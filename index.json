[{"categories":["documentation"],"content":"Hugo provides multiple built-in shortcodes for author convenience and to keep your markdown content clean.","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/","tags":["shortcodes"],"title":"Theme Documentation - Built-in Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"Hugo provides multiple built-in shortcodes for author convenience and to keep your markdown content clean. Hugo uses Markdown for its simple content format. However, there are a lot of things that Markdown doesn’t support well. You could use pure HTML to expand possibilities. But this happens to be a bad idea. Everyone uses Markdown because it’s pure and simple to read even non-rendered. You should avoid HTML to keep it as simple as possible. To avoid this limitations, Hugo created shortcodes. A shortcode is a simple snippet that can generate reasonable HTML code and conforms to Markdown’s design philosophy. Hugo ships with a set of predefined shortcodes that represent very common usage. These shortcodes are provided for author convenience and to keep your markdown content clean. ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:0:0","tags":["shortcodes"],"title":"Theme Documentation - Built-in Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"1 figure Documentation of figure Example figure input: {{\u003c figure src=\"/images/lighthouse.jpg\" title=\"Lighthouse (figure)\" \u003e}} The rendered output looks like this: Lighthouse (figure) The HTML looks like this: \u003cfigure\u003e \u003cimg src=\"/images/lighthouse.jpg\"/\u003e \u003cfigcaption\u003e \u003ch4\u003eLighthouse (figure)\u003c/h4\u003e \u003c/figcaption\u003e \u003c/figure\u003e ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:1:0","tags":["shortcodes"],"title":"Theme Documentation - Built-in Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"2 gist Documentation of gist Example gist input: {{\u003c gist spf13 7896402 \u003e}} The rendered output looks like this: The HTML looks like this: \u003cscript type=\"application/javascript\" src=\"https://gist.github.com/spf13/7896402.js\"\u003e\u003c/script\u003e ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:2:0","tags":["shortcodes"],"title":"Theme Documentation - Built-in Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"3 highlight Documentation of highlight Example highlight input: {{\u003c highlight html \u003e}} \u003csection id=\"main\"\u003e \u003cdiv\u003e \u003ch1 id=\"title\"\u003e{{ .Title }}\u003c/h1\u003e {{ range .Pages }} {{ .Render \"summary\"}} {{ end }} \u003c/div\u003e \u003c/section\u003e {{\u003c /highlight \u003e}} The rendered output looks like this: \u003csection id=\"main\"\u003e \u003cdiv\u003e \u003ch1 id=\"title\"\u003e{{ .Title }}\u003c/h1\u003e {{ range .Pages }} {{ .Render \"summary\"}} {{ end }} \u003c/div\u003e \u003c/section\u003e ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:3:0","tags":["shortcodes"],"title":"Theme Documentation - Built-in Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"4 instagram Documentation of instagram Example instagram input: {{\u003c instagram BWNjjyYFxVx hidecaption \u003e}} The rendered output looks like this: ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:4:0","tags":["shortcodes"],"title":"Theme Documentation - Built-in Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"5 param Documentation of param Example param input: {{\u003c param description \u003e}} The rendered output looks like this: Hugo provides multiple built-in shortcodes for author convenience and to keep your markdown content clean. ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:5:0","tags":["shortcodes"],"title":"Theme Documentation - Built-in Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"6 ref and relref Documentation of ref and relref ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:6:0","tags":["shortcodes"],"title":"Theme Documentation - Built-in Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"7 tweet Documentation of tweet Example tweet input: {{\u003c tweet 877500564405444608 \u003e}} The rendered output looks like this: Hugo 0.24 Released: Big archetype update + @Netlify _redirects etc. file supporthttps://t.co/X94FmYDEZJ #gohugo #golang @spf13 @bepsays — GoHugo.io (@GoHugoIO) June 21, 2017 ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:7:0","tags":["shortcodes"],"title":"Theme Documentation - Built-in Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"8 vimeo Documentation of vimeo Example vimeo input: {{\u003c vimeo 146022717 \u003e}} The rendered output looks like this: ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:8:0","tags":["shortcodes"],"title":"Theme Documentation - Built-in Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"9 youtube Documentation of youtube Example youtube input: {{\u003c youtube w7Ft2ymGmfc \u003e}} The rendered output looks like this: ","date":"2020-03-04","objectID":"/theme-documentation-built-in-shortcodes/:9:0","tags":["shortcodes"],"title":"Theme Documentation - Built-in Shortcodes","uri":"/theme-documentation-built-in-shortcodes/"},{"categories":["documentation"],"content":"uBlogger theme provides multiple shortcodes on top of built-in ones in Hugo.","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"uBlogger theme provides multiple shortcodes on top of built-in ones in Hugo. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:0:0","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"1 style Note\r\rHugo extended version is necessary for style shortcode.\r\r style is a shortcode to insert custom style in your post. The style shortcode has two positional parameters. The first one is the custom style content, which supports nesting syntax in  SASS and \u0026 referring to this parent HTML element. And the second one is the tag name of the HTML element wrapping the content you want to change style, and whose default value is div. Example style input: {{\u003c style \"text-align:right; strong{color:#00b1ff;}\" \u003e}} This is a **right-aligned** paragraph. {{\u003c /style \u003e}} The rendered output looks like this: This is a right-aligned paragraph. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:1:0","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"2 link link shortcode is an alternative to Markdown link syntax. link shortcode can provide some other features and can be used in code blocks. The complete usage of local resource references is supported. The link shortcode has the following named parameters: href [required] (first positional parameter) Destination of the link. content [optional] (second positional parameter) Content of the link, default value is the value of href parameter. Markdown or HTML format is supported. title [optional] (third positional parameter) title attribute of the HTML a tag, which will be shown when hovering on the link. class [optional] class attribute of the HTML a tag. rel [optional] Additional rel attributes of the HTML a tag. Example link input: {{\u003c link \"https://assemble.io\" \u003e}} Or {{\u003c link href=\"https://assemble.io\" \u003e}} {{\u003c link \"mailto:contact@revolunet.com\" \u003e}} Or {{\u003c link href=\"mailto:contact@revolunet.com\" \u003e}} {{\u003c link \"https://assemble.io\" Assemble \u003e}} Or {{\u003c link href=\"https://assemble.io\" content=Assemble \u003e}} The rendered output looks like this: https://assemble.io mailto:contact@revolunet.com Assemble Example link input with a title: {{\u003c link \"https://github.com/upstage/\" Upstage \"Visit Upstage!\" \u003e}} Or {{\u003c link href=\"https://github.com/upstage/\" content=Upstage title=\"Visit Upstage!\" \u003e}} The rendered output looks like this (hover over the link, there should be a tooltip): Upstage ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:2:0","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"3 image image shortcode is an alternative to figure shortcode. image shortcode can take full advantage of the dependent libraries of lazysizes and lightgallery.js. The complete usage of local resource references is supported. The image shortcode has the following named parameters: src [required] (first positional parameter) URL of the image to be displayed. alt [optional] (second positional parameter) Alternate text for the image if the image cannot be displayed, default value is the value of src parameter. Markdown or HTML format is supported. caption [optional] (third positional parameter) Image caption. Markdown or HTML format is supported. title [optional] Image title that will be shown when hovering on the image. class [optional] class attribute of the HTML figure tag. src_s [optional] URL of the image thumbnail, used for lightgallery, default value is the value of src parameter. src_l [optional] URL of the HD image, used for lightgallery, default value is the value of src parameter. height [optional] height attribute of the image. width [optional] width attribute of the image. linked [optional] Whether the image needs to be hyperlinked, default value is true. rel [optional] Additional rel attributes of the HTML a tag, if linked parameter is set to true. Example image input: {{\u003c image src=\"/images/lighthouse.jpg\" caption=\"Lighthouse (`image`)\" src_s=\"/images/lighthouse-small.jpg\" src_l=\"/images/lighthouse-large.jpg\" \u003e}} The rendered output looks like this: Lighthouse (image)\"\rLighthouse (image)\r ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:3:0","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"4 admonition The admonition shortcode supports 12 types of banners to help you put notice in your page. Markdown or HTML format in the content is supported. Note\r\rA note banner\r\r Abstract\r\rAn abstract banner\r\r Info\r\rA info banner\r\r Tip\r\rA tip banner\r\r Success\r\rA success banner\r\r Question\r\rA question banner\r\r Warning\r\rA warning banner\r\r Failure\r\rA failure banner\r\r Danger\r\rA danger banner\r\r Bug\r\rA bug banner\r\r Example\r\rAn example banner\r\r Quote\r\rA quote banner\r\r The admonition shortcode has the following named parameters: type [optional] (first positional parameter) Type of the admonition banner, default value is note. title [optional] (second positional parameter) Title of the admonition banner, default value is the value of type parameter. open [optional] (third positional parameter) Whether the content will be expandable by default, default value is true. Example admonition input: {{\u003c admonition type=tip title=\"This is a tip\" open=false \u003e}} A **tip** banner {{\u003c /admonition \u003e}} Or {{\u003c admonition tip \"This is a tip\" false \u003e}} A **tip** banner {{\u003c /admonition \u003e}} The rendered output looks like this: This is a tip\r\rA tip banner\r\r ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:4:0","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"5 mermaid mermaid is a library helping you to generate diagram and flowcharts from text, in a similar manner as Markdown. Just insert your mermaid code in the mermaid shortcode and that’s it. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:5:0","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"5.1 Flowchart Example flowchart mermaid input: {{\u003c mermaid \u003e}} graph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] {{\u003c /mermaid \u003e}} The rendered output looks like this: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:5:1","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"5.2 Sequence Diagram Example sequence diagram mermaid input: {{\u003c mermaid \u003e}} sequenceDiagram participant Alice participant Bob Alice-\u003e\u003eJohn: Hello John, how are you? loop Healthcheck John-\u003eJohn: Fight against hypochondria end Note right of John: Rational thoughts \u003cbr/\u003eprevail... John--\u003eAlice: Great! John-\u003eBob: How about you? Bob--\u003eJohn: Jolly good! {{\u003c /mermaid \u003e}} The rendered output looks like this: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:5:2","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"5.3 GANTT Example GANTT mermaid input: {{\u003c mermaid \u003e}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and jison :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to mermaid :1d {{\u003c /mermaid \u003e}} The rendered output looks like this: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:5:3","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"5.4 Class Diagram Example class diagram mermaid input: {{\u003c mermaid \u003e}} classDiagram Class01 \u003c|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --\u003e C2 : Where am i? Class09 --* C3 Class09 --|\u003e Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 \u003c--\u003e C2: Cool label {{\u003c /mermaid \u003e}} The rendered output looks like this: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:5:4","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"5.5 State Diagram Example state diagram mermaid input: {{\u003c mermaid \u003e}} stateDiagram [*] --\u003e Still Still --\u003e [*] Still --\u003e Moving Moving --\u003e Still Moving --\u003e Crash Crash --\u003e [*] {{\u003c /mermaid \u003e}} The rendered output looks like this: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:5:5","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"5.6 Git Graph Example git graph mermaid input: {{\u003c mermaid \u003e}} gitGraph: options { \"nodeSpacing\": 100, \"nodeRadius\": 10 } end commit branch newbranch checkout newbranch commit commit checkout master commit commit merge newbranch {{\u003c /mermaid \u003e}} The rendered output looks like this: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:5:6","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"5.7 Pie Example pie mermaid input: {{\u003c mermaid \u003e}} pie \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15 {{\u003c /mermaid \u003e}} The rendered output looks like this: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:5:7","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"6 echarts ECharts is a library helping you to generate interactive data visualization. The basic chart types ECharts supports include line series, bar series, scatter series, pie charts, candle-stick series, boxplot series for statistics, map series, heatmap series, lines series for directional information, graph series for relationships, treemap series, sunburst series, parallel series for multi-dimensional data, funnel series, gauge series. And it’s extremely easy to create a combinition of them with ECharts. Just insert your ECharts option in JSON/YAML/TOML format in the echarts shortcode and that’s it. Example echarts input in JSON format: {{\u003c echarts \u003e}} { \"title\": { \"text\": \"Summary Line Chart\", \"top\": \"2%\", \"left\": \"center\" }, \"tooltip\": { \"trigger\": \"axis\" }, \"legend\": { \"data\": [\"Email Marketing\", \"Affiliate Advertising\", \"Video Advertising\", \"Direct View\", \"Search Engine\"], \"top\": \"10%\" }, \"grid\": { \"left\": \"5%\", \"right\": \"5%\", \"bottom\": \"5%\", \"top\": \"20%\", \"containLabel\": true }, \"toolbox\": { \"feature\": { \"saveAsImage\": { \"title\": \"Save as Image\" } } }, \"xAxis\": { \"type\": \"category\", \"boundaryGap\": false, \"data\": [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"] }, \"yAxis\": { \"type\": \"value\" }, \"series\": [ { \"name\": \"Email Marketing\", \"type\": \"line\", \"stack\": \"Total\", \"data\": [120, 132, 101, 134, 90, 230, 210] }, { \"name\": \"Affiliate Advertising\", \"type\": \"line\", \"stack\": \"Total\", \"data\": [220, 182, 191, 234, 290, 330, 310] }, { \"name\": \"Video Advertising\", \"type\": \"line\", \"stack\": \"Total\", \"data\": [150, 232, 201, 154, 190, 330, 410] }, { \"name\": \"Direct View\", \"type\": \"line\", \"stack\": \"Total\", \"data\": [320, 332, 301, 334, 390, 330, 320] }, { \"name\": \"Search Engine\", \"type\": \"line\", \"stack\": \"Total\", \"data\": [820, 932, 901, 934, 1290, 1330, 1320] } ] } {{\u003c /echarts \u003e}} The same in YAML format: {{\u003c echarts \u003e}}title:text:Summary Line Charttop:2%left:centertooltip:trigger:axislegend:data:- Email Marketing- Affiliate Advertising- Video Advertising- Direct View- Search Enginetop:10%grid:left:5%right:5%bottom:5%top:20%containLabel:truetoolbox:feature:saveAsImage:title:Save as ImagexAxis:type:categoryboundaryGap:falsedata:- Monday- Tuesday- Wednesday- Thursday- Friday- Saturday- SundayyAxis:type:valueseries:- name:Email Marketingtype:linestack:Totaldata:- 120- 132- 101- 134- 90- 230- 210- name:Affiliate Advertisingtype:linestack:Totaldata:- 220- 182- 191- 234- 290- 330- 310- name:Video Advertisingtype:linestack:Totaldata:- 150- 232- 201- 154- 190- 330- 410- name:Direct Viewtype:linestack:Totaldata:- 320- 332- 301- 334- 390- 330- 320- name:Search Enginetype:linestack:Totaldata:- 820- 932- 901- 934- 1290- 1330- 1320{{\u003c /echarts \u003e}} The same in TOML format: {{\u003c echarts \u003e}} [title] text = \"Summary Line Chart\" top = \"2%\" left = \"center\" [tooltip] trigger = \"axis\" [legend] data = [ \"Email Marketing\", \"Affiliate Advertising\", \"Video Advertising\", \"Direct View\", \"Search Engine\" ] top = \"10%\" [grid] left = \"5%\" right = \"5%\" bottom = \"5%\" top = \"20%\" containLabel = true [toolbox] [toolbox.feature] [toolbox.feature.saveAsImage] title = \"Save as Image\" [xAxis] type = \"category\" boundaryGap = false data = [ \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\" ] [yAxis] type = \"value\" [[series]] name = \"Email Marketing\" type = \"line\" stack = \"Total\" data = [ 120.0, 132.0, 101.0, 134.0, 90.0, 230.0, 210.0 ] [[series]] name = \"Affiliate Advertising\" type = \"line\" stack = \"Total\" data = [ 220.0, 182.0, 191.0, 234.0, 290.0, 330.0, 310.0 ] [[series]] name = \"Video Advertising\" type = \"line\" stack = \"Total\" data = [ 150.0, 232.0, 201.0, 154.0, 190.0, 330.0, 410.0 ] [[series]] name = \"Direct View\" type = \"line\" stack = \"Total\" data = [ 320.0, 332.0, 301.0, 334.0, 390.0, 330.0, 320.0 ] [[series]] name = \"Search Engine\" type = \"line\" stack = \"Total\" data = [ 820.0, 932.0, 901.0, 934.0, 1290.0, 1330.0, 1320.0 ] {{\u003c /echarts \u003e}} The rendered output looks like this: The echarts shortcode has also","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:6:0","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"7 mapbox Mapbox GL JS is a JavaScript library that uses WebGL to render interactive maps from vector tiles and Mapbox styles. The mapbox shortcode has the following named parameters to use Mapbox GL JS: lng [required] (first positional parameter) Longitude of the inital centerpoint of the map, measured in degrees. lat [required] (second positional parameter) Latitude of the inital centerpoint of the map, measured in degrees. zoom [optional] (third positional parameter) The initial zoom level of the map, default value is 10. marked [optional] (fourth positional parameter) Whether to add a marker at the inital centerpoint of the map, default value is true. light-style [optional] (fifth positional parameter) Style for the light theme, default value is the value set in the front matter or the site configuration. dark-style [optional] (sixth positional parameter) Style for the dark theme, default value is the value set in the front matter or the site configuration. navigation [optional] Whether to add NavigationControl, default value is the value set in the front matter or the site configuration. geolocate [optional] Whether to add GeolocateControl, default value is the value set in the front matter or the site configuration. scale [optional] Whether to add ScaleControl, default value is the value set in the front matter or the site configuration. fullscreen [optional] Whether to add FullscreenControl, default value is the value set in the front matter or the site configuration. width [optional] Width of the map, default value is 100%. height [optional] Height of the map, default value is 20rem. Example simple mapbox input: {{\u003c mapbox 121.485 31.233 12 \u003e}} Or {{\u003c mapbox lng=121.485 lat=31.233 zoom=12 \u003e}} The rendered output looks like this: Example mapbox input with the custom style: {{\u003c mapbox -122.252 37.453 10 false \"mapbox://styles/mapbox/navigation-preview-day-v4\" \"mapbox://styles/mapbox/navigation-preview-night-v4\" \u003e}} Or {{\u003c mapbox lng=-122.252 lat=37.453 zoom=10 marked=false light-style=\"mapbox://styles/mapbox/navigation-preview-day-v4\" dark-style=\"mapbox://styles/mapbox/navigation-preview-night-v4\" \u003e}} The rendered output looks like this: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:7:0","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"8 music The music shortcode embeds a responsive music player based on APlayer and MetingJS. There are three ways to use it the music shortcode. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:8:0","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"8.1 Custom Music URL The complete usage of local resource references is supported. The music shortcode has the following named parameters by custom music URL: server [required] URL of the custom music. name [optional] Name of the custom music. artist [optional] Artist of the custom music. cover [required] URL of the custom music cover. Example music input by custom music URL: {{\u003c music url=\"/music/Wavelength.mp3\" name=Wavelength artist=oldmanyoung cover=\"/images/Wavelength.jpg\" \u003e}} The rendered output looks like this: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:8:1","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"8.2 Music Platform URL Automatic Identification The music shortcode has one named parameter by music platform URL automatic identification: auto [required] (first positional parameter) URL of the music platform URL for automatic identification, which supports netease, tencent and xiami music platform. Example music input by music platform URL automatic identification: {{\u003c music auto=\"https://music.163.com/#/playlist?id=60198\" \u003e}} Or {{\u003c music \"https://music.163.com/#/playlist?id=60198\" \u003e}} The rendered output looks like this: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:8:2","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"8.3 Custom Server, Type and ID The music shortcode has the following named parameters by custom music platform: server [required] (first positional parameter) [netease, tencent, kugou, xiami, baidu] Music platform. type [required] (second positional parameter) [song, playlist, album, search, artist] Type of the music. id [required] (third positional parameter) Song ID, or playlist ID, or album ID, or search keyword, or artist ID. Example music input by custom music platform: {{\u003c music server=\"netease\" type=\"song\" id=\"1868553\" \u003e}} Or {{\u003c music netease song 1868553 \u003e}} The rendered output looks like this: ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:8:3","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"8.4 Other Parameters The music shortcode has other named parameters applying to the above three ways: theme [optional] Main color of the music player, default value is #448aff. fixed [optional] Whether to enable fixed mode, default value is false. mini [optional] Whether to enable mini mode, default value is false. autoplay [optional] Whether to autoplay music, default value is false. volume [optional] Default volume when the player is first opened, which will be remembered in the browser, default value is 0.7. mutex [optional] Whether to pause other players when this player starts playing, default value is true. The music shortcode has the following named parameters only applying to the type of music list: loop [optional] [all, one, none] Loop mode of the music list, default value is none. order [optional] [list, random] Play order of the music list, default value is list. list-folded [optional] Whether the music list should be folded at first, default value is false. list-max-height [optional] Max height of the music list, default value is 340px. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:8:4","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"9 bilibili The bilibili shortcode embeds a responsive video player for bilibili videos. When the video only has one part, only the BV id of the video is required, e.g.: https://www.bilibili.com/video/BV1Sx411T7QQ Example bilibili input: {{\u003c bilibili BV1Sx411T7QQ \u003e}} Or {{\u003c bilibili id=BV1Sx411T7QQ \u003e}} The rendered output looks like this: \rWhen the video has multiple parts, in addition to the BV id of the video, p is also required, whose default value is 1, e.g.: https://www.bilibili.com/video/BV1TJ411C7An?p=3 Example bilibili input with p: {{\u003c bilibili BV1TJ411C7An 3 \u003e}} Or {{\u003c bilibili id=BV1TJ411C7An p=3 \u003e}} The rendered output looks like this: \r","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:9:0","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"10 typeit The typeit shortcode provides typing animation based on TypeIt. Just insert your content in the typeit shortcode and that’s it. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:10:0","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"10.1 Simple Content Simple content is allowed in Markdown format and without rich block content such as images and more… Example typeit input: {{\u003c typeit \u003e}} This is a *paragraph* with **typing animation** based on [TypeIt](https://typeitjs.com/)... {{\u003c /typeit \u003e}} The rendered output looks like this: \rAlternatively, you can use custom HTML tags. Example typeit input with h4 tag: {{\u003c typeit tag=h4 \u003e}} This is a *paragraph* with **typing animation** based on [TypeIt](https://typeitjs.com/)... {{\u003c /typeit \u003e}} The rendered output looks like this: \r","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:10:1","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"10.2 Code Content Code content is allowed and will be highlighted by named parameter code for the type of code language. Example typeit input with code: {{\u003c typeit code=java \u003e}} public class HelloWorld { public static void main(String []args) { System.out.println(\"Hello World\"); } } {{\u003c /typeit \u003e}} The rendered output looks like this: \r","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:10:2","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"10.3 Group Content All typing animations start at the same time by default. But sometimes you may want to start a set of typeit contents in order. A set of typeit contents with the same value of named parameter group will start typing animation in sequence. Example typeit input with group: {{\u003c typeit group=paragraph \u003e}} **First** this paragraph begins {{\u003c /typeit \u003e}} {{\u003c typeit group=paragraph \u003e}} **Then** this paragraph begins {{\u003c /typeit \u003e}} The rendered output looks like this: \r\r","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:10:3","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["documentation"],"content":"11 script script is a shortcode to insert custom  Javascript in your post. Note\r\rThe script content can be guaranteed to be executed in order after all third-party libraries are loaded. So you are free to use third-party libraries.\r\r Example script input: {{\u003c script \u003e}} console.log('Hello uBlogger!'); {{\u003c /script \u003e}} You can see the output in the console of the developer tool. ","date":"2020-03-03","objectID":"/theme-documentation-extended-shortcodes/:11:0","tags":["shortcodes"],"title":"Theme Documentation - Extended Shortcodes","uri":"/theme-documentation-extended-shortcodes/"},{"categories":["Markdown"],"content":"This article shows the basic Markdown syntax and format.","date":"2019-12-01","objectID":"/basic-markdown-syntax/","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files. Note\r\rThis article is a shameful copy of the great Grav original page. If you want to know about the extented Markdown syntax of uBlogger theme, please read extended Markdown syntax page. \r\r Let’s face it: Writing content for the Web is tiresome. WYSIWYG editors help alleviate this task, but they generally result in horrible code, or worse yet, ugly web pages. Markdown is a better way to write HTML, without all the complexities and ugliness that usually accompanies it. Some of the key benefits are: Markdown is simple to learn, with minimal extra characters, so it’s also quicker to write content. Less chance of errors when writing in Markdown. Produces valid XHTML output. Keeps the content and the visual display separate, so you cannot mess up the look of your site. Write in any text editor or Markdown application you like. Markdown is a joy to use! John Gruber, the author of Markdown, puts it like this: The overriding design goal for Markdown’s formatting syntax is to make it as readable as possible. The idea is that a Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions. While Markdown’s syntax has been influenced by several existing text-to-HTML filters, the single biggest source of inspiration for Markdown’s syntax is the format of plain text email. – John Gruber Without further delay, let us go over the main elements of Markdown and what the resulting HTML looks like! Tip\r\r Bookmark this page for easy future reference!\r\r ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:0:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"1 Headings Headings from h2 through h6 are constructed with a # for each level: ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading The HTML looks like this: \u003ch2\u003eh2 Heading\u003c/h2\u003e \u003ch3\u003eh3 Heading\u003c/h3\u003e \u003ch4\u003eh4 Heading\u003c/h4\u003e \u003ch5\u003eh5 Heading\u003c/h5\u003e \u003ch6\u003eh6 Heading\u003c/h6\u003e Heading IDs\r\rTo add a custom heading ID, enclose the custom ID in curly braces on the same line as the heading: ### A Great Heading {#custom-id} The HTML looks like this: \u003ch3 id=\"custom-id\"\u003eA Great Heading\u003c/h3\u003e \r\r ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:1:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"2 Comments Comments should be HTML compatible. \u003c!-- This is a comment --\u003e Comment below should NOT be seen: ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:2:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"3 Horizontal Rules The HTML \u003chr\u003e element is for creating a “thematic break” between paragraph-level elements. In Markdown, you can create a \u003chr\u003e with any of the following: ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks The rendered output looks like this: ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:3:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"4 Body Copy Body copy written as normal, plain text will be wrapped with \u003cp\u003e\u003c/p\u003e tags in the rendered HTML. So this body copy: Lorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad. The HTML looks like this: \u003cp\u003eLorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad.\u003c/p\u003e A line break can be done with one blank line. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:4:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"5 Inline HTML If you need a certain HTML tag (with a class) you can simply use HTML: Paragraph in Markdown. \u003cdiv class=\"class\"\u003e This is \u003cb\u003eHTML\u003c/b\u003e \u003c/div\u003e Paragraph in Markdown. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:5:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"6 Emphasis ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Bold For emphasizing a snippet of text with a heavier font-weight. The following snippet of text is rendered as bold text. **rendered as bold text** __rendered as bold text__ The HTML looks like this: \u003cstrong\u003erendered as bold text\u003c/strong\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:1","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Italics For emphasizing a snippet of text with italics. The following snippet of text is rendered as italicized text. *rendered as italicized text* _rendered as italicized text_ The HTML looks like this: \u003cem\u003erendered as italicized text\u003c/em\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:2","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Strikethrough In GFMGitHub flavored Markdown you can do strikethroughs. ~~Strike through this text.~~ The rendered output looks like this: Strike through this text. The HTML looks like this: \u003cdel\u003eStrike through this text.\u003c/del\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:3","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Combination Bold, italics, and strikethrough can be used in combination. ***bold and italics*** ~~**strikethrough and bold**~~ ~~*strikethrough and italics*~~ ~~***bold, italics and strikethrough***~~ The rendered output looks like this: bold and italics strikethrough and bold strikethrough and italics bold, italics and strikethrough The HTML looks like this: \u003cem\u003e\u003cstrong\u003ebold and italics\u003c/strong\u003e\u003c/em\u003e \u003cdel\u003e\u003cstrong\u003estrikethrough and bold\u003c/strong\u003e\u003c/del\u003e \u003cdel\u003e\u003cem\u003estrikethrough and italics\u003c/em\u003e\u003c/del\u003e \u003cdel\u003e\u003cem\u003e\u003cstrong\u003ebold, italics and strikethrough\u003c/strong\u003e\u003c/em\u003e\u003c/del\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:4","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"7 Blockquotes For quoting blocks of content from another source within your document. Add \u003e before any text you want to quote: \u003e **Fusion Drive** combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. The rendered output looks like this: Fusion Drive combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. The HTML looks like this: \u003cblockquote\u003e \u003cp\u003e \u003cstrong\u003eFusion Drive\u003c/strong\u003e combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. \u003c/p\u003e \u003c/blockquote\u003e Blockquotes can also be nested: \u003e Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. \u003e\u003e Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. The rendered output looks like this: Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:7:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"8 Lists ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Unordered A list of items in which the order of the items does not explicitly matter. You may use any of the following symbols to denote bullets for each list item: * valid bullet - valid bullet + valid bullet For example: * Lorem ipsum dolor sit amet * Consectetur adipiscing elit * Integer molestie lorem at massa * Facilisis in pretium nisl aliquet * Nulla volutpat aliquam velit * Phasellus iaculis neque * Purus sodales ultricies * Vestibulum laoreet porttitor sem * Ac tristique libero volutpat at * Faucibus porta lacus fringilla vel * Aenean sit amet erat nunc * Eget porttitor lorem The rendered output looks like this: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Phasellus iaculis neque Purus sodales ultricies Vestibulum laoreet porttitor sem Ac tristique libero volutpat at Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem The HTML looks like this: \u003cul\u003e \u003cli\u003eLorem ipsum dolor sit amet\u003c/li\u003e \u003cli\u003eConsectetur adipiscing elit\u003c/li\u003e \u003cli\u003eInteger molestie lorem at massa\u003c/li\u003e \u003cli\u003eFacilisis in pretium nisl aliquet\u003c/li\u003e \u003cli\u003eNulla volutpat aliquam velit \u003cul\u003e \u003cli\u003ePhasellus iaculis neque\u003c/li\u003e \u003cli\u003ePurus sodales ultricies\u003c/li\u003e \u003cli\u003eVestibulum laoreet porttitor sem\u003c/li\u003e \u003cli\u003eAc tristique libero volutpat at\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003eFaucibus porta lacus fringilla vel\u003c/li\u003e \u003cli\u003eAenean sit amet erat nunc\u003c/li\u003e \u003cli\u003eEget porttitor lorem\u003c/li\u003e \u003c/ul\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:1","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Ordered A list of items in which the order of items does explicitly matter. 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa 4. Facilisis in pretium nisl aliquet 5. Nulla volutpat aliquam velit 6. Faucibus porta lacus fringilla vel 7. Aenean sit amet erat nunc 8. Eget porttitor lorem The rendered output looks like this: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem The HTML looks like this: \u003col\u003e \u003cli\u003eLorem ipsum dolor sit amet\u003c/li\u003e \u003cli\u003eConsectetur adipiscing elit\u003c/li\u003e \u003cli\u003eInteger molestie lorem at massa\u003c/li\u003e \u003cli\u003eFacilisis in pretium nisl aliquet\u003c/li\u003e \u003cli\u003eNulla volutpat aliquam velit\u003c/li\u003e \u003cli\u003eFaucibus porta lacus fringilla vel\u003c/li\u003e \u003cli\u003eAenean sit amet erat nunc\u003c/li\u003e \u003cli\u003eEget porttitor lorem\u003c/li\u003e \u003c/ol\u003e Tip\r\rIf you just use 1. for each number, Markdown will automatically number each item. For example: 1. Lorem ipsum dolor sit amet 1. Consectetur adipiscing elit 1. Integer molestie lorem at massa 1. Facilisis in pretium nisl aliquet 1. Nulla volutpat aliquam velit 1. Faucibus porta lacus fringilla vel 1. Aenean sit amet erat nunc 1. Eget porttitor lorem The rendered output looks like this: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem \r\r ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:2","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Task Lists Task lists allow you to create a list of items with checkboxes. To create a task list, add dashes (-) and brackets with a space ([ ]) before task list items. To select a checkbox, add an x in between the brackets ([x]). - [x] Write the press release - [ ] Update the website - [ ] Contact the media The rendered output looks like this: Write the press release Update the website Contact the media ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:3","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"9 Code ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Inline Code Wrap inline snippets of code with `. In this example, `\u003csection\u003e\u003c/section\u003e` should be wrapped as **code**. The rendered output looks like this: In this example, \u003csection\u003e\u003c/section\u003e should be wrapped as code. The HTML looks like this: \u003cp\u003e In this example, \u003ccode\u003e\u0026lt;section\u0026gt;\u0026lt;/section\u0026gt;\u003c/code\u003e should be wrapped with \u003cstrong\u003ecode\u003c/strong\u003e. \u003c/p\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:1","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Indented Code Or indent several lines of code by at least four spaces, as in: // Some comments line 1 of code line 2 of code line 3 of code The rendered output looks like this: // Some comments\rline 1 of code\rline 2 of code\rline 3 of code\r The HTML looks like this: \u003cpre\u003e \u003ccode\u003e // Some comments line 1 of code line 2 of code line 3 of code \u003c/code\u003e \u003c/pre\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:2","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Block Fenced Code Use “fences” ``` to block in multiple lines of code with a language attribute. ```markdown Sample text here... ``` The HTML looks like this: \u003cpre language-html\u003e \u003ccode\u003eSample text here...\u003c/code\u003e \u003c/pre\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:3","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Syntax Highlighting GFMGitHub Flavored Markdown also supports syntax highlighting. To activate it, simply add the file extension of the language you want to use directly after the first code “fence”, ```js, and syntax highlighting will automatically be applied in the rendered HTML. For example, to apply syntax highlighting to JavaScript code: ```js grunt.initConfig({ assemble: { options: { assets: 'docs/assets', data: 'src/data/*.{json,yml}', helpers: 'src/custom-helpers.js', partials: ['src/partials/**/*.{hbs,md}'] }, pages: { options: { layout: 'default.hbs' }, files: { './': ['src/templates/pages/index.hbs'] } } } }; ``` The rendered output looks like this: grunt.initConfig({ assemble: { options: { assets: 'docs/assets', data: 'src/data/*.{json,yml}', helpers: 'src/custom-helpers.js', partials: ['src/partials/**/*.{hbs,md}'] }, pages: { options: { layout: 'default.hbs' }, files: { './': ['src/templates/pages/index.hbs'] } } } }; Note\r\rSyntax highlighting page in Hugo Docs introduces more about syntax highlighting, including highlight shortcode.\r\r ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:4","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"10 Tables Tables are created by adding pipes as dividers between each cell, and by adding a line of dashes (also separated by bars) beneath the header. Note that the pipes do not need to be vertically aligned. | Option | Description | | ------ | ----------- | | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | The rendered output looks like this: Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. The HTML looks like this: \u003ctable\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003eOption\u003c/th\u003e \u003cth\u003eDescription\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd\u003edata\u003c/td\u003e \u003ctd\u003epath to data files to supply the data that will be passed into templates.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eengine\u003c/td\u003e \u003ctd\u003eengine to be used for processing templates. Handlebars is the default.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eext\u003c/td\u003e \u003ctd\u003eextension to be used for dest files.\u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/table\u003e Right or center aligned text\r\rAdding a colon on the right side of the dashes below any heading will right align text for that column. Adding colons on both sides of the dashes below any heading will center align text for that column. | Option | Description | |:------:| -----------:| | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | The rendered output looks like this: Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. \r\r ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:10:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"11 Links ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Basic Link \u003chttps://assemble.io\u003e \u003ccontact@revolunet.com\u003e [Assemble](https://assemble.io) The rendered output looks like this (hover over the link, there is no tooltip): https://assemble.io contact@revolunet.com Assemble The HTML looks like this: \u003ca href=\"https://assemble.io\"\u003ehttps://assemble.io\u003c/a\u003e \u003ca href=\"mailto:contact@revolunet.com\"\u003econtact@revolunet.com\u003c/a\u003e \u003ca href=\"https://assemble.io\"\u003eAssemble\u003c/a\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:1","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Add a Title [Upstage](https://github.com/upstage/ \"Visit Upstage!\") The rendered output looks like this (hover over the link, there should be a tooltip): Upstage The HTML looks like this: \u003ca href=\"https://github.com/upstage/\" title=\"Visit Upstage!\"\u003eUpstage\u003c/a\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:2","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Named Anchors Named anchors enable you to jump to the specified anchor point on the same page. For example, each of these chapters: ## Table of Contents * [Chapter 1](#chapter-1) * [Chapter 2](#chapter-2) * [Chapter 3](#chapter-3) will jump to these sections: ## Chapter 1 \u003ca id=\"chapter-1\"\u003e\u003c/a\u003e Content for chapter one. ## Chapter 2 \u003ca id=\"chapter-2\"\u003e\u003c/a\u003e Content for chapter one. ## Chapter 3 \u003ca id=\"chapter-3\"\u003e\u003c/a\u003e Content for chapter one. Note\r\rThe specific placement of the anchor tag seems to be arbitrary. They are placed inline here since it seems to be unobtrusive, and it works.\r\r ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:3","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"12 Footnotes Footnotes allow you to add notes and references without cluttering the body of the document. When you create a footnote, a superscript number with a link appears where you added the footnote reference. Readers can click the link to jump to the content of the footnote at the bottom of the page. To create a footnote reference, add a caret and an identifier inside brackets ([^1]). Identifiers can be numbers or words, but they can’t contain spaces or tabs. Identifiers only correlate the footnote reference with the footnote itself — in the output, footnotes are numbered sequentially. Add the footnote using another caret and number inside brackets with a colon and text ([^1]: My footnote.). You don’t have to put footnotes at the end of the document. You can put them anywhere except inside other elements like lists, block quotes, and tables. This is a digital footnote[^1]. This is a footnote with \"label\"[^label] [^1]: This is a digital footnote [^label]: This is a footnote with \"label\" This is a digital footnote1. This is a footnote with “label”2 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:12:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"13 Images Images have a similar syntax to links but include a preceding exclamation point. ![Minion](https://octodex.github.com/images/minion.png) \r or: ![Alt text](https://octodex.github.com/images/stormtroopocat.jpg \"The Stormtroopocat\") \rAlt textThe Stormtroopocat \"\rThe Stormtroopocat\r Like links, images also have a footnote style syntax: ![Alt text][id] \rAlt textThe Dojocat \"\rThe Dojocat\r With a reference later in the document defining the URL location: [id]: https://octodex.github.com/images/dojocat.jpg \"The Dojocat\" Tip\r\ruBlogger theme has special shortcode for image, which provides more features.\r\r This is a digital footnote ↩︎ This is a footnote with “label” ↩︎ ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:13:0","tags":["Markdown","HTML"],"title":"Basic Markdown Syntax","uri":"/basic-markdown-syntax/"},{"categories":[],"content":"sky description","date":"2021-02-22","objectID":"/chnsdocumentation/","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"The China Health and Nutrition Survey (CHNS), an ongoing open cohort, international collaborative project between the Carolina Population Center at the University of North Carolina at Chapel Hill and the National Institute for Nutrition and Health (NINH, former National Institute of Nutrition and Food Safety) at the Chinese Center for Disease Control and Prevention (CCDC), was designed to examine the effects of the health, nutrition, and family planning policies and programs implemented by national and local governments and to see how the social and economic transformation of Chinese society is affecting the health and nutritional status of its population. The impact on nutrition and health behaviors and outcomes is gauged by changes in community organizations and programs as well as by changes in sets of household and individual economic, demographic, and social factors. The survey was conducted by an international team of researchers whose backgrounds include nutrition, public health, economics, sociology, Chinese studies, and demography. The survey took place over a 7-day period using a multistage, random cluster process to draw a sample of about 7,200 households with over 30,000 individuals in 15 provinces and municipal cities that vary substantially in geography, economic development, public resources, and health indicators. In addition, detailed community data were collected in surveys of food markets, health facilities, family planning officials, and other social services and community leaders. Following are official documentation of CHNS. A careful review of these materials will help you: Navigate the questionnaires and data files more efficiently Determine which files should be combined to create the analyses files you need Alert you to features and problems in the data that may affect your analysis. ","date":"2021-02-22","objectID":"/chnsdocumentation/:0:0","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"ID Variables - case identifiers, how to merge files across survey years Two systems of ID variables have been employed in CHNS data files: the original system used in surveys before 2004, and a revised system used thereafter. All old IDs have been changed to new IDs in all files. ","date":"2021-02-22","objectID":"/chnsdocumentation/:1:0","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Household ID (HHID) HHID is a nine-digit numeric variable that uniquely identified each household that had been seven digits in the old cross-sectional files. The variables T1 through T5 (documented in the questionnaire) were concatenated to form the HHID. Each HHID value represented one household. When the unit of analysis for a file/table was household, 1 HHID = 1 row = 1 household = 1 observation. Observations in these files were sorted by HHID and survey year (Wave). Thus the key sort variables for these files were HHID and Wave. ","date":"2021-02-22","objectID":"/chnsdocumentation/:1:1","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Individual ID (IDind) The IDind is a twelve-digit numeric variable that uniquely identified for all participants. Each participant will have the same ID in all datasets and in all survey years. The unique ID will not change over time and will facilitate data merges across datasets and survey years. Each IDind value represented one individual. When the unit of analysis for a file/table was individual, 1 IDind = 1 row = 1 individual = 1 observation. Observations in these files are sorted by IDind, and Wave, i.e., the key sort variables were IDind, and Wave. ","date":"2021-02-22","objectID":"/chnsdocumentation/:1:2","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Community ID (COMMID) A third ID variable, COMMID, is a six-digit numeric variable that uniquely identified each community. The variables T1 through T4 were concatenated to create COMMID. Each COMMID value represented one community. When the unit of analysis for a file was community, 1 COMMID = 1 community = 1 observation. Observations in these files were sorted by COMMID and Wave, i.e., the key sort variables were COMMID and Wave. Although COMMID was not required for most file merges, this variable was included on all data sets to facilitate merges with community-level files. When the unit of analysis was something other than individual, household or community (e.g., job, livestock type, food item, health facility), a variable that identified this unit was included on the file (e.g., JOB, F11, FOODCODE, Q1). For the files/tables where job was the unit of analysis, for example, each value of the variable JOB represented one occupation. That is, 1 JOB = 1 row = 1 occupation = 1 observation. Observations in these files were sorted by HHID, LINE, and JOB, i.e., the key sort variables were HHID, LINE, and JOB. ","date":"2021-02-22","objectID":"/chnsdocumentation/:1:3","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Survey Year (WAVE) The WAVE is a four-digit numeric variable that identified survey year (i.e, 1989, 1991, 1993, 1997, 2000, 2004, 2006, 2009, 2011, 2015). This variable was used in master longitudinal files only. ","date":"2021-02-22","objectID":"/chnsdocumentation/:1:4","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Special LINE Numbers - important LINE numbers Some special LINE numbers used in the data files are: 21 = first new household member added to the roster in 1991 22 = second new household member added to the roster in 1991, and so on 31 = first new household member added to the roster in 1993 32 = second new household member added to the roster in 1993, and so on 41 = first new household member added to the roster in 1997 42 = second new household member added to the roster in 1997, and so on 61 = first new household member added to the roster in 2000 62 = second new household member added to the roster in 2000, and so on 77 = grandparent 88 = uncle/aunt 99 = other -2 = guest For LINE numbers 21, 22, 31, 32, 41, 42, 61, 62, etc., the first digit identifies the round in which the individual first appeared in the Household Member Roster (2=1991, 3=1993, 4 \u0026 5=1997, 6=2000). LINE numbers 77, 88, and 99 are used in the Childcare section of the Household Survey. LINE number -2 is used in the Nutrition Survey. ","date":"2021-02-22","objectID":"/chnsdocumentation/:2:0","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Unit of Analysis - critical concept for analysis of multi-level data The CHNS data have been collected at many different levels: community, household, individual, job, food item, etc. The unit of analysis for a file can often be determined by a careful review of the appropriate questionnaire section. Because of the multi-level nature of the data and the large number of files, most users of the CHNS data must perform complex file merges on a regular basis. A thorough understanding of complex file structures is critical to the successful use of these data. To merge CHNS files properly, users must know the unit of analysis for each data file, be familiar with the case identifiers (ID Variables), and understand exactly how to use their software to combine files. ","date":"2021-02-22","objectID":"/chnsdocumentation/:3:0","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Variable Names - variable-naming system Most CHNS variable names consist of an alphabetic prefix (A, B, etc.) followed by a number. Each table/section of the questionnaire employs a different prefix. Variable names are normally printed below or to the right of the corresponding question on the questionnaire. Some variables were renamed to facilitate use of the data. For example, variables that identify line numbers in tables were renamed “LINE” (see ID Variables). The suffix “_91” was added to the 1991 variables to distinguish them from similarly named 1989 variables, and “_93” was added to the 1993 variables. However, “_97” and “_00” suffixes were not added to the 1997 and 2000 variables. Suffixes are not indicated in the questionnaires. Note that some variables documented in the questionnaires do not appear in the data files, particularly in 1989 data sets. Many of these variables were never entered into a computer file. Also note that variables T1 through T5 were dropped from 1989, 1991 and 1993 files since they were combined to form household and community identifiers (see ID Variables). The variables T1 through T5 were not dropped from 1997 and 2000 files. Finally the variables in the data files are not necessarily in the same order that they appear in the documentation. ","date":"2021-02-22","objectID":"/chnsdocumentation/:4:0","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Missing Values - missing value indicators Negative numbers (-9, -99, -999, -8, -88, -888, etc.) are typically used to identify missing values. Large positive numbers (88, 888, 99, 999, etc.) are also used for some variables. Many missing values are not documented in the questionnaire. ","date":"2021-02-22","objectID":"/chnsdocumentation/:5:0","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Date Variables - how date variables are handled Date information was originally entered using separate date variables (year, month, day). In 1989, 1991, and 1993 files, these variables were concatenated to form 4- and 6-digit variables of the form YYMM and YYMMDD, where YY=year, MM=month, and DD=day. Although they are documented in the questionnaires, the original year, month, and day variables have been deleted from 1989, 1991, and 1993 data files. Separate date variables (year, month, day) still exist in 1997 and 2000 files. The 1997 and 2000 year variables have the form YYYYS dates are used in the Master File. ","date":"2021-02-22","objectID":"/chnsdocumentation/:6:0","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Data Cleaning - general cleanup effort Basic data cleaning has included: Corrections to ID variables Corrections to the household roster Deletion of duplicate records Deletion of blank records Recoding out-of-range values to missing status. Note that only values that were clearly impossible were recoded to missing. The significance of extreme values still remaining in the files is left to the discretion of individual researchers. ","date":"2021-02-22","objectID":"/chnsdocumentation/:7:0","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Notes / Data Problems - important data features and problems ","date":"2021-02-22","objectID":"/chnsdocumentation/:8:0","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Sampling-Unit Variables (T1-T5) The original sampling-unit variables T1 through T5 were concatenated to form community and household ID variables (COMMID, HHID). They are still kept in master longitudinal files, although they contribute no unique information. ","date":"2021-02-22","objectID":"/chnsdocumentation/:8:1","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Area of Residence Variables Area of residence variables (STRATUM and URBAN) can be constructed from the original sampling-unit variables. The short SAS program that constructed these variables is available upon request by sending an email message to chns@unc.edu. ","date":"2021-02-22","objectID":"/chnsdocumentation/:8:2","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Incorrect Variable Names Several variables were named incorrectly in the original 1997 and 2000 Chinese questionnaires. The translated English questionnaires include the original name, with the corrected name as it appears in the data file enclosed in [brackets] to the right or below the original name. ","date":"2021-02-22","objectID":"/chnsdocumentation/:8:3","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Biomarker Data The biomarker data collected in CHNS 2009 involves the release of 26 fasting blood measures on individuals aged 7 and older. This included major cardiovascular biomarkers (lipids, diabetes such as HbA1c, glucose, insulin, triglycerides, CRP) and important nutrition biomarkers (transferrin, hemoglobin, and ferritin). An overview of some of the key results was published in: Yan, Shengkai, J. Li, S. Li, B. Zhang, S. Du, P. Gordon-Larsen, L. Adair, B.M. Popkin (2012) The expanding burden of cardiometabolic risk in China: the China Health and Nutrition Survey. Obesity Reviews 13 (9): 810-21. PMCID: PMC3429648. The full documentation of the collection and laboratory measurement are also placed as documents online: Protocols used to collect and process blood samples are available [here](https://www.cpc.unc.edu/projects/china/data/datasets/Blood Collection Protocol_English.pdf). A list of biomarkers and methods used to measure them is available here. Codebook of the biomarker dataset is available here. ","date":"2021-02-22","objectID":"/chnsdocumentation/:9:0","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Constructed Variables ","date":"2021-02-22","objectID":"/chnsdocumentation/:10:0","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Income Measures The constructed household income variables have been completely revised in 2011, and individual income variables have been constructed as well. These measures are fully documented below in PDF format: [Household Income Variable Construction](https://www.cpc.unc.edu/projects/china/data/datasets/Household Income Variable Construction.pdf), 92 KB [Individual Income Variable Construction](https://www.cpc.unc.edu/projects/china/data/datasets/Individual Income Variable Construction.pdf), 44 KB These files are longitudinal, with one observation per household or individual per wave. Income measures include: **Household Income (HHINC_10.sas7bdat) Total Annual Household Income, Nominal Total Annual Household Income, Inflated to 2015 Per Capita Household Income Components of Household Income (Business, Wages, etc.) Click here to review the codebook. Individual Income (INDINC_10.sas7bdat) ​ Total Annual Individual Income, Nominal ​ Total Annual Individual Income, inflated to 2015 ​ Components of individual income (business, wages, etc.) Click here to review the codebook. Follow the link below to “Download Constructed Variables” and select “Longitudinal” to find the constructed income files. ","date":"2021-02-22","objectID":"/chnsdocumentation/:10:1","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Nutrient Intake Variables (C12DIET.sas7bdat) Nutrient intake variables are constructed. The unit of analysis for these files is individual. Nutrient intake variables include: Average Daily Calorie Intake (in kilocalories) for Each Survey YearAverage Daily Protein Intake (in grams) for Each Survey YearAverage Daily Fat Intake (in grams) for Each Survey YearAverage Percent Calories From Fat for Each Survey Year Click here to review the codebook. Download Constructed Variables ","date":"2021-02-22","objectID":"/chnsdocumentation/:10:2","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":[],"content":"Longitudinal Data Master Files by chns@unc.edu — last modified Jun 12, 2018 11:13 AM General Description These pages: Describe the longitudinal data Allow users to download data directly from the Website Researchers can now download datasets known as CHNS Longitudinal Master Files. These new Master Files are designed to make longitudinal analysis of the CHNS Survey data much easier. The new Master Files consolidate and standardize data from multiple survey years into a select number of Master Files, and they address the following problems: ID (identification numbers) problems are corrected and consistent across all survey years. In addition, individuals who have lived in multiple households and had multiple IDs, now have all their data stored under their most current ID. Their previous IDs have been saved in the Master ID file. Birth dates are corrected and stored only in the Master ID File. And birth dates are now stored as both Western and Lunar dates. Gender has been corrected as needed and is stored only in the Master ID File and the Master PE File. Household Interview Dates are determined for all longitudinal files that may need to calculate age. Age is generally calculated as ((Household Interview Date - Western DOB) / 365.25). Variable names are standardized across all survey years when they represent the same survey question. With a few exceptions, they are standardized to the 2000 variable name. Data formatting is standardized across all survey years. All dates are now stored in the YYYYMMDD format. Click here to obtain a detailed pdf file that provides a full layout of the contents and organization of these files. Download the Data ","date":"2021-02-22","objectID":"/chnsdocumentation/:11:0","tags":["Academic","Database"],"title":"China Health and Nutrition Survey (CHNS) Documentation","uri":"/chnsdocumentation/"},{"categories":["Markdown"],"content":"Guide to emoji usage in Hugo and uBlogger.","date":"2019-10-01","objectID":"/emoji-support/","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Emoji can be enabled in a Hugo project in a number of ways. The emojify function can be called directly in templates or Inline Shortcodes. To enable emoji globally, set enableEmoji to true in your site configuration and then you can type emoji shorthand codes directly in content files. These begin and end with a colon and include the code of an emoji: Gone camping! :tent: Be back soon. That is so funny! :joy: The rendered output looks like this: Gone camping! ⛺ Be back soon. That is so funny! 😂 The following cheat sheet is a useful reference for emoji shorthand codes. ","date":"2019-10-01","objectID":"/emoji-support/:0:0","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Smileys \u0026 Emotion ","date":"2019-10-01","objectID":"/emoji-support/:1:0","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Face Smiling icon code icon code 😀 grinning 😃 smiley 😄 smile 😁 grin 😆 laughing satisfied 😅 sweat_smile 🤣 rofl 😂 joy 🙂 slightly_smiling_face 🙃 upside_down_face 😉 wink 😊 blush 😇 innocent ","date":"2019-10-01","objectID":"/emoji-support/:1:1","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Face Affection icon code icon code 😍 heart_eyes 😘 kissing_heart 😗 kissing ☺️ relaxed 😚 kissing_closed_eyes 😙 kissing_smiling_eyes ","date":"2019-10-01","objectID":"/emoji-support/:1:2","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Face Tongue icon code icon code 😋 yum 😛 stuck_out_tongue 😜 stuck_out_tongue_winking_eye 😝 stuck_out_tongue_closed_eyes 🤑 money_mouth_face ","date":"2019-10-01","objectID":"/emoji-support/:1:3","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Face Hand icon code icon code 🤗 hugs 🤔 thinking ","date":"2019-10-01","objectID":"/emoji-support/:1:4","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Face Neutral Skeptical icon code icon code 🤐 zipper_mouth_face 😐 neutral_face 😑 expressionless 😶 no_mouth 😏 smirk 😒 unamused 🙄 roll_eyes 😬 grimacing 🤥 lying_face ","date":"2019-10-01","objectID":"/emoji-support/:1:5","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Face Sleepy icon code icon code 😌 relieved 😔 pensive 😪 sleepy 🤤 drooling_face 😴 sleeping ","date":"2019-10-01","objectID":"/emoji-support/:1:6","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Face Unwell icon code icon code 😷 mask 🤒 face_with_thermometer 🤕 face_with_head_bandage 🤢 nauseated_face 🤧 sneezing_face 😵 dizzy_face ","date":"2019-10-01","objectID":"/emoji-support/:1:7","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Face Hat icon code icon code 🤠 cowboy_hat_face ","date":"2019-10-01","objectID":"/emoji-support/:1:8","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Face Glasses icon code icon code 😎 sunglasses 🤓 nerd_face ","date":"2019-10-01","objectID":"/emoji-support/:1:9","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Face Concerned icon code icon code 😕 confused 😟 worried 🙁 slightly_frowning_face ☹ frowning_face 😮 open_mouth 😯 hushed 😲 astonished 😳 flushed 😦 frowning 😧 anguished 😨 fearful 😰 cold_sweat 😥 disappointed_relieved 😢 cry 😭 sob 😱 scream 😖 confounded 😣 persevere 😞 disappointed 😓 sweat 😩 weary 😫 tired_face ","date":"2019-10-01","objectID":"/emoji-support/:1:10","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Face Negative icon code icon code 😤 triumph 😡 pout rage 😠 angry 😈 smiling_imp 👿 imp 💀 skull ☠️ skull_and_crossbones ","date":"2019-10-01","objectID":"/emoji-support/:1:11","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Face Costume icon code icon code 💩 hankey poop shit 🤡 clown_face 👹 japanese_ogre 👺 japanese_goblin 👻 ghost 👽 alien 👾 space_invader 🤖 robot ","date":"2019-10-01","objectID":"/emoji-support/:1:12","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Cat Face icon code icon code 😺 smiley_cat 😸 smile_cat 😹 joy_cat 😻 heart_eyes_cat 😼 smirk_cat 😽 kissing_cat 🙀 scream_cat 😿 crying_cat_face 😾 pouting_cat ","date":"2019-10-01","objectID":"/emoji-support/:1:13","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Monkey Face icon code icon code 🙈 see_no_evil 🙉 hear_no_evil 🙊 speak_no_evil ","date":"2019-10-01","objectID":"/emoji-support/:1:14","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Emotion icon code icon code 💋 kiss 💌 love_letter 💘 cupid 💝 gift_heart 💖 sparkling_heart 💗 heartpulse 💓 heartbeat 💞 revolving_hearts 💕 two_hearts 💟 heart_decoration ❣️ heavy_heart_exclamation 💔 broken_heart ❤️ heart 💛 yellow_heart 💚 green_heart 💙 blue_heart 💜 purple_heart 🖤 black_heart 💯 100 💢 anger 💥 boom collision 💫 dizzy 💦 sweat_drops 💨 dash 🕳️ hole 💣 bomb 💬 speech_balloon 👁️‍🗨️ eye_speech_bubble 🗯️ right_anger_bubble 💭 thought_balloon 💤 zzz ","date":"2019-10-01","objectID":"/emoji-support/:1:15","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"People \u0026 Body ","date":"2019-10-01","objectID":"/emoji-support/:2:0","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Hand Fingers Open icon code icon code 👋 wave 🤚 raised_back_of_hand 🖐️ raised_hand_with_fingers_splayed ✋ hand raised_hand 🖖 vulcan_salute ","date":"2019-10-01","objectID":"/emoji-support/:2:1","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Hand Fingers Partial icon code icon code 👌 ok_hand ✌️ v 🤞 crossed_fingers 🤘 metal 🤙 call_me_hand ","date":"2019-10-01","objectID":"/emoji-support/:2:2","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Hand Single Finger icon code icon code 👈 point_left 👉 point_right 👆 point_up_2 🖕 fu middle_finger 👇 point_down ☝️ point_up ","date":"2019-10-01","objectID":"/emoji-support/:2:3","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Hand Fingers Closed icon code icon code 👍 +1 thumbsup 👎 -1 thumbsdown ✊ fist fist_raised 👊 facepunch fist_oncoming punch 🤛 fist_left 🤜 fist_right ","date":"2019-10-01","objectID":"/emoji-support/:2:4","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Hands icon code icon code 👏 clap 🙌 raised_hands 👐 open_hands 🤝 handshake 🙏 pray ","date":"2019-10-01","objectID":"/emoji-support/:2:5","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Hand Prop icon code icon code ✍️ writing_hand 💅 nail_care 🤳 selfie ","date":"2019-10-01","objectID":"/emoji-support/:2:6","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Body Parts icon code icon code 💪 muscle 👂 ear 👃 nose 👀 eyes 👁️ eye 👅 tongue 👄 lips ","date":"2019-10-01","objectID":"/emoji-support/:2:7","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Person icon code icon code 👶 baby 👦 boy 👧 girl :blonde_man: blonde_man person_with_blond_hair 👨 man 👩 woman 👱‍♀️ blonde_woman 👴 older_man 👵 older_woman ","date":"2019-10-01","objectID":"/emoji-support/:2:8","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Person Gesture icon code icon code 🙍‍♀️ frowning_woman person_frowning 🙍‍♂️ frowning_man 🙎‍♀️ person_with_pouting_face pouting_woman 🙎‍♂️ pouting_man 🙅‍♀️ ng_woman no_good no_good_woman 🙅‍♂️ ng_man no_good_man 🙆‍♀️ ok_woman 🙆‍♂️ ok_man 💁‍♀️ information_desk_person sassy_woman tipping_hand_woman 💁‍♂️ sassy_man tipping_hand_man 🙋‍♀️ raising_hand raising_hand_woman 🙋‍♂️ raising_hand_man 🙇‍♂️ bow bowing_man 🙇‍♀️ bowing_woman 🤦‍♂️ man_facepalming 🤦‍♀️ woman_facepalming 🤷‍♂️ man_shrugging 🤷‍♀️ woman_shrugging ","date":"2019-10-01","objectID":"/emoji-support/:2:9","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Person Role icon code icon code 👨‍⚕️ man_health_worker 👩‍⚕️ woman_health_worker 👨‍🎓 man_student 👩‍🎓 woman_student 👨‍🏫 man_teacher 👩‍🏫 woman_teacher 👨‍⚖️ man_judge 👩‍⚖️ woman_judge 👨‍🌾 man_farmer 👩‍🌾 woman_farmer 👨‍🍳 man_cook 👩‍🍳 woman_cook 👨‍🔧 man_mechanic 👩‍🔧 woman_mechanic 👨‍🏭 man_factory_worker 👩‍🏭 woman_factory_worker 👨‍💼 man_office_worker 👩‍💼 woman_office_worker 👨‍🔬 man_scientist 👩‍🔬 woman_scientist 👨‍💻 man_technologist 👩‍💻 woman_technologist 👨‍🎤 man_singer 👩‍🎤 woman_singer 👨‍🎨 man_artist 👩‍🎨 woman_artist 👨‍✈️ man_pilot 👩‍✈️ woman_pilot 👨‍🚀 man_astronaut 👩‍🚀 woman_astronaut 👨‍🚒 man_firefighter 👩‍🚒 woman_firefighter 👮‍♂️ cop policeman 👮‍♀️ policewoman 🕵 detective male_detective 🕵️‍♀️ female_detective 💂‍♂️ guardsman 💂‍♀️ guardswoman 👷‍♂️ construction_worker construction_worker_man 👷‍♀️ construction_worker_woman 🤴 prince 👸 princess 👳‍♂️ man_with_turban 👳‍♀️ woman_with_turban 👲 man_with_gua_pi_mao 🤵‍♂️ man_in_tuxedo 👰 bride_with_veil 🤰 pregnant_woman ","date":"2019-10-01","objectID":"/emoji-support/:2:10","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Person Fantasy icon code icon code 👼 angel 🎅 santa 🤶 mrs_claus ","date":"2019-10-01","objectID":"/emoji-support/:2:11","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Person Activity icon code icon code 💆‍♀️ massage massage_woman 💆‍♂️ massage_man 💇‍♀️ haircut haircut_woman 💇‍♂️ haircut_man 🚶‍♂️ walking walking_man 🚶‍♀️ walking_woman 🏃‍♂️ runner running running_man 🏃‍♀️ running_woman 💃 dancer 🕺 man_dancing 🕴️ business_suit_levitating 👯‍♀️ dancers dancing_women 👯‍♂️ dancing_men ","date":"2019-10-01","objectID":"/emoji-support/:2:12","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Person Sport icon code icon code 🤺 person_fencing 🏇 horse_racing ⛷️ skier 🏂 snowboarder 🏌️‍♂️ golfing_man 🏌️‍♀️ golfing_woman 🏄‍♂️ surfer surfing_man 🏄‍♀️ surfing_woman 🚣‍♂️ rowboat rowing_man 🚣‍♀️ rowing_woman 🏊‍♂️ swimmer swimming_man 🏊‍♀️ swimming_woman ⛹️‍♂️ basketball_man ⛹️‍♀️ basketball_woman 🏋️‍♂️ weight_lifting_man 🏋️‍♀️ weight_lifting_woman 🚴‍♂️ bicyclist biking_man 🚴‍♀️ biking_woman 🚵‍♂️ mountain_bicyclist mountain_biking_man 🚵‍♀️ mountain_biking_woman 🤸‍♂️ man_cartwheeling 🤸‍♀️ woman_cartwheeling 🤼‍♂️ men_wrestling 🤼‍♀️ women_wrestling 🤽‍♂️ man_playing_water_polo 🤽‍♀️ woman_playing_water_polo 🤾‍♂️ man_playing_handball 🤾‍♀️ woman_playing_handball 🤹‍♂️ man_juggling 🤹‍♀️ woman_juggling ","date":"2019-10-01","objectID":"/emoji-support/:2:13","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Person Resting icon code icon code 🛀 bath 🛌 sleeping_bed ","date":"2019-10-01","objectID":"/emoji-support/:2:14","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Family icon code icon code 👭 two_women_holding_hands 👫 couple 👬 two_men_holding_hands 👩‍❤️‍💋‍👨 couplekiss_man_woman 👨‍❤️‍💋‍👨 couplekiss_man_man 👩‍❤️‍💋‍👩 couplekiss_woman_woman 👩‍❤️‍👨 couple_with_heart couple_with_heart_woman_man 👨‍❤️‍👨 couple_with_heart_man_man 👩‍❤️‍👩 couple_with_heart_woman_woman 👨‍👩‍👦 family family_man_woman_boy 👨‍👩‍👧 family_man_woman_girl 👨‍👩‍👧‍👦 family_man_woman_girl_boy 👨‍👩‍👦‍👦 family_man_woman_boy_boy 👨‍👩‍👧‍👧 family_man_woman_girl_girl 👨‍👨‍👦 family_man_man_boy 👨‍👨‍👧 family_man_man_girl 👨‍👨‍👧‍👦 family_man_man_girl_boy 👨‍👨‍👦‍👦 family_man_man_boy_boy 👨‍👨‍👧‍👧 family_man_man_girl_girl 👩‍👩‍👦 family_woman_woman_boy 👩‍👩‍👧 family_woman_woman_girl 👩‍👩‍👧‍👦 family_woman_woman_girl_boy 👩‍👩‍👦‍👦 family_woman_woman_boy_boy 👩‍👩‍👧‍👧 family_woman_woman_girl_girl 👨‍👦 family_man_boy 👨‍👦‍👦 family_man_boy_boy 👨‍👧 family_man_girl 👨‍👧‍👦 family_man_girl_boy 👨‍👧‍👧 family_man_girl_girl 👩‍👦 family_woman_boy 👩‍👦‍👦 family_woman_boy_boy 👩‍👧 family_woman_girl 👩‍👧‍👦 family_woman_girl_boy 👩‍👧‍👧 family_woman_girl_girl ","date":"2019-10-01","objectID":"/emoji-support/:2:15","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Person Symbol icon code icon code 🗣 speaking_head 👤 bust_in_silhouette 👥 busts_in_silhouette 👣 footprints ","date":"2019-10-01","objectID":"/emoji-support/:2:16","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Animals \u0026 Nature ","date":"2019-10-01","objectID":"/emoji-support/:3:0","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Animal Mammal icon code icon code 🐵 monkey_face 🐒 monkey 🦍 gorilla 🐶 dog 🐕 dog2 🐩 poodle 🐺 wolf 🦊 fox_face 🐱 cat 🐈 cat2 🦁 lion 🐯 tiger 🐅 tiger2 🐆 leopard 🐴 horse 🐎 racehorse 🦄 unicorn 🦌 deer 🐮 cow 🐂 ox 🐃 water_buffalo 🐄 cow2 🐷 pig 🐖 pig2 🐗 boar 🐽 pig_nose 🐏 ram 🐑 sheep 🐐 goat 🐪 dromedary_camel 🐫 camel 🐘 elephant 🦏 rhinoceros 🐭 mouse 🐁 mouse2 🐀 rat 🐹 hamster 🐰 rabbit 🐇 rabbit2 🐿️ chipmunk 🦇 bat 🐻 bear 🐨 koala 🐼 panda_face 🐾 feet paw_prints ","date":"2019-10-01","objectID":"/emoji-support/:3:1","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Animal Bird icon code icon code 🦃 turkey 🐔 chicken 🐓 rooster 🐣 hatching_chick 🐤 baby_chick 🐥 hatched_chick 🐦 bird 🐧 penguin 🕊 dove 🦅 eagle 🦆 duck 🦉 owl ","date":"2019-10-01","objectID":"/emoji-support/:3:2","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Animal Amphibian icon code icon code 🐸 frog ","date":"2019-10-01","objectID":"/emoji-support/:3:3","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Animal Reptile icon code icon code 🐊 crocodile 🐢 turtle 🦎 lizard 🐍 snake 🐲 dragon_face 🐉 dragon ","date":"2019-10-01","objectID":"/emoji-support/:3:4","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Animal Marine icon code icon code 🐳 whale 🐋 whale2 🐬 dolphin flipper 🐟 fish 🐠 tropical_fish 🐡 blowfish 🦈 shark 🐙 octopus 🐚 shell ","date":"2019-10-01","objectID":"/emoji-support/:3:5","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Animal Bug icon code icon code 🐌 snail 🦋 butterfly 🐛 bug 🐜 ant 🐝 bee honeybee 🪲 beetle 🕷️ spider 🕸️ spider_web 🦂 scorpion ","date":"2019-10-01","objectID":"/emoji-support/:3:6","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Plant Flower icon code icon code 💐 bouquet 🌸 cherry_blossom 💮 white_flower 🏵️ rosette 🌹 rose 🥀 wilted_flower 🌺 hibiscus 🌻 sunflower 🌼 blossom 🌷 tulip ","date":"2019-10-01","objectID":"/emoji-support/:3:7","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Plant Other icon code icon code 🌱 seedling 🌲 evergreen_tree 🌳 deciduous_tree 🌴 palm_tree 🌵 cactus 🌾 ear_of_rice 🌿 herb ☘️ shamrock 🍀 four_leaf_clover 🍁 maple_leaf 🍂 fallen_leaf 🍃 leaves ","date":"2019-10-01","objectID":"/emoji-support/:3:8","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Food \u0026 Drink ","date":"2019-10-01","objectID":"/emoji-support/:4:0","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Food Fruit icon code icon code 🍇 grapes 🍈 melon 🍉 watermelon 🍊 mandarin orange tangerine 🍋 lemon 🍌 banana 🍍 pineapple 🍎 apple 🍏 green_apple 🍐 pear 🍑 peach 🍒 cherries 🍓 strawberry 🥝 kiwi_fruit 🍅 tomato ","date":"2019-10-01","objectID":"/emoji-support/:4:1","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Food Vegetable icon code icon code 🥑 avocado 🍆 eggplant 🥔 potato 🥕 carrot 🌽 corn 🌶️ hot_pepper 🥒 cucumber 🍄 mushroom 🥜 peanuts 🌰 chestnut ","date":"2019-10-01","objectID":"/emoji-support/:4:2","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Food Prepared icon code icon code 🍞 bread 🥐 croissant 🥖 baguette_bread 🥞 pancakes 🧀 cheese 🍖 meat_on_bone 🍗 poultry_leg 🥓 bacon 🍔 hamburger 🍟 fries 🍕 pizza 🌭 hotdog 🌮 taco 🌯 burrito 🥙 stuffed_flatbread 🥚 egg 🍳 fried_egg 🥘 shallow_pan_of_food 🍲 stew 🥗 green_salad 🍿 popcorn ","date":"2019-10-01","objectID":"/emoji-support/:4:3","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Food Asian icon code icon code 🍱 bento 🍘 rice_cracker 🍙 rice_ball 🍚 rice 🍛 curry 🍜 ramen 🍝 spaghetti 🍠 sweet_potato 🍢 oden 🍣 sushi 🍤 fried_shrimp 🍥 fish_cake 🍡 dango ","date":"2019-10-01","objectID":"/emoji-support/:4:4","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Food Marine icon code icon code 🦀 crab 🦐 shrimp 🦑 squid ","date":"2019-10-01","objectID":"/emoji-support/:4:5","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Food Sweet icon code icon code 🍦 icecream 🍧 shaved_ice 🍨 ice_cream 🍩 doughnut 🍪 cookie 🎂 birthday 🍰 cake 🍫 chocolate_bar 🍬 candy 🍭 lollipop 🍮 custard 🍯 honey_pot ","date":"2019-10-01","objectID":"/emoji-support/:4:6","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Drink icon code icon code 🍼 baby_bottle 🥛 milk_glass ☕ coffee 🍵 tea 🍶 sake 🍾 champagne 🍷 wine_glass 🍸 cocktail 🍹 tropical_drink 🍺 beer 🍻 beers 🥂 clinking_glasses 🥃 tumbler_glass ","date":"2019-10-01","objectID":"/emoji-support/:4:7","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Dishware icon code icon code 🍽️ plate_with_cutlery 🍴 fork_and_knife 🥄 spoon 🔪 hocho knife 🏺 amphora ","date":"2019-10-01","objectID":"/emoji-support/:4:8","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Travel \u0026 Places ","date":"2019-10-01","objectID":"/emoji-support/:5:0","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Place Map icon code icon code 🌍 earth_africa 🌎 earth_americas 🌏 earth_asia 🌐 globe_with_meridians 🗺️ world_map 🗾 japan ","date":"2019-10-01","objectID":"/emoji-support/:5:1","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Place Geographic icon code icon code 🏔 mountain_snow ⛰️ mountain 🌋 volcano 🗻 mount_fuji 🏕️ camping ⛱ beach_umbrella 🏜️ desert 🏝️ desert_island 🏞️ national_park ","date":"2019-10-01","objectID":"/emoji-support/:5:2","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Place Building icon code icon code 🏟️ stadium 🏛️ classical_building 🏗️ building_construction 🏘 houses 🏚 derelict_house 🏠 house 🏡 house_with_garden 🏢 office 🏣 post_office 🏤 european_post_office 🏥 hospital 🏦 bank 🏨 hotel 🏩 love_hotel 🏪 convenience_store 🏫 school 🏬 department_store 🏭 factory 🏯 japanese_castle 🏰 european_castle 💒 wedding 🗼 tokyo_tower 🗽 statue_of_liberty ","date":"2019-10-01","objectID":"/emoji-support/:5:3","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Place Religious icon code icon code ⛪ church 🕌 mosque 🕍 synagogue ⛩️ shinto_shrine 🕋 kaaba ","date":"2019-10-01","objectID":"/emoji-support/:5:4","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Place Other icon code icon code ⛲ fountain ⛺ tent 🌁 foggy 🌃 night_with_stars 🏙️ cityscape 🌄 sunrise_over_mountains 🌅 sunrise 🌆 city_sunset 🌇 city_sunrise 🌉 bridge_at_night ♨️ hotsprings 🎠 carousel_horse 🎡 ferris_wheel 🎢 roller_coaster 💈 barber 🎪 circus_tent ","date":"2019-10-01","objectID":"/emoji-support/:5:5","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Transport Ground icon code icon code 🚂 steam_locomotive 🚃 railway_car 🚄 bullettrain_side 🚅 bullettrain_front 🚆 train2 🚇 metro 🚈 light_rail 🚉 station 🚊 tram 🚝 monorail 🚞 mountain_railway 🚋 train 🚌 bus 🚍 oncoming_bus 🚎 trolleybus 🚐 minibus 🚑 ambulance 🚒 fire_engine 🚓 police_car 🚔 oncoming_police_car 🚕 taxi 🚖 oncoming_taxi 🚗 car red_car 🚘 oncoming_automobile 🚙 blue_car 🚚 truck 🚛 articulated_lorry 🚜 tractor 🏎️ racing_car 🏍 motorcycle 🛵 motor_scooter 🚲 bike 🛴 kick_scooter 🚏 busstop 🛣️ motorway 🛤️ railway_track 🛢️ oil_drum ⛽ fuelpump 🚨 rotating_light 🚥 traffic_light 🚦 vertical_traffic_light 🛑 stop_sign 🚧 construction ","date":"2019-10-01","objectID":"/emoji-support/:5:6","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Transport Water icon code icon code ⚓ anchor ⛵ boat sailboat 🛶 canoe 🚤 speedboat 🛳️ passenger_ship ⛴️ ferry 🛥️ motor_boat 🚢 ship ","date":"2019-10-01","objectID":"/emoji-support/:5:7","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Transport Air icon code icon code ✈️ airplane 🛩️ small_airplane 🛫 flight_departure 🛬 flight_arrival 💺 seat 🚁 helicopter 🚟 suspension_railway 🚠 mountain_cableway 🚡 aerial_tramway 🛰️ artificial_satellite 🚀 rocket ","date":"2019-10-01","objectID":"/emoji-support/:5:8","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Hotel icon code icon code 🛎️ bellhop_bell ","date":"2019-10-01","objectID":"/emoji-support/:5:9","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Time icon code icon code ⌛ hourglass ⏳ hourglass_flowing_sand ⌚ watch ⏰ alarm_clock ⏱️ stopwatch ⏲️ timer_clock 🕰️ mantelpiece_clock 🕛 clock12 🕧 clock1230 🕐 clock1 🕜 clock130 🕑 clock2 🕝 clock230 🕒 clock3 🕞 clock330 🕓 clock4 🕟 clock430 🕔 clock5 🕠 clock530 🕕 clock6 🕡 clock630 🕖 clock7 🕢 clock730 🕗 clock8 🕣 clock830 🕘 clock9 🕤 clock930 🕙 clock10 🕥 clock1030 🕚 clock11 🕦 clock1130 ","date":"2019-10-01","objectID":"/emoji-support/:5:10","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Sky \u0026 Weather icon code icon code 🌑 new_moon 🌒 waxing_crescent_moon 🌓 first_quarter_moon 🌔 moon waxing_gibbous_moon 🌕 full_moon 🌖 waning_gibbous_moon 🌗 last_quarter_moon 🌘 waning_crescent_moon 🌙 crescent_moon 🌚 new_moon_with_face 🌛 first_quarter_moon_with_face 🌜 last_quarter_moon_with_face 🌡️ thermometer ☀️ sunny 🌝 full_moon_with_face 🌞 sun_with_face ⭐ star 🌟 star2 🌠 stars 🌌 milky_way ☁️ cloud ⛅ partly_sunny ⛈ cloud_with_lightning_and_rain 🌤 sun_behind_small_cloud 🌥 sun_behind_large_cloud 🌦 sun_behind_rain_cloud 🌧 cloud_with_rain 🌨 cloud_with_snow 🌩 cloud_with_lightning 🌪️ tornado 🌫️ fog 🌬 wind_face 🌀 cyclone 🌈 rainbow 🌂 closed_umbrella ☂️ open_umbrella ☂️ umbrella ⛱️ parasol_on_ground ⚡ zap ❄️ snowflake ☃️ snowman_with_snow ☃️ snowman ☄️ comet 🔥 fire 💧 droplet 🌊 ocean ","date":"2019-10-01","objectID":"/emoji-support/:5:11","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Activities ","date":"2019-10-01","objectID":"/emoji-support/:6:0","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Event icon code icon code 🎃 jack_o_lantern 🎄 christmas_tree 🎆 fireworks 🎇 sparkler ✨ sparkles 🎈 balloon 🎉 tada 🎊 confetti_ball 🎋 tanabata_tree 🎍 bamboo 🎎 dolls 🎏 flags 🎐 wind_chime 🎑 rice_scene 🎀 ribbon 🎁 gift 🎗️ reminder_ribbon 🎟 tickets 🎫 ticket ","date":"2019-10-01","objectID":"/emoji-support/:6:1","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Award Medal icon code icon code 🎖️ medal_military 🏆 trophy 🏅 medal_sports 🥇 1st_place_medal 🥈 2nd_place_medal 🥉 3rd_place_medal ","date":"2019-10-01","objectID":"/emoji-support/:6:2","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Sport icon code icon code ⚽ soccer ⚾ baseball 🏀 basketball 🏐 volleyball 🏈 football 🏉 rugby_football 🎾 tennis 🎳 bowling 🦗 cricket 🏑 field_hockey 🏒 ice_hockey 🏓 ping_pong 🏸 badminton 🥊 boxing_glove 🥋 martial_arts_uniform 🥅 goal_net ⛳ golf ⛸️ ice_skate 🎣 fishing_pole_and_fish 🎽 running_shirt_with_sash 🎿 ski ","date":"2019-10-01","objectID":"/emoji-support/:6:3","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Game icon code icon code 🎯 dart 🎱 8ball 🔮 crystal_ball 🎮 video_game 🕹️ joystick 🎰 slot_machine 🎲 game_die ♠️ spades ♥️ hearts ♦️ diamonds ♣️ clubs 🃏 black_joker 🀄 mahjong 🎴 flower_playing_cards ","date":"2019-10-01","objectID":"/emoji-support/:6:4","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Arts \u0026 Crafts icon code icon code 🎭 performing_arts 🖼 framed_picture 🎨 art ","date":"2019-10-01","objectID":"/emoji-support/:6:5","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Objects ","date":"2019-10-01","objectID":"/emoji-support/:7:0","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Clothing icon code icon code 👓 eyeglasses 🕶️ dark_sunglasses 👔 necktie 👕 shirt tshirt 👖 jeans 👗 dress 👘 kimono 👙 bikini 👚 womans_clothes 👛 purse 👜 handbag 👝 pouch 🛍️ shopping 🎒 school_satchel 👞 mans_shoe shoe 👟 athletic_shoe 👠 high_heel 👡 sandal 👢 boot 👑 crown 👒 womans_hat 🎩 tophat 🎓 mortar_board ⛑️ rescue_worker_helmet 📿 prayer_beads 💄 lipstick 💍 ring 💎 gem ","date":"2019-10-01","objectID":"/emoji-support/:7:1","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Sound icon code icon code 🔇 mute 🔈 speaker 🔉 sound 🔊 loud_sound 📢 loudspeaker 📣 mega 📯 postal_horn 🔔 bell 🔕 no_bell ","date":"2019-10-01","objectID":"/emoji-support/:7:2","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Music icon code icon code 🎼 musical_score 🎵 musical_note 🎶 notes 🎙️ studio_microphone 🎚️ level_slider 🎛️ control_knobs 🎤 microphone 🎧 headphones 📻 radio ","date":"2019-10-01","objectID":"/emoji-support/:7:3","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Musical Instrument icon code icon code 🎷 saxophone 🎸 guitar 🎹 musical_keyboard 🎺 trumpet 🎻 violin 🥁 drum ","date":"2019-10-01","objectID":"/emoji-support/:7:4","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Phone icon code icon code 📱 iphone 📲 calling ☎️ phone telephone 📞 telephone_receiver 📟 pager 📠 fax ","date":"2019-10-01","objectID":"/emoji-support/:7:5","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Computer icon code icon code 🔋 battery 🔌 electric_plug 💻 computer 🖥️ desktop_computer 🖨️ printer ⌨️ keyboard 🖱 computer_mouse 🖲️ trackball 💽 minidisc 💾 floppy_disk 💿 cd 📀 dvd ","date":"2019-10-01","objectID":"/emoji-support/:7:6","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Light \u0026 Video icon code icon code 🎥 movie_camera 🎞️ film_strip 📽️ film_projector 🎬 clapper 📺 tv 📷 camera 📸 camera_flash 📹 video_camera 📼 vhs 🔍 mag 🔎 mag_right 🕯️ candle 💡 bulb 🔦 flashlight 🏮 izakaya_lantern lantern ","date":"2019-10-01","objectID":"/emoji-support/:7:7","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Book Paper icon code icon code 📔 notebook_with_decorative_cover 📕 closed_book 📖 book open_book 📗 green_book 📘 blue_book 📙 orange_book 📚 books 📓 notebook 📒 ledger 📃 page_with_curl 📜 scroll 📄 page_facing_up 📰 newspaper 🗞️ newspaper_roll 📑 bookmark_tabs 🔖 bookmark 🏷️ label ","date":"2019-10-01","objectID":"/emoji-support/:7:8","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Money icon code icon code 💰 moneybag 💴 yen 💵 dollar 💶 euro 💷 pound 💸 money_with_wings 💳 credit_card 💹 chart ","date":"2019-10-01","objectID":"/emoji-support/:7:9","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Mail icon code icon code ✉️ email envelope 📧 📧 📨 incoming_envelope 📩 envelope_with_arrow 📤 outbox_tray 📥 inbox_tray 📦 package 📫 mailbox 📪 mailbox_closed 📬 mailbox_with_mail 📭 mailbox_with_no_mail 📮 postbox 🗳 ballot_box ","date":"2019-10-01","objectID":"/emoji-support/:7:10","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Writing icon code icon code ✏️ pencil2 ✒️ black_nib 🖋 fountain_pen 🖊 pen 🖌 paintbrush 🖍 crayon 📝 memo pencil ","date":"2019-10-01","objectID":"/emoji-support/:7:11","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Office icon code icon code 💼 briefcase 📁 file_folder 📂 open_file_folder 🗂️ card_index_dividers 📅 date 📆 calendar 🗒 spiral_notepad 🗓 spiral_calendar 📇 card_index 📈 chart_with_upwards_trend 📉 chart_with_downwards_trend 📊 bar_chart 📋 clipboard 📌 pushpin 📍 round_pushpin 📎 paperclip 🖇 paperclips 📏 straight_ruler 📐 triangular_ruler ✂️ scissors 🗃️ card_file_box 🗄️ file_cabinet 🗑️ wastebasket ","date":"2019-10-01","objectID":"/emoji-support/:7:12","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Lock icon code icon code 🔒 lock 🔓 unlock 🔏 lock_with_ink_pen 🔐 closed_lock_with_key 🔑 key 🗝️ old_key ","date":"2019-10-01","objectID":"/emoji-support/:7:13","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Tool icon code icon code 🔨 hammer ⛏️ pick ⚒️ hammer_and_pick 🛠️ hammer_and_wrench 🗡 dagger ⚔️ crossed_swords 🔫 gun 🏹 bow_and_arrow 🛡️ shield 🔧 wrench 🔩 nut_and_bolt ⚙️ gear 🗜 clamp ⚖ balance_scale 🔗 link ⛓️ chains ","date":"2019-10-01","objectID":"/emoji-support/:7:14","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Science icon code icon code ⚗️ alembic 🔬 microscope 🔭 telescope 🛰️ satellite ","date":"2019-10-01","objectID":"/emoji-support/:7:15","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Medical icon code icon code 💉 syringe 💊 pill ","date":"2019-10-01","objectID":"/emoji-support/:7:16","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Household icon code icon code 🚪 door 🛏️ bed 🛋️ couch_and_lamp 🚽 toilet 🚿 shower 🛁 bathtub 🛒 shopping_cart ","date":"2019-10-01","objectID":"/emoji-support/:7:17","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Other Object icon code icon code 🚬 smoking ⚰️ coffin ⚱️ funeral_urn 🗿 moyai ","date":"2019-10-01","objectID":"/emoji-support/:7:18","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Symbols ","date":"2019-10-01","objectID":"/emoji-support/:8:0","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Transport Sign icon code icon code 🏧 atm 🚮 put_litter_in_its_place 🚰 potable_water ♿ wheelchair 🚹 mens 🚺 womens 🚻 restroom 🚼 baby_symbol 🚾 wc 🛂 passport_control 🛃 customs 🛄 baggage_claim 🛅 left_luggage ","date":"2019-10-01","objectID":"/emoji-support/:8:1","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Warning icon code icon code ⚠️ warning 🚸 children_crossing ⛔ no_entry 🚫 no_entry_sign 🚳 no_bicycles 🚭 no_smoking 🚯 do_not_litter 🚱 🚱 🚷 no_pedestrians 📵 no_mobile_phones 🔞 underage ☢ radioactive ☣ biohazard ","date":"2019-10-01","objectID":"/emoji-support/:8:2","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Arrow icon code icon code ⬆️ arrow_up ↗️ arrow_upper_right ➡️ arrow_right ↘️ arrow_lower_right ⬇️ arrow_down ↙️ arrow_lower_left ⬅️ arrow_left ↖️ arrow_upper_left ↕️ arrow_up_down ↔️ left_right_arrow ↩️ leftwards_arrow_with_hook ↪️ arrow_right_hook ⤴️ arrow_heading_up ⤵️ arrow_heading_down 🔃 arrows_clockwise 🔄 arrows_counterclockwise 🔙 back 🔚 end 🔛 on 🔜 soon 🔝 top ","date":"2019-10-01","objectID":"/emoji-support/:8:3","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Religion icon code icon code 🛐 place_of_worship ⚛️ atom_symbol 🕉 om ✡️ star_of_david ☸️ wheel_of_dharma ☯️ yin_yang ✝️ latin_cross ☦️ orthodox_cross ☪️ star_and_crescent ☮️ peace_symbol 🕎 menorah 🔯 six_pointed_star ","date":"2019-10-01","objectID":"/emoji-support/:8:4","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Zodiac icon code icon code ♈ aries ♉ taurus ♊ gemini ♋ cancer ♌ leo ♍ virgo ♎ libra ♏ scorpius ♐ sagittarius ♑ capricorn ♒ aquarius ♓ pisces ⛎ ophiuchus ","date":"2019-10-01","objectID":"/emoji-support/:8:5","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Av Symbol icon code icon code 🔀 twisted_rightwards_arrows 🔁 repeat 🔂 repeat_one ▶️ arrow_forward ⏩ fast_forward ⏭ next_track_button ⏯ play_or_pause_button ◀️ arrow_backward ⏪ rewind ⏮️ previous_track_button 🔼 arrow_up_small ⏫ arrow_double_up 🔽 arrow_down_small ⏬ arrow_double_down ⏸ pause_button ⏹ stop_button ⏺ record_button 🎦 cinema 🔅 low_brightness 🔆 high_brightness 📶 signal_strength 📳 vibration_mode 📴 mobile_phone_off ","date":"2019-10-01","objectID":"/emoji-support/:8:6","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Math icon code icon code ✖️ heavy_multiplication_x ➕ heavy_plus_sign ➖ heavy_minus_sign ➗ heavy_division_sign ","date":"2019-10-01","objectID":"/emoji-support/:8:7","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Punctuation icon code icon code ‼️ bangbang ⁉️ interrobang ❓ question ❔ grey_question ❕ grey_exclamation ❗ exclamation heavy_exclamation_mark 〰️ wavy_dash ","date":"2019-10-01","objectID":"/emoji-support/:8:8","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Currency icon code icon code 💱 currency_exchange 💲 heavy_dollar_sign ","date":"2019-10-01","objectID":"/emoji-support/:8:9","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Keycap icon code icon code #️⃣ hash *️⃣ asterisk 0️⃣ zero 1️⃣ one 2️⃣ two 3️⃣ three 4️⃣ four 5️⃣ five 6️⃣ six 7️⃣ seven 8️⃣ eight 9️⃣ nine 🔟 keycap_ten ","date":"2019-10-01","objectID":"/emoji-support/:8:10","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Alphabet icon code icon code 🔠 capital_abcd 🔡 abcd 🔢 1234 🔣 symbols 🔤 abc 🅰️ a 🆎 ab 🅱️ b 🆑 cl 🆒 cool 🆓 free ℹ️ information_source 🆔 id ⓜ️ m 🆕 new 🆖 ng 🅾️ o2 🆗 ok 🅿️ parking 🆘 sos 🆙 up 🆚 vs 🈁 koko 🈂️ sa 🈷️ u6708 🈶 u6709 🈯 u6307 🉐 ideograph_advantage 🈹 u5272 🈚 u7121 🈲 u7981 🉑 accept 🈸 u7533 🈴 u5408 🈳 u7a7a ㊗️ congratulations ㊙️ secret 🈺 u55b6 🈵 u6e80 ","date":"2019-10-01","objectID":"/emoji-support/:8:11","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Geometric icon code icon code 🔴 red_circle 🔵 large_blue_circle ⚫ black_circle ⚪ white_circle ⬛ black_large_square ⬜ white_large_square ◼️ black_medium_square ◻️ white_medium_square ◾ black_medium_small_square ◽ white_medium_small_square ▪️ black_small_square ▫️ white_small_square 🔶 large_orange_diamond 🔷 large_blue_diamond 🔸 small_orange_diamond 🔹 small_blue_diamond 🔺 small_red_triangle 🔻 small_red_triangle_down 💠 diamond_shape_with_a_dot_inside 🔘 radio_button 🔳 white_square_button 🔲 black_square_button ","date":"2019-10-01","objectID":"/emoji-support/:8:12","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Other Symbol icon code icon code ♻️ recycle ⚜️ fleur_de_lis 🔱 trident 📛 name_badge 🔰 beginner ⭕ o ✅ white_check_mark ☑️ ballot_box_with_check ✔️ heavy_check_mark ❌ x ❎ negative_squared_cross_mark ➰ curly_loop ➿ loop 〽️ part_alternation_mark ✳️ eight_spoked_asterisk ✴️ eight_pointed_black_star ❇️ sparkle ©️ copyright ®️ registered ™️ tm ","date":"2019-10-01","objectID":"/emoji-support/:8:13","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Flags ","date":"2019-10-01","objectID":"/emoji-support/:9:0","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Common Flags icon code icon code 🏁 checkered_flag 🚩 triangular_flag_on_post 🎌 crossed_flags 🏴 black_flag 🏳 white_flag 🏳️‍🌈 rainbow_flag ","date":"2019-10-01","objectID":"/emoji-support/:9:1","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Country and Region Flags icon code icon code 🇦🇩 andorra 🇦🇪 united_arab_emirates 🇦🇫 afghanistan 🇦🇬 antigua_barbuda 🇦🇮 anguilla 🇦🇱 albania 🇦🇲 armenia 🇦🇴 angola 🇦🇶 antarctica 🇦🇷 argentina 🇦🇸 american_samoa 🇦🇹 austria 🇦🇺 australia 🇦🇼 aruba 🇦🇽 aland_islands 🇦🇿 azerbaijan 🇧🇦 bosnia_herzegovina 🇧🇧 barbados 🇧🇩 bangladesh 🇧🇪 belgium 🇧🇫 burkina_faso 🇧🇬 bulgaria 🇧🇭 bahrain 🇧🇮 burundi 🇧🇯 benin 🇧🇱 st_barthelemy 🇧🇲 bermuda 🇧🇳 brunei 🇧🇴 bolivia 🇧🇶 caribbean_netherlands 🇧🇷 brazil 🇧🇸 bahamas 🇧🇹 bhutan 🇧🇼 botswana 🇧🇾 belarus 🇧🇿 belize 🇨🇦 canada 🇨🇨 cocos_islands 🇨🇩 congo_kinshasa 🇨🇫 central_african_republic 🇨🇬 congo_brazzaville 🇨🇭 switzerland 🇨🇮 cote_divoire 🇨🇰 cook_islands 🇨🇱 chile 🇨🇲 cameroon 🇨🇳 cn 🇨🇴 colombia 🇨🇷 costa_rica 🇨🇺 cuba 🇨🇻 cape_verde 🇨🇼 curacao 🇨🇽 christmas_island 🇨🇾 cyprus 🇨🇿 czech_republic 🇩🇪 de 🇩🇯 djibouti 🇩🇰 denmark 🇩🇲 dominica 🇩🇴 dominican_republic 🇩🇿 algeria 🇪🇨 ecuador 🇪🇪 estonia 🇪🇬 egypt 🇪🇭 western_sahara 🇪🇷 eritrea 🇪🇸 es 🇪🇹 ethiopia 🇪🇺 eu european_union 🇫🇮 finland 🇫🇯 fiji 🇫🇰 falkland_islands 🇫🇲 micronesia 🇫🇴 faroe_islands 🇫🇷 fr 🇬🇦 gabon 🇬🇧 gb uk 🇬🇩 grenada 🇬🇪 georgia 🇬🇫 french_guiana 🇬🇬 guernsey 🇬🇭 ghana 🇬🇮 gibraltar 🇬🇱 greenland 🇬🇲 gambia 🇬🇳 guinea 🇬🇵 guadeloupe 🇬🇶 equatorial_guinea 🇬🇷 greece 🇬🇸 south_georgia_south_sandwich_islands 🇬🇹 guatemala 🇬🇺 guam 🇬🇼 guinea_bissau 🇬🇾 guyana 🇭🇰 hong_kong 🇭🇳 honduras 🇭🇷 croatia 🇭🇹 haiti 🇭🇺 hungary 🇮🇨 canary_islands 🇮🇩 indonesia 🇮🇪 ireland 🇮🇱 israel 🇮🇲 isle_of_man 🇮🇳 india 🇮🇴 british_indian_ocean_territory 🇮🇶 iraq 🇮🇷 iran 🇮🇸 iceland 🇮🇹 it 🇯🇪 jersey 🇯🇲 jamaica 🇯🇴 jordan 🇯🇵 jp 🇰🇪 kenya 🇰🇬 kyrgyzstan 🇰🇭 cambodia 🇰🇮 kiribati 🇰🇲 comoros 🇰🇳 st_kitts_nevis 🇰🇵 north_korea 🇰🇷 kr 🇰🇼 kuwait 🇰🇾 cayman_islands 🇰🇿 kazakhstan 🇱🇦 laos 🇱🇧 lebanon 🇱🇨 st_lucia 🇱🇮 liechtenstein 🇱🇰 sri_lanka 🇱🇷 liberia 🇱🇸 lesotho 🇱🇹 lithuania 🇱🇺 luxembourg 🇱🇻 latvia 🇱🇾 libya 🇲🇦 morocco 🇲🇨 monaco 🇲🇩 moldova 🇲🇪 montenegro 🇲🇬 madagascar 🇲🇭 marshall_islands 🇲🇰 macedonia 🇲🇱 mali 🇲🇲 myanmar 🇲🇳 mongolia 🇲🇴 macau 🇲🇵 northern_mariana_islands 🇲🇶 martinique 🇲🇷 mauritania 🇲🇸 montserrat 🇲🇹 malta 🇲🇺 mauritius 🇲🇻 maldives 🇲🇼 malawi 🇲🇽 mexico 🇲🇾 malaysia 🇲🇿 mozambique 🇳🇦 namibia 🇳🇨 new_caledonia 🇳🇪 niger 🇳🇫 norfolk_island 🇳🇬 nigeria 🇳🇮 nicaragua 🇳🇱 netherlands 🇳🇴 norway 🇳🇵 nepal 🇳🇷 nauru 🇳🇺 niue 🇳🇿 new_zealand 🇴🇲 oman 🇵🇦 panama 🇵🇪 peru 🇵🇫 french_polynesia 🇵🇬 papua_new_guinea 🇵🇭 philippines 🇵🇰 pakistan 🇵🇱 poland 🇵🇲 st_pierre_miquelon 🇵🇳 pitcairn_islands 🇵🇷 puerto_rico 🇵🇸 palestinian_territories 🇵🇹 portugal 🇵🇼 palau 🇵🇾 paraguay 🇶🇦 qatar 🇷🇪 reunion 🇷🇴 romania 🇷🇸 serbia 🇷🇺 ru 🇷🇼 rwanda 🇸🇦 saudi_arabia 🇸🇧 solomon_islands 🇸🇨 seychelles 🇸🇩 sudan 🇸🇪 sweden 🇸🇬 singapore 🇸🇭 st_helena 🇸🇮 slovenia 🇸🇰 slovakia 🇸🇱 sierra_leone 🇸🇲 san_marino 🇸🇳 senegal 🇸🇴 somalia 🇸🇷 suriname 🇸🇸 south_sudan 🇸🇹 sao_tome_principe 🇸🇻 el_salvador 🇸🇽 sint_maarten 🇸🇾 syria 🇸🇿 swaziland 🇹🇨 turks_caicos_islands 🇹🇩 chad 🇹🇫 french_southern_territories 🇹🇬 togo 🇹🇭 thailand 🇹🇯 tajikistan 🇹🇰 tokelau 🇹🇱 timor_leste 🇹🇲 turkmenistan 🇹🇳 tunisia 🇹🇴 tonga 🇹🇷 tr 🇹🇹 trinidad_tobago 🇹🇻 tuvalu 🇹🇼 taiwan 🇹🇿 tanzania 🇺🇦 ukraine 🇺🇬 uganda 🇺🇸 us 🇺🇾 uruguay 🇺🇿 uzbekistan 🇻🇦 vatican_city 🇻🇨 st_vincent_grenadines 🇻🇪 venezuela 🇻🇬 british_virgin_islands 🇻🇮 us_virgin_islands 🇻🇳 vietnam 🇻🇺 vanuatu 🇼🇫 wallis_futuna 🇼🇸 samoa 🇽🇰 kosovo 🇾🇪 yemen 🇾🇹 mayotte 🇿🇦 south_africa 🇿🇲 zambia 🇿🇼 zimbabwe ","date":"2019-10-01","objectID":"/emoji-support/:9:2","tags":["emoji"],"title":"Emoji Support","uri":"/emoji-support/"},{"categories":[],"content":"全球衍生品市场现状 场外市场成交量远大于城内市场但不方便统计。 农产品：成交量前10的合约我国占8个 能源：原油期货最为活跃，Brent WTI为主，上期货原油期货排第四 股权衍生品：印度、韩国占世界大头 金属衍生品：螺纹钢成交量列世界第一 外汇衍生品：国内暂时没有场内合约 利率衍生品：国债期货(中金所，由于监管考量，国有商业银行未进入市场，因此我国国债期货体量很小) 交易所排名 国家证券交易所（National Stock Exchange of India，NSE） 芝商所(CME) 国内衍生品市场 发展时间较短，发展的动力来自于政府(国外衍生品市场的发展则来自于市场) 新中国成立后 计划经济时代，价格官定，没有对冲需求 80s，价格双轨制后开始出现对冲需求 目前我国衍生品市场的潜力与需求很大，但需要官方自上而下推动 目前期货公司不允许自营业务，发展空间很小 ","date":"2021-03-04","objectID":"/globalderivativesmarkets/:0:0","tags":[],"title":"Global Derivatives Markets","uri":"/globalderivativesmarkets/"},{"categories":["Monetary Finance"],"content":"2.1 Introduction The neoclassical growth model due to Ramsey (1928) and Solow (1956) provides the basic framework for much of modern macroeconomics. When the assumption of a fixed savings rate is replaced by a model of forward looking households choosing savings and labor supply to maximize lifetime utility, the Solow model becomes the foundation for dynamic stochastic general equilibrium(DSGE) models of the business cycle. The neoclassical growth model is a model of a nonmonetary economy, and although goods are exchanged and transactions must be taking place, there is no medium of exchange—that is, no ‘‘money’’—that is used to facilitate these transactions. Three general approaches to incorporating money into general equilibrium models have been followed: assume that money yields direct utility by incorporating money balances into the utility functions of the agents of the model (Sidrauski 1967); impose transaction costs of some form that give rise to a demand for money. treat money like any other asset used to transfer resources intertemporally (Samuelson 1958). This chapter develops the first of the three approaches by incorporating into the basic neoclassical model agents whose utility depends directly on their consumption of goods and their holdings of money. ","date":"2021-03-03","objectID":"/2.moneyinutility/:1:0","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.2 The Basic MIU Model To develop the basic MIU approach, we will initially ignore uncertainty and any labor-leisure choice, focusing instead on the implications of the model for money demand, the value of money, and the costs of inflation. Suppose that the utility function of the representative household takes the form: $$ U_{t}=u\\left(c_{t}, z_{t}\\right) $$ where $z_t $ is the flow of services yielded by money holdings and $ c_t $ is time t per capita consumption. $ \\lim _{z \\rightarrow 0} u_{z}(c, z)=\\infty $ for all $ c, $ where $ u_{z}=\\partial u(c, z) / \\partial z $. $$ z_{t}=\\frac{M_{t}}{P_{t} N_{t}} \\equiv m_{t} $$ where $M_t$ is total nominal money, $P_t$ is price level, $N_t$ is population. The representative household is viewed as choosing time paths for consumption and real money balances subject to budget constraints to be specified, with total utility given by: $$ W=\\sum_{t=0}^{\\infty} \\beta^{t} u\\left(c_{t}, m_{t}\\right) \\tag{2.1} $$ where $ \\beta $ is a subjective rate of discount($ 0\u003c\\beta\u003c1$). the aggregate economywide budget constraint of the household sector takes the form: $$ Y_{t}+\\tau_{t} N_{t}+(1-\\delta) K_{t-1}+\\frac{\\left(1+i_{t-1}\\right) B_{t-1}}{P_{t}}+\\frac{M_{t-1}}{P_{t}}=C_{t}+K_{t}+\\frac{M_{t}}{P_{t}}+\\frac{B_{t}}{P_{t}} \\tag{2.2} $$ where $Y_t$ is aggregate output, $K_t$is the aggregate stock of capital at the start of period t, $\\delta$ is the rate of depreciation of physical capital, $\\tau_t N_t$ is the aggregate real value of any lump-sum transfers or taxes, $B_t$ is the amount of bond, $i_t$ is nominal interest rate. The timing implicit in this specification of the MIU model assumes that it is the household’s real money holdings at the end of the period, $M_t/P_t$, after having purchased consumption goods, that yield utility. The aggregate production function relates output $ Y_{t} $ to the available capital stock $ K_{t-1} $ and employment $ N_{t}: Y_{t}=F\\left(K_{t-1}, N_{t}\\right) .^{7} $ Assuming that this production function is linear homogeneous with constant returns to scale, output per capita $ y_{t} $ will be a function of the per capita capital stock $ k_{t-1}: 8 $ $ y_{t}=f\\left(\\frac{k_{t-1}}{1+n}\\right) $ Dividing both sides of the budget constraint (2.2) by population , the per capita version becomes: $ \\omega_{t} \\equiv f\\left(\\frac{k_{t-1}}{1+n}\\right)+\\tau_{t}+\\left(\\frac{1-\\delta}{1+n}\\right) k_{t-1}+\\frac{\\left(1+i_{t-1}\\right) b_{t-1}+m_{t-1}}{\\left(1+\\pi_{t}\\right)(1+n)}=c_{t}+k_{t}+m_{t}+b_{t} $ where $ \\pi_{t} $ is the rate of inflation, $ b_{t}=B_{t} / P_{t} N_{t}, $ and $ m_{t}=M_{t} / P_{t} N_{l} $ The household’s problem is to choose paths for to maximize (2.1) subject to (2.4). This is a problem in dynamic optimization, and it is convenient to formulate the problem in terms of a value function. The value function gives the maximized present discounted value of utility that the household can achieve by optimally choosing consumption, capital holdings, bond holdings, and money balances, given its current state. The state variable for the problem is the household’s initial level of resources , and the value function is defined by: $$ V\\left(\\omega_{t}\\right)=\\max _{c_{t}, k_{t}, b_{t}, m_{t}}\\left{u\\left(c_{t}, m_{t}\\right)+\\beta V\\left(\\omega_{t+1}\\right)\\right} $$ Using (2.4) to express $ k_{t} $ as $ \\omega_{t}-c_{t}-m_{t}-b_{t} $ and making use of the definition of (2.5) can be written as $ \\omega_{t+1} $ $$ \\begin{aligned} V\\left(\\omega_{t}\\right)=\\max _{c_{t}, b_{t}, m_{t}}{\u0026 u\\left(c_{t}, m_{t}\\right)+\\beta V\\left(\\frac{f\\left(\\omega_{t}-c_{t}-m_{t}-b_{t}\\right)}{1+n}+\\tau_{t+1}\\right.\\ \u0026\\left.\\left.+\\left(\\frac{1-\\delta}{1+n}\\right)\\left(\\omega_{t}-c_{t}-m_{t}-b_{t}\\right)+\\frac{\\left(1+i_{t}\\right) b_{t}+m_{t}}{\\left(1+\\pi_{t+1}\\right)(1+n)}\\right)\\right} \\end{aligned} $$ with the maximization problem now an unconstrained one over $ c_{t}, b_{t}, $ and $ m_{t} . $ The first-order necessary conditions for this problem are $ u_{c}\\left(c_{t}, m_{t}\\right)-\\f","date":"2021-03-03","objectID":"/2.moneyinutility/:2:0","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.2.1 Steady-State Equilibrium Consider the properties of this economy when it is in a steady-state equilibrium with and the nominal supply of money growing at the rate . Let the superscript denote values evaluated at the steady state. Note that with real money balances constant in the steady state, it must be that the prices are growing at the same rate as the nominal stock of money, or . Using (2.10) to eliminate , the equilibrium conditions can be written as: $ u_{c}\\left(c^{s s}, m^{s s}\\right)-\\beta\\left[f_{k}\\left(k^{s s}\\right)+1-\\delta\\right] u_{c}\\left(c^{s s}, m^{s s}\\right)=0 $ $ \\frac{1+i^{s s}}{1+\\theta}-\\left[f_{k}\\left(k^{s s}\\right)+1-\\delta\\right]=0 $ $ u_{m}\\left(c^{s s}, m^{s s}\\right)-\\beta\\left[f_{k}\\left(k^{s s}\\right)+1-\\delta\\right] u_{c}\\left(c^{s s}, m^{s s}\\right)+\\frac{\\beta u_{c}\\left(c^{s s}, m^{s s}\\right)}{1+\\theta}=0 $ $ f\\left(k^{s s}\\right)+\\tau^{s s}+(1-\\delta) k^{s s}+\\frac{m^{s s}}{1+\\theta}=c^{s s}+k^{s s}+m^{s s} $ where $ \\omega^{s s}=f\\left(k^{s s}\\right)+\\tau^{s s}+(1-\\delta) k^{s s}+m^{s s} /(1+\\pi) . $ In $ (2.14)-(2.17), $ use has been made of the fact that in the equilibrium of this representative agent model, $ b=0 $. Equation (2.15) is the steady-state form of the Fisher relationship linking real and nominal interest rates. This can be seen by noting that the real return on capital (net of depreciation) is $ r^{s s} \\equiv f_{k}\\left(k^{s s}\\right)-\\delta, $ so (2.15) can be written as $ 1+i^{s s}=\\left(1+r^{s^{s}}\\right)(1+\\theta)=\\left(1+r^{s s}\\right)\\left(1+\\pi^{s s}\\right) $ neutrality of money Notice that in (2.14)–(2.17) money appears only in the form of real money balances. Thus, any change in the nominal quantity of money that is matched by a proportional change in the price level, leaving unchanged, has no effect on the economy’s real equilibrium. Dividing (2.14) by $ u_{c}\\left(c^{s s}, m^{s s}\\right) $ yields $ 1-\\beta\\left[f_{k}\\left(k^{s s}\\right)+1-\\delta\\right]=0,0 r $ $ f_{k}\\left(k^{s S}\\right)=\\frac{1}{\\beta}-1+\\delta $ This equation defines the steady-state capital-labor ratio $ k^{s s} $ as a function of $ \\beta $ and $ \\delta $. If the production function is Cobb-Douglas, say $ f(k)=k^{\\alpha} $ for $ 0\u003c\\alpha \\leq 1, $ then $ f_{k}(k)=\\alpha k^{\\alpha-1} $ and $ k^{s s}=\\left[\\frac{\\alpha \\beta}{1+\\beta(\\delta-1)}\\right]^{1 /(1-\\alpha)} $ Because changes in the nominal quantity of money are engineered in this model by making lump-sum transfers to the public, the real value of these transfers must equal $ \\left(M_{t}-M_{t-1}\\right) / P_{t}=\\theta M_{t-1} / P_{t}=\\theta m_{t-1} /\\left(1+\\pi_{t}\\right) . $ Hence, steady-state transfers are given by $ \\tau^{s s}=\\theta m^{s s} /\\left(1+\\pi^{s s}\\right)=\\theta m^{s s} /(1+\\theta), $ and the budget constraint (2.17) reduces to the economy’s resource constraint $ c^{s s}=f\\left(k^{s s}\\right)-\\delta k^{s s} $ Assuming that $ f(k)=k^{\\alpha}, $ then $ k^{s s} $ is given by (2.20) and $$ c^{s s}=\\left[\\frac{\\alpha \\beta}{1+\\beta(\\delta-1)}\\right]^{\\alpha /(1-\\alpha)}-\\delta\\left[\\frac{\\alpha \\beta}{1+\\beta(\\delta-1)}\\right]^{1 /(1-\\alpha)} $$ superneutrality of money the steady-state values of the capital stock, consumption, and output are all independent of the rate of growth of the nominal money stock. Since the real rate of interest is equal to the marginal product of capital, it also is invariant across steady states that differ only in their rates of money growth. the Sidrauski MIU model possesses the properties of both neutrality and superneutrality. To understand why superneutrality holds, note that from $ (2.10), u_{c}=V_{\\omega}\\left(\\omega_{t}\\right), $ so using (2.6) $ u_{c}\\left(c_{t}, m_{t}\\right)=\\beta\\left[f_{k}\\left(k_{t}\\right)+1-\\delta\\right] u_{c}\\left(c_{t+1}, m_{t+1}\\right) $ or $ \\frac{u_{c}\\left(c_{t+1}, m_{t+1}\\right)}{u_{c}\\left(c_{t}, m_{t}\\right)}=\\frac{1 / \\beta}{f_{k}\\left(k_{t}\\right)+1-\\delta} $ Recall from (2.19) that the right side of this expression is equal to 1 in the steady stat","date":"2021-03-03","objectID":"/2.moneyinutility/:2:1","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.2.2 Steady States with a Time-Varying Money Stock when the focus is on the relationship between money and prices, one might be more interested in a steady state in which real quantities such as consumption and the capital stock are constant but the growth rate of money varies over time. Assume that all t. Setting population growth n to zero and using (2.10), the equilibrium conditions (2.6) and (2.7) can be written as: $ u_{c}\\left(c^{*}, m_{t}\\right)=\\beta\\left[f_{k}\\left(k^{*}\\right)+1-\\delta\\right] u_{c}\\left(c^{*}, m_{t+1}\\right) $ $ \\frac{1+i_{t}}{\\left(1+\\pi_{t+1}\\right)}=\\left[f_{k}\\left(k^{*}\\right)+1-\\delta\\right] $ and (2.12) implies $ \\frac{u_{m}\\left(c^{*}, m_{t}\\right)}{u_{c}\\left(c^{*}, m_{t}\\right)}=\\frac{i_{t}}{1+i_{t}} $ The budget constraint becomes $ c^{*}=f\\left(k^{*}\\right)-\\delta k^{*} $ and the evolution of the real stock of money is given by $ m_{t}=\\left(\\frac{1+\\theta_{t}}{1+\\pi_{t}}\\right) m_{t-1} $ If $ \\theta $ is constant, one has the situation previously studied. There is a steady state with inflation equal to the rate of growth of money $ (\\pi=\\theta) $, and real money balances are constant. With $ m $ constant, (2.24) uniquely determines the capital stock such that $ \\beta\\left[f_{k}\\left(k^{s s}\\right)+1-\\delta\\right]=1 $. The economy’s resource constraint then determines $ c^{*} $. There may also be steady-state equilibria in which is changing over time. Reis (2007) investigated how monetary policies that allow the monetary stock to be time-varying can alter the steady-state values of consumption and capital. consider (2.24) for, then: $ \\frac{u_{c}\\left(c^{*}, m_{t+1}\\right)}{u_{c}\\left(c^{*}, m_{t}\\right)}=\\frac{1}{\\beta\\left[f_{k}\\left(k^{*}\\right)+1-\\delta\\right]}\u003e1 $ a steady state that satisfies (2.28) may not be feasible. If, following Fischer $ (1979 \\mathrm{~b}) $, the utility function takes the form $ u(c, m)=\\frac{\\left(c^{1-\\gamma} m^{\\prime}\\right)^{1-\\eta}}{1-\\eta} $ with $ \\eta\u003c1 $ and $ \\gamma \\in(0,1) $, then (2.28) requires that real money balances evolve according to $ \\left(\\frac{m_{t+1}}{m_{t}}\\right)=\\left{\\frac{1}{\\beta\\left[f_{k}\\left(k^{*}\\right)+1-\\delta\\right]}\\right}^{1 /(1-\\gamma(1-\\eta))} . $ Rather than characterize the steady state in terms of the growth rate of the nominal stock of money, Reis (2007) examined the behavior of the nominal interest rate directly, since central banks today generally employ a nominal interest rate and not a nominal quantity as their policy instrument. The equilibrium condition (2.26) implicitly defines a money demand function of the form $ m_{t}=\\phi\\left(i_{t}, c^{*}\\right) $ so (2.29) implies the path of the nominal rate must satisfy $ \\frac{\\phi\\left(i_{t+1}, c^{*}\\right)}{\\phi\\left(i_{t}, c^{*}\\right)}=\\left{\\frac{1}{\\beta\\left[f_{k}\\left(k^{*}\\right)+1-\\delta\\right]}\\right}^{1 /(1-\\gamma(1-\\eta))} $ Returning to $ (2.12), $ this equation characterizes the demand for real money balances as a function of the nominal rate of interest and real consumption. For example, suppose that the utility function in consumption and real balances is of the constant elasticity of substitution (CES) form: $ u\\left(c_{t}, m_{t}\\right)=\\left[a c_{t}^{1-b}+(1-a) m_{t}^{1-b}\\right]^{1 /(1-b)} $ with $ 0\u003ca\u003c1 $ and $ b\u003e0, b \\neq 1 $. Then $ \\frac{u_{m}}{u_{c}}=\\left(\\frac{1-a}{a}\\right)\\left(\\frac{c_{t}}{m_{t}}\\right)^{b} $ and (2.12) can be written $ \\mathrm{as}^{21} $ $ m_{t}=\\left(\\frac{1-a}{a}\\right)^{1 / b}\\left(\\frac{i}{1+i}\\right)^{-1 / b} c_{t} $ In terms of the more common log specification used to model empirical money demand equations, $ \\log \\frac{M_{t}}{P_{t} N_{t}}=\\frac{1}{b} \\log \\left(\\frac{1-a}{a}\\right)+\\log c-\\frac{1}{b} \\log \\frac{i}{1+i}, $ The interest elasticity of money demand The elasticity of money demand with respect to the opportunity cost variable . For simplicity, this will often be referred to as the interest elasticity of money demand. While the parameter b governs the interest elasticity of demand, the steady-state","date":"2021-03-03","objectID":"/2.moneyinutility/:2:2","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.2.4 Limitations In the MIU model, there is a clearly defined reason for individuals to hold money—it provides utility. However, this essentially solves the problem of generating a positive demand for money by assumption; it doesn’t address the reasons that money, particularly money in the form of unbacked pieces of paper, might yield utility. ","date":"2021-03-03","objectID":"/2.moneyinutility/:2:3","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.3 The Welfare Cost of Inflation Because money holdings yield direct utility and higher inflation reduces real money balances, inflation generates a welfare loss. This raises two questions: How large is the welfare cost of inflation? Is there an optimal rate of inflation that maximizes the steady-state welfare of the representative household? The second question—the optimal rate of inflation—was originally addressed by Bailey (1956) and M. Friedman (1969).The optimal rate of inflation is a rate of deflation approximately equal to the real return on capital. The major criticism of this result is due to Phelps (1973), who pointed out that money growth generates revenue for the government—the inflation tax. Now let’s return to the first question—what is the welfare cost of inflation? Beginning with Bailey (1956), this welfare cost has been calculated from the area under the money demand curve (showing money demand as a function of the nominal rate of interest) because this provides a measure of the consumer surplus lost as a result of having a positive nominal rate of interest. Figure 2.2 real money balances \r Welfare costs of inflation as measured by the area under the demanded curve. Nominal interest rates reflect expected inflation, so calculating the area under the money demand curve provides a measure of the costs of anticipated inflation and is therefore appropriate for evaluating the costs of alternative constant rates of inflation. There are other costs of inflation associated with tax distortions and with variability in the rate of inflation; these are discussed in the survey on the costs of inflation by Driffill, Mizon, and Ulph (1990); relative price distortions generated by inflation when prices are sticky are discussed in chapter 8 . Lucas (1994) provided estimates of the welfare costs of inflation by starting with the following specification of the instantaneous utility function: $ u(c, m)=\\frac{1}{1-\\sigma}\\left{\\left[c \\varphi\\left(\\frac{m}{c}\\right)\\right]^{1-\\sigma}-1\\right} $ With this utility function, (2.12) becomes $ \\frac{u_{m}}{u_{c}}=\\frac{\\varphi^{\\prime}(x)}{\\varphi(x)-x \\varphi^{\\prime}(x)}=\\frac{i}{1+i}=\\Upsilon $ Lucas proposed to measure the costs of inflation by the percentage increase in steady-state consumption necessary to make the household indifferent between a nominal interest rate of i and a nominal rate of 0. If this cost is denoted , it is defined by: $ u(1+w(\\Upsilon), m(\\Upsilon)) \\equiv u\\left(1, m^{*}\\right) $ where denotes the solution of (2.34) for real money balances evaluated at steady-state consumption . Lucas (2000) calculated the welfare costs of inflation for two alternative specifications of money demand. The first takes the form $ \\ln (m)=\\ln (A)-\\eta \\ln (i) $ the second takes the form $ \\ln (m)=\\ln (B)-\\xi i $ ","date":"2021-03-03","objectID":"/2.moneyinutility/:3:0","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.4 Extensions ","date":"2021-03-03","objectID":"/2.moneyinutility/:4:0","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.4.1 Interest on Money If the welfare costs of inflation are related to the positive private opportunity costs of holding money, paying explicit interest on money would be an alternative to deflation as a means of eliminating these costs. There are obvious technical difficulties in paying interest on cash, but ignoring these, assume that the government pays a nominal interest rate of $ i^{m} $ on money balances. Assume further that these interest payments are financed by lump-sum taxes $ s $. The household’s budget constraint, (2.4) , now becomes (setting $ n=0) $ $$ f\\left(k_{t-1}\\right)-s_{t}+\\tau_{t}+(1-\\delta) k_{t-1}+\\left(1+r_{t-1}\\right) b_{t-1}+\\frac{1+i_{t}^{m}}{1+\\pi_{t}} m_{t-1}=c_{t}+k_{t}+m_{t}+b_{t} $$ and the first-order condition (2.8) becomes $ -u_{c}\\left(c_{t}, m_{t}\\right)+u_{m}\\left(c_{t}, m_{t}\\right)+\\frac{\\beta\\left(1+i_{t}^{m}\\right) V_{\\omega}\\left(\\omega_{t+1}\\right)}{\\left(1+\\pi_{t+1}\\right)}=0 $ whereas (2.12) is now $ \\frac{u_{m}\\left(c_{t}, m_{t}\\right)}{u_{c}\\left(c_{t}, m_{t}\\right)}=\\frac{i_{t}-i_{t}^{m}}{1+i_{t}} $ the optimal quantity of money can be achieved as long as , regardless of the rate of inflation. If , so that the rate of inflation in the steady state is also zero, the optimal quantity of money is obtained with a positive nominal interest rate as long as . 2.4.2 Nonsuperneutrality suppose utility depends on consumption, real money holdings, and leisure: The economy’s production function becomes where n is employment. If the total supply of time is normalized to equal 1, then n=1-l. The additional first-order condition implied by the optimal choice of leisure is: $ \\frac{u_{l}(c, m, l)}{u_{c}(c, m, l)}=f_{n}(k, 1-l) $ Now, both steady-state labor supply and consumption may be affected by variations in the rate of inflation. Specifically, an increase in the rate of inflation reduces holdings of real money balances. If this affects the marginal utility of leisure, then (2.43) implies the supply of labor will be affected, leading to a change in the steady-state per capita stock of capital, output, and consumption. Equation (2.43) suggests that if $ u_{l} / u_{c} $ were independent of $ m, $ then superneutrality would hold. This is the case because the steady-state values of $ k, c, $ and $ l $ could then be found from $ \\frac{u_{l}}{u_{c}}=f_{n}\\left(k^{s s}, 1-l^{s s}\\right), $ $ f_{k}\\left(k^{s s}, 1-l^{s s}\\right)=\\frac{1}{\\beta}-1+\\delta $ and $ c^{s s}=f\\left(k^{s s}, 1-l^{s s}\\right)+\\delta k^{s s} . $ ","date":"2021-03-03","objectID":"/2.moneyinutility/:4:1","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.5 Dynamics in an MIU Model The analysis of the MIU approach has, up to this point, focused on steady-state properties. It is also important to understand the model’s implications for the dynamic behavior of the economy as it adjusts to exogenous disturbances. One way to study the model’s dynamics is to employ numerical methods to carry out simulations using the model. ","date":"2021-03-03","objectID":"/2.moneyinutility/:5:0","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.5.1 The Decision Problem When the household chooses how much labor to supply, current income is no longer predetermined from the perspective of the household’s choices of money, bonds, and capital investment. Consequently, income (output) cannot be part of the state vector for period t. Instead, let $ a_{t}=\\tau_{t}+\\left[\\left(1+i_{t-1}\\right) /\\left(1+\\pi_{t}\\right)\\right] b_{t-1}+\\left[1 /\\left(1+\\pi_{t}\\right)\\right] m_{t-1} $ be the household’s real financial wealth plus transfer at the start of period t. output per household is given by: $$ y_{t}=f\\left(k_{t-1}, n_{t}, z_{t}\\right) $$ The household’s decision problem is defined by $$ V\\left(a_{t}, k_{t-1}\\right)=\\max \\left{u\\left(c_{t}, m_{t}, 1-n_{t}\\right)+\\beta \\mathrm{E}_{t} V\\left(a_{t+1}, k_{t}\\right)\\right}, $$ where the maximization is over $ \\left(c_{t}, m_{t}, b_{t}, k_{t}, n_{t}\\right) $ and is subject to $$ \\begin{array}{l} f\\left(k_{t-1}, n_{t}, z_{t}\\right)+(1-\\delta) k_{t-1}+a_{t} \\geq c_{t}+k_{t}+b_{t}+m_{t} \\ a_{t+1}=\\tau_{t+1}+\\left(\\frac{1+i_{t}}{1+\\pi_{t+1}}\\right) b_{t}+\\frac{m_{t}}{1+\\pi_{t+1}} \\end{array} $$ Equation (2.45) will always hold with equality (as long as $ \\left.u_{c}\u003e0\\right) $; it can be used to eliminate $ k_{t} $, and (2.46) can be used to substitute for $ a_{t+1} $, allowing the value function to be rewritten as $$ \\begin{array}{c} V\\left(a_{t}, k_{t-1}\\right)=\\max _{c_{t}, n_{t}, b_{t}, m_{t}}\\left{u\\left(c_{t}, m_{t}, 1-n_{t}\\right)+\\beta \\mathrm{E}_{t} V\\left(\\tau_{t+1}+\\left(\\frac{1+i_{t}}{1+\\pi_{t+1}}\\right) b_{t}+\\frac{m_{t}}{1+\\pi_{t+1}}\\right.\\right. \\ \\left.\\left.f\\left(k_{t-1}, n_{t}, z_{t}\\right)+(1-\\delta) k_{t-1}+a_{t}-c_{t}-b_{t}-m_{t}\\right)\\right} \\end{array} $$ where this is now an unconstrained maximization problem. The first-order necessary conditions with respect to $ c_{t}, n_{t}, b_{t}, $ and $ m_{t} $ are $ u_{c}\\left(c_{t}, m_{t}, 1-n_{t}\\right)-\\beta \\mathrm{E}_{t} V_{k}\\left(a_{t+1}, k_{t}\\right)=0 $ $ -u_{l}\\left(c_{t}, m_{t}, 1-n_{t}\\right)+\\beta \\mathrm{E}_{t} V_{k}\\left(a_{t+1}, k_{t}\\right) f_{n}\\left(k_{t-1}, n_{t}, z_{t}\\right)=0 $ $ \\beta \\mathrm{E}_{t}\\left(\\frac{1+i_{t}}{1+\\pi_{t+1}}\\right) V_{a}\\left(a_{t+1}, k_{t}\\right)-\\beta \\mathrm{E}_{t} V_{k}\\left(a_{t+1}, k_{t}\\right)=0 $ $ u_{m}\\left(c_{t}, m_{t}, 1-n_{t}\\right)+\\beta \\mathrm{E}_{t}\\left[\\frac{V_{a}\\left(a_{t+1}, k_{t}\\right)}{1+\\pi_{t+1}}\\right]-\\beta \\mathrm{E}_{t} V_{k}\\left(a_{t+1}, k_{t}\\right)=0 $ and the envelope theorem yields $ V_{a}\\left(a_{t}, k_{t-1}\\right)=\\beta \\mathrm{E}_{t} V_{k}\\left(a_{t+1}, k_{t}\\right) $ $ V_{k}\\left(a_{t}, k_{t-1}\\right)=\\beta \\mathrm{E}_{t} V_{k}\\left(a_{t+1}, k_{t}\\right)\\left[1-\\delta+f_{k}\\left(k_{t-1}, n_{t}, z_{t}\\right)\\right] $ Updating (2.52) one period and using $ (2.51), $ one obtains $ V_{k}\\left(a_{t+1}, k_{t}\\right)=\\mathrm{E}_{t}\\left{\\left[1-\\delta+f_{k}\\left(k_{t}, n_{t+1}, z_{t+1}\\right)\\right] V_{a}\\left(a_{t+1}, k_{t}\\right)\\right} $ Now substituting this for $ V_{k}\\left(a_{t+1}, k_{t}\\right) $ in (2.47) yields $ u_{c}\\left(c_{t}, m_{t}, 1-n_{t}\\right)-\\beta \\mathrm{E}_{t}\\left{\\left[1-\\delta+f_{k}\\left(k_{t}, n_{t+1}, z_{t+1}\\right)\\right] V_{a}\\left(a_{t+1}, k_{t}\\right)\\right}=0 $ When it is recognized that $ u_{c}\\left(c_{t}, m_{t}, 1-n_{t}\\right)=\\beta \\mathrm{E}_{t} V_{k}\\left(a_{t+1}, k_{t}\\right),(2.50),(2.53), $ and (2.51) take the same form as (2.8),(2.6) , and $ (2.10), $ the first-order conditions for the basic Sidrauski model, which did not include a labor-leisure choice. The only new condition is (2.48) , which can be written, using (2.47) , as $ \\frac{u_{l}\\left(c_{t}, m_{t}, 1-n_{t}\\right)}{u_{c}\\left(c_{t}, m_{t}, 1-n_{t}\\right)}=f_{n}\\left(k_{t-1}, n_{t}, z_{t}\\right) $ The equilibrium values of consumption, capital, money holdings, and labor supply must satisfy the conditions given in $ (2.47)-(2.51) . $ These conditions can be simplified, however. Note that $ (2.47),(2.49), $ and (2.51) imply that $$ u_{c}\\left(c_{t}, m_{t}, 1-n_{t}\\right)=\\beta\\left(1+i_{t}","date":"2021-03-03","objectID":"/2.moneyinutility/:5:1","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.5.2 The Steady State Suppose the utility function is separable in money so that neither the marginal utility of leisure nor the marginal utility of consumption depend on the household’s holdings of real money balances. Then (2.62) becomes: $ \\frac{u_{l}\\left(\\bar{\\phi} n^{s s}, 1-n^{s s}\\right)}{u_{c}\\left(\\bar{\\phi} n^{s s}, 1-n^{s s}\\right)}=\\phi\\left(\\frac{k^{s s}}{n^{s s}}\\right)-\\left(\\frac{k^{s s}}{n^{s s}}\\right) \\phi^{\\prime}\\left(\\frac{k^{s s}}{n^{s s}}\\right) $ If utility is nonseparable, so that either $ u_{l} $ or $ u_{c} $ (or both) depend on $ m^{s s} $, then money is not superneutral. Variations in average inflation that affect the opportunity cost of holding money will affect $ m^{s s} $. Different levels of $ m^{s s} $ will change the value of $ n^{s s} $ that satisfies (2.62) . Since $ 1+i^{$ s}=\\left(1+r^{s s}\\right)\\left(1+\\pi^{s s}\\right)=\\beta^{-1}\\left(1+\\theta^{s s}\\right) $, equation (2.54) can be rewritten as $ \\frac{u_{m}\\left(\\bar{\\phi} n^{s s}, m^{s s}, 1-n^{s s}\\right)}{u_{c}\\left(\\bar{\\phi} n^{s s}, m^{s s}, 1-n^{s s}\\right)}=\\left(\\frac{i^{s s}}{1+i^{s S}}\\right)=\\frac{1+\\theta^{s s}-\\beta}{1+\\theta^{s s}} . $ ","date":"2021-03-03","objectID":"/2.moneyinutility/:5:2","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.5.3 The Linear Approximation To further explore the effects of money outside the steady state, it is useful to approximate the model’s equilibrium conditions around the steady state. Percentage deviations of a variable qt around its steady-state value will be denoted by ,where . $ q_{t} \\equiv q^{s s}\\left(1+\\hat{q}_{t}\\right) $ As is standard, the production function is taken to be Cobb-Douglas with constant returns to scale, so $ y_{t}=e^{z_{t}} k_{t-1}^{\\alpha} n_{t}^{1-\\alpha} $ with $ 0\u003c\\alpha\u003c1 $. For the utility function, it is assumed that $ u\\left(c_{t}, m_{t}, 1-n_{t}\\right)=\\frac{\\left[a c_{t}^{1-b}+(1-a) m_{t}^{1-b}\\right]^{(1-\\Phi) /(1-b)}}{1-\\Phi}+\\Psi \\frac{\\left(1-n_{t}\\right)^{1-\\eta}}{1-\\eta} . $ To this system of eight endogenous variables, it will be convenient to add investment, $ x_{t}, $ given by $ x_{t}=k_{t}-(1-\\delta) k_{t-1}, $ and to define $ \\lambda_{t} $ as the marginal utility of consumption. The linearized expression for $ \\hat{\\lambda}_{t} $ is $ \\hat{\\lambda}_{t}=\\Omega_{1} \\hat{c}_{t}+\\Omega_{2} \\hat{m}_{t} $ where $ \\Omega_{1}=[(b-\\Phi) \\gamma-b], \\Omega_{2}=(b-\\Phi)(1-\\gamma), $ and the parameter $ \\gamma $ is equal to $ a\\left(c^{s s}\\right)^{1-b} /\\left[a\\left(c^{s s}\\right)^{1-b}+(1-a)\\left(m^{s s}\\right)^{1-b}\\right] $ Then, in linearized form, the equilibrium conditions include (2.65) and (see the chapter appendix): $ \\left(\\frac{x^{s s}}{k^{s s}}\\right) \\hat{x}_{t}=\\hat{k}_{t}-(1-\\delta) \\hat{k}_{t-1} $ $ \\hat{y}_{t}=\\alpha \\hat{k}_{t-1}+(1-\\alpha) \\hat{n}_{t}+z_{t} $ $ \\left(\\frac{y^{s s}}{k^{s s}}\\right) \\hat{y}_{t}=\\left(\\frac{c^{s s}}{k^{s s}}\\right) \\hat{c}_{t}+\\delta \\hat{x}_{t} $ $ \\hat{r}{t}=\\alpha\\left(\\frac{y^{s s}}{k^{s s}}\\right)\\left(\\mathrm{E}{t} \\hat{y}{t+1}-\\hat{k}{t}\\right) $ $ \\hat{\\lambda}{t}=\\mathrm{E}{t} \\hat{\\lambda}{t+1}+\\hat{r}{t} $ $ -\\hat{\\lambda}{t}+\\eta\\left(\\frac{n^{s s}}{1-n^{s s}}\\right) \\hat{n}{t}=\\hat{y}{t}-\\hat{n}{t} $ $ \\hat{\\imath}{t}=\\hat{r}{t}+\\mathrm{E}{t} \\hat{\\pi}{t+1} $ $ \\hat{m}{t}-\\hat{c}{t}=-\\left(\\frac{1}{b}\\right)\\left(\\frac{1}{i^{s s}}\\right) \\hat{\\imath}{t} $ $ \\hat{m}{t}=\\hat{m}{t-1}-\\hat{\\pi}{t}+u_{t} $ Consistent with the real-business-cycle literature, a stochastic disturbance to total factor productivity is incorporated that follows an AR(1) process: $ z_{t}=\\rho_{z} z_{t-1}+e_{t} $ Assume that $ e_{t} $ is a serially uncorrelated mean zero process and $ \\left|\\rho_{z}\\right|\u003c1 . $ Note the timing convention in (2.67): the capital carried over from period $ t-1, K_{t-1}, $ is available for use in producing output during period $ t $. It is also necessary to specify the process followed by the nominal stock of money. In previous sections, $ \\theta $ denoted the growth rate of the nominal money supply. Assume then that the average growth rate is $ \\theta^{s s}, $ and let $ u_{t} \\equiv \\theta_{t}-\\theta^{s s} $ be the deviation in period $ t $ of the growth rate from its unconditional average value. This deviation will be treated as a stochastic process given by $ u_{t}=\\rho_{u} u_{t-1}+\\phi z_{t-1}+\\varphi_{t}, \\quad 0 \\leq \\gamma\u003c1 $ where $ \\varphi_{t} $ is a white noise process and $ \\left|\\rho_{u}\\right|\u003c1 $. This formulation allows the growth rate of the money stock to display persistence $ \\left(\\right. $ if $ \\left.\\rho_{u}\u003e0\\right) $, respond to the real productivity shock $ z, $ and be subject to random disturbances through the realizations of $ \\varphi_{l} $ Separability allows the real equilibrium to be solved independent of money and inflation, but it has more commonly been used in monetary economics to allow the study of inflation and money growth to be conducted independent of the real equilibrium. When $ \\Phi=b,(2.73) $ and (2.74) constitute a two-equations system in inflation and real money balances, with $ u $ representing an exogenous random disturbance and $ \\hat{c} $ and $ \\hat{r} $ determined by $ (2.67)-(2.71) $ and exogenous to the determination of inflation and real money balances. Equation (2.73) can then ","date":"2021-03-03","objectID":"/2.moneyinutility/:5:3","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.5.4 Calibration ","date":"2021-03-03","objectID":"/2.moneyinutility/:5:4","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.5.5 Simulation Results \r \r \r ","date":"2021-03-03","objectID":"/2.moneyinutility/:5:5","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"2.6 Summary Assuming that holdings of real money balances yield direct utility is a means of ensuring a positive demand for money so that, in equilibrium, money is held and has value. This assumption is clearly a shortcut; it does not address the issue of why money yields utility or why certain pieces of paper that we call money yield utility but other pieces of paper presumably do not. The Sidrauski model, because it assumes that agents act systematically to maximize utility, allows welfare comparisons to be made. The model can be used to assess the welfare costs of inflation and to determine the optimal rate of inflation. Friedman’s conclusion that the optimal inflation rate is the rate that produces a zero nominal rate of interest is quite robust (see chapter 4). Finally, by developing a linear approximation to the basic money-in-the-utility function model (augmented to include a labor supply choice), it was shown how the effects of variations in the growth rate of the money supply on the short-run dynamic adjustment of the economy depended on the effect of money holdings on the marginal utility of consumption and leisure. ","date":"2021-03-03","objectID":"/2.moneyinutility/:6:0","tags":["Notes"],"title":"2. Money-in-the-Utility Function","uri":"/2.moneyinutility/"},{"categories":["Monetary Finance"],"content":"The course mainly teaches and discusses monetary and financial related theories, including monetary demand and monetary supply theory, the empirical evidence on money, price, output and interest rate; monetary and public finance, financial development and economic growth, developing countries' financial repression and financial deepening, the credit creation of the shadow bank, asset substitution and financial crisis, new monetary policy tool and monetary policy operation mode, financial risk and financial supervision, asset price bubble, financial crisis and macro-prudential policy, analysis of the cause of Chinese stock market bubble, and other more related theories and research. Monetary economics investigates the relationship between real economic variables at the aggregate level (such as real output, real rates of interest, employment, and real exchange rates) and nominal variables (such as the inflation rate, nominal interest rates, nominal exchange rates, and the supply of money). Monetary economics has considerable overlap with macroeconomics more generally, and these two fields have to a large degree shared a common history over most of the past 50 years. During the 1970s the monetarist/Keynesian debates led to a reintegration of monetary economics with macroeconomics. The seminal work of Robert Lucas (1972) provided theoretical foundations for models of economic fluctuations in which money was the fundamental driving factor behind movements in real output. The rise of real-business-cycle models during the 1980s and early 1990s, building on the contribution of Kydland and Prescott (1982) and focusing explicitly on nonmonetary factors as the driving forces behind business cycles, tended to separate monetary economics from macroeconomics. ","date":"2021-03-03","objectID":"/1.monetaryfinanceintroduction/:0:0","tags":["Notes"],"title":"1.MonetaryFinanceIntroduction","uri":"/1.monetaryfinanceintroduction/"},{"categories":[],"content":"Data Indexing / selection pandas.DataFrame.loc The basics of indexing are as follows: Operation Syntax Result Select column df[col] Series Select row by label df.loc[label] Series Select row by integer location df.iloc[loc] Series Slice rows df[5:10] DataFrame Select rows by boolean vector df[bool_vec] DataFrame Convert float into integer pandas.to_numeric pandas.DataFrame.astype pandas.DataFrame.round Bug solution: Pandas: ValueError: cannot convert float NaN to integer Merge data Merge, join, concatenate and compare pandas.DataFrame.merge Duplicated value pandas.DataFrame.duplicated: judge whether duplicated E.g. data[data.duplicated(subset = [\"Id\"], keep = False)] pandas.DataFrame.drop_duplicates Missing data pandas.DataFrame.dropna ","date":"2021-02-28","objectID":"/pandas/:0:0","tags":["Python","Coding"],"title":"Pandas Notes","uri":"/pandas/"},{"categories":[],"content":" ├─biomarker_09 │ biomarker_09.sas7bdat │ ├─Master_Agriculture_201804 │ cropt_12.sas7bdat │ farmg_12.sas7bdat │ farmh_12.sas7bdat │ fishh_12.sas7bdat │ fishi_12.sas7bdat │ foods_12.sas7bdat │ gardh_12.sas7bdat │ ├─Master_Asset_201804 │ asset_12.sas7bdat │ ├─Master_Business_201804 │ busi_12.sas7bdat │ busn_12.sas7bdat │ ├─Master_Caltrac_201410 │ en_00.sas7bdat: Individual Energy/CALTRAC File 1997-2000 │ ├─Master_Childcare_201804 │ carec_12.sas7bdat: Individual Child Care File 1989-2015 │ careh_12.sas7bdat: Household Child Care 1991-2006 │ ├─Master_Constructed_Income_201804: summary of different kind of income │ hhinc_10.sas7bdat │ indinc_10.sas7bdat │ ├─Master_Educ_201804 │ educ_12.sas7bdat: Individual Education File 1989-2015 │ ├─Master_EverMarriedWomen_201804 │ birthmast_pub_12.sas7bdat: Birth History Individual ID File through 2015 │ birth_12.sas7bdat: Birth History File 1991-1993, 2000-2015 │ emw_12.sas7bdat: Ever Married Women File 1991-2015 │ preg_12.sas7bdat: Pregnancy History File 1991-2015 │ wed_12.sas7bdat: Marriage History File 1991-2015 │ ├─Master_HealthCare_201804 │ hlth_12.sas7bdat: Individual Health Care File 1989-2015 │ ins_12.sas7bdat: Individual Medical Insurance File 1989-2015 │ ├─Master_ID_201908 │ dataset relationship.pdf │ mast_pub_12.sas7bdat: Marriage History File (wed_12) 1991-2015 │ rst_12.sas7bdat: Roster File 1989-2015 │ surveys_pub_12.sas7bdat: Survey File 1989-2015 │ ├─Master_Income_Categories_201804 │ jobs_12.sas7bdat │ oinc_12.sas7bdat │ subf_12.sas7bdat │ subh_12.sas7bdat │ subi_12.sas7bdat │ wages_12.sas7bdat │ ├─Master_InfantFeeding_201410 │ infed_00.sas7bdat: Infant Feeding File 1989-1993 │ infnt_00.sas7bdat: Infant Feeding Survey 1989 │ ├─Master_Livestock_201804 │ liveh_12.sas7bdat: Household Livestock File 1991-2015 │ livei_12.sas7bdat: Individual Livestock Income File 1989-2015 │ livet_12.sas7bdat: Household Livestock Type 1989-2015 │ ├─Master_Macronutrients_201410 │ c12diet.sas7bdat: Household Food Inventory 1989-2011 │ ├─Master_Media_201410 │ media_00.sas7bdat: EMW MASS MEDIA File 2000-2011 │ medsv_00.sas7bdat: Household Medical Facilities 1989-2006 │ ├─Master_PE_PA_201908 │ pact_12.sas7bdat: Physical Activity 1989-2015 │ pexam_pub_12.sas7bdat: Physical Examination 1989-2015 │ pstress_12.sas7bdat: Physical Examination 2015-2015 │ ├─Master_Relationship_201410 │ relationmast_pub_00.sas7bdat: Mast Relationship File (rst_00) 1989-2011 │ ├─Master_TimeUse_201804 │ timea_12.sas7bdat │ └─Master_UrbanIndex_201804 urban_11.sas7bdat Master_ID_201908/surveys_pub_12存放了最基本的个人数据，我们以此为基本的个体标识。 ","date":"2021-02-23","objectID":"/chnstutorials/:0:0","tags":[],"title":"CHNS Tutorials","uri":"/chnstutorials/"},{"categories":[],"content":"Household ID (HHID) HHID is a nine-digit numeric variable that uniquely identified each household that had been seven digits in the old cross-sectional files. The variables T1 through T5 (documented in the questionnaire) were concatenated to form the HHID. Each HHID value represented one household. When the unit of analysis for a file/table was household, 1 HHID = 1 row = 1 household = 1 observation. Observations in these files were sorted by HHID and survey year (Wave). Thus the key sort variables for these files were HHID and Wave. ","date":"2021-02-23","objectID":"/chnstutorials/:0:1","tags":[],"title":"CHNS Tutorials","uri":"/chnstutorials/"},{"categories":[],"content":"Individual ID (IDind) The IDind is a twelve-digit numeric variable that uniquely identified for all participants. Each participant will have the same ID in all datasets and in all survey years. The unique ID will not change over time and will facilitate data merges across datasets and survey years. Each IDind value represented one individual. When the unit of analysis for a file/table was individual, 1 IDind = 1 row = 1 individual = 1 observation. Observations in these files are sorted by IDind, and Wave, i.e., the key sort variables were IDind, and Wave. ","date":"2021-02-23","objectID":"/chnstutorials/:0:2","tags":[],"title":"CHNS Tutorials","uri":"/chnstutorials/"},{"categories":[],"content":"Community ID (COMMID) A third ID variable, COMMID, is a six-digit numeric variable that uniquely identified each community. The variables T1 through T4 were concatenated to create COMMID. Each COMMID value represented one community. When the unit of analysis for a file was community, 1 COMMID = 1 community = 1 observation. Observations in these files were sorted by COMMID and Wave, i.e., the key sort variables were COMMID and Wave. Although COMMID was not required for most file merges, this variable was included on all data sets to facilitate merges with community-level files. When the unit of analysis was something other than individual, household or community (e.g., job, livestock type, food item, health facility), a variable that identified this unit was included on the file (e.g., JOB, F11, FOODCODE, Q1). For the files/tables where job was the unit of analysis, for example, each value of the variable JOB represented one occupation. That is, 1 JOB = 1 row = 1 occupation = 1 observation. Observations in these files were sorted by HHID, LINE, and JOB, i.e., the key sort variables were HHID, LINE, and JOB. ","date":"2021-02-23","objectID":"/chnstutorials/:0:3","tags":[],"title":"CHNS Tutorials","uri":"/chnstutorials/"},{"categories":[],"content":"Province ID Province Frequency Percent 11 Beijing, Added 2011 2607 1.82 21 Liaoning, Missed 1997 12327 8.59 23 Heilongjiang, Added 1997 9012 6.28 31 Shanghai, Added 2011 2860 1.99 32 Jiangsu 14823 10.33 37 Shandong 14474 10.08 41 Henan 16982 11.83 42 Hubei 15906 11.08 43 Hunan 15449 10.76 45 Guangxi 19030 13.26 52 Guizhou 17311 12.06 55 Chongqing, Added 2011 2783 1.94 ","date":"2021-02-23","objectID":"/chnstutorials/:0:4","tags":[],"title":"CHNS Tutorials","uri":"/chnstutorials/"},{"categories":[],"content":"NATIONALITY ID NATIONALITY Frequency Percent -9 Unknown 7 0.02 . Missing 1853 4.17 1 Han 37853 85.15 2 Mongolian 36 0.08 3 Hui 99 0.22 4 Tibetian 2 0 5 Vaguer 3 0.01 6 Miao 1116 2.51 7 Yi 3 0.01 8 Zhuang 310 0.7 9 Buyi 854 1.92 10 Korean 44 0.1 11 Man 890 2 12 Dong 14 0.03 Education ID Education Frequency Percent . Missing 6185 4.61 0 None 33411 24.89 1 Grad from primary 28137 20.96 2 Lower middle school degree 38815 28.91 3 Upper middle school degree 14935 11.12 4 Technical or vocational degree 6234 4.64 5 University or college degree 6195 4.61 6 Master’s degree or higher 192 0.14 9 Unknown 152 0.11 MARITAL STATUS ID MARITAL STATUS Frequency Percent . Missing 49183 27.24 OTHER Unknown or Invalid Response 275 0.15 1 Never married 34895 19.32 2 Married 87597 48.51 3 Divorced 1015 0.56 4 Widowed 7269 4.03 5 Separated 348 0.19 DOCTOR’S DIAGNOSIS OF ILLENESS/INJURY ID Disease Frequency Percent -9 Unknown 596 0.47 -32 23 464 0.36 . Missing 115370 90.3 0 No diagnosis 913 0.71 1 Infectious/parasitic disease 162 0.13 2 Heart disease 678 0.53 3 Tumor 95 0.07 4 Respiratory disease 3555 2.78 5 Injury 257 0.2 6 Alcohol poisoning 10 0.01 7 Endocrine disorder 213 0.17 8 Hematological disease 152 0.12 9 Mental/psychiatric disorder 95 0.07 10 Mental retardation 9 0.01 11 Neurological disorder 255 0.2 12 Eye/ear/nose/throat/teeth disease 360 0.28 13 Digestive disease 1199 0.94 14 Urinary disease 222 0.17 15 Sexual dysfunction 3 0 16 Obstetrical/gynecological disease 133 0.1 17 Neonatal disease 44 0.03 18 Dermatological disease 246 0.19 19 Muscular/rheumatological disease 489 0.38 20 Genetic disease 65 0.05 21 Old age/mid-life syndrome 361 0.28 22 Other 1815 1.42 ID Disease -9 Unknown -32 23 . Missing 0 No diagnosis 1 Infectious/parasitic disease 2 Heart disease 3 Tumor 4 Respiratory disease 5 Injury 6 Alcohol poisoning 7 Endocrine disorder 8 Hematological disease 9 Mental/psychiatric disorder 10 Mental retardation 11 Neurological disorder 12 Eye/ear/nose/throat/teeth disease 13 Digestive disease 14 Urinary disease 15 Sexual dysfunction 16 Obstetrical/gynecological disease 17 Neonatal disease 18 Dermatological disease 19 Muscular/rheumatological disease 20 Genetic disease 21 Old age/mid-life syndrome 22 Other CURRENT HEALTH STATUS (SELF-REPORT) ID Type Frequency Percent . Missing 52644 45.7 1 Excellent 8815 7.65 2 Good 30217 26.23 3 Fair 19463 16.89 4 Poor 3604 3.13 5 Very Poor 157 0.14 9 Unknown 301 0.26 ","date":"2021-02-23","objectID":"/chnstutorials/:0:5","tags":[],"title":"CHNS Tutorials","uri":"/chnstutorials/"},{"categories":[],"content":"Biomarker Data The biomarker data collected in CHNS 2009 involves the release of 26 fasting blood measures on individuals aged 7 and older. This included major cardiovascular biomarkers (lipids, diabetes such as HbA1c, glucose, insulin, triglycerides, CRP) and important nutrition biomarkers (transferrin, hemoglobin, and ferritin). An overview of some of the key results was published in: Yan, Shengkai, J. Li, S. Li, B. Zhang, S. Du, P. Gordon-Larsen, L. Adair, B.M. Popkin (2012) The expanding burden of cardiometabolic risk in China: the China Health and Nutrition Survey. Obesity Reviews 13 (9): 810-21. PMCID: PMC3429648. The full documentation of the collection and laboratory measurement are also placed as documents online: Protocols used to collect and process blood samples are available [here](https://www.cpc.unc.edu/projects/china/data/datasets/Blood Collection Protocol_English.pdf). A list of biomarkers and methods used to measure them is available here. Codebook of the biomarker dataset is available here. Master_Asset_201804: mainly record what they have in home ","date":"2021-02-23","objectID":"/chnstutorials/:1:0","tags":[],"title":"CHNS Tutorials","uri":"/chnstutorials/"},{"categories":[],"content":"Income Master_Agriculture_201804: Agriculture income, work hours, and cost Master_Business_201804: Business Income \u0026 work hours Master_Income_Categories_201804: record different categories of income Master_Constructed_Income_201804: summary of different kind of income ","date":"2021-02-23","objectID":"/chnstutorials/:2:0","tags":[],"title":"CHNS Tutorials","uri":"/chnstutorials/"},{"categories":["documentation"],"content":"Discover what the Hugo - uBlog theme is all about and the core-concepts behind it.","date":"2020-03-06","objectID":"/theme-documentation-basics/","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"Discover what the Hugo - uBlog theme is all about and the core-concepts behind it. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:0:0","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"1 Requirements Thanks to the simplicity of Hugo, Hugo is the only dependency of this theme. Just install latest version of  Hugo (\u003e 0.62.0) for your OS (Windows, Linux, macOS). Why not support earlier versions of Hugo?\r\rSince Markdown Render Hooks was introduced in the Hugo Christmas Edition, this theme only supports Hugo versions above 0.62.0.\r\r Hugo extended version is recommended\r\rSince some features of this theme need to processes  SCSS to  CSS, it is recommended to use Hugo extended version for better experience.\r\r ","date":"2020-03-06","objectID":"/theme-documentation-basics/:1:0","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2 Installation The following steps are here to help you initialize your new website. If you don’t know Hugo at all, we strongly suggest you learn more about it by following this great documentation for beginners. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:0","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.1 Create Your Project Hugo provides a new command to create a new website: hugo new site my_website cd my_website ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:1","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.2 Install the Theme The uBlogger theme’s repository is: https://github.com/upagge/uBlogger. You can download the latest release  .zip file of the theme and extract it in the themes directory. Alternatively, clone this repository to the themes directory: git clone https://github.com/upagge/uBlogger.git themes/uBlogger Or, create an empty git repository and make this repository a submodule of your site directory: git init git submodule add https://github.com/upagge/uBlogger.git themes/uBlogger ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:2","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.3 Basic Configuration The following is a basic configuration for the uBlogger theme: baseURL = \"http://example.org/\" # [en, zh-cn, fr, ...] determines default content language defaultContentLanguage = \"en\" # language code languageCode = \"en\" title = \"My New Hugo Site\" # Change the default theme to be use when building the site with Hugo theme = \"uBlogger\" [params] # uBlogger theme version version = \"1.3.X\" [menu] [[menu.main]] identifier = \"posts\" # you can add extra information before the name (HTML format is supported), such as icons pre = \"\" # you can add extra information after the name (HTML format is supported), such as icons post = \"\" name = \"Posts\" url = \"/posts/\" # title will be shown when you hover on this menu link title = \"\" weight = 1 [[menu.main]] identifier = \"tags\" pre = \"\" post = \"\" name = \"Tags\" url = \"/tags/\" title = \"\" weight = 2 [[menu.main]] identifier = \"categories\" pre = \"\" post = \"\" name = \"Categories\" url = \"/categories/\" title = \"\" weight = 3 # Markup related configuration in Hugo [markup] # Syntax Highlighting (https://gohugo.io/content-management/syntax-highlighting) [markup.highlight] # false is a necessary configuration noClasses = false Note\r\rWhen building the website, you can set a theme by using --theme option. However, we suggest you modify the configuration file (config.toml) and set the theme as the default.\r\r ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:3","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.4 Create Your First Post Here is the way to create your first post: hugo new posts/first_post.md Feel free to edit the post file by adding some sample content and replacing the title value in the beginning of the file. Note\r\rBy default all posts and pages are created as a draft. If you want to render these pages, remove the property draft: true from the metadata, set the property draft: false or add -D/--buildDrafts parameter to hugo command.\r\r Archetypes\rYou can copy /uBlogger/archetypes/default.md to your archetypes folder to create a pre-configured post.\r\r ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:4","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.5 Launching the Website Locally Launch by using the following command: hugo serve Go to http://localhost:1313. Tip\r\rWhen you run hugo serve, when the contents of the files change, the page automatically refreshes with the changes.\r\r Note\r\rSince the theme use .Scratch in Hugo to implement some features, it is highly recommended that you add --disableFastRender parameter to hugo server command for the live preview of the page you are editing. hugo serve --disableFastRender \r\r ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:5","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"2.6 Build the Website When your site is ready to deploy, run the following command: hugo A public folder will be generated, containing all static content and assets for your website. It can now be deployed on any web server. Tip\r\rThe website can be automatically published and hosted with Netlify (Read more about Automated HUGO deployments with Netlify). Alternatively, you can use AWS Amplify, Github pages, Render and more…\r\r ","date":"2020-03-06","objectID":"/theme-documentation-basics/:2:6","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"3 Configuration ","date":"2020-03-06","objectID":"/theme-documentation-basics/:3:0","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"3.1 Site Configuration In addition to Hugo global configuration and menu configuration, uBlogger lets you define the following parameters in your site configuration (here is a config.toml, whose values are default). Please open the code block below to view the complete sample configuration : [params] # uBlogger theme version version = \"1.3.X\" # site description description = \"This is My New Hugo Site\" # site keywords keywords = [\"Theme\", \"Hugo\"] # site default theme (\"light\", \"dark\", \"auto\") defaultTheme = \"auto\" # public git repo url only then enableGitInfo is true gitRepo = \"\" # which hash function used for SRI, when empty, no SRI is used # (\"sha256\", \"sha384\", \"sha512\", \"md5\") fingerprint = \"\" # date format dateFormat = \"2006-01-02\" # website images for Open Graph and Twitter Cards images = [\"/logo.png\"] # App icon config [params.app] # optional site title override for the app when added to an iOS home screen or Android launcher title = \"uBlogger\" # whether to omit favicon resource links noFavicon = false # modern SVG favicon to use in place of older style .png and .ico files svgFavicon = \"\" # Android browser theme color themeColor = \"#ffffff\" # Safari mask icon color iconColor = \"#5bbad5\" # Windows v8-10 tile color tileColor = \"#da532c\" # Search config [params.search] enable = true # type of search engine (\"lunr\", \"algolia\") type = \"lunr\" # max index length of the chunked content contentLength = 4000 # placeholder of the search bar placeholder = \"\" # max number of results length maxResultLength = 10 # snippet length of the result snippetLength = 30 # HTML tag name of the highlight part in results highlightTag = \"em\" # whether to use the absolute URL based on the baseURL in search index absoluteURL = false [params.search.algolia] index = \"\" appID = \"\" searchKey = \"\" # Header config [params.header] # desktop header mode (\"fixed\", \"normal\", \"auto\") desktopMode = \"fixed\" # mobile header mode (\"fixed\", \"normal\", \"auto\") mobileMode = \"auto\" # Header title config [params.header.title] # URL of the LOGO logo = \"\" # title name name = \"\" # you can add extra information before the name (HTML format is supported), such as icons pre = \"\" # you can add extra information after the name (HTML format is supported), such as icons post = \"\" # whether to use typeit animation for title name typeit = false # Footer config [params.footer] enable = true # Custom content (HTML format is supported) custom = '' # whether to show Hugo and theme info hugo = true # whether to show copyright info copyright = true # whether to show the author author = true # Site creation time since = 2019 # ICP info only in China (HTML format is supported) icp = \"\" # license info (HTML format is supported) license = '\u003ca rel=\"license external nofollow noopener noreffer\" href=\"https://creativecommons.org/licenses/by-nc/4.0/\" target=\"_blank\"\u003eCC BY-NC 4.0\u003c/a\u003e' # Section (all posts) page config [params.section] # special amount of posts in each section page paginate = 20 # date format (month and day) dateFormat = \"01-02\" # amount of RSS pages rss = 10 # List (category or tag) page config [params.list] # special amount of posts in each list page paginate = 20 # date format (month and day) dateFormat = \"01-02\" # amount of RSS pages rss = 10 # Home page config [params.home] # amount of RSS pages rss = 10 # Home page profile [params.home.profile] enable = true # Gravatar Email for preferred avatar in home page gravatarEmail = \"\" # URL of avatar shown in home page avatarURL = \"/images/avatar.png\" # title shown in home page (HTML format is supported) title = \"\" # subtitle shown in home page subtitle = \"This is My New Hugo Site\" # whether to use typeit animation for subtitle typeit = true # whether to show social links social = true # disclaimer (HTML format is supported) disclaimer = \"\" # Home page posts [params.home.posts] enable = true # special amount of posts in each home posts page paginate = 6 # replaced with hiddenFromHomePage in params.page # default behavior when you don'","date":"2020-03-06","objectID":"/theme-documentation-basics/:3:1","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"3.2 Favicons, Browserconfig, Manifest It is recommended to put your own favicons: apple-touch-icon.png (180x180) favicon-32x32.png (32x32) favicon-16x16.png (16x16) mstile-150x150.png (150x150) android-chrome-192x192.png (192x192) android-chrome-512x512.png (512x512) into /static. They’re easily created via https://realfavicongenerator.net/. Customize browserconfig.xml and site.webmanifest to set theme-color and background-color. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:3:2","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"3.3 Style Customization Note\r\rHugo extended version is necessary for the style customization.\r\r uBlogger theme has been built to be as configurable as possible by defining custom .scss style files. The directory including the custom .scss style files is assets/css relative to your project root directory. In assets/css/_override.scss, you can override the variables in themes/uBlogger/assets/css/_variables.scss to customize the style. Here is a example: @import url('https://fonts.googleapis.com/css?family=Fira+Mono:400,700\u0026display=swap\u0026subset=latin-ext'); $code-font-family: Fira Mono, Source Code Pro, Menlo, Consolas, Monaco, monospace; In assets/css/_custom.scss, you can add some css style code to customize the style. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:3:3","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"4 Multilingual and i18n uBlogger theme is fully compatible with Hugo multilingual mode, which provides in-browser language switching. \rLanguage Switch \"\rLanguage Switch\r ","date":"2020-03-06","objectID":"/theme-documentation-basics/:4:0","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"4.1 Compatibility Language Hugo Code HTML lang Attribute Theme Docs Lunr.js Support English en en Simplified Chinese zh-cn zh-CN French fr fr Polish pl pl Brazilian Portuguese pt-br pt-BR Italian it it Spanish es es German de de German de de Serbian sr sr Russian ru ru Romanian ro ro Vietnamese vi vi ","date":"2020-03-06","objectID":"/theme-documentation-basics/:4:1","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"4.2 Basic Configuration After learning how Hugo handle multilingual websites, define your languages in your site configuration. For example with English, Chinese and French website: # [en, zh-cn, fr, pl, ...] determines default content language defaultContentLanguage = \"en\" [languages] [languages.en] weight = 1 title = \"My New Hugo Site\" languageCode = \"en\" languageName = \"English\" [[languages.en.menu.main]] identifier = \"posts\" pre = \"\" post = \"\" name = \"Posts\" url = \"/posts/\" title = \"\" weight = 1 [[languages.en.menu.main]] identifier = \"tags\" pre = \"\" post = \"\" name = \"Tags\" url = \"/tags/\" title = \"\" weight = 2 [[languages.en.menu.main]] identifier = \"categories\" pre = \"\" post = \"\" name = \"Categories\" url = \"/categories/\" title = \"\" weight = 3 [languages.zh-cn] weight = 2 title = \"我的全新 Hugo 网站\" # language code, CN only here languageCode = \"zh-CN\" languageName = \"简体中文\" # whether to include Chinese/Japanese/Korean hasCJKLanguage = true [[languages.zh-cn.menu.main]] identifier = \"posts\" pre = \"\" post = \"\" name = \"文章\" url = \"/posts/\" title = \"\" weight = 1 [[languages.zh-cn.menu.main]] identifier = \"tags\" pre = \"\" post = \"\" name = \"标签\" url = \"/tags/\" title = \"\" weight = 2 [[languages.zh-cn.menu.main]] identifier = \"categories\" pre = \"\" post = \"\" name = \"分类\" url = \"/categories/\" title = \"\" weight = 3 [languages.fr] weight = 3 title = \"Mon nouveau site Hugo\" languageCode = \"fr\" languageName = \"Français\" [[languages.fr.menu.main]] identifier = \"posts\" pre = \"\" post = \"\" name = \"Postes\" url = \"/posts/\" title = \"\" weight = 1 [[languages.fr.menu.main]] identifier = \"tags\" pre = \"\" post = \"\" name = \"Balises\" url = \"/tags/\" title = \"\" weight = 2 [[languages.fr.menu.main]] identifier = \"categories\" name = \"Catégories\" pre = \"\" post = \"\" url = \"/categories/\" title = \"\" weight = 3 Then, for each new page, append the language code to the file name. Single file my-page.md is split in three files: in English: my-page.en.md in Chinese: my-page.zh-cn.md in French: my-page.fr.md Note\r\rBe aware that only translated pages are displayed in menu. It’s not replaced with default language content.\r\r Tip\r\rUse Front Matter parameter to translate urls too.\r\r ","date":"2020-03-06","objectID":"/theme-documentation-basics/:4:2","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"4.3 Overwrite Translation Strings Translations strings are used for common default values used in the theme. Translations are available in some languages, but you may use another language or want to override default values. To override these values, create a new file in your local i18n folder i18n/\u003clanguageCode\u003e.toml and inspire yourself from themes/uBlogger/i18n/en.toml. By the way, as these translations could be used by other people, please take the time to propose a translation by  making a PR to the theme! ","date":"2020-03-06","objectID":"/theme-documentation-basics/:4:3","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"5 Search Based on Lunr.js or algolia, searching is supported in uBlogger theme. ","date":"2020-03-06","objectID":"/theme-documentation-basics/:5:0","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"5.1 Output Configuration In order to generate index.json for searching, add JSON output file type to the home of the outputs part in your site configuration. [outputs] home = [\"HTML\", \"RSS\", \"JSON\"] ","date":"2020-03-06","objectID":"/theme-documentation-basics/:5:1","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"5.2 Search Configuration Based on index.json generated by Hugo, you could activate searching. Here is the search configuration in your site configuration: [params.search] enable = true # type of search engine (\"lunr\", \"algolia\") type = \"lunr\" # max index length of the chunked content contentLength = 4000 # placeholder of the search bar placeholder = \"\" # max number of results length maxResultLength = 10 # snippet length of the result snippetLength = 30 # HTML tag name of the highlight part in results highlightTag = \"em\" # whether to use the absolute URL based on the baseURL in search index absoluteURL = false [params.search.algolia] index = \"\" appID = \"\" searchKey = \"\" How to choose search engine?\r\rThe following is a comparison of two search engines: lunr: simple, no need to synchronize index.json, no limit for contentLength, but high bandwidth and low performance (Especially for Chinese which needs a large segmentit library) algolia: high performance and low bandwidth, but need to synchronize index.json and limit for contentLength The content of the post is separated by h2 and h3 HTML tag to improve query performance and basically implement full-text search. contentLength is used to limit the max index length of the part starting with h2 and h3 HTML tag. \r\r Tips about algolia\r\rYou need to upload index.json files to algolia to activate searching. You could upload the index.json files by browsers but a CLI tool may be better. Algolia Atomic is a good choice. To be compatible with Hugo multilingual mode, you need to upload different index.json for each language to the different index of algolia, such as zh-cn/index.json or fr/index.json…\r\r","date":"2020-03-06","objectID":"/theme-documentation-basics/:5:2","tags":["installation","configuration"],"title":"Theme Documentation - Basics","uri":"/theme-documentation-basics/"},{"categories":["documentation"],"content":"Find out how to create and organize your content quickly and intuitively in uBlogger theme.","date":"2020-03-05","objectID":"/theme-documentation-content/","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Find out how to create and organize your content quickly and intuitively in uBlogger theme. ","date":"2020-03-05","objectID":"/theme-documentation-content/:0:0","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"1 Contents Organization A few suggestions to help you get a good looking site quickly: Keep post pages in the content/posts directory, for example: content/posts/my-first-post.md Keep other pages in the content directory, for example: content/about.md Local resources organization Local Resource Reference\r\r There are three ways to reference local resources such as images and music: Using page resources in page bundles. You can reference page resources by the value for Resources.GetMatch or the filepath of the resource relative to the page directory directly. Store resources in the assets directory, which is /assets by default. The filepath of the resource to reference in the post is relative to the assets directory. Store resources in the static directory, which is /static by default. The filepath of the resource to reference in the post is relative to the static directory. The priority of references is also in the above order. There are many places in the theme where the above local resource references can be used, such as links, images, image shortcode, music shortcode and some params in the front matter. Images in page resources or assets directory processing will be supported in the future. It’s really cool! \r\r ","date":"2020-03-05","objectID":"/theme-documentation-content/:1:0","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"2 Front Matter Hugo allows you to add front matter in yaml, toml or json to your content files. Note\r\rNot all of the below front matters need to be set in each of your posts. It is necessary only if the front matters and the page part in your site configuration are inconsistent.\r\r Here is a front matter example: ---title:\"My First Post\"subtitle:\"\"date:2020-03-04T15:58:26+08:00lastmod:2020-03-04T15:58:26+08:00draft:true# author author:\"\"# authorLink authorLink:\"\"description:\"\"license:\"\"images:[]# Article Update Informationupd:\"\"# Author's comment, is shown above all commentsauthorComment:\"\"# article design themetheme:\"\"# Allows you to hide the preview image on the article pagehiddenFeaturedImage:false# Post display settings on the pagesummaryStyle:# Display previews on the page of postshiddenImage:false# Allows you to hide the descriptionhiddenDescription:false# Allows you to hide the titlehiddenTitle:truetags:# One of the options for displaying tagstheme:\"image\"# Text colorcolor:\"white\"# Backing colorbackground:\"black\"# Tag transparencytransparency:0.9tags:[]categories:[]featuredImage:\"\"featuredImagePreview:\"\"hiddenFromHomePage:falsehiddenFromSearch:falsetwemoji:falselightgallery:trueruby:truefraction:truefontawesome:truelinkToMarkdown:truerssFullText:falsetoc:enable:trueauto:truecode:copy:true# ...math:enable:true# ...mapbox:accessToken:\"\"# ...share:enable:true# ...comment:enable:true# ...library:css:# someCSS = \"some.css\"# located in \"assets/\"# Or# someCSS = \"https://cdn.example.com/some.css\"js:# someJS = \"some.js\"# located in \"assets/\"# Or# someJS = \"https://cdn.example.com/some.js\"seo:images:[]# ...--- title: the title for the content. subtitle: the subtitle for the content. date: the datetime assigned to this page, which is usually fetched from the date field in front matter, but this behaviour is configurabl in the site configuration. lastmod: the datetime at which the content was last modified. draft: if true, the content will not be rendered unless the --buildDrafts/-D flag is passed to the hugo command. author: the author for the content. authorLink: the link of the author. description: the description for the content. license: the special lisence for this content. images: page images for Open Graph and Twitter Cards. tags: the tags for the content. categories: the categories for the content. featuredImage: the featured image for the content. featuredImagePreview: the featured image for the content preview in the home page. hiddenFromHomePage: if true, the content will not be shown in the home page. hiddenFromSearch: if true, the content will not be shown in the search results. twemoji: if true, the content will enable the twemoji. lightgallery: if true, images in the content will be shown as the gallery. ruby: if true, the content will enable the ruby extended syntax. fraction: if true, the content will enable the fraction extended syntax. fontawesome: if true, the content will enable the Font Awesome extended syntax. linkToMarkdown: if true, the footer of the content will be shown the link to the orignal Markdown file. rssFullText: if true, the full text content will be shown in RSS. toc: the same as the params.page.toc part in the site configuration. code: the same as the params.page.code part in the site configuration. math: the same as the params.page.math part in the site configuration. mapbox: the same as the params.page.mapbox part in the site configuration. share: the same as the params.page.share part in the site configuration. comment: the same as the params.page.comment part in the site configuration. library: the same as the params.page.library part in the site configuration. seo: the same as the params.page.seo part in the site configuration. Tip\r\r featuredImage and featuredImagePreview support the complete usage of local resource references. If the page resource with name: featured-image or name: featured-image-preview is set in the front matter, it is not necessary to set the parameter featuredImage or","date":"2020-03-05","objectID":"/theme-documentation-content/:2:0","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Theme You can choose one of three topics for an article. The differences between them are in the location of the image and the title of the article with metadata. All themes are adaptable to any screen size. Available values: classic, wide, full. This page uses design - full. Idea\rIf you combine them with toc, you can get even more styling options.\r\r classic \rTheme classic \"\rTheme classic\r wide \rTheme wide \"\rTheme wide\r full \rTheme full \"\rTheme full\r ","date":"2020-03-05","objectID":"/theme-documentation-content/:2:1","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Author Many authors are now supported - you can write more than one article. For your convenience, author data is stored as json in the data/authors folder. Don’t forget to create it. Example\rYou can see the author’s example at the end of this article.\r\r Format JSON: { \"name\": \"Struchkov Mark\", \"nickname\": \"uPagge\", \"about\": \"Trusted user pc\", \"avatar\": \"https://upagge.ru/img/ava.jpg\", \"link\": \"https://uPagge.ru\", \"email\": \"me@upagge.ru\", \"ps\": \"If you like this topic, put an asterisk in GitHub, it will be a pleasure for me\" } ","date":"2020-03-05","objectID":"/theme-documentation-content/:2:2","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"🥖 Breadcrumbs This is quite a useful chip for SEO optimization, it is implemented in all three themes. Support for opengraph. Traditionally, Breadcrumbs are placed above the header. They contain a path by sections to the current page, with links to sections. For posts it is the main page of the site, then the category of the post and the name of the post without a link. ","date":"2020-03-05","objectID":"/theme-documentation-content/:2:3","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Disable content for H1-H2 You can disable default characters ‘#’, ‘|’ by setting the correct id. It must start with ‘u-’. You can use this to set emoji or partition number. \rExample with Emoji \"\rExample with Emoji\r ","date":"2020-03-05","objectID":"/theme-documentation-content/:2:4","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Summary display settings You can customize the summary view to your taste by changing the following parameters. summaryStyle:hiddenImage:falsehiddenDescription:falsehiddenTitle:truetags:theme:\"image\"color:\"white\"background:\"black\"transparency:0.9 What kinds of tags are available: image. Tags on the picture footer. Tags in footer summary under-footer. Tags under summary These parameters, together with the scss override, allow you to customize the display quite flexibly. $article-summary-border-radius: 32px; $article-summary-image-border-radius: 30px; The global setting is, as always, available in the main configuration file ","date":"2020-03-05","objectID":"/theme-documentation-content/:2:5","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"3 Content Summaries uBlogger theme uses the summary of the content to display abstract information in the home page. Hugo can generate summaries of your content. \rSummary Preview \"\rSummary Preview\r ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:0","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Automatic Summary Splitting By default, Hugo automatically takes the first 70 words of your content as its summary. You may customize the summary length by setting summaryLength in the site configuration. If you are creating content in a CJKChinese/Japanese/Korean language and want to use Hugo’s automatic summary splitting, set hasCJKLanguage to true in your site configuration. ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:1","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Manual Summary Splitting Alternatively, you may add the \u003c!--more--\u003e summary divider where you want to split the article. Content that comes before the summary divider will be used as that content’s summary. Note\r\rBe careful to enter \u003c!--more--\u003e exactly; i.e., all lowercase and with no whitespace.\r\r ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:2","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Front Matter Summary You might want your summary to be something other than the text that starts the article. In this case you can provide a separate summary in the summary variable of the article front matter. ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:3","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Use Description as Summary You might want your description in the description variable of the article front matter as the summary. You may add the \u003c!--more--\u003e summary divider at the start of the article. Keep content that comes before the summary divider empty. Then uBlogger theme will use your description as the summary. ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:4","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Priority Order of Summary Selection Because there are multiple ways in which a summary can be specified it is useful to understand the order. It is as follows: If there is a \u003c!--more--\u003e summary divider present in the article but no content is before the divider, the description will be used as the summary. If there is a \u003c!--more--\u003e summary divider present in the article the text up to the divider will be provided as per the manual summary split method. If there is a summary variable in the article front matter the value of the variable will be provided as per the front matter summary method. The text at the start of the article will be provided as per the automatic summary split method. Note\r\rIt is not recommended to include rich text block elements in the summary, which will cause typographic errors. Such as code blocks, pictures, tables, etc.\r\r ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:5","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"TimeAgo TimeAgo allows you to display the date of publication of a post in the style of social networks, as opposed to the current time. For example, “4 minutes ago,” “one day ago” \rtime agoHere is what it looks like \"\rHere is what it looks like\r ","date":"2020-03-05","objectID":"/theme-documentation-content/:3:6","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"4 Basic Markdown Syntax This part is shown in the basic markdown syntax page. ","date":"2020-03-05","objectID":"/theme-documentation-content/:4:0","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"5 Extended Markdown Syntax uBlogger theme has some extended syntax elements for you to write articles. ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:0","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Emoji Support This part is shown in the emoji support page. ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:1","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Mathematical Formula uBlogger theme supports mathematical formulas based on $ \\KaTeX $. Set the property enable = true under [params.math] in your site configuration and the property math: true of the article front matter to enable the automatic rendering of mathematical formulas. Tip\r\rHere is a list of $ \\TeX $ functions supported by $ \\KaTeX $.\r\r Block Formula The default block delimiters are $$/$$ and \\\\[/\\\\]: $$ c = \\pm\\sqrt{a^2 + b^2} $$ \\\\[ f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi \\\\] The rendered output looks like this: $$ c = \\pm\\sqrt{a^2 + b^2} $$ \\[ f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi \\] Inline Formula The default inline delimiters are $/$ and \\\\(/\\\\): $ c = \\pm\\sqrt{a^2 + b^2} $ and \\\\( f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi \\\\) The rendered output looks like this: $ c = \\pm\\sqrt{a^2 + b^2} $ and \\( f(x)=\\int_{-\\infty}^{\\infty} \\hat{f}(\\xi) e^{2 \\pi i \\xi x} d \\xi \\) Tip\r\rYou can add more block and inline delimiters in your site configuration.\r\r Copy-tex Copy-tex is an extension for $ \\KaTeX $. By the extension, when selecting and copying $ \\KaTeX $ rendered elements, copies their $ \\LaTeX $ source to the clipboard. Set the property copyTex = true under [params.math] in your site configuration to enable Copy-tex. Select and copy the formula rendered in the previous section, and you can find that the copied content is the LaTeX source code. mhchem mhchem is an extension for $ \\KaTeX $. By the extension, you can write beautiful chemical equations easily in the article. Set the property mhchem = true under [params.math] in your site configuration to enable mhchem. $$ \\ce{CO2 + C -\u003e 2 CO} $$ $$ \\ce{Hg^2+ -\u003e[I-] HgI2 -\u003e[I-] [Hg^{II}I4]^2-} $$ The rendered output looks like this: $$ \\ce{CO2 + C -\u003e 2 CO} $$ $$ \\ce{Hg^2+ -\u003e[I-] HgI2 -\u003e[I-] [Hg^{II}I4]^2-} $$ ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:2","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Ruby Annotation An extended Markdown syntax for ruby annotation is supported in uBlogger theme: [Hugo]^(An open-source static site generator) The rendered output looks like this: HugoAn open-source static site generator ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:3","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Fraction An extended Markdown syntax for fraction is supported in uBlogger theme: [Light]/[Dark] [99]/[100] The rendered output looks like this: Light/Dark 90/100 ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:4","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Font Awesome uBlogger theme uses Font Awesome as the icon library. You can easily use these icons in your articles. Get the class of icons you wanted from the Font Awesome website. Gone camping! :(fas fa-campground fa-fw): Be back soon. That is so funny! :(far fa-grin-tears): The rendered output looks like this: Gone camping!  Be back soon. That is so funny! ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:5","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":["documentation"],"content":"Escape character In some special cases (when writing this theme documentation ), your content will conflict with basic or extended Markdown syntax, and it is inevitable. The escape character syntax can help you build the content you wanted: {?X} -\u003e X For example, two : will enable emoji syntax, which is not the behavior you want. The escape character syntax is like this: {?:}joy: The rendered output looks like this: :joy: instead of 😂 Tip\r\rThis is related to an issue for Hugo, which has not been resolved.\r\r Another example is: [link{?]}(#escape-character) The rendered output looks like this: [link](#escape-character) instead of link. ","date":"2020-03-05","objectID":"/theme-documentation-content/:5:6","tags":["content","Markdown"],"title":"Theme Documentation - Content","uri":"/theme-documentation-content/"},{"categories":null,"content":"  uBlogger is a clean, elegant but advanced blog theme for Hugo developed by uPagge. It is based on the original LeaveIt Theme and KeepIt Theme. \rHugo Theme uBlogger \"\rHugo Theme uBlogger\r ","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"About uBlogger","uri":"/about/"},{"categories":null,"content":"Features Performance and SEO  Optimized for performance: 99/100 on mobile and 100/100 on desktop in Google PageSpeed Insights  Optimized SEO performance with a correct SEO SCHEMA based on JSON-LD  Google Analytics supported  Fathom Analytics supported  Search engine verification supported (Google, Bind, Yandex and Baidu)  CDN for third-party libraries supported  Automatically converted images with Lazy Load by lazysizes Appearance and Layout / Responsive layout / Light/Dark mode  Globally consistent design language  Pagination supported  Easy-to-use and self-expanding table of contents  Multilanguage supported and i18n ready  Beautiful CSS animation Social and Comment Systems  Gravatar supported by Gravatar  Local Avatar supported  Up to 64 social links supported  Up to 28 share sites supported  Disqus comment system supported by Disqus  Gitalk comment system supported by Gitalk  Valine comment system supported by Valine  Facebook comments system supported by Facebook  Telegram comments system supported by Comments  Commento comment system supported by Commento  Utterances comment system supported by Utterances Extended Features  Search supported by Lunr.js or algolia  Twemoji supported  Automatically highlighting code  Copy code to clipboard with one click  Images gallery supported by lightgallery.js  Extended Markdown syntax for Font Awesome icons  Extended Markdown syntax for ruby annotation  Extended Markdown syntax for fraction  Mathematical formula supported by $ \\KaTeX $  Diagrams shortcode supported by mermaid  Interactive data visualization shortcode supported by ECharts  Mapbox shortcode supported by Mapbox GL JS  Music player shortcode supported by APlayer and MetingJS  Bilibili player shortcode  Kinds of admonitions shortcode  Custom style shortcode  Custom script shortcode  Animated typing supported by TypeIt  Dynamic scroll supported by Smooth Scroll  Cookie consent banner supported by cookieconsent … ","date":"2019-08-02","objectID":"/about/:0:1","tags":null,"title":"About uBlogger","uri":"/about/"},{"categories":null,"content":"License uBlogger is licensed under the MIT license. Check the LICENSE file for details. Thanks to the authors of following resources included in the theme: normalize.css Font Awesome Simple Icons Animate.css Smooth Scroll autocomplete.js Lunr.js algoliasearch lazysizes object-fit-images Twemoji lightgallery.js clipboard.js Sharer.js TypeIt $ \\KaTeX $ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine cookieconsent ","date":"2019-08-02","objectID":"/about/:0:2","tags":null,"title":"About uBlogger","uri":"/about/"},{"categories":null,"content":"4. Dynamic Games with Incomplete Information and Perfect Bayesian Equilibrium ","date":"0001-01-01","objectID":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/:0:0","tags":null,"title":"","uri":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/"},{"categories":null,"content":"4.1 Perfect Bayesian Equilibrium Beliefs and Sequential Rationality MWG Chapter 9:C JR Chapter 7.3.7 The notion of PBE will be used later in the following chapters: Adverse selection and signalling (MWG 13.B, 13.C, JR 8.1.1, 8.1.2) When the notion of subgame perfect Nash equilibrium is of no use to refine Nash equilibria. e.g. MWG (Page 283) Example 9.C.1: \r All Nash equilibria are SPNEs (two pure-strategy N.E./SPNEs are shown in blue). \r How to eliminate the unreasonable equilibrium, e.g. (E plays Out; I plays Fight if entry occurs)? In the spirt of sequential rationality, we might insist that Firm I’s action be optimal for some belief. Viewed in this light, we can conclude that “fight if entry occurs” is not an optimal choice for any belief that Firm I might have. Roughly speaking, we want to require strategies being optimal given beliefs and beliefs be consistent with strategies. We need to introduce the notions of a system of beliefs and sequential rationality of strategies. 【Definition】A system of beliefs $\\mu$ in extensive form game $\\Gamma_{E}$ is a specification of a probability $\\mu(x) \\in[0,1]$ for each decision node $x$ in $\\Gamma_{E}$ such that $\\sum_{x \\in H} \\mu(x)=1$ for all information sets $H$. A system of beliefs specify, for each information set, a probabilistic assessment by the player who moves there, conditional upon play having reached that information set. Let $E\\left[u_{i} | H, \\mu, \\sigma_{i}, \\sigma_{-i}\\right]$ be Player $i$ ’s expected utility starting at $H$ if she has beliefs $\\mu$ and follows strategy $\\sigma_{i}$ and other players use $\\sigma_{-i}$ 【Definition】A strategy profile $\\sigma$ in extensive form game $\\Gamma_{E}$ is **sequentially rational** at information set $H$ given a system of beliefs $\\mu$ if, denoting by $\\iota(H)$ the player who moves at $H$, we have $$ E\\left[u_{\\iota(H)} | H, \\mu, \\sigma_{\\iota(H)}, \\sigma_{-\\iota(H)}\\right] \\geq E\\left[u_{\\iota(H)} | H, \\mu, \\tilde{\\sigma}_{\\iota(H)}, \\sigma_{-\\iota(H)}\\right] $$ for all $\\tilde{\\sigma}_{\\iota(H)} \\in \\Delta\\left(S_{\\iota(H)}\\right)$. If strategy profile $\\sigma$ satisfies this condition for all information sets $H$, then we say that $\\sigma$ is sequentially rational given belief system. Consider completely mixed strategy: each player’s strategy assigns a strictly positive probability to each possible action at every one of her information sets. (So every information set in the game is reached with positive probability.) For each $x$ in a given player’s information set $H$, the player should compute the conditional probability of reaching that node given play of strategies $\\sigma$ : $$ \\operatorname{Pr}(x | H, \\sigma)=\\frac{\\operatorname{Pr}(x | \\sigma)}{\\sum_{x^{\\prime} \\in H} \\operatorname{Pr}\\left(x^{\\prime} | \\sigma\\right)} $$ \r 【structurally consistent】Also called the “common beliefs” restriction in JR Section 7.3.7: players with identical information have identical beliefs. $\\mu=0.5, \\forall \\sigma_{1} \\in(0,1)$ \r Extensive-form games with incomplete information: Sequential-move $+$ Bayesian games (players has types) 【PBE】We are going to introduce Perfect Bayesian Equilibrium as the solution concept. Strategy profile satisfies sequential rationality: Each player’s strategies specify optimal actions, given the strategies of the other players, and given his beliefs. Consistency of beliefs: the beliefs are consistent with Bayes‘ rule, whenever possible. Useful notions: separating (perfect Bayesian) equilibrium, pooling (perfect Bayesian) equilibrium. 【Separating equilibrium】 is a Perfect Bayesian Equilibrium of extensive-form incomplete information game in which different types of player i adopt different strategies. 【Pooling equilibrium】 is a Perfect Bayesian Equilibrium in which different types of player i adopt the same strategy. ","date":"0001-01-01","objectID":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/:1:0","tags":null,"title":"","uri":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/"},{"categories":null,"content":"4.2 5-step to find pure-strategy PBEs Specify a strategy profile for the privately informed player, either separating or pooling. In our above example, there are only four possible strategy profiles for the privately informed monetary authority: two separating strategy profiles, High $^{S}$ Low $^{W}$ and Low $^{S}$ High $^{W}$ and two pooling strategy profiles, High $^{S}$ High $^{W}$ and Low $^{S}$ Low $^{W}$. (For future reference, it might be helpful to shade the branches corresponding to the strategy profile we test.) Update the uninformed player’s beliefs using Bayes' rule, whenever possible. In our above example, we need to specify beliefs $\\mu$ and $\\gamma$ which arise after the labor union observes a high or a low inflation announcement, respectively. Given the uninformed player’s updated beliefs, find his optimal response. In our above example, we first determine the optimal response of the labor union $(\\mathrm{H} \\text { or } \\mathrm{L}$ ) upon observing a high-inflation announcement (given its updated belief $\\mu$ ), we then determine its optimal response (H or L) after observing a low-inflation announcement (given its updated belief $\\gamma$ ). (Also for future reference, it might be helpful to shade the branches corresponding to the optimal responses we ust found Given the optimal response of the uninformed player, find the optimal action (message) for the informed player. In our previous example, we first check if the Strong monetary authority prefers to make a high or low inflation announcement (given the labor union’s responses determined in step 3 ). We then operate similarly for the Weak type of monetary authority. Then check if this strategy profile for the informed player coincides with the profile suggested in step 1 If it coincides, then this strategy profile, updated beliefs and optimal responses can be supported as a PBE of the incomplete information game. Otherwise, we say that this strategy profile cannot be sustained as a PBE of the game. ","date":"0001-01-01","objectID":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/:2:0","tags":null,"title":"","uri":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/"},{"categories":null,"content":"4.2.1 Example Let us consider the following sequential game with incomplete information: A monetary authority (such as the Federal Reserve Bank) privately observes its real degree of commitment with maintaining low information levels. After knowing its type (either Strong or Weak), the monetary authority decides whether to announce that the expectation for information is either High or Low. A labor union, observing the message sent by the monetary authority, decides whether to ask for high or low salary raises(denoted as H or L, respectively) \r Let us next separately apply this procedure to test each of the four candidate strategy profiles: two separating strategy profiles: $\\text { High }^{S} \\text { Low }^{W} $ $\\text { Low }^{S} \\text { High}^{W}$ And two pooling strategy profiles: $\\text { High }^{S} \\text { High }^{W} $ $\\text { Low }^{S} \\text { Low}^{W}$ Separating equilibrium Let us first check separating strategy profile: $\\text { Low }^{S} \\text { High}^{W}$ Step 1: Specifying strategy profile $\\text { Low }^{S} \\text { High}^{W}$ that we will test. (See shaded branches in the figure.) \r Step 2: Updating beliefs (a) After high inflation announcement (left-hand side) $$ \\mu=\\frac{0.6 \\alpha^{\\text {Strong}}}{0.6 \\alpha^{\\text {Strong}}+0.4 \\alpha^{\\text {Weak}}}=\\frac{0.6 \\times 0}{0.6 \\times 0+0.4 \\times 1}=0 $$ where $\\alpha^{\\text{Strong}}$ is the probability that monetary authority choose high inflation given type is strong. This implies that after high inflation, the labor union restricts its belief to the lower left-hand corner (see box), since $\\mu=0$ and $1-\\mu=1$ (b) After low inflation announcement (right-hand side) $$ \\gamma=\\frac{0.6\\left(1-\\alpha^{\\text {Strong}}\\right)}{0.6\\left(1-\\alpha^{\\text {Strong}}\\right)+0.4\\left(1-\\alpha^{\\text {Weak}}\\right)}=\\frac{0.6 \\times 1}{0.6 \\times 1+0.4 \\times 0}=1 $$ This implies that, after low inflation, the labor union restricts its belief to the upper right-hand corner (see box), since $\\gamma=1$ and $1-\\gamma=0$. Step 3: Optimal response (a) After high inflation announcement, respond with $\\mathrm{H}$ since $$ 0\u003e-100 $$ in the lower left-hand corner of the figure. (b) After low inflation announcement, respond with $\\mathrm{L}$ since $$ 0 \u003e-100 $$ in the upper right-hand corner of the figure. We can hence summarize the optimal responses we just found, by shading them in the figure: $\\mathrm{H}$ after high inflation, but $\\mathrm{L}$ after low inflation. Step 4: Optimal messages by the informed player (a) When the monetary authority is Strong, if it chooses Low (as prescribed), its payoff is $$ 300$. while if it deviates, its payoff decreases to $$ 0$. No incentives to deviate. \r (b) When the monetary authority is Weak, if it chooses High (as prescribed), its payoff is $$ 100$. While if it deviates, its payoff decreases to $$ 50$. No incentives to deviate either. \r Since no type of privately informed player (monetary authority) has incentives to deviate, The separating strategy profile $\\text{Low}^{S} \\text{High}^W$ can be sustained as a PBE. Redo the 5-step, we can check that $\\text { High }^{S} \\text { Low }^{W} $ cannot be sustained as a PBE. Pooling equilibrium Let us now test the pooling strategy profile $\\text { High }^{S} \\text { High }^{W} $ Step 1: Specifying strategy profile High' High W that we will test. (See shaded branches in the figure.) \r Step 2: Updating beliefs (a) After high inflation announcement $$ \\mu=\\frac{0.6 \\alpha^{\\text {Strong}}}{0.6 \\alpha^{\\text {Strong}}+0.4 \\alpha \\text {Weak}}=\\frac{0.6 \\times 1}{0.6 \\times 1+0.4 \\times 1}=0.6 $$ so the high inflation announcement is uninformative. (b) After low inflation announcement (off-the-equilibrium path) $$ \\gamma=\\frac{0.6\\left(1-\\alpha^{\\text {Strong}}\\right)}{0.6\\left(1-\\alpha^{\\text {Strong}}\\right)+0.4\\left(1-\\alpha^{\\text {Weak}}\\right)}=\\frac{0.6 \\times 0}{0.6 \\times 0+0.4 \\times 0}=\\frac{0}{0} \\ $$ Hence, $\\gamma \\in[0,1]$. Step 3: Optimal response (a) After high inflation announcement","date":"0001-01-01","objectID":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/:2:1","tags":null,"title":"","uri":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/"},{"categories":null,"content":"4.3 The Intuitive Criterion The Intuitive Criterion (Cho and Kreps 1987) is to refine the set of PBEs. It is mostly useful in the signalling games (with both discrete and continuous message space). One player is privately informed. e.g. he knows information about the market demand, his own production cost, etc. He uses his actions (e.g. his production decision, investment in capacity, etc.) to communicate/conceal this information to the other uninformed player. In particular, let us precisely describe the time structure of the signalling game: (用观测到的动作更新nature的信息) Nature reveals to player $i$ some piece of private information, $\\theta_{i} \\in \\Theta_{i}$ Then, player $i,$ who privately observes $\\theta_{i},$ chooses an action (or message $m$ ) which is observed by the other player $j$ Player j observes message $m,$ but does not know player i’s type. He knows the prior probability distribution that nature selects a give type $\\theta_{i}$ from $\\Theta_{i}$ e.g. the prior probability for $\\Theta_{i}={\\text { Strong, Weak }}$ is 0.6 and 0.4 After observing player i’s message $m,$ player $j$ updates his beliefs about player i’s type. Let $\\mu\\left(\\theta_{i} | m\\right)$ denote $j$ ’s belief about i’s type being $\\theta_{i}$ Given the beliefs, player $j$ selects an optimal action, $a,$ as a best response to the message $m$ The Intuitive Criterion is an equilibrium selection for PBE. So we start with an already-established PBE. Consider a PBE with its corresponding equilibrium payoff for player $i$ as $u_{i}^{*}\\left(\\theta_{i}\\right) .$ **Apply the Intuitive Criterion in two steps**: ","date":"0001-01-01","objectID":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/:3:0","tags":null,"title":"","uri":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/"},{"categories":null,"content":"4.3.1 Step 1 Figure out which type of the informed player $i$ (the monetary authority in the previous example) could benefit by deviating from the equilibrium message? Let us focus on those types of informed player i who can obtain a higher payoff by deviating form equilibrium than by keeping their equilibrium message unaltered. $$ \\Theta_{i}^{* *}(m)={\\theta_{i} \\in \\Theta_{i} | \\underbrace{u_{i}^{*}\\left(\\theta_{i}\\right)}_{\\text {Equil. Payoff }} \\leq \\underbrace{\\max _{a \\in A^{*}\\left(\\Theta_{i}, m\\right)} u_{i}\\left(m, a, \\theta_{i}\\right)}_{\\text {Highest Payoff from Deviating to message } m}} $$ where $A^{*}\\left(\\Theta_{i}, m\\right)$ denotes the set of best responses of the uninformed player $j,$ who has a belief over the entire type space $\\Theta_{i}$ We restrict out attention to those types of informed player for which sending the off-the-equilibrium message $m$ could give them a higher payoff than that in equilibrium. If $m$ does not satisfy this inequality, we say that $m$ is “equilibrium donimated.” ","date":"0001-01-01","objectID":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/:3:1","tags":null,"title":"","uri":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/"},{"categories":null,"content":"4.3.2 Step 2 If deviations can only come from the type of player $i$ identified in the First Step, is the lowest payoff from deviating higher than the original equilibrium payoff? If the answer is yes, then the equilibrium violates the Intuitive Criterion. (And we can safely refine it out.) If the answer is no, then the equilibrium survives the Intuitive Criterion. Take the subset of types of player $i$ for which the off-the-equilibrium message $m$ is not equilibrium dominated, $\\Theta_{i}^{* *}(m),$ and check if the original equilibrium strategy profile $\\left(m^{*}, a^{*}\\right),$ with associated equilibrium payoff for the informed player $u_{i}^{*}\\left(\\theta_{i}\\right),$ satisfies: $$ \\underbrace{\\min _{a \\in A^{*}\\left(\\theta_{i}^{*}(m), m\\right)} u_{i}\\left(m, a, \\theta_{i}\\right)}_{\\text {Lowest Payoff from Deviating to } m}\u003e\\underbrace{U_{i}^{*}\\left(\\theta_{i}\\right)}_{\\text {Equil. Payott }} $$ where $A^{*}\\left(\\Theta_{i}^{* *}(m), m\\right)$ denotes the set of best responses of the uninformed player $j,$ who is now focusing on the subset of types $\\Theta_{i}^{* *}(m)$ after applying Step 1. If there is a type for which this condition holds, then the equilibrium strategy profile $\\left(m^{}, a^{}\\right)$ violates the Intuitive Criterion. Imagine the possible speech to the uninformed player from the informed player who has incentives to deviate: “It is clear that my type is in $\\Theta_{i}^{* *}(m)$. If my type was outside $\\Theta_{i}^{* *}(m)$ । would have no chance of improving my payoff over what I can obtain at the equilibrium (condition (1)). We can therefore agree that $m$ y type is in $\\Theta_{i}^{* *}(m) .$ Hence, update your beliefs as you wish, but restricting my type to be in $\\Theta_{i}^{* *}(m)$. Given these beliefs, any of your best responses to my message improves my payoff over what 1 would obtain with my equilibrium strategy (condition (2)). For this reason, 1 am sending you such off-the-equilibrium message.” ","date":"0001-01-01","objectID":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/:3:2","tags":null,"title":"","uri":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/"},{"categories":null,"content":"4.3.3 Example: Binary Types + Binary Actions Let us come back to the signally game between a monetary authority and a labor union: The only two strategy profiles that can be support as a PBE are A pooling PBE with both types choosing (High, High) with $\\gamma\u003c1 / 2 ;$ and A separating PBE with $\\left(L o w^{S}, H i g h^{W}\\right)$ Let us check if (High, High) survives the Intuitive Criterion. \r First Step: which types of monetary authority have incentives to deviate towards Low inflation? Low inflation is an off-the-equilibrium message. Let us first apply condition (1) to the Strong type $$ \\begin{aligned} \\underbrace{u_{i}^{*}(\\text { High } | \\text { Strong })}_{\\text {Equil. Payoff }}\u0026\u003c\\underbrace{\\max _{a_{j}} u_{i}(\\text { Low } | \\text { Strong })}_{\\text {Highest payoff from deviating to Low inflation }} ?\\ 200\u0026\u003c300 \\end{aligned} $$ Hence, the Strong type of monetary authority has incentives to deviate towards Low inflation. Graphically, we can represent the incentives of the Strong monetary authority to deviate towards Low inflation as follows: \r Let us now check if the Weak type also has incentives to deviate towards Low inflation: $$ \\begin{aligned} \\underbrace{u_{i}^{*}(\\text { High } | \\text { Weak })}_{\\text {Equil. Payoff }}\u0026\u003c\\underbrace{\\max _{a_{j}} u_{i}(\\text { Low } | \\text { Weak })}_{\\text {Highest payoff from deviating to Low inflation }} ?\\ 150\u0026\u003e50 \\end{aligned} $$ Thus, the Weak type of monetary authority does not have incentives to deviate towards Low inflation. Hence, the only type of monetary authority with incentives to deviate is the Strong type, $\\Theta_{i}^{* *}(\\mathrm{Low})={\\text { Strong }}$ Thus, the labor union believes that after observing a “Low-inflation” deviation it must be from the Strong type. Intuition. Imagine the possible speech to the uninformed player from the informed player with incentives to deviate (the Strong type): “It is now clear that my type is in $\\Theta_{i}^{* *}(L O W)={\\text { Strong }}$. Given this new belief, your best response to my message improves my payoff over what I would obtain with my original equilibrium strategy (condition (2)). For this reason, 1 am sending you such off-the-equilibrium message of Low inflation.” Second Step: to check if there is a type of monetary authority and a message it could use such that condition (2) is satisfied: $$ \\begin{aligned} \\min {a \\in A^{-}\\left(\\Theta ;^{*}(m), m\\right)} u{i}\\left(m, a, \\theta_{i}\\right) \u0026\u003eu_{i}^{*}\\left(\\theta_{i}\\right) ? \\ 300 \u0026\u003e200 \\end{aligned} $$ Therefore, the pooling PBE of (High, High) with $\\gamma\u003c1 / 2$ violates the Intuitive Criterion: There exists a type of informed player (Strong type) and a message (Low inflation announcement) which gives to this informed player a higher payoff than in the original equilibrium. In the binary example, there is only one equilibrium of this signalling game that survives the Intuitive Criterion,: the separating PBE with (Low, High): Why don’t we need to apply the Intuitive Criterion to the separating PBE to check if this equilibrium survives? This is because in the separating PBE there is no off-the-equilibrium message, since both messages are used by either type of the informed player. So the two-step checking is already included in the process when we establish this separating PBE. Recall that in the Intuitive Criterion we start by checking if an informed player has incentives to deviate towards an off-the-equilibrium message, then we restrict the uninformed player’s off-the-equilibrium beliefs. Our previous result implies that, in signalling games with the binary structure (two types of the informed player and two available messages for each type), the separating PBE always survives the Intuitive Criterion. Question and exercise: what if two types of the informed player can choose among three or more possible messages? e.g. Type t1 uses message m1, type t2 uses message m2, and nobody uses message m3. Then m3 is an off-the-equilibrium message. In this case","date":"0001-01-01","objectID":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/:3:3","tags":null,"title":"","uri":"/4.-dynamic-games-with-incomplete-information-and-perfect-bayesian-equilibrium/"},{"categories":null,"content":"5. Information Economics: Signalling and Screening Information economics: extensive-form (dynamic) games with incomplete (asymmetric) information. Sequential rationality + consistent beliefs ) Perfect Bayesian Equilibrium Signalling (MWG 13.B, 13.C, JR 8.1.1, 8.1.2) Competitive Screening (MWG 13.D, JR 8.1.3) (NOTE: this is to be distinguished from “monopolistic screening” of MWG 14.C!) Asymmetric information exists among agents when one or more agents possess information that others don’t, and it affects agents’ payoffs in the game. ","date":"0001-01-01","objectID":"/5.-information-economics-signalling-and-screening/:0:0","tags":null,"title":"","uri":"/5.-information-economics-signalling-and-screening/"},{"categories":null,"content":"5.1 Adverse Selection ","date":"0001-01-01","objectID":"/5.-information-economics-signalling-and-screening/:1:0","tags":null,"title":"","uri":"/5.-information-economics-signalling-and-screening/"},{"categories":null,"content":"5.1.1 The Market for Lemons (Akerlof 1972) The adverse selection problem One party/player has hidden characteristics (about oneself), where asymmetric information exists before the parties enter into a relationship. e.g. In the trading game, seller has information about what good he/she is (potentially) selling, before seeing the price and trading with the buyer. The Market for Lemons (Akerlof 1972) In the market, buyers often do not observe the quality of the cars, which is privately information of the sellers. Due to the common existence of low quality used cars (the “lemons”), buyers will be reluctant to pay high price for a high quality car (the “orange”), since they cannot tell its quality. As a consequence of low market prices, high quality sellers are driven out of the market (they lose if they sell), and whoever sells on the market is more likely to be selling a low quality car. The market mechanism selects the adverse (bad) type of the good, thus it is called “adverse selection.” Imagine a decision task. Half of you will act as a “Seller” and the other half of you a “Buyer.” Each seller will be assigned a good that values $v_{s}$ to the himself/herself. The value of $v_{s}$ depends on the type of the good one receives: orange or lemon. An orange values $v_{s}=20$ and a lemon values $v_{s}=5$ to the seller. Half of the sellers will receive an orange and the other half will receive a lemon. But whoever has received orange or lemon is one’s private information. Each buyer’s value of the good is $v_{b} .$ An orange values $v_{b}=30$ and a lemon values $v_{b}=7$ to the buyer. The timing and decisions Each buyer proposes a price $P$ at which you would like to purchase. This price will then be communicated to a corresponding seller. The seller observes the price $P$ and decide “Sell” or “Not-Sell.” Payoffs If the seller chooses “Not-Sell” then both of them get 0 (no trade). If the seller chooses “Sell” then trade happens and the buyer pays the price $P$ to the seller. If the seller had an orange, seller’s net gain is $P-20$ and buyer’s net gain $30-P$ If the seller had a lemon, seller’s net gain is $P-5$ and buyer’s net gain $7-P$. Efficiency: since $v_{b}\u003ev_{s}$ for both oranges and lemons, if trade happens in both markets, all sellers and buyers enjoy gain from trade $10+2=12$. Question: will trade happen for both orange and lemon between rational sellers and buyers? Ans: No. With buyers' rational expectation, there will be no trade on the market for oranges. First, will buyer offer a price $P\u003e20 ?$ No. With 0.5 probability buyer will face a seller with a lemon; $7-P\u003c0$ if $P\u003e20$ Buyer at most willing to offer a price $P \\leq 0.5 \\cdot 30+0.5 \\cdot 7=18.5$ Will the orange seller sell at such price? No. Seller would rather not trade: $$ P-20\u003c0 \\text { if } P \\leq 18.5 $$ The only PBE (also SPNE) outcome of the game is $P \\leq 18.5,$ orange seller does not sell, and lemon seller sells. The gain from trade is only 2 and the market does not achieve efficiency. One of the players knows something pertinent to the outcomes that the other player don’t know: An employer knows much less about the skills of a potential employee than does the employee himself, An insurance company knows much less about health or driving skills of someone applying for medical or auto insurance than does the applicant. The seller of a used car knows a lot about the car from long experience while a potential buyer can at best get a little information by inspection. ","date":"0001-01-01","objectID":"/5.-information-economics-signalling-and-screening/:1:1","tags":null,"title":"","uri":"/5.-information-economics-signalling-and-screening/"},{"categories":null,"content":"5.1.2 Solutions to adverse selection Signaling: the seller of a high quality car can (costly) signal to potential buyers by offering a long warranty, which is not affordable to low quality car sellers, as a way to distinguish them from low quality car sellers. e.g. 保修 Screening: the uninformed party offers a term of exchange such that the informed party can choose to accept or reject. e.g. An uninformed buyer can ask for a long enough warranty to screen out low quality sellers. Or an uninformed insurance company, who is uninformed of health conditions of the clients, offers different insurance packages, and ideally, each type of clients only find one package acceptable. Nobel prize (2001) in economics was awarded to George Akerlof, Michael Spence, and Joseph Stiglitz for their contribution on signaling and screening models. ","date":"0001-01-01","objectID":"/5.-information-economics-signalling-and-screening/:1:2","tags":null,"title":"","uri":"/5.-information-economics-signalling-and-screening/"},{"categories":null,"content":"5.2 Job Market Signaling ","date":"0001-01-01","objectID":"/5.-information-economics-signalling-and-screening/:2:0","tags":null,"title":"","uri":"/5.-information-economics-signalling-and-screening/"},{"categories":null,"content":"5.2.1 Assumption One way to resolve the inefficient market equilibria caused by asymmetric information. Key idea of the job-market signaling model A worker knows about his/her ability while the firm does not. The worker can use education level to “signal” ability. Education is time-consuming and effort-taking. Hence it’s more painful for the low-ability workers. If the firm sets up the wages appropriately depending on education levels, then the high-ability workers are able to distinguish themselves out, even if education could be otherwise useless. We consider a model with two types of workers, $\\theta_{H}$ and $\\theta_{L},$ where $$ \\theta_{H}\u003e\\theta_{L}\u003e0 \\text { and } \\lambda=\\operatorname{Pr}\\left(\\theta=\\theta_{H}\\right) \\in(0,1) $$ Before entering the job market, a worker can get some education. **The amount of education a worker receives is observable.** Education does nothing for a worker’s productivity. Assume “useless education.” The cost of obtaining education level $e$ for a type $\\theta$ worker is given by the twice continuous differentiable function $c(e, \\theta),$ with $c(0, \\theta)=0 .$ Zero education, zero cost for both types. $c_{e}(e, \\theta)\u003e0$ and $c_{e e}(e, \\theta)\u003e0 .$ cost is increasing (marginal cost positive) and convex (marginal cost increasing) with education level. $c_{\\theta}(e, \\theta)\u003c0$ for all $e\u003e0 .$ cost decreases with ability (type); i.e. a given level (e.g. $e=16$ yrs) of education is less costly for a high-ability and more costly for a low-ability worker: $c\\left(16, \\theta_{H}\\right)\u003cc\\left(16, \\theta_{L}\\right)$ $c_{e \\theta}(e, \\theta)\u003c0 .$ High-ability worker has lower marginal cost of education than Iow-ability worker does; i.e. $$ \\frac{\\partial c\\left(e, \\theta_{H}\\right)}{\\partial e}\u003c\\frac{\\partial c\\left(e, \\theta_{L}\\right)}{\\partial e} $$ The utility of a type $\\theta$ worker who chooses education level $e$ and receives wage $w$ is $u(w, e | \\theta)=w-c(e, \\theta)$ A worker of type $\\theta$ can earn $r(\\theta)$ by working at home. We assume that $r\\left(\\theta_{H}\\right)=r\\left(\\theta_{L}\\right)=0$ (for simplicity) In the absence of the ability of signal, all workers are employed at $w^{*}=E[\\theta],$ which is Pareto inefficient. Note: we model a single worker of unknown type, but this could be thought of as multiple workers. \r For each type $\\theta$, one indifference curve represents a utility level $\\bar{u}$ of this worker, i.e. $$ w-c(e, \\theta)=\\bar{u} \\Rightarrow \\frac{\\partial w}{\\partial e}=c_{e}(e, \\theta) $$ Indifference curves are also convex since $c_{e e}(e, \\theta)\u003e0$ The property that $\\frac{\\partial c\\left(e, \\theta_{H}\\right)}{\\partial e}\u003c\\frac{\\partial c\\left(e, \\theta_{L}\\right)}{\\partial e}$ implies type $\\theta_{H}$ ’s indifference curves are flatter and type $\\theta_{L}$ ’s indifference curves are steeper. Exercise: A Numerical Example $$ u(w, e | \\theta)=w-\\frac{e^{3}}{\\theta} $$ That is, the cost $c(e, \\theta)=e^{3} / \\theta .$ Check for the 4 assumptions about the cost function. Fixed a value of $\\theta,$ draw the graph of $c(e, \\theta)$ as a function of $e$ Suppose $\\theta_{L}=2, \\theta_{H}=4 .$ Draw the families of indifference curves for both types of workers. For each type of workers, what are the utility levels represented by the indifference curve passing through point $(e=0, w=2),(e=2, w=4)$ or $(e=\\sqrt[3]{4}, w=4) ?$ Timeline of the dynamic game Random move of nature determines the type of the worker. Conditional on her type, the worker chooses how much education to obtain. The worker enters the job market. Conditional on the observed education level, two firms simultaneously make wage offers. \r Worker decides whether to work and, if so, for which firm. We look at perfect Bayesian equilibrium and focus on pure strategies. A set of strategies and a belief function $\\mu\\left(\\theta_{H} | e\\right) \\in[0,1]$ giving the firms' common probability assessment that the worker is of high ability after observing $e$ is a Perfect Bayesian Equilibrium if T","date":"0001-01-01","objectID":"/5.-information-economics-signalling-and-screening/:2:1","tags":null,"title":"","uri":"/5.-information-economics-signalling-and-screening/"},{"categories":null,"content":"5.2.2 Separating PBE “Perfect Bayesian Equilibria with Separating Types\": two types of workers choose different education levels. 【Lemma 1】In any separating perfect Bayesian equilibrium, $w^{}\\left(e^{}\\left(\\theta_{H}\\right)\\right)=\\theta_{H}$ and $w^{*}\\left(e^{*}\\left(\\theta_{L}\\right)\\right)=\\theta_{L}$ Proof: In any PBE, beliefs on the equilibrium path must be correctly derived from the equilibrium strategies using Bayes' rule. Upon seeing education level $e^{*}\\left(\\theta_{L}\\right),$ firms must assign probability 1 to the worker being type $\\theta_{L}$. So $w^{*}\\left(e^{*}\\left(\\theta_{L}\\right)\\right)=\\theta_{L} .$ Otherwise, firms will either have negative profit, or the low-ability worker reject the offer. similarly, upon seeing $e^{*}\\left(\\theta_{H}\\right),$ assign probability 1 to the worker being type $\\theta_{H}$ 【Lemma 2】In any separating perfect Bayesian equilibrium, $e^{*}\\left(\\theta_{L}\\right)=0 ;$ that is, a low-ability worker chooses to get no education. Proof and an illustration: Suppose not. Suppose that $\\theta_{L}$ chooses $\\hat{e}\u003e0$. If this is an equilibrium, then the low-ability worker will receive $\\theta_{L}$ according to the previous lemma. However, if $e=0$, the worker can still receive at least $\\theta_{L}$ (for any belief on $\\left.\\theta_{L} \\text { and } \\theta_{H}\\right)$.That presents a profitable deviation. Let us construct a few separating PBEs. The equilibrium needs to satisfy: For the firm (uninformed player), we need to figure out the consistent belief along equilibrium path (Bayes' Rule), the optimal response of wage schedule $w(e)$ given this belief, and beliefs off-the-equilibrium. For each type of worker (informed player), given $w(e),$ no deviation to choose the other type’s education level, or any off-the-equilibrium education level. (Question: what properties needed for $w(e)$ to ensure this no-deviation of each-type-worker?) Note: in the graphs we use abbreviation $e_{L}$ to represent $e^{*}\\left(\\theta_{L}\\right)$ and $e_{H}$ to represent $e^{*}\\left(\\theta_{H}\\right)$. Separating PBE 1 With One Possible Wage Schedule Firm’s beliefs On-the-equilibrium: after observing $e_{L}(=0)$ or $e_{H}(\u003e0),$ firm assigns $\\mu^{*}\\left(\\theta_{H} | e_{L}\\right)=0$ and $\\mu^{*}\\left(\\theta_{H} | e_{H}\\right)=1$ Off-the-equilibrium: if observing an education level $e \\neq e_{L}, e \\neq e_{H}$ and $e\u003e0,$ firm assigns $\\mu^{*}\\left(\\theta_{H} | e\\right) \\in[0,1] .$ **Arbitrary belief as in PBE, but to be further restricted later (by the Intuitive Criterion)**. Firm’s optimal response $w^{*}(e)$ given the belief In general, conditional on each possible education choice $e,$ wage schedule satisfies $w^{}(e)=\\mu^{}\\left(\\theta_{H} | e\\right) \\cdot \\theta_{H}+\\left(1-\\mu^{*}\\left(\\theta_{H} | e\\right)\\right) \\cdot \\theta_{L}$ On-the-equilibrium: $w^{*}\\left(e_{L}\\right)=\\theta_{L}$ and $w^{*}\\left(e_{H}\\right)=\\theta_{H}$ Off-the-equilibrium: $w^{*}(e) \\in\\left[\\theta_{L}, \\theta_{H}\\right],$ plus some additional properties (see the illustration). For the low type $e^{*}\\left(\\theta_{L}\\right)=0,$ the Incentive Compatibility conditions: Choosing any other $e\u003e0, \\neq e_{H}$ is more costly (lower utility) to acquire, the wage schedule should make it indeed optimal for $\\theta_{L}$ to choose $e^{*}\\left(\\theta_{L}\\right)=0$ In particular, $\\theta_{L}$ should have NO incentive to mimic the high type $\\theta_{H}$; i.e. by acquiring education level $e_{H}$ and being paid $w^{*}\\left(e_{H}\\right)=\\theta_{H},$ type $\\theta_{L}$ does not get higher utility. In the graph, deviation to education level $e_{1}\u003e0$ or $e_{2}\u003e0$ are unprofitable. The indifference curves lying on $(w, e)$ -pairs $A$ and $B$ should be associated to a lower utility level than the indifference curve passing through$\\left(w=\\theta_{L}, e=0\\right)$ \r For the high type $e^{*}\\left(\\theta_{H}\\right)\u003e0,$ the Incentive Compatibility conditions: He chooses the prescribed education level $e_{H}\u003e0$ there is NO incentive to mimic the low-t","date":"0001-01-01","objectID":"/5.-information-economics-signalling-and-screening/:2:2","tags":null,"title":"","uri":"/5.-information-economics-signalling-and-screening/"},{"categories":null,"content":"5.2.3 Pooling Equilibria “Perfect Bayesian Equilibria with Pooling Types”: two types of workers choose same education level; i.e.$e^{*}\\left(\\theta_{L}\\right)=e^{*}\\left(\\theta_{H}\\right)=e^{*}$ In any pooling equilibrium, firms' belief and wage on-the-equilibrium (upon observing $e^{*}$ ): $$ \\begin{aligned} \\mu\\left(\\theta_{H} | e^{*}\\right) \u0026=\\lambda \\ w^{*}\\left(e^{*}\\right) \u0026=\\lambda \\cdot \\theta_{H}+(1-\\lambda) \\cdot \\theta_{L}=E(\\theta) \\end{aligned} $$ Firm’s belief and wage off-the-equilibrium: upon observing $e=0: \\mu\\left(\\theta_{H} | e=0\\right)=0$ and $w^{*}(e=0)=\\theta_{L}$ upon observing $0\u003ce \\neq e^{*}: \\mu\\left(\\theta_{H} | e\\right) \\in[0,1]$ and $w^{*}(e) \\in\\left[\\theta_{L}, \\theta_{H}\\right]$ with the additional properties that $w^{*}(e)$ lies below both types' indifference curves. \r Consider the pooling education level $e^{}$ such that $E(\\theta)-c\\left(e^{}, \\theta_{L}\\right)=\\theta_{L}-c\\left(0, \\theta_{L}\\right)$ The low type $\\theta_{L}$ ’s Incentive Compatibility conditions: Given $w^{}(e),$ deviation from the pooling education level $e^{}$ (e.g. to $e_{1}$ or $e_{2}$ ) yields $(w, e)$ -pairs $C$ or $D,$ which entails a lower utility than point $A$ $\\left(\\left(E(\\theta), e^{*}\\right)\\right)$ In particular, deviating to $e=0$ does not result in a strictly higher utility for type $\\theta_{L}$ \r The high type $\\theta_{H}$ ’s Incentive Compatibility conditions: Given $w^{}(e),$ deviation from $e^{}$ (e.g. to $e_{1}$ or $e_{2}$ ) is also unprofitable; i.e. $(w, e)$ -pairs $\\mathrm{C}$ or $\\mathrm{D}$ are both below high-ability type’s indifference curve which passes through point $\\mathrm{A}\\left(\\left(E(\\theta), e^{*}\\right)\\right)$ \r Finally, we have \r There are also multiple pooling PBEs of the job-market signalling game: Same pooling strategies of the workers (choice of education levels), but with a different off-equilibrium wage schedule. As long as $w^{*}(e)$ lies below both types' indifference curves. Pooling equilibria with a difference choice of the pooling education level $e^{\\prime}$. Any education level $e^{\\prime} \\in\\left[0, e^{*}\\right]$ can be supported as different pooling PBEs. Intuition, if in a pooling equilibrium with some $e^{\\prime}\u003ce^{}$, both types would actually be happier than in the $e^{}$ case: they would still receive $w=E(\\theta)$ while incur less education cost. The education levels that cannot be sustained as a pooling PBE: $e\u003ee^{},$ low-ability type would rather deviate to $e=0$ and receive $w^{}(e=0)=\\theta_{L} . $ An illustration. Again, we can Pareto rank all the pooling equilibria: $e^{\\prime}=0$ no education (no-signaling-via-edu) dominates all the others. ","date":"0001-01-01","objectID":"/5.-information-economics-signalling-and-screening/:2:3","tags":null,"title":"","uri":"/5.-information-economics-signalling-and-screening/"},{"categories":null,"content":"5.2.4 Intuitive Criterion Refinement There are multiple equilibria (both separating and pooling equilibria). We are going to adopt the Intuitive Criterion to refine them. The multiplicity stems from the freedom to choose beliefs off the equilibrium path. Can we put any reasonable restrictions on such beliefs? e.g. Consider one separating PBE in which $\\left{e^{*}\\left(\\theta_{L}\\right)=0, e^{*}\\left(\\theta_{H}\\right)=\\hat{e}_{H}\\right} .$ To sustain $\\hat{e}_{H}$ as the equilibrium education level of high-ability workers, firms must believe that any worker with an education level $e \\in\\left[\\tilde{e}_{H}, \\hat{e}_{H}\\right]$ has a positive probability of being of type $\\theta_{L} .$ Is this a reasonable belief? Note that a type $\\theta_{L}$ worker could never be made better off choosing such an education level than she is getting at $e=0,$ regardless of what firms believe about her. If workers with high productivity expect $ w(e)=\\theta_L $, for $\\tilde{e}{H}\u003ce\u003c\\hat{e}{H}$. Than he has incentive to move from $\\hat{e}_H$ to $e$. So for $\\tilde{e}{H}\u003ce\u003c\\hat{e}{H},$ any belief other than $\\mu\\left(\\theta_{H} | e\\right)=1$ seems unreasonable. Only the high-ability worker would deviate to education level $e$. This implies the firm to set a wage such that $w(e)=\\theta_{H}$ Separating PBEs Let us check it the separating equilibrium $e_{L}^{*}=0$ and $e_{H}^{*}=e_{2}$ survives the Intuitive Criterion. Hence, let us consider any off-the-equilibrium message $e \\in\\left(e_{1}, e_{2}\\right) .$ (Green color) \r The $\\theta_{L}$ -type of worker doesn’t have incentives to deviate towards e since: $$ \\underbrace{u_{L}^{*}\\left(\\theta_{L}\\right)}_{\\text {Equil. Payoff }}\u003e\\underbrace{\\max _{w \\in W^{*}(\\theta, m)} u_{L}\\left(e, w, \\theta_{L}\\right)}_{\\text {Highest payoff from deviating towards } e} $$ [Given that any Indifference Curve passing through any $e \\in\\left(e_{1}, e_{2}\\right) \\text { and } w=\\theta_{H} $ lies below the equilibrium $IC_L$]. But the $\\theta_{H}$ -type of worker has incentives to deviate: $$ \\underbrace{u_{i}^{*}\\left(\\theta_{H}\\right)}_{\\text {Equil. payoff }}\u003c\\underbrace{\\max _{w \\in W^{*}(\\theta, e)} u_{H}\\left(e, w, \\theta_{H}\\right)}_{\\text {Highest payoff of deviating towards } e} $$ [since any Indifference Curve passing through any $e \\in\\left(e_{1}, e_{2}\\right)$ and $\\left.w=\\theta_{H} \\text { lies above the equilibrium } I C_{H}\\right]$ Therefore, education levels in $e \\in\\left(e_{1}, e_{2}\\right)$ can only come from the $\\theta_{H}$ -worker, and $\\Theta^{* *}(e)=\\left{\\theta_{H}\\right}$ Given that $e$ only comes from $\\theta_{H},$ the firm offers a wage $w(e)=\\theta_{H}$ after observing $e$ $$ \\underbrace{\\min _{w \\in W^{*}\\left(\\Theta^{* *}(e), e\\right)} u_{H}\\left(e, w, \\theta_{H}\\right)}_{\\theta_{H}-c\\left(e, \\theta_{H}\\right)}\u003e\\underbrace{u_{H}^{*}\\left(\\theta_{H}\\right)}_{\\theta_{H}-c\\left(e_{2}, \\theta_{H}\\right)} $$ since, $e_{2}\u003ee,$ then $c\\left(e_{2}, \\theta_{H}\\right)\u003ec\\left(e, \\theta_{H}\\right) .$ Hence $$ \\theta_{H}-c\\left(e, \\theta_{H}\\right)\u003e\\theta_{H}-c\\left(e_{2}, \\theta_{H}\\right) $$ Intuitively, the lowest payoff that the $\\theta_{H}$ -worker obtains by deviating towards $e$ is higher than his equilibrium payoff. Therefore, the separating PBE $$ \\left{e_{L}^{*}\\left(\\theta_{L}\\right), e_{H}^{*}\\left(\\theta_{H}\\right)\\right}=\\left{0, e_{2}\\right} $$ violates the Intuitive Criterion. Finally,we can find that the only separating PBE that survive the intuitive criterion is $$ \\left{e_{L}^{*}\\left(\\theta_{L}\\right), e_{H}^{*}\\left(\\theta_{H}\\right)\\right}=\\left{0, e_{1}\\right} $$ Pooling PBEs Which of all the poollng PBEs we found survive the Intuitive criterion? None! \r 1st Step: If the firm observes an off-the-equilibrium message $e^{\\prime}(\\text { see next figure }),$ it can understand that such a message is never equilibrium deviated for the high-productive worker since $$ u^{}\\left(E[\\theta], e^{} | \\theta_{H}\\right)\u003c\\max _{w} u\\left(w, e^{\\prime} | \\theta_{H}\\right) $$ while this inequal","date":"0001-01-01","objectID":"/5.-information-economics-signalling-and-screening/:2:4","tags":null,"title":"","uri":"/5.-information-economics-signalling-and-screening/"},{"categories":null,"content":"5.3 Asymmetric Information and Screening The major problem with using education as a signaling device: multiple equilibria, some of which are not Pareto efficient. An alternative approach: uninformed parties take steps to try to distinguish, or screen, the various types of individuals on the other side of the market. Firms can design multiple contracts for the workers to choose. Each contract consists of a wage level and a task with certain difficulty. No pooling equilibrium exists. Low-ability can never pretend to be a high-ability and get high wage. The high-ability can distinguish themselves from the low-ability by choosing the high wage and harder task. ","date":"0001-01-01","objectID":"/5.-information-economics-signalling-and-screening/:3:0","tags":null,"title":"","uri":"/5.-information-economics-signalling-and-screening/"},{"categories":null,"content":"5.3.1 Assumption Suppose there are only two types of workers, $\\theta_{L}$ and $\\theta_{H},$ with $\\theta_{H}\u003e\\theta_{L}$ and where the fraction of workers of type $\\theta_{H}$ is $\\lambda \\in(0,1)$. Further assume that $r\\left(\\theta_{L}\\right)=r\\left(\\theta_{H}\\right)=0$ Suppose that jobs may differ in the “task level” required of the worker. We assume that higher task levels add nothing to the output of the worker, and their only effect is to lower the worker’s utility. This is to say that the output of type $\\theta$ worker remains to be $\\theta$ regardless of the task level. Utility of a type $\\theta$ worker who receives $w$ and faces task level $t \\geq 0$ is $u(w, t | \\theta)=w-c(t, \\theta)$ We assume that $c(0, \\theta)=0, c_{t}(t, \\theta)\u003e0, c_{t t}(t, \\theta)\u003e0, c_{\\theta}(t, \\theta)\u003c0$ for all $t\u003e0,$ and $c_{t \\theta}(t, \\theta)\u003c0$ Timing of the game: Two firms simultaneously announce sets of offered contracts. A contract is pair $(w, t) .$ Each firm may announce any finite number of contracts. Given the offers made by the firms, workers of each type choose whether to accept a contract and, if so, which one. Indifference is resolved in favor of lower task level and employment. Indifference between firms is resolved by a flip of a coin. Note: since the firms move before workers' move, there is no information based on which the firms would update “belief”. ","date":"0001-01-01","objectID":"/5.-information-economics-signalling-and-screening/:3:1","tags":null,"title":"","uri":"/5.-information-economics-signalling-and-screening/"},{"categories":null,"content":"5.3.2 Equilibrium 【Lemma (13.D.1)】In any equilibrium, whether pooling or separating, both firms must earn zero profits. Proof: Let $\\left(w_{L}, t_{L}\\right)$ and $\\left(w_{H}, t_{H}\\right)$ be the contracts chosen by the low- and high-ability workers. They could be the same. Suppose $\\Pi\u003e0$ is the aggregate profits. Firms are making positive profit. Consider that this firm deviates and offer $\\left(w_{L}+\\epsilon, t_{L}\\right)$ and $\\left(w_{H}+\\epsilon, t_{H}\\right)$ for $\\epsilon\u003e0 .$ The contracts will attract all respective types of workers. Incentive compatibility ( $i$, j be the two types of workers): $$ w_{i}-c\\left(t_{i}, \\theta_{i}\\right) \\geq w_{j}-c\\left(t_{j}, \\theta_{i}\\right) \\Rightarrow\\left(w_{i}+\\epsilon\\right)-c\\left(t_{i}, \\theta_{i}\\right) \\geq\\left(w_{j}+\\epsilon\\right)-c\\left(t_{j}, \\theta_{i}\\right) $$ For $\\epsilon$ arbitrarily small, the firm’s profit will be sufficiently close to $\\Pi$. This implies that $\\Pi \\leq 0$, but note that not offering any contract guarantees zero profit. 【Lemma (13.D.2)】No pooling equilibria exist. Proof: suppose that firm $j$ is offering contract $\\left(w_{p}, t_{p}\\right)$ Then firm $k \\neq j$ can profitably deviate by offering $(\\tilde{w}, \\tilde{t}),$ where $\\tilde{w}\u003c\\theta_{H}$ This contract will attract all type $\\theta_{H}$ workers and none of type $\\theta_{L}$, who prefer $\\left(w_{p}, t_{p}\\right)$ to $(\\tilde{w}, \\tilde{t})$ Furthermore, $\\tilde{w}\u003c\\theta_{H}$ implies strictly positive profits. \r 【Lemma (13.D.3)】If $\\left(w_{L}, t_{L}\\right)$ and $\\left(w_{H}, t_{H}\\right)$ are the contracts signed by the low- and high-ability workers in a separating equilibrium, then (for each firm) both contracts yield zero profits; that is, $w_{L}=\\theta_{L}$ and $w_{H}=\\theta_{H}$ Proof: Suppose first that $w_{L}\u003c\\theta_{L}$. Then either firm could earn strictly positive profits by instead offering only contract $\\left(\\tilde{w}{L}, t{L}\\right),$ where $\\theta_{L}\u003e\\tilde{w}_{L}\u003ew_{L}$ All low-ability workers would accept this contract. The deviating firm earns strictly positive profits from any worker (of low or high ability) who accepts it. The above lemma says this is impossible. So, we must have $w_{L} \\geq \\theta_{L}$ in any separating equilibrium. But, $w_{L}\u003e\\theta_{L}$ will result in a profit loss. Next, suppose that $w_{H}\u003c\\theta_{H}$. The fact that we have a separating equilibrium implies that $w_{L}$ must lie in the shaded region in the next figure (each type $\\theta_{H}, \\theta_{L}$ prefers the contract designed for him/her to the contract of the other type.) suppose firm $j$ is offering $\\left(w_{L}, t_{L}\\right)$ in the next figure. Then, firm $k \\neq j$ could earn strictly positive profits by deviating and offering only a contract such as $(\\tilde{w}, \\tilde{t})$ This implies we must have $w_{H} \\geq \\theta_{H} .$ But, $w_{H}\u003e\\theta_{H}$ will result in a profit loss. Breaking even further implies that $w_{L}=\\theta_{L}$ and $w_{H}=\\theta_{H}$ \r We then identify properties of contracts in any separating equilibrium: 【Proposition】In any subgame perfect Nash equilibrium of the screening game, low-ability workers accept contract $\\left(\\theta_{L}, 0\\right),$ and high-ability workers accept contract $\\left(\\theta_{H}, \\hat{t}_{H}\\right),$ where $\\hat{t}_{H}$ satisfies $\\theta_{H}-c\\left(\\hat{t}_{H}, \\theta_{L}\\right)=\\theta_{L}-c\\left(0, \\theta_{L}\\right)$ Proof: If the low-ability is offered a contract with $t_{L}^{\\prime}\u003e0$, then one firm can make strictly positive payoff by offering a contract with a lower. So we know $\\left(w_{L}, t_{L}\\right)=\\left(\\theta_{L}, 0\\right)$ and that $w_{H}=\\theta_{H}$ Note that low-ability workers are indifferent between contracts $\\left(\\theta_{L}, 0\\right)$ and $\\left(\\theta_{H}, \\hat{t}_{H}\\right),$ and so $\\theta_{H}-c\\left(\\hat{t}_{H}, \\theta_{L}\\right)=\\theta_{L}-c\\left(0, \\theta_{L}\\right)$ Suppose that the high-ability contract $\\left(\\theta_{H}, t_{H}\\right)$ has $t_{H}\u003e\\hat{t}_{H}$ . Then, there is a profitable deviation.","date":"0001-01-01","objectID":"/5.-information-economics-signalling-and-screening/:3:2","tags":null,"title":"","uri":"/5.-information-economics-signalling-and-screening/"},{"categories":null,"content":"6. Contract Theory Moral hazard: the principal-agent problem (MWG 14.B, JR 8.2) Monopolistic screening (MWG 14.C) Different from the labor market competitive screening model (MWG 13.D, multiple firms), in the monopolistic screening model we have a monopolistic employer. ","date":"0001-01-01","objectID":"/6.-contract-theory/:0:0","tags":null,"title":"","uri":"/6.-contract-theory/"},{"categories":null,"content":"6.1 Moral Hazard Problem Moral hazard models hidden action, where asymmetric information forms after the parties enter into a relationship. Consider that a firm/employer/dean (the principal) hires a worker/manager/professor (the agent) to work on a project, which succeeds with some probability if the worker/manager/professor exerts effort. If the firm only observes the outcome of the project but not the agent’s effort level. In such a situation, the firm’s payment contract can only depend on the outcome, which is an imperfect indicator of the worker’s effort level. If the worker is paid fixed wage or if the payment conditional on success is not high enough, since effort is costly, the worker will shirk – moral hazard arises. ","date":"0001-01-01","objectID":"/6.-contract-theory/:1:0","tags":null,"title":"","uri":"/6.-contract-theory/"},{"categories":null,"content":"6.1.1 Setup Time Structure Principal offers a contract. Agent decides to accept or reject the contract. Upon acceptance, the agent exerts a non-observable effort level e. Nature determines how effort transforms into principal’s profit. The agent selects an effort level $e \\in \\mathbb{R}_{+}$ Profits $\\pi \\in[\\underline{\\pi}, \\bar{\\pi}]$ are affected by effort $e$, as follows $$ f(\\pi | e)\u003e0, \\forall e\u003e0 $$ which indicates that a given profit value $\\pi$ can arise from any effort level $e$. e.g. a million dollars in profit can arise from a high effort (with a higher conditional probability, but it can also arise from a low effort level (which means the agent is really lucky.. and it occurs with a lower conditional probability) For simplicity, we restrict the effort level to be binary $e \\in\\left{e_{L}, e_{H}\\right}$. How to make sure there is a conflict between principal’s and agent’s interest? Assuming that a high effort is more likely to yield a high profit than a low effort does The principal seeks to induce a high effort, while the agent would prefer a low effort (if he receive the same salary, to shirk as much as possible) How should we put this assumption more formally? we would like to use the notion of First Order Stochastic Dominance (FOSD). $$ \\begin{aligned} F\\left(\\pi | e_{H}\\right)\u0026\u003cF\\left(\\pi | e_{L}\\right) \\text { for all profits } \\pi \\in[\\pi, \\bar{\\pi}] \\ \\text{or } 1-F\\left(\\pi | e_{H}\\right) \u0026\u003e 1-F\\left(\\pi | e_{L}\\right) \\text { for all profits } \\pi \\in[\\pi, \\bar{\\pi}] \\end{aligned} $$ that is, the probability that $e_{H}$ induces profits equal to $\\pi$ or higher is larger than that of $e_{L}$ \r ??? Note that by MWG Proposition 6.D.1 (from Adv Micro 1), the above condition (FOSD) is equivalent to: $$ \\int \\pi \\cdot f\\left(\\pi | \\boldsymbol{e}{H}\\right) d \\pi\u003e\\int \\pi \\cdot f\\left(\\pi | e{L}\\right) d \\pi $$ which, in words, says that the expected profits the principal obtains from inducing a high effort are larger than from inducing a low effort. Agent’s payoff His Bernoulli utility function is $u(w, e)=v(w)-g(e),$ where $v(w)$ represents his utility from the salary he receives, and $g(e)$ represents his dis-utility from exerting effort e We assume $v^{\\prime}(w)\u003e0, v^{\\prime \\prime}(w) \\leq 0$ (risk averse), and $g\\left(e_{H}\\right)\u003eg\\left(e_{l}\\right)$ This entails that the agent is risk-averse. Principal’s payoff Her Bernoulli utility function is $\\pi-w$ Thus, the principal is risk-neutral Exercise: risk-averse principal (MWG 14.B.2). ","date":"0001-01-01","objectID":"/6.-contract-theory/:1:1","tags":null,"title":"","uri":"/6.-contract-theory/"},{"categories":null,"content":"6.1.2 Benchmark: Effort is Observable The principal must offer at least a reservation utility level $\\bar{u}$ to the agent. In particular, the principal’s problem is $$ \\begin{array}{cl} \\max _{w(\\pi)} \u0026 \\int \\left(\\pi-w(\\pi)\\right) \\cdot f(\\pi | e) d \\pi \\ \\text { s.t. } \u0026 \\int v(w(\\pi)) \\cdot f(\\pi | e) d \\pi-g(e) \\geq \\bar{u} \\end{array} $$ Re-arrange terms, $$ \\int(\\pi-w(\\pi)) \\cdot f(\\pi | e) d \\pi=\\int \\pi \\cdot f(\\pi | e) d \\pi-\\int w(\\pi) \\cdot f(\\pi | e) d \\pi $$ Then, for a given effort $e$, the above maximization problem is equivalent to the following minimization problem $$ \\begin{array}{cl} \\min {w(\\pi)} \u0026 \\int w(\\pi) \\cdot f(\\pi | e) d \\pi \\ \\text { s.t.} \u0026 \\int v(w(\\pi)) \\cdot f(\\pi | e) d \\pi-g(e) \\geq \\bar{u} \\end{array} $$ Form the Lagrangian function and take first-order condition with respect to $w$ (given each level of $\\pi$, and each $e \\in\\left{e{L}, e_{H}\\right}$ ) yields: $$ \\begin{aligned} \u0026-f(\\pi | e)+\\gamma \\cdot v^{\\prime}(w(\\pi)) \\cdot f(\\pi | e)=0 \\ \\Rightarrow \\quad \u0026 f(\\pi | e) \\cdot\\left[\\gamma \\cdot V^{\\prime}(w(\\pi))-1\\right]=0 \\ \\Rightarrow \\quad \u0026 \\frac{1}{v^{\\prime}(w(\\pi))}=\\gamma \\end{aligned} $$ with $v^{\\prime}(w)\u003e0$, we have $\\gamma\u003e0$, it means that the constraint is binding. $v^{\\prime}(w(\\pi))$ is decreasing in $w,$ because $v^{\\prime \\prime}(w)\u003c0 .$ Therefore, $\\frac{1}{v^{\\prime}(w(x))}$ is increasing in $w$. \r The principal thus provides a “fixed” wage payment $(w(\\pi)$ does not change with $\\pi$ ) that solves $\\frac{1}{v^{\\prime}(w(\\pi))}=\\gamma$ Intuition: A risk-sharing arrangement. the risk-neutral principal offers a contract to the risk-averse agent that guarantees him a fixed payoff $w_{e}^{*}$ (which is still a function of the effort he exerts, because effort is observable in this context), and is not affected by the profit level $\\pi$. Hence, the principal offers the minimum salary $w_{e}^{*}$ that guarantees agent’s acceptance of the contract: $$ v\\left(w_{e}^{*}\\right)-g(e)=\\bar{u} \\Leftrightarrow w_{e}^{*}=v^{-1}(\\bar{u}+g(e)) $$ for each effort level $e \\in\\left{e_{L}, e_{H}\\right} .$ Notationally, instead of writing $\\int v(w(\\pi)) \\cdot f(\\pi | e) d \\pi,$ we write $v\\left(w_{e}^{*}\\right)$ since the principal pays the same salary $w_{e}^{*}$ for all profit levels. \r In addition, since $g\\left(e_{H}\\right)\u003eg\\left(e_{L}\\right),$ we have $$ w_{e_{H}}^{*}=v^{-1}\\left(\\bar{u}+g\\left(e_{H}\\right)\\right)\u003ew_{e_{L}}^{*}=v^{-1}\\left(\\bar{u}+g\\left(e_{L}\\right)\\right) $$ that is, the salary inducing a high effort level should be larger than that inducing a low effort level. A numerical exercise: $v(w)=\\sqrt{w}, g(e)=e^{3}, \\bar{u}=10$ Derive the optimal wages that induce $e_{L}, e_{H},$ respectively. How does the principal’s expected payoff depends on the effort levels? \r Question: which effort level maximizes the principal’s expected payoff? Insert the expression of $w_{i}$ where $e \\in\\left{e_{l}, e_{H}\\right}$ into the principal’s payoff function: $$ \\underbrace{\\int \\pi \\cdot f(\\pi | e) d \\pi}_{\\text {Expected profit at each effort level }}-\\underbrace{v^{-1}(\\bar{u}+g(e))}_{w_{e}^{*} \\text { for each effort level }} $$ the expression represents the expected profit tor the principal minus the fixed salary she has to pay. $e_{H}$ or $e_{L} ?$ It depends. If $e_{H}$ increases the expected profits by a larger extent than the increase in the salary, then it is better for the principal to induce $e_{H}$. Otherwise, it is better for her to induce $e_{L}$. ","date":"0001-01-01","objectID":"/6.-contract-theory/:1:2","tags":null,"title":"","uri":"/6.-contract-theory/"},{"categories":null,"content":"6.1.3 Main Model: Effort is Not Observable In the moral hazard setting where effort $ e $ is not observable, the principal’s contract need to ensure that the agent has sufficient incentives to choose the effort level in equilibrium. In particular, $$ \\begin{aligned} \\min _{w(\\pi)} \u0026 \\int w(\\pi) \\cdot f(\\pi | e) d \\pi \\ \\text { s.t.} \u0026 \\int v(w(\\pi)) \\cdot f(\\pi | e) d \\pi-g(e) \\geq \\bar{u} \\quad \\text{(P.C.)}\\ \\text { e solves } \u0026 \\max _{\\tilde{e} \\in {e_L, e_H}} \\int v(w(\\pi)) \\cdot f(\\pi | \\tilde{e}) d \\pi-g(\\tilde{e}) \\quad \\text{(I.C.)} \\end{aligned} $$ where P.C. denotes the participation constraint condition, and I.C. denotes the incentive compatibility condition. In the following analysis, we will first try to get rid of some of the constraints by understanding which salary will induce $ e_{L} $ and which salary will induce $e_{H}$. What if the principal wishes to choose a salary to induce effort level $e_{L}$ ? To offer the agent fixed wage payment (“fixed” with respect to profit levels $\\pi$) $$ w_{e_{L}}^{*}=v^{-1}\\left(\\bar{u}+g\\left(e_{L}\\right)\\right) $$ Check the PC constraint: the agent shall have utility level exactly $\\bar{u}$ $$ \\begin{aligned} \u0026 \\int v\\left(w_{e_{L}}^{*}\\right) \\cdot f\\left(\\pi | {e}_{L}\\right) d \\pi-g\\left(e_{L}\\right) \\ =\u0026 v\\left(v^{-1}\\left(\\bar{u}+g\\left({e}_{L}\\right)\\right)\\right) \\cdot \\int f\\left(\\pi | {e}_{L}\\right) d \\pi-g\\left(e_{L}\\right)=\\bar{u} \\end{aligned} $$ Then check the IC constraint: the salary would not induce the alternative effort level $e_H$ because $g(e_H) \u003e g(e_L)$. $$ \\begin{aligned} \u0026\\quad \\int v\\left(w_{e_{L}}^{*}\\right) \\cdot f\\left(\\pi | e_{H}\\right) d \\pi-g\\left(e_{H}\\right) \\ =\u0026\\quad\\left(\\bar{u}+g\\left(e_{L}\\right)\\right) \\cdot \\int f\\left(\\pi | \\theta_{H}\\right) d \\pi-g\\left(e_{H}\\right) \\ =\u0026\\quad \\bar{u}+g\\left(e_{L}\\right)-g\\left(e_{H}\\right) \\ \u003c\u0026\\quad \\bar{u}=\\int v\\left(w_{e_L}^*\\right) \\cdot f\\left(\\pi | e_{L}\\right) d \\pi-g\\left(e_{L}\\right) \\end{aligned} $$ Intuitively, the wage $w_{e_L}^*$ is fixed with respect to $\\pi$, thus it is also unaffected by agents effort level. So the agent will choose the effort level $e$ that involves the lowest disutility. Note that this salary $w_{\\dot{e}_{2}}^{*}=v^{-1}\\left(\\bar{u}+g\\left(e_{L}\\right)\\right)$ coincides with the salary we found when effort is observable. It satisfies P.C.-L binding and makes I.C.-L slack. It minimizes the salary expenses from the principal to the agent: if there were a higher salary, then it could still be reduced to induce the agent to participate with effort level $e_{L}$, where as if the salary were lower, then it would deter the agent from participating, and the agent would just refuse to sign the contract. Then, what is the salary to induce effort level $e_{H}$ ? Note that, the agent will choose effort $e_{H}$ rather than $e_{L}$ if his Incentive Compatibility condition (I.C.-H) holds: $$ \\underbrace{\\int v(w(\\pi)) \\cdot f\\left(\\pi | e_{H}\\right) d \\pi-g\\left(e_{H}\\right)}_{\\text {Salar minus disutility of high effort }} \\geq \\underbrace{\\int v(w(\\pi)) \\cdot f\\left(\\pi | e_{L}\\right) d \\pi-g\\left(e_{L}\\right)}_{\\text {Salary minus disutility of low effort }} $$ Hence, the principal’s optimization problem, when she seeks to induce $e_{H}$ from the agent, becomes $$ \\begin{aligned} \\min {w(\\pi)} \u0026\\int w(\\pi) \\cdot f\\left(\\pi | e{H}\\right) d \\pi \\ \\text { s.t. } \u0026\\int v(w(\\pi)) \\cdot f\\left(\\pi | e_{H}\\right) d \\pi-g\\left(e_{H}\\right) \\geq \\bar{U} \\quad(P . C .-H) \\ \u0026\\int v(w(\\pi)) \\cdot f\\left(\\pi | e_{H}\\right) d \\pi-g\\left(e_{H}\\right) \\geq \\int v(w(\\pi)) \\cdot f\\left(\\pi | e_{L}\\right) d \\pi-g\\left(e_{L}\\right) \\quad(I . C .-H) \\end{aligned} $$ Let $\\gamma$ and $\\mu$ be the Lagrangian multipliers of constraints P.C.-H and I.C.-H, respectively. Take the first order condition with respect to $w$ : $$ \\begin{array}{l} \\quad-f\\left(\\pi | e_{H}\\right)+\\gamma \\cdot v^{\\prime}(w(\\pi)) \\cdot f\\left(\\pi | e_{H}\\right) \\ +\\quad \\mu \\cdot v^{\\prime}(w(\\pi)) \\cdot f\\left(\\pi | e_{H}\\right)","date":"0001-01-01","objectID":"/6.-contract-theory/:1:3","tags":null,"title":"","uri":"/6.-contract-theory/"},{"categories":null,"content":"6.2 Monopolistic Screening (Hidden Information Model) Potential employer(s) have incomplete information regarding the types (e.g. high-ability or low-ability) of workers. Hidden information with more-than-one employers The labor market competitive screening model. (MWG 13.C) Multiple firms offering different contracts (w; t) to workers of different types. Hidden information with one employer (“Principal”) Monopolistic screening model. (MWG 14.C) A single firm offering different contracts (w, t) to workers of different types. ","date":"0001-01-01","objectID":"/6.-contract-theory/:2:0","tags":null,"title":"","uri":"/6.-contract-theory/"},{"categories":null,"content":"6.2.1 Model Setup We still consider a setting with asymmetric information between a principal and an agent. However, the effort is now perfectly observable. What is unobservable in this model? We shall consider the unobservable element be the disutility/cost $ g(e) $ that the agent experiences from exerting effort . The principal’s payoff: $\\pi(e)-w$ Where profits $\\pi(e)$ are increasing, but at a decreasing rate, in effort; i.e. $\\pi^{\\prime}(e)\u003e0$ and $\\pi^{\\prime \\prime}(e)\u003c0$ No profit arises if the agent exerts no effort; i.e. $\\pi(0)=0$ The principal is risk-neutral. Indifference curve illustration: The principal is better off if salary is lower given an effort level, or if effort is higher given a profit level. \r The agent’s payoff: $u(w, e, \\theta)=v(w-g(e, \\theta))$ $g(e, \\theta)$ measures the disutility from effort, and $g(0, \\theta) = 0$. $g_{e}(e, \\theta) \\geq 0, g_{e}(0, \\theta)=0,$ and $g_{e e}(e, \\theta)\u003e0 ;$ i.e. the disutility is increasing and convex in effort. $g_{\\theta}(e, \\theta)\u003c0 ;$ i.e. given an effort, the disutility is lower for higher type. $g_{e \\theta}(e, \\theta) \\leq 0 ;$ i.e. the marginal disutility is lower for higher type. In addition, $v^{\\prime}\u003e0$ and $v^{\\prime \\prime}\u003c0,$ which implies that the agent is risk-averse. For simplicity, we consider binary types $\\Theta=\\left{\\theta_{H}, \\theta_{L}\\right}$ with probability $p$ and $1-p$ \r The difference curve is totally depend on $g(\\cdot)$. The agent is better off for higher salary (if fix the effort), or for lower effort (if fix the salary). \r The slope of the agent’s two types' indifference curves: since $g_{e \\theta}(e, \\theta)\u003c0$, the marginal disutility of effort satisfies $g_{e}\\left(e, \\theta_{L}\\right)\u003eg_{e}\\left(e, \\theta_{H}\\right) .$ So the high type’s indifference curve is flatter. \r ","date":"0001-01-01","objectID":"/6.-contract-theory/:2:1","tags":null,"title":"","uri":"/6.-contract-theory/"},{"categories":null,"content":"6.2.2 Benchmark: Observable $g(\\cdot)$ In the benchmark model, a contract specifies the salary as a function of both the effort and type $\\theta$ (both are observable). That is, a salary-effort pair $(w_{H}, e_{H})$ for type $\\theta_{H}$, and another pair $\\left(w_{l}, e_{l}\\right)$ for type $\\theta_{L}$. Consider maximizing the ex-ante payoffs, the principal’s problem is $$ \\begin{aligned} \\max {w{L}, e_{L}, w_{H}, e_{H}} \u0026p \\cdot\\left[\\pi\\left(e_{H}\\right)-w_{H}\\right]+(1-p) \\cdot\\left[\\pi\\left(\\theta_{L}\\right)-w_{L}\\right] \\ \\text { s.t. } \u0026p v\\left(w_{H}-g\\left(e_{H}, \\theta_{H}\\right)\\right)+(1-p) v\\left(w_{L}-g\\left(e_{L}, \\theta_{L}\\right)\\right) \\geq \\bar{u} \\quad (P.C.) \\end{aligned} $$ Let the Lagrangian multiplier to the P.C. be $\\gamma$. Exercise: Note that, if we instead consider the interim payoff; i.e. the agents P.C. after knowing his type we can write the above P.C. into two conditions and use Lagrangian multiplier $\\gamma_{1}, \\gamma_{2}$ for each constraint (**the solution is the same**) First order conditions are: $$ \\begin{aligned} \\partial w_{H}:\u0026 \\quad-p+\\gamma \\cdot p \\cdot v^{\\prime}\\left(w_{H}^{*}-g\\left(e_{H}^{*}, \\theta_{H}\\right)\\right)=0 \\ \\partial w_{L}:\u0026 \\quad-(1-p)+\\gamma \\cdot(1-p) \\cdot v^{\\prime}\\left(w_{L}^{*}-g\\left(e_{L}^{*}, \\theta_{L}\\right)\\right)=0\\ \\partial e_{H}:\u0026 \\quad p \\cdot \\pi^{\\prime}\\left(e_{H}^{*}\\right)-\\gamma \\cdot p \\cdot v^{\\prime}\\left(w_{H}^{*}-g\\left(e_{H}^{*}, \\theta_{H}\\right)\\right) \\cdot g_{e}\\left(e_{H}^{*}, \\theta_{H}\\right)=0 \\ \\partial \\theta_{L}:\u0026 \\quad(1-p) \\cdot \\pi^{\\prime}\\left(e_{L}^{*}\\right)-\\gamma \\cdot(1-p) \\cdot v^{\\prime}\\left(w_{L}^{*}-g\\left(e_{L}^{*}, \\theta_{L}\\right)\\right) \\cdot g_{e}\\left(e_{L}^{*}, \\theta_{L}\\right)=0 \\end{aligned} $$ Note that if we had written the P.C. into two constraints, it is just a re-scaling of the multiplier; i.e. $\\gamma_{1}=\\gamma p$ and $\\gamma_{2}=\\gamma(1-p)$. Salaries $w_{\\mu}^{*}$ and $w_{i}^{*}$ are positive To guarantee acceptance under observability; i.e. $w_{i}^{*}=v^{-1}(\\bar{u})+g\\left(e_{i}, \\theta_{i}\\right), i=H, L$ only if $w_{i}^{*}\u003e0$. Effort level $e_H^$ and $e_L^$ are positive Suppose otherwise; i.e. $e_{H}=0 .$ The 3rd FOC becomes $$ p \\cdot \\underbrace{ \\pi ^{\\prime}(0)}{\u003e0}-\\gamma \\cdot p \\cdot v^{\\prime}\\left(w{H}^{*}-g\\left(0, \\theta_{H}\\right)\\right) \\cdot \\underbrace{g_{e}\\left(0, \\theta_{H}\\right)}_{=0}=p \\cdot \\pi^{\\prime}(0)\u003e0 $$ which indicates that the principal’s marginal payoff can be further improved by increasing $e_H$ from zero to a positive amount. Similar argument applies to $e_{H}\u003e0$ Solving for $\\gamma$ in the first two FOCs and re-arrange terms: $$ v^{\\prime}\\left(w_{H}^{*}-g\\left(e_{\\mu}^{*}, \\theta_{H}\\right)\\right)=v^{\\prime}\\left(w_{L}^{*}-g\\left(e_{L}^{*}, \\theta_{L}\\right)\\right) $$ because $v(\\cdot)$ is a monotone increasing function, the above also implies $$ w_{н}^{*}-g\\left(e_{Н}^{*}, \\theta_{H}\\right)=w_{L}^{*}-g\\left(e_{L}^{*}, \\theta_{L}\\right) $$ which further implies the utility also equals $$ v\\left(w_{H}^{*}-g\\left(e_{H}^{*}, \\theta_{H}\\right)\\right)=v\\left(w_{L}^{*}-g\\left(e_{L}^{*}, \\theta_{L}\\right)\\right) $$ Therefore, the risk averse agent obtains the same marginal utility, payoff, and total utility for both types. When the principal’s payoff maximizes, P.C. must be binding (otherwise there will be room for more profit by principal): $$ p v\\left(w_{H}^{*}-g\\left(e_{H}^{*}, \\theta_{H}\\right)\\right)+(1-p) v\\left(w_{L}^{*}-g\\left(e_{L}^{*}, \\theta_{L}\\right)\\right)=\\bar{u} $$ Since $v\\left(w_{H}^{*}-g\\left(e_{H}^{*}, \\theta_{H}\\right)\\right)=v\\left(w_{L}^{*}-g\\left(e_{L}^{*}, \\theta_{L}\\right)\\right),$ the P.C. becomes $$ \\begin{aligned} \u0026\\quad p v\\left(w_{H}^{*}-g\\left(e_{H}^{*}, \\theta_{H}\\right)\\right)+(1-p) v\\left(w_{H}^{*}-g\\left(e_{H}^{*}, \\theta_{H}\\right)\\right)=\\bar{u} \\ \\Rightarrow \u0026\\quad v\\left(w_{H}^{*}-g\\left(e_{H}^{*}, \\theta_{H}\\right)\\right)=\\bar{u} \\end{aligned} $$ So the optimal salaries satisfy $w_{i}^{*}=v^{-1}(\\bar{u})+g\\left(e_{i}^","date":"0001-01-01","objectID":"/6.-contract-theory/:2:2","tags":null,"title":"","uri":"/6.-contract-theory/"},{"categories":null,"content":"6.2.3 Main Model: Unobservable $ g(\\cdot) $ When only the agent observes the true type $\\theta_{i}$ while the principal does not, would the agent have incentive to truthfully reveal the type information to the principal and then choose the contract accordingly? Or alternatively, can the principal still offer the same contracts $\\left(w_{H}^{*}, e_{H}^{*}\\right)$ and $\\left(w_{L}^{*}, e_{L}^{*}\\right)$ and let the agent to self-select them into different contracts? The answer is no. In the previous figure, type $\\theta_{H}$ agent will have incentive to lie to say that his type is $\\theta_{L}$, because the pair $w_{L}^{*}, e_{L}^{*}$ lies above his current indifference curve, which means if he lies, he can achieve a higher utility. Offering a high type a $\\left(w_{L}^{*}, \\theta_{L}^{*}\\right)$ certainly reduces the principal’s profits. Given this problem, what should the optimal contract be under unobservable types? In particular, the principal chooses contract $\\left(w_{H}, e_{H}\\right)$ and $\\left(w_{L}, e_{L}\\right)$ to solve the maximization problem: $$ \\begin{aligned} \\max _{w_{L}, e_{L}, w_{H}, e_{H}} \u0026p \\cdot\\left[\\pi\\left(e_{H}\\right)-w_{H}\\right]+(1-p) \\cdot\\left[\\pi\\left(e_{L}\\right)-w_{L}\\right] \\ \\text { s.t. } \u0026v\\left(w_{H}-g\\left(e_{H}, \\theta_{H}\\right)\\right) \\geq \\bar{u} \\quad(P . C .-H) \\ \\quad \u0026v\\left(w_{L}-g\\left(e_{L}, \\theta_{L}\\right)\\right) \\geq \\bar{u} \\quad(P . C .-L) \\ \\quad \u0026v\\left(w_{H}-g\\left(e_{H}, \\theta_{H}\\right)\\right) \\geq v\\left(w_{L}-g\\left(e_{L}, \\theta_{H}\\right)\\right) \\quad(lI. C .-H) \\ \\quad \u0026v\\left(w_{L}-g\\left(e_{L}, \\theta_{L}\\right)\\right) \\geq v\\left(w_{H}-g\\left(e_{H}, \\theta_{L}\\right)\\right) \\quad(I . C .-L) \\end{aligned} $$ Note that the P.C. conditions can be re-arranged as $$ w_{i}-g\\left(e_{i}, \\theta_{i}\\right) \\geq v^{-1}(\\bar{u}), i=H, L $$ The I.C. conditions can also be re-arranged as: $$ \\begin{array}{l} \\underbrace{w_{H}-g\\left(e_{H}, \\theta_{H}\\right)}_{\\text {High type chooses his contract }} \\geq \\underbrace{w_{L}-g\\left(e_{L}, \\theta_{H}\\right)}_{\\text {High type chooses low type’s contract }} (I . C .-H)\\ \\underbrace{w_{L}-g\\left(e_{L}, \\theta_{L}\\right)}_{\\text {Low type chooses his contract }} \\geq \\underbrace{w_{H}-g\\left(e_{H}, \\theta_{L}\\right)}_{\\text {Low type chooses high type’s contract }} (I . C .-H) \\end{array} $$ The I.C. conditions ensures that the agents are truth-telling and self-select themselves into the designated contract. Four steps to solve the principal’s maximization problem 1st Step: Simplify the constraints. P.C.-H is satisfied when all other constraints hold $$ \\begin{aligned} w_{H}-g\\left(e_{H}, \\theta_{H}\\right) \u0026 \\geq w_{L}-g\\left(e_{L}, \\theta_{H}\\right) \\quad(I . C .-H) \\ \u0026 \\geq w_{L}-g\\left(e_{L}, \\theta_{L}\\right) \\geq v^{-1}(\\bar{u}) \\quad(P . C .-L) \\end{aligned} $$ since $g\\left(e_{L}, \\theta_{H}\\right)\u003cg\\left(e_{H}, \\theta_{H}\\right)$. The last inequality implies that (P.C.-H) must hold. Therefore, the principal’s maximization problem reduces to: $$ \\begin{aligned} \\max {w{L}, e_{L}, w_{H}, e_{H}} \u0026p \\cdot\\left[\\pi\\left(e_{H}\\right)-w_{H}\\right]+(1-p) \\cdot\\left[\\pi\\left(e_{L}\\right)-w_{L}\\right] \\ \\text { s.t. } \u0026w_{L}-g\\left(e_{L}, \\theta_{L}\\right) \\geq v^{-1}(\\bar{u}) \u0026 (P . C .-L) \\ \u0026w_{H}-g\\left(e_{H}, \\theta_{H}\\right) \\geq w_{L}-g\\left(e_{L}, \\theta_{H}\\right) \u0026 (I . C .-H) \\ \u0026w_{L}-g\\left(e_{L}, \\theta_{L}\\right) \\geq w_{H}-g\\left(e_{H}, \\theta_{L}\\right) \u0026 (I . C .-L) \\end{aligned} $$ Let $\\gamma, \\phi_{H}, \\phi_{L}$ be the Lagrangian multiplier for the three constraints. **Kuhn-Tucker conditions** are: $$ \\begin{aligned} \\partial w_{H}: \u0026 \\quad-p+\\phi_{H}-\\phi_{L}=0 \\quad \u0026(1) \\ \\partial w_{L}: \u0026 \\quad -(1-p)+\\gamma-\\phi_{H}+\\phi_{L}=0 \\quad \u0026(2) \\ \\partial e_{H}: \u0026 \\quad p \\pi^{\\prime}\\left(e_{H}\\right)-\\phi_{H} g_{e}\\left(e_{H}, \\theta_{H}\\right)+\\phi_{L} g_{e}\\left(e_{H}, \\theta_{L}\\right)=0 \\quad \u0026(3) \\ \\partial e_{L}: \u0026 \\quad (1-p) \\pi^{\\prime}\\left(e_{L}\\right)-\\left(\\gamma+\\phi_{L}\\right) g_{e}\\left(e_{L}, \\","date":"0001-01-01","objectID":"/6.-contract-theory/:2:3","tags":null,"title":"","uri":"/6.-contract-theory/"},{"categories":null,"content":"6.2.4 Benchmark model vs. Unobservable Types The same high effort $e_{H}$ at optimality ; i.e. in both the benchmark and the main model (unobservable types), $e_{H}^{*}$ solves $\\pi^{\\prime}\\left(e_{H}\\right)=g_{e}\\left(e_{H}, \\theta_{H}\\right)$ The optimal $e_{L}$ ’s are different in the benchmark and the main model. In the benchmark, $e_{L}^{*}$ solves $\\pi^{\\prime}\\left(e_{L}\\right)=g_{e}\\left(e_{L}, \\theta_{L}\\right)$ In the main model, optimal $e_{L}$ solves $$ \\underbrace{\\pi^{\\prime}\\left(e_{L}\\right)-g_{e}\\left(e_{L}, \\theta_{L}\\right)}_{\u003e0}+\\frac{p}{1-p}[\\underbrace{g_{e}\\left(e_{L}, \\theta_{H}\\right)-g_{e}\\left(e_{L}, \\theta_{L}\\right)}_{\u003c0}]=0 $$ The first term is positive for all $e_{L}\u003ce_{L}^{*},$ and the second term is always negative. Thus, we must have $e_{L}\u003ce_{L}^{*}$ for this equation to hold. ","date":"0001-01-01","objectID":"/6.-contract-theory/:2:4","tags":null,"title":"","uri":"/6.-contract-theory/"},{"categories":null,"content":"7. Auction Theory Auction theory (FPA and SPA). Part of MWG Chapter 23.B It also related to topics on “incentives and mechanism design” MWG Chapter 23.A – 23.D, JR Chapter 9 Auctions are a large part of the economic landscape: Since Babylon in 500 BC, and Rome in 193 AC Auction houses Shotheby‘s and Christi’s founded in 1744 and 1766. \r Munch‘s “The Scream,” sold for US $119.9 million in 2012. ","date":"0001-01-01","objectID":"/7.-auction-theory/:0:0","tags":null,"title":"","uri":"/7.-auction-theory/"},{"categories":null,"content":"7.1 Settings One seller and $N$ bidders, each bidder $i$ with a valuation $v_i$ for the object. ","date":"0001-01-01","objectID":"/7.-auction-theory/:1:0","tags":null,"title":"","uri":"/7.-auction-theory/"},{"categories":null,"content":"7.1.1 Auction Mechanisms We can design many different rules for the auction: First price auction: the winner is the bidder submitting the highest bid, and he/she must pay the highest bid (which is his/hers). Second price auction: the winner is the bidder submitting the highest bid, but he/she must pay the second highest bid. Third price auction: the winner is the bidder submitting the highest bid, but he/she must pay the third highest bid. All-pay auction: the winner is the bidder submitting the highest bid, but every single bidder must pay the price he/she submitted. Lottery auction: bidder $i$ get the object with $prob\\text {(win) }=\\frac{b_i}{b_{1}+b_{2}+\\ldots b_{N}}$, where $b_i$ is bidder $i$ ’s bid price. All auctions can be interpreted as allocation mechanisms with the following ingredients: an allocation rule (who gets the object): The allocation rule for most auctions determines the object is allocated to the individual submitting the highest bid However, we could assign the object by a lottery, where prob $ prob\\text {(win) }=\\frac{b_i}{b_{1}+b_{2}+\\ldots b_{N}} $ a payment rule (how much every bidder must pay): The payment rule in the FPA determines that the individual submitting the highest bid pays his bid, while everybody else pays zero The payment rule in the SPA determines that the individual submitting the highest bid pays the second highest bid, while everybody else pays zero. The payment rule in the APA determines that every individual must pay the bid he/she submitted. rivate valuations. ","date":"0001-01-01","objectID":"/7.-auction-theory/:1:1","tags":null,"title":"","uri":"/7.-auction-theory/"},{"categories":null,"content":"7.1.2 Private Valuations Symmetric incomplete information: I know my own valuation for the object, $v_{i}$. I don’t know your valuation for the object, $v_{j}$, but I know that it is drawn from a distribution function. Easiest case: $$ v_j= \\begin{cases} 10\u0026,\\text{with probability 0.4, or}\\ 5\u0026, \\text{with probability 0.6} \\end{cases} $$ More generally, CDF of $ v_j $ $$ F(v)=\\operatorname{prob}\\left(v_{j}\u003cv\\right) $$ Assume that every bidder’s valuation for the object is drawn from a uniform distribution function between 0 and 1. Uniform distribution function $U[0,1]$ \r If bidder $i$ ’s valuation is $v,$ then all points in the horizontal axis where $v_{j}\u003cv$, entail probability $\\operatorname{prob}\\left(v_{i}\u003cv\\right)=F(v)$ in the vertical axis. I.e. $v=F(v)$. Similarly, valuations where $v_{j}\u003ev$ (horizontal axis) entail probability $\\operatorname{prob}\\left(v_{j}\u003ev\\right)=1-F(v)$ in the vertical axis. I.e. $1-F(v)=1-v$. Since all bidders are ex-ante symmetric, they will all be using the same bidding function: $$ b_{i}: v_i \\in [0,1] \\rightarrow \\mathbb{R}_{+} \\text{for every bidder } i $$ They might, however, submit different bids, depending on their different valuation for the object. Example: A valuation of $v_{i}=0.4$ inserted into a bidding function $b_{i}\\left(v_{i}\\right)=\\frac{v_{i}}{2},$ implies a bid of $b_{i}(0.4)=$ 0.2$ A bidder with a higher valuation of $v_{i}=0.9$ implies, in contrast, a bid of $b_{i}(0.9)=\\frac{0.9}{2}=$ 0.45$ Even if bidders are symmetric in the bidding function they use, they can be asymmetric in the actual bid they submit ","date":"0001-01-01","objectID":"/7.-auction-theory/:1:2","tags":null,"title":"","uri":"/7.-auction-theory/"},{"categories":null,"content":"7.2 First-price Auctions / Dutch Auction The Dutch Auction is strategically equivalent to FPA with sealed bid. Dutch auction is commonly used in many auction houses nowadays. An auctioneer begins by calling out a price high enough so that presumably no bidder is interested in buying the object at that price. The price is gradually lowered until some bidder indicates his/her interest. The object is sold to the bidder at the current price. Therefore, each bidder needs to have a “target price” (his bid) in mind, and to raise hand to indicate his interest when that price is called. ","date":"0001-01-01","objectID":"/7.-auction-theory/:2:0","tags":null,"title":"","uri":"/7.-auction-theory/"},{"categories":null,"content":"7.2.1 Risk Neutral Bidders For risk neutral bidders, assume linear utility function $$ u_i = v_i-b_i $$ Then start by ruling out bidding strategies that yield negative (or zero) payoffs, regardless of what your opponent does, i.e., deleting dominated bidding strategies. Never bid above your value, $b_{i}\u003ev_{i},$ since it yields a negative payoff if winning. $$ E U_{i}\\left(b_{i} | v_{i}\\right)=\\operatorname{prob}(\\operatorname{win}) \\cdot \\underbrace{\\left(v_{i}-b_{i}\\right)}_{\\text{negtive}}+\\operatorname{prob}(\\operatorname{lose})\\cdot 0\u003c0 $$ Never bid your value, $b_{i}=v_{i},$ since it yields a zero payoff if winning. $$ E U_{i}\\left(b_{i} | v_{i}\\right)=\\operatorname{prob}(\\operatorname{win}) \\cdot \\underbrace{\\left(v_{i}-b_{i}\\right)}_{0}+\\operatorname{prob}(\\text {lose}) \\cdot 0=0 $$ Therefore, the only bidding strategies that can arise in equilibrium imply “bid shading”, That is, $b_{i}\u003cv_{i}$ More specifically, $b_{i}\\left(v_{i}\\right)=a \\cdot v_{i},$ where constant $a \\in(0,1)$ \r But, what is the precise value of parameter $a \\in(0,1)$. That is, how much bid shadding? Before answering that question, we must provide a more specific expression for the probability of winning in bidder $i$ ’s expected utility of submitting a bid $x$, $$ E U_{i}\\left(x | v_{i}\\right)=\\operatorname{prob}(\\operatorname{win}) \\cdot\\left(v_{i}-x\\right) $$ Uniform distribution $ v_i $ Two Bidders Case Given symmetry in the bidding function, bidder $j$ can “recover” the valuation that produces a bid of exactly $$ x$. From the vertical to the horizontal axis. Solving for $v_{i}$ in function $x=a \\cdot v_{i},$ yields $v_{i}=\\frac{x}{a}$. \r What is, then, the probability of winning when submitting a bid $x$ is $prob(b_j \u003c b_i )$ in the vertical axis, or $ prob( v_j \u003c \\frac{x}{a} ) $ in the horizontal axis. \r And since valuations are uniformly distributed $ prob( v_j \u003c \\frac{x}{a} ) = \\frac{x}{a}$ which implies that the expected utility of submitting a bid $x$ is. $$ E U_{i}\\left(x | v_{i}\\right)=\\underbrace{\\frac{x}{a}}_{\\text {prob}(\\text {win})}\\left(v_{i}-x\\right) $$ And simplifying $$ =\\frac{x v_{i}-x^{2}}{a} $$ Taking first-order conditions of $\\frac{x v_{i}-x^{2}}{a}$ with respect to $x,$ we obtain $$ \\frac{v_{i}-2 x}{a}=0 $$ and solving for $x$ yields an optimal bidding function of $$ x\\left(v_{i}\\right)=\\frac{1}{2} v_{i} $$ it means bid shadding in half. \r N Bidders Case The expected utility is similar, but the probability of winning differs $$ \\begin{aligned} \\operatorname{prob}(\\operatorname{win}) \u0026=\\frac{x}{a} \\cdot \\ldots \\cdot \\frac{x}{a} \\cdot \\frac{x}{a} \\cdot \\ldots \\cdot \\frac{x}{a} \\ \u0026=\\left(\\frac{x}{a}\\right)^{N-1} \\end{aligned} $$ Hence, the expected utility of submitting a bid $x$ is $$ E U_{i}\\left(x | v_{i}\\right)=\\left(\\frac{x}{a}\\right)^{N-1}\\left(v_{i}-x\\right)+\\left[1-\\left(\\frac{x}{a}\\right)^{N-1}\\right] \\cdot 0 $$ Taking first-order conditions with respect to his bid, $x,$ we obtain $$ -\\left(\\frac{x}{a}\\right)^{N-1}+(N-1)\\left(\\frac{x}{a}\\right)^{N-2}\\left(\\frac{1}{a}\\right)\\left(v_{i}-x\\right)=0 $$ Rearranging, $$ \\left(\\frac{x}{a}\\right)^{N} \\frac{a}{x^{2}}\\left[(N-1) v_{i}-N x\\right]=0 $$ and solving for $x$, we find bidder $i$ ’s optimal bidding function, $$ x\\left(v_{i}\\right)=\\frac{N-1}{N} v_{i} $$ \r Comparative statics: Bid shadding diminishes as N increases. Bidding function approaches $45^o$ line. Generalization $ v_i $ Let us now allow for valuations to be drawn from any CDF $F\\left(v_{i}\\right)$ (not necessarily uniform) First, note that, for a given bidding strategy $s:v_i \\in [0,1] \\rightarrow \\mathbb{R}_{+}$, i.e., $s\\left(v_{i}\\right)=x_{i},$ we can define its inverse $s^{-1}\\left(x_{i}\\right)=v_{i}$ implying that the CDF can be rewritten as $$ F\\left(v_{i}\\right)=F\\left(s^{-1}\\left(x_{i}\\right)\\right) $$ Then bidder i’s UMP becomes $$ \\max {x{i}} \\underbrace{\\left[F\\left(s^{-1}\\left(x_{i}\\right)\\right)\\right]^{n-1}}_{\\text {prob }(\\text {win})}\\left(v_{i}-x_{i}\\right) $$ Taking first-order conditions w","date":"0001-01-01","objectID":"/7.-auction-theory/:2:1","tags":null,"title":"","uri":"/7.-auction-theory/"},{"categories":null,"content":"7.2.2 Risk-averse Bidders Utility function is concave in income, $x,$ e.g., $u(x)=x^{\\alpha}$ where $0\u003c\\alpha \\leq 1$ denotes bidder $i$ ’s risk-aversion parameter. Note that when $\\alpha=1$, the bidder is risk neutral. Consider two bidders with $ v_i \\sim U[0,1] $. The expected utility of submitting a bid $x$ is $$ E U_{i}\\left(x | v_{i}\\right)=\\underbrace{\\frac{x}{a}}_{\\text {prob }(\\text {win})}\\left(v_{i}-x\\right)^{\\alpha} $$ Taking first-order conditions with respect to his bid, $x$, $$ \\frac{1}{a}\\left(v_{i}-x\\right)^{\\alpha}-\\frac{x}{a} \\alpha\\left(v_{i}-x\\right)^{\\alpha-1}=0 $$ and solving for $x,$ we find the optimal bidding function, $$ x\\left(v_{i}\\right)=\\frac{v_{i}}{1+\\alpha} $$ Under risk-neutral bidders, $\\alpha=1$, this function becomes $x\\left(v_{i}\\right)=\\frac{v_{i}}{2}$. But, what happens when $\\alpha$ decreases (more risk aversion)? \r Bid shading is ameliorated as bidders' risk aversion increases: That is, the bidding function approaches the $ 45^o $ line when a approaches zero. Intuition: for a risk-averse bidder: the positive effect of slightly lowering his bid, arising from getting the object at a cheaper price, is offset by… the negative effect of increasing the probability that he loses the auction. Ultimately, the bidders' incentives to shade his bid are diminished. ","date":"0001-01-01","objectID":"/7.-auction-theory/:2:2","tags":null,"title":"","uri":"/7.-auction-theory/"},{"categories":null,"content":"7.3 Second-price Auctions / English Auction The English Auction is strategically equivalent to SPA with sealed bid. English auction is also a commonly used form in auction houses. An auctioneer begins by calling out a low price and gradually raise it. Whoever wants to exit can exit the auction. The auctioneer keep on increasing the price as long as there are at least two interested bidders. The auction stops when there is only one interested bidder left. The object is sold to this bidder at the current price. Therefore, each bidder needs to have a target price (the bid) in mind and wait till the price reaches this price to drop out. **Bidding your own valuation, $b_{i}\\left(v_{i}\\right)=v_{i}$, is a weakly dominant strategy**, i.e., it yields a larger (or the same) payoff than submitting any other bid. In order to show this, let us find the expected payoff from submitting A bid that coincides with your own valuation, $b_{i}\\left(v_{i}\\right)=v_{i}$ A bid that lies below your own valuation, $b_{i}\\left(v_{i}\\right)\u003cv_{i}$ A bid that lies above your own valuation, $b_{i}\\left(v_{i}\\right)\u003ev_{i}$ We can then compare which bidding strategy yields the largest expected payoff Bidding your own valuation, $b_{i}\\left(v_{i}\\right)=v_{i}$ \r Bidding your own valuation: If his bid lies below the highest competing bid, i.e. $b_{i}\u003ch_{i}$ where $h_{i}=\\max _{j \\neq i}\\left{b_{j}\\right}$ then bidder $i$ loses the auction, obtaining a zero payoff. Case 1 b: If his bid lies above the highest competing bid, i.e. $b_{i}\u003eh_{i},$ then bidder $i$ wins. He obtains a net payoff of $v_{i}-h_{i}$ end-price auctions. Case 1 c: If, instead, his bid coincides with the highest competing bid, i.e., $b_{i}=h_{i},$ then a tie occurs. For simplicity, ties are solved by randomly assigning the object to the bidders who submitted the highest bids as a consequence, bidder i’s expected payoff becomes $\\frac{1}{2}\\left(v_{i}-h_{i}\\right)$. Bidding below your valuation, $b_{i}\\left(v_{i}\\right)\u003cv_{i}$ \r Bidding below your valuation ( compare to $v=v_i$ case by case): Case 2 a: If his bid lies below the highest competing bid, i.e., $ b_i \u003c h_i $, then bidder $i$ loses, obtaining a zero payoff. Case 2 b: if his bid lies above the highest competing bid, i.e. $b_{i}\u003eh_{i}$, then bidder $i$ wins, obtaining a net payoff of $v_{i}-h_{i}$ Case 2 c: If, instead, his bid coincides with the highest competing bid, i.e., $b_{i}=h_{i},$ then a tie occurs, and the object is randomly assigned, yielding an expected payoff of $\\frac{1}{2}\\left(v_{i}-h_{i}\\right)$. Bidding above your valuation, $b_{i}\\left(v_{i}\\right)\u003ev_{i}$ \r Bidding above your valuation ( compare to $v=v_i$ case by case): Case 3 a: if his bid lies below the highest competing bid, i.e. $b_{i}\u003ch_{i}$, then bidder $i$ loses, obtaining a zero payoff. Case 3 b: if his bid lies above the highest competing bid, i.e. $b_{i}\u003eh_{i},$ then bidder $i$ wins. His payoff becomes $v_{i}-h_{i},$ which is positive if $v_{i}\u003eh_{i},$ or negative otherwise. Case 3 c: If, instead, his bid coincides with the highest competing bid, i.e., $b_{i}=h_{i},$ then a tie occurs. The object is randomly assigned, yielding an expected payoff of $\\frac{1}{2}\\left(v_{i}-h_{i}\\right)$, which is negative. Summary: There is no bidding strategy that provides a strictly higher payoff than $b_{i}\\left(v_{i}\\right)=v_{i}$ in the SPA. All players bid their own valuation, without shading their bids, unlike in the optimal bidding function in FPA. The above equilibrium bidding strategy in the SPA is unaffected by: the number of bidders who participate in the auction, N, or their risk-aversion preferences. ","date":"0001-01-01","objectID":"/7.-auction-theory/:3:0","tags":null,"title":"","uri":"/7.-auction-theory/"},{"categories":null,"content":"7.4 Efficiency in auctions The object is assigned to the bidder with the highest valuation. Otherwise, the outcome of the auction cannot be efficient, since there exist alternative reassignments that would still improve welfare. Hence, FPA and SPA are efficient, since: the player with the highest valuation submits the highest bid and wins the auction. Lottery auctions are not necessarily efficient, since it’s random allocation between bidders. ","date":"0001-01-01","objectID":"/7.-auction-theory/:4:0","tags":null,"title":"","uri":"/7.-auction-theory/"},{"categories":null,"content":"8. Mechanism Design There are many situations in which some central authority wishes to implement a decision that depends on the private information of a set of players. Government may wish to choose the design of a public-work project based on preferences of its citizens who have private information about their preferences. Monopolistic firms may wish to determine a set of consumers' willingness to pay for different products it can produce with the goal of making as high a profit as possible. A seller of an object wants to design an auction to maximize her revenue when each bidder’s valuation is private information. Mechanism design is a study of what kinds of mechanism that the central authority can devise in order to reveal the private information of players. Central authority is the mechanism designer. ","date":"0001-01-01","objectID":"/8.-mechanism-design/:0:0","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.1 Set up ","date":"0001-01-01","objectID":"/8.-mechanism-design/:1:0","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.1.1 Mechanism as Bayesian Games A set of players $ N={1,2, \\ldots, n} $. A set of public alternatives $ X $ that could represent many kinds of alternatives. e.g., an alternative $ x \\in X $ could represent the attributes of a public good or service, like investment in education or in preserving the environment. The reason that $ X $ is called as public alternatives is the chosen alternative affects all the players in $ N $, e.g., in an auction, if one player gets a private good then the consequence is that everyone else does not Each player $ i $ privately observes his type $ \\theta_{i} \\in \\Theta_{i} $ which determines his preferences. Let $ \\theta=\\left(\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{n}\\right) $ be the **state of the world**. State $ \\theta $ is drawn randomly from the state space $$ \\Theta \\equiv \\Theta_{1} \\times \\Theta_{2} \\times \\cdots \\times \\Theta_{n} $$ The draw of $ \\theta $ is according to some prior distribution $ \\phi(\\cdot) $ over $ \\Theta $. $ \\theta_{i} $ is player $ i^{\\prime} $ s private information; $ \\phi(\\cdot) $ is common knowledge knowledge. ","date":"0001-01-01","objectID":"/8.-mechanism-design/:1:1","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.1.2 Players Each player $ i $ has quasi-linear preference $$ v_{i}\\left(x, m, \\theta_{i}\\right)=u_{i}\\left(x, \\theta_{i}\\right)+m_{i} $$ Alternatives have a “money-equivalent” value, and preferences are additive in money. $ m_{i} $ is the amount of money that is given to individual $ i . m_{i} $ can be negative, representing that money is taken away from individual $ i $. $ u_{i}\\left(x, \\theta_{i}\\right) $ is money-equivalent **value of alternative** $ x \\in X $ when $ i $ ’s type is $ \\theta_{i} $. An outcome (alternative+money) is represented as $ y=\\left(x, m_{1}, \\ldots, m_{n}\\right) $. ","date":"0001-01-01","objectID":"/8.-mechanism-design/:1:2","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.1.3 Mechanism Designer The mechanism designer has the objective of achieving an outcome that depends on the types of players. Assume that mechanism designer does not have a source of funds to pay the players (Government rather than firm); i.e. monetary payments have to be self-financed, which is $ \\sum_{i=1}^{n} m_{i} \\leq 0 $. When $ \\sum_{i=1}^{n} m_{i}\u003c0, $ it means that mechanism designer keeps some of the money that he raises from players. The set of outcomes is defined as: $$ Y=\\left{\\left(x, m_{1}, \\ldots, m_{n}\\right): x \\in X, m_{i} \\in \\mathbb{R}, \\forall i \\in N, \\sum_{i=1}^{n} m_{i} \\leq 0\\right} $$ The mechanism designer’s objective is given by a choice rule/ strategy: $$ f(\\theta)=\\left(x(\\theta), m_{1}(\\theta), \\ldots, m_{n}(\\theta)\\right) $$ where $ x(\\theta) \\in X $ and $ \\sum_{i=1}^{n} m_{i} \\leq 0 $. $ x(\\theta) $ is the decision rule (function); $ \\left(m_{1}(\\theta), \\ldots, m_{n}(\\theta)\\right) $ is the **transfer rule**. Example 1: Let $ X=[0, \\bar{x}] $ be the size of a water treatment plant. The plant will benefit some citizens and may displease others. The citizens are the group of players, $ N $ Player i’s willingness to pay/ utility from $ x \\in X $ of the plant is $$ u_{i}\\left(x, \\theta_{i}\\right) $$ The utilitarian mechanism designer **maximizes the sum of the players' valuations** by choosing the value of $ x $. So his decision rule $ x(\\theta) $ would maximize $ \\sum_{i=1}^{n} u_{i}\\left(x, \\theta_{i}\\right) $ Example 2: A good: a license to use a certain portion of the electromagnetic spectrum for cell coverage. The license can be allocated to one of a group of cellular carriers $ i \\in N $. $ x_{i} \\in{0,1} $ indicates whether player $ i $ receives the license $$ \\left(x_{i}=1\\right) \\text { or not }\\left(x_{i}=0\\right) $$ The possible set of alternatives is $$ X=\\left{\\left(x_{1}, \\ldots, x_{n}\\right)\\right} $$ such that $ x_{i} \\in{0,1} $ and $ \\sum_{i=1}^{n} x_{i}=1 $. Player $ i $ ’s willingness to pay for the license is $ u_{i}\\left(x, \\theta_{i}\\right)=\\theta_{i} x_{i} $. The mechanism designer maximizes the sum of the players' valuations by choosing $ x $. So his decision rule $ x(\\theta) $ would maximize $ \\sum_{i=1}^{n} u_{i}\\left(x, \\theta_{i}\\right) $. ","date":"0001-01-01","objectID":"/8.-mechanism-design/:1:3","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.2 The Mechanism Game The mechanism designer desires to implement a choice rule $$ f: \\Theta \\rightarrow Y $$ The problem is that the mechanism designer’s choice rule depends on the unobserved state $ \\Theta $. Two ways that the mechanism designer have to solve the problem. The first method is to ask each player directly. But are they willing to share their true preference? Example 3 (MWG 23.B.1): Consider an abstract case, where we are given a set of alternatives $ X={x, y, z} $ and two players Suppose that agent 1 has one possible type, so that $ \\Theta_{1}=\\left{\\bar{\\theta}_{1}\\right} $ and that agent 2 has two possible types, so that$ \\Theta_{2}=\\left{\\theta_{2}^{\\prime}, \\theta_{2}^{\\prime \\prime}\\right} $. The agents' possible preference orderings are given as $$ \\begin{array}{c|ccc} \u0026 \\bar{\\theta}{1} \u0026 \\theta{2}^{\\prime} \u0026 \\theta_{2}^{\\prime \\prime} \\ \\hline \\text { Best } \u0026 x \u0026 z \u0026 y \\ \\text { Middle } \u0026 y \u0026 y \u0026 x \\ \\text { Worst } \u0026 z \u0026 x \u0026 z \\end{array} $$ Suppose that the agents wish to implement the ex post efficient social choice function $ f(\\cdot) $ with $$ f\\left(\\bar{\\theta}{1}, \\theta{2}^{\\prime}\\right)=y \\quad \\text { and } \\quad f\\left(\\bar{\\theta}{1}, \\theta{2}^{\\prime \\prime}\\right)=x $$ If so, then agent 2 must be relied upon to truthfully reveal his preferences. It is apparent, however, that he will not find it in his interest to do so. When $ \\theta_{2}=\\theta_{2}^{\\prime \\prime}, $ **agent 2 will wish to lie** and claim that his type is $ \\theta_{2}^{\\prime} $. In the words of a famous American TV Doctor, “Everyone lies.” Let’s look at the alternative way. In the second way, the mechanism designer designs some clever game that ends up revealing the players' private information.(like Screening) The rules of the game endows each player $ i $ with an action set $ A_{i} $. Following the choice $ a_{i} \\in A_{i} $ by each player, there is some **outcome function** $ g\\left(a_{1}, \\ldots, a_{n}\\right) $. That makes a choice of an outcome $ y \\in Y $. The payoffs of player $ i $ over outcomes is $ v_{i}\\left(g(s), \\theta_{i}\\right) $. 【Definition】A mechanism, $ \\Gamma=\\left{A_{1}, A_{2}, \\ldots, A_{n}, g(\\cdot)\\right} $ is a collection of $ n $ action sets $ A_{1}, A_{2}, \\ldots, A_{n} $ and an outcome function $ g: A_{1} \\times A_{2} \\times \\cdots \\times A_{n} \\rightarrow Y $. A pure strategy for player $ i $ in the mechanism $ \\Gamma $ is a function that maps types into actions, $ s_{i}: \\Theta_{i} \\rightarrow A_{i} $. The payoffs of the players are given by $ v_{i}\\left(g(s), \\theta_{i}\\right) $. ","date":"0001-01-01","objectID":"/8.-mechanism-design/:2:0","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.3 Bayesian Implementation ","date":"0001-01-01","objectID":"/8.-mechanism-design/:3:0","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.3.1 Example: FPA as an Allocation Mechanism MWG Example 23.B.5. Consider an FPA with one seller (player 0) and two bidders (player 1 and 2). Each bidder with valuation $ \\theta_{i} \\sim U[0,1] . $ BNE of the auction is $ b_{i}\\left(\\theta_{i}\\right)=\\theta_{i} / 2, \\forall i $. The corresponding choice rule $ f(\\theta)=\\left(x_{0}(\\theta), x_{1}(\\theta), x_{2}(\\theta), m_{0}(\\theta), m_{1}(\\theta), m_{2}(\\theta)\\right) $ is $$ \\begin{array}{l} x_{1}(\\theta)=1 \\text { if } \\theta_{1} \\geq \\theta_{2} ;=0 \\text { if } \\theta_{1}\u003c\\theta_{2} \\ x_{2}(\\theta)=1 \\text { if } \\theta_{1}\u003c\\theta_{2} ;=0 \\text { if } \\theta_{1} \\geq \\theta_{2} \\ m_{1}(\\theta)=-0.5 \\theta_{1} \\cdot x_{1}(\\theta) \\ m_{2}(\\theta)=-0.5 \\theta_{2} \\cdot x_{2}(\\theta) \\ x_{0}(\\theta)=0, \\forall \\theta, m_{0}(\\theta)=-\\left(m_{1}(\\theta)+m_{2}(\\theta)\\right) \\end{array} $$ ","date":"0001-01-01","objectID":"/8.-mechanism-design/:3:1","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.3.2 Definition 【Definition】The strategy profile $ s^{*}(\\cdot)=\\left(s_{1}^{*}(\\cdot), \\ldots, s_{n}^{*}(\\cdot)\\right) $ is a **Bayesian Nash equilibrium** of the mechanism $ \\Gamma=\\left{A_{1}, \\ldots, A_{n}, g(\\cdot)\\right} $ if for every $ i \\in N $ and for every $ \\theta_{i} \\in \\Theta_{i} $ $$ \\begin{aligned} \u0026 E_{\\theta_{-i}}\\left[v_{i}\\left(g\\left(s_{i}^{*}\\left(\\theta_{i}\\right), s_{-i}^{*}\\left(\\theta_{-i}\\right)\\right), \\theta_{i}\\right) | \\theta_{i}\\right] \\ \\geq \u0026 E_{\\theta_{-i}}\\left[v_{i}\\left(g\\left(a_{i}^{\\prime}, s_{-i}^{*}\\left(\\theta_{-i}\\right)\\right), \\theta_{i}\\right) | \\theta_{i}\\right] \\quad \\text { for all } a_{i}^{\\prime} \\in A_{i} \\end{aligned} $$ That is, if player $ i $ believes that other players are playing according to $ s_{-i}^{*}(\\theta) $ then he maximizes his expected payoff by following the behavior prescribed by $ s_{i}^{*}\\left(\\theta_{i}\\right) $ regardless of which type player $ i $ is. The mechanism designer designs a mechanism in which $ s_{i}^{*} \\rightarrow A_{i} $ such that the outcome is exactly what the mechanism designer desires given each $ \\theta_{i} $. $$ \\text { for all } \\theta \\in \\Theta, g\\left(s_{1}^{*}\\left(\\theta_{1}\\right), s_{2}^{*}\\left(\\theta_{2}\\right), \\ldots, s_{n}^{*}\\left(\\theta_{n}\\right)\\right)=f(\\theta) $$ 【Definition】A mechanism $ \\Gamma $ **implements** the choice rule $ f(\\cdot) $ if there exists a Bayesian Nash equilibrium of the mechanism $ \\Gamma $ $$ \\begin{array}{l} \\left(s_{1}^{*}\\left(\\theta_{1}\\right), s_{2}^{*}\\left(\\theta_{2}\\right), \\ldots, s_{n}^{*}\\left(\\theta_{n}\\right)\\right), \\text { such that } \\ g\\left(s_{1}^{*}\\left(\\theta_{1}\\right), s_{2}^{*}\\left(\\theta_{2}\\right), \\ldots, s_{n}^{*}\\left(\\theta_{n}\\right)\\right)=f(\\theta) \\text { for all } \\theta \\in \\Theta \\end{array} $$ That is, it instead implements $ f(\\cdot) $ after knowing the true $ \\theta $ the mechanism does what the mechanism designer wants to $ \\mathrm{do}: g(a(\\theta))=f(\\theta) $. It is a partial implementation because it requires that the desired outcome be an equilibrium, but allows for other, undersirable equilibrium outcomes as well. The implementation without “bad equilibria” is called full implementation. \r ","date":"0001-01-01","objectID":"/8.-mechanism-design/:3:2","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.3.3 The Revelation Principle The mechanism game is a Bayesian game. It is useful when the mechanism designer cannot get players to reveal their types. There is a particular mechanism which is also a Bayesian game in which the mechanism designer asks players directly to reveal their types in order to implement $ f(\\cdot) $. The mechanism designer implements $ f(\\hat{\\theta}), $ with $ \\hat{\\theta} $ is announced by the players. 【Definition】$ \\Gamma=\\left{\\Theta_{1}, \\ldots, \\Theta_{n}, f(\\cdot)\\right} $ is a **direct revelation mechanism** for choice rule $ f(\\cdot) $ if $ A_{i}=\\Theta_{i} $ for all $ i \\in N $ and $ g(\\theta)=f(\\theta) $ for all $ \\theta \\in \\Theta $. The straightforward direct revelation mechanism will actually have an equilibrium that implements the mechanism designer’s intended outcome.??? 【Definition】The choice rule $ f(\\cdot) $ is truthfully implementable in Bayesian Nash equilibrium if for all $ \\theta $ the direct revelation mechanism $ \\Gamma=\\left{\\Theta_{1}, \\ldots, \\Theta_{l}, f(\\cdot)\\right} $ has a Bayesian Nash equilibrium $ s_{i}^{*}\\left(\\theta_{i}\\right)=\\theta_{i} $ for all $ i . $ Equivalently, for all $ i $ $$ \\begin{aligned} E_{\\theta_{-i}}\\left[v_{i}\\left(f\\left(\\theta_{i}, \\theta_{-i}\\right), \\theta_{i}\\right) | \\theta_{i}\\right] \u0026 \\geq E_{\\theta_{-i}}\\left[v_{i}\\left(f\\left(\\hat{\\theta}_{i}, \\theta_{-i}\\right), \\theta_{i}\\right) | \\theta_{i}\\right] \\ \\text { for all } \\hat{\\theta}_{i} \u0026 \\in \\Theta_{i} \\end{aligned} $$ That is, $ f(\\cdot) $ is truthfully implementable in Bayesian Nash equilibrium if truthtelling is a Bayesian Nash equilibrium strategy in the direct revelation mechanism. If every player $ i $ believes that all other players are reporting their types truthfully, then player $ i $ is also willing to report truthfully. 【Example 4】$ (\\mathrm{MWG} 23 . \\mathrm{B} .7): $ Consider a first-price sealed-bid auction where two potential buyers have valuations $ \\theta_{i} $ that are drawn from a uniform distribution on [0,1]. Recall that the equilibrium bidding function for each player $ i $ is $ b_{i}\\left(\\theta_{i}\\right)=\\frac{1}{2} \\theta_{i} $. When facing the direct revelation mechanism $ \\Gamma=\\left{\\Theta_{1}, \\ldots, \\Theta_{l}, f(\\cdot)\\right} $ (所有人同时报真实底价然后密封，价高者得，且按照报价的一半支付), buyer 1 ’s optimal announcement $ \\hat{\\theta}_{1} $, when he has type $ \\theta_{1} $ solves $$ \\begin{array}{l} \\max _{\\hat{\\theta}_{1}}\\left(\\theta_{1}-\\frac{1}{2} \\hat{\\theta}_{1}\\right) \\operatorname{Pr}\\left(\\theta_{2} \\leq \\hat{\\theta}_{1}\\right) \\ \\quad=\\max _{\\hat{\\theta}_{1}}\\left(\\theta_{1}-\\frac{1}{2} \\hat{\\theta}_{1}\\right) \\hat{\\theta}_{1} \\end{array} $$ The first-order condition is $$ \\theta_{1}-\\hat{\\theta}_{1}=0 \\Longrightarrow \\hat{\\theta}_{1}=\\theta_{1} $$ Hence, truth telling is buyer 1 ’s **optimal strategy given that buyer 2 always tells the truth**. A similar conclusion follows for buyer 2. Thus, the social choice function implemented by the first-price sealed-bid auction (in a Bayesian Nash equilibrium) can also be truthfully implemented (in a Bayesian Nash equilibrium) through a direct revelation mechanism! 【Proposition】The Revelation Principle for Bayesian Nash Implementation: A choice rule $ f(\\cdot) $ is implementable in Bayesian Nash equilibrium if and only if it is truthfully implementable in Bayesian Nash equilibrium. Proof: IF part: By definition, if $ f(\\cdot) $ is truthfully implementable in Bayesian Nash equilibrium then it is implementable in Bayesian Nash equilibrium using the direct revelation mechanism. ONLY IF part: Suppose that there exists some mechanism $ \\Gamma=\\left(A_{1}, \\ldots, A_{n}, g(\\cdot)\\right) $ that implements $ f(\\cdot) $ using the equilibrium strategy profile $ s^{*}(\\cdot)=\\left(s_{1}^{*}(\\cdot), \\ldots, s_{n}^{*}(\\cdot)\\right) $ and $ g\\left(s^{*}(\\cdot)\\right)=f(\\cdot), $ so that for every $ i \\in N $ and $ \\theta_{i} \\in \\Theta_{i} $ $$ \\begin{aligned} \u0026 E_{\\theta_{-i}}\\left[v_{i}\\left(g\\left(s_{i}^{*}\\left(\\theta_{i}\\right), s_{-i}^{","date":"0001-01-01","objectID":"/8.-mechanism-design/:3:3","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.3.4 BIC and Revenue Equivalence Theorem The concept, truthfully implementable in Bayesian Nash Equilibrium is also known as Bayesian Incentive Compatible (BIC). Consider a generalized auction mechanism as follows: Agent 0 is the seller of an indivisible object from which he derives no values; and agents $ 1, \\ldots, n $ are potential buyers. Each agent’s type (valuation) is generated $ \\theta_{i} \\in \\Theta_{i}=\\left[\\underline{\\theta}_{i}, \\bar{\\theta}_{i}\\right] $ following $ \\operatorname{CDF} \\Phi_{i}\\left(\\theta_{i}\\right) $, $ \\text { with density } \\phi_{i}\\left(\\theta_{i}\\right)\u003e0, \\forall \\theta_{i} \\in \\Theta_{i} $. Let $ x_{i}(\\theta) $ denote buyer $ i $ ’s probability of getting the item when the reported types of all bidders are $ \\theta=\\left(\\theta_{1}, \\ldots, \\theta_{n}\\right) $ Let $ m_{i}(\\theta) $ denote bidder $ i $ ’s payment in the auction given the reported type profile $ \\theta $ Bidder $ i $ ’s utility $ v_{i}\\left(x, m_{i}, \\theta_{i}\\right)=\\theta_{i} \\cdot x_{i}(\\theta)+m_{i}(\\theta) $ A type $ \\theta_{i} $ buyer $ i $ ’s expected transfer if he reports his type to be $ \\hat{\\theta}_{i} $ and all other buyers truthfully reports their types $ \\theta_{-i} $ $$ \\bar{m}_{i}\\left(\\hat{\\theta}_{i}\\right) \\doteq E_{\\theta_{-i}}\\left[m_{i}\\left(\\hat{\\theta}_{i}, \\theta_{-i}\\right)\\right] $$ A type $ \\theta_{i} $ buyer $ i $ ’s probability of receiving the object if he reports his type to be $ \\hat{\\theta}_{i} $ and all other buyers truthfully reports their types $ \\theta_{-i} $ $$ \\bar{x}_{i}\\left(\\hat{\\theta}_{i}\\right) \\doteq E_{\\theta_{-i}}\\left[x_{i}\\left(\\hat{\\theta}_{i}, \\theta_{-i}\\right)\\right] $$ A type $ \\theta_{i} $ buyer $ i $ ’s expected utility if he reports his type to be $ \\hat{\\theta}_{i} $ and all other buyers truthfully reports their types $ \\theta_{-i} $ $$ \\begin{aligned} \u0026 E_{\\theta_{-i}}\\left[v_{i}\\left(x(\\cdot), m_{i}(\\cdot), \\hat{\\theta}_{i}\\right) | \\theta_{i}\\right] \\ =\u0026 E_{\\theta_{-i}}\\left[\\theta_{i} \\cdot x_{i}\\left(\\hat{\\theta}_{i}, \\theta_{-i}\\right)+m_{i}\\left(\\hat{\\theta}_{i}, \\theta_{-i}\\right)\\right] \\ =\u0026 \\theta_{i} \\cdot \\bar{x}_{i}\\left(\\hat{\\theta}_{i}\\right)+\\bar{m}_{i}\\left(\\hat{\\theta}_{i}\\right) \\end{aligned} $$ A type $ \\theta_{i} $ buyer $ i $ ’s expected utility if he truthfully reports his type $ \\theta_{i} $ $$ V_{i}\\left(\\theta_{i}\\right)=\\theta_{i} \\cdot \\bar{x}_{i}\\left(\\theta_{i}\\right)+\\bar{m}_{i}\\left(\\theta_{i}\\right) $$ 【Proposition】23.D.2 Necessary and Sufficient Conditions for Bayesian Incentive Compatibility: The choice rule $ f(\\cdot)=\\left(x(\\cdot), m_{1}(\\cdot), \\ldots, m_{n}(\\cdot)\\right) $ is Bayesian Incentive Compatible if and only if, for all $ i=1, \\ldots, n, $ the following conditions hold: $ \\bar{x}_{i}(\\cdot) $ is non-decreasing. For all $ \\theta_{i} $ of buyer $ i $ $$ V_{i}\\left(\\theta_{i}\\right)=V_{i}\\left(\\underline{\\theta}_{i}\\right)+\\int_{\\underline{\\theta}_{i}}^{\\theta_{i}} \\bar{x}_{i}(s) d s $$ **Proof for Necessity** $ \" \\Rightarrow \" $ WLOG, consider $ \\hat{\\theta}{i}\u003e\\theta{i} $. Bayesian incentive compatibility (BIC) implies (此时是对同一个买者，$ \\theta $是随机变量，可能取到不同的值): $$ \\begin{aligned} \\operatorname{type} \\theta_{i} \\operatorname{BIC}: V_{i}\\left(\\theta_{i}\\right) \u0026 \\geq \\theta_{i} \\cdot \\bar{x}_{i}\\left(\\hat{\\theta}_{i}\\right)+\\bar{m}_{i}\\left(\\hat{\\theta}_{i}\\right) \\ \u0026=\\theta_{i} \\cdot \\bar{x}_{i}\\left(\\hat{\\theta}_{i}\\right)+\\bar{m}_{i}\\left(\\hat{\\theta}_{i}\\right)+\\hat{\\theta}_{i} \\cdot \\bar{x}_{i}\\left(\\hat{\\theta}_{i}\\right)-\\hat{\\theta}_{i} \\cdot \\bar{x}_{i}\\left(\\hat{\\theta}_{i}\\right) \\ \u0026=V_{i}\\left(\\hat{\\theta}_{i}\\right)+\\left(\\theta_{i}-\\hat{\\theta}_{i}\\right) \\cdot \\bar{x}_{i}\\left(\\hat{\\theta}_{i}\\right) \\ \\operatorname{type} \\hat{\\theta}_{i} \\operatorname{BIC}: V_{i}\\left(\\hat{\\theta}_{i}\\right) \u0026 \\geq V_{i}\\left(\\theta_{i}\\right)+\\left(\\hat{\\theta}_{i}-\\theta_{i}\\right) \\cdot \\bar{x}_{i}\\left(\\theta_{i}\\right) \\end{aligned} $$ Combining the two: $$ \\bar{x}_{i}\\left(\\hat{\\theta}_{i}\\right) \\geq\\left(V_{i}\\left(","date":"0001-01-01","objectID":"/8.-mechanism-design/:3:4","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.4 Dominant Strategy Implementation ","date":"0001-01-01","objectID":"/8.-mechanism-design/:4:0","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.4.1 Example: SPA as an Allocation Mechanism 【Example MWG 23.B.6.】Consider an SPA with one seller (player 0 ) and two bidders (player 1 and 2). Each bidder with valuation $ \\theta_{i} \\sim U[0,1] $. Dominant-strategy equilibrium of the auction is $ b_{i}\\left(\\theta_{i}\\right)=\\theta_{i}, \\forall i$. The corresponding choice rule $ f(\\theta)=\\left(x_{0}(\\theta), x_{1}(\\theta), x_{2}(\\theta), m_{0}(\\theta), m_{1}(\\theta), m_{2}(\\theta)\\right) $ is $$ \\begin{array}{l} x_{1}(\\theta)=1 \\text { if } \\theta_{1} \\geq \\theta_{2} ;=0 \\text { if } \\theta_{1}\u003c\\theta_{2} \\ x_{2}(\\theta)=1 \\text { if } \\theta_{1}\u003c\\theta_{2} ;=0 \\text { if } \\theta_{1} \\geq \\theta_{2} \\ m_{1}(\\theta)=-\\theta_{2} \\cdot x_{1}(\\theta) \\ m_{2}(\\theta)=-\\theta_{1} \\cdot x_{2}(\\theta) \\ x_{0}(\\theta)=0, \\forall \\theta, m_{0}(\\theta)=-\\left(m_{1}(\\theta)+m_{2}(\\theta)\\right) \\end{array} $$ ","date":"0001-01-01","objectID":"/8.-mechanism-design/:4:1","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.4.2 Definition 【Definition】The strategy profile $ s^{*}(\\cdot)=\\left(s_{1}^{*}(\\cdot), \\ldots, s_{n}^{*}(\\cdot)\\right) $ is a **dominant strategy equilibrium** of the mechanism $ \\Gamma=\\left{A_{1}, A_{2}, \\ldots, A_{n}, g(\\cdot)\\right} $ if for every $ i \\in N $ and for every $ \\theta_{i} \\in \\Theta_{i} $ $$ v_{i}\\left(g\\left(s_{i}^{*}(\\theta), a_{-i}\\right), \\theta\\right) \\geq v_{i}\\left(g\\left(a_{i}^{\\prime}, a_{-i}\\right), \\theta\\right) $$ for all $ a_{i}^{\\prime} \\in A_{i} $ and for all $ a_{-i} \\in A_{-i} $ Is there a mechanism $ \\Gamma $ that implements $ f(\\cdot) $ in dominant strategies? Since a dominant strategy equilibrium is a special case of a Bayesian equilibrium, the revelation principle applies. So we only check that $ f(\\cdot) $ is implementable in dominant strategies directly to see if $ f(\\cdot) $ is implementable in dominant strategies. That is $$ v_{i}\\left(f\\left(\\theta_{i}, \\theta_{-i}\\right), \\theta_{i}\\right) \\geq v_{i}\\left(f\\left(\\hat{\\theta}_{i}, \\theta_{-i}\\right), \\theta_{i}\\right) $$ for all $ \\hat{\\theta}_{i} \\in \\Theta_{i}, $ and for all $ \\theta_{-i} \\in \\Theta_{-i} $. Dominant strategy implementation is also known as dominant strategy incentive compatibility (DSIC). Then, DSIC is a more demanding property than BIC, in particular, DSIC requires that players find truthtelling optimal regardless of the specific types of their opponents and their specific strategies in equilibrium. In contrast, BIC from the previous section only requires truthtelling to be utility maximizing in expectation and given that all other players are truthfully report their types. ","date":"0001-01-01","objectID":"/8.-mechanism-design/:4:2","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.4.3 VicKery-Clarke-Groves Mechanism Recall that our quasi-linear preferences are additive in money $$ v_{i}\\left(x, m_{i}, \\theta_{i}\\right)=u_{i}\\left(x, \\theta_{i}\\right)+m_{i} $$ There is a nice feature of this quasi-linear environment: monetary transfer can benefit the whole group. Imagine player $ i $ with type $ \\theta_{i} $ and player $ j $ with type $ \\theta_{j} $ such that $$ u_{i}\\left(x^{\\prime}, \\theta_{i}\\right)\u003eu_{i}\\left(x, \\theta_{i}\\right), u_{j}\\left(x, \\theta_{j}\\right)\u003eu_{j}\\left(x^{\\prime}, \\theta_{j}\\right) $$ Intuitively, player $ i $ would prefer alternative $ x^{\\prime} $ and player $ j $ would prefer alternative $ x $. Furthermore, let $$ u_{i}\\left(x^{\\prime}, \\theta_{i}\\right)-u_{i}\\left(x, \\theta_{i}\\right)\u003eu_{j}\\left(x, \\theta_{j}\\right)-u_{j}\\left(x^{\\prime}, \\theta_{j}\\right) $$ it implies that the gains received by player $ i $ when implementing alternative $ x^{\\prime} $ are greater than the gains received by player $ j $ when implementing alternative $ x $. Then there must be an amount of money $ k\u003e0 $ satisfying $$ u_{i}\\left(x^{\\prime}, \\theta_{i}\\right)-u_{i}\\left(x, \\theta_{i}\\right)\u003ek\u003eu_{j}\\left(x, \\theta_{j}\\right)-u_{j}\\left(x^{\\prime}, \\theta_{j}\\right) $$ This $ k $ is the transfer amount from player $ i $ to $ j $ when implementing $ x^{\\prime} . $ (**Coase’s Spirit** is back!) 【Proposition】In the quasilinear environment, given a state of the world $ \\theta \\in \\Theta, $ an alternative $ x^{*} \\in X $ is Pareto optimal if and only if it is a solution to $$ \\max {x \\in X} \\sum{i=1}^{I} u_{i}\\left(x, \\theta_{i}\\right) $$ **Proof:** If an alternative a did not maximize this sum, then there was another $ x^{\\prime} $ that did. Then money transfers among players that would ensure the gains of some players more than compensate for the losses of others. 【Definition】We call a decision rule $ x^{}(\\cdot) $ the first-best decision rule if for all $ \\theta \\in \\Theta, x^{}(\\theta) $ is Pareto optimal. $$ x^{*}(\\theta) \\in \\arg \\max {x \\in X} \\sum{i=1}^{I} u_{i}\\left(x, \\theta_{i}\\right) \\forall \\theta \\in \\Theta $$ When faced with the Pareto optimal choice rule $ \\left(x^{*}(\\cdot), m_{1}(\\cdot), \\ldots, m_{n}(\\cdot)\\right), $ will truth-telling be a dominant strategy for each player in the direct revelation mechanism? No, when $ m_{i}\\left(\\hat{\\theta}_{i}, \\hat{\\theta}_{-i}\\right) \\equiv 0\\left(\\hat{\\theta}_{i} \\text { is announced by player } i\\right) . $ The reason is that each player $ i $ only maximizes his own payoff, not the total surplus. This problem could be solved by having a clever transfer rule $ m_{i}\\left(\\hat{\\theta}_{i}, \\hat{\\theta}_{-i}\\right) $ to **let player internalize the externality**. 【Definition】Given announcements $ \\hat{\\theta}, $ the choice rule $ f(\\hat{\\theta})=\\left(x^{*}(\\hat{\\theta}), m_{1}(\\hat{\\theta}), \\ldots, m_{n}(\\hat{\\theta})\\right) $ is a **Vickrey-Clarke-Groves** **(VCG) mechanism** if $ x^{*}(\\cdot) $ is the **first-best decision rule** and if for all $ i \\in N $ $$ m_{i}(\\hat{\\theta})=\\sum_{j \\neq i} u_{j}\\left(x^{*}\\left(\\hat{\\theta}_{i}, \\hat{\\theta}_{-i}\\right), \\hat{\\theta}_{j}\\right)+h_{i}\\left(\\hat{\\theta}_{-i}\\right) $$ where $ h_{i}\\left(\\theta_{-i}\\right) $ is an arbitrary function of $ \\hat{\\theta}_{-i} $. 【Proposition】Any VCG mechanism is truthfully implementable in dominant strategies. Proof: In the VCG mechanism every player i solves $$ \\begin{aligned} \u0026\\max {\\theta{i} \\in \\Theta_{i}} u_{i}\\left(x^{*}\\left(\\hat{\\theta}_{i}, \\hat{\\theta}_{-i}\\right), \\theta_{i}\\right)+m_{i}\\left(\\hat{\\theta}_{i}, \\hat{\\theta}_{-i}\\right) \\ =\u0026\\max _{\\theta_{i} \\in \\Theta_{i}} \\underbrace{u_{i}\\left(x^{*}\\left(\\hat{\\theta}_{i}, \\hat{\\theta}_{-i}\\right), \\theta_{i}\\right)+\\sum_{j \\neq i} u_{j}\\left(x^{*}\\left(\\hat{\\theta}_{i}, \\hat{\\theta}_{-i}\\right), \\hat{\\theta}_{j}\\right)}_{\\text { total surplus }}+h_{i}\\left(\\hat{\\theta}_{-i}\\right) \\end{aligned} $$ $ h_{i}\\left(\\hat{\\theta}_{-i}\\right) $ does not affect $ i^{\\prime} s $ choice. Player $ i $ indeed","date":"0001-01-01","objectID":"/8.-mechanism-design/:4:3","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"8.4.4 Pivotal mechanism Pivotal mechanism suggested by Clarke (1971) is a particular VCG mechanism. It is obtained by setting $$ h_{i}\\left(\\hat{\\theta}_{-i}\\right)=-\\sum_{j \\neq i} u_{j}\\left(x_{-i}^{*}\\left(\\hat{\\theta}_{-i}\\right), \\hat{\\theta}_{j}\\right) $$ where $$ x_{-i}^{*}\\left(\\hat{\\theta}_{-i}\\right) \\in \\arg \\max _{x \\in X} \\sum_{j \\neq i} u_{j}\\left(x, \\hat{\\theta}_{j}\\right) $$ is **the optimal choice of $ x $ for a society from which player $ i $ was absent**. Thus $$ m_{i}(\\hat{\\theta})=\\sum_{j \\neq i} u_{j}\\left(x^{*}\\left(\\hat{\\theta}_{i}, \\hat{\\theta}_{-i}\\right), \\hat{\\theta}_{j}\\right)-\\sum_{j \\neq i} u_{j}\\left(x_{-i}^{*}\\left(\\hat{\\theta}_{-i}\\right), \\hat{\\theta}_{j}\\right) $$ Pivotal mechanism lets player $ i $ make his announcement that affects the outcome had he not been part of society. There are relevant cases: Case $ 1: x^{*}\\left(\\hat{\\theta}{i}, \\hat{\\theta}{-i}\\right)=x_{-i}^{*}\\left(\\hat{\\theta}_{-i}\\right) $ where player i’s announcement does not change what would have happened if he were not part of society. Then the mechanism specifies a transfer of zero to $ i $. Case $ 2: x^{*}\\left(\\hat{\\theta}{i}, \\hat{\\theta}{-i}\\right) \\neq x_{-i}^{*}\\left(\\hat{\\theta}_{-i}\\right) $ where player $ i $ is pivotal that his announcement changes what would have happened without him. **His transfer ends up taxing him for the externality that his announcement imposes on the other players**. Returning to Example 2 , where the mechanism designer is trying to determine who to give a license to use a certain portion of the electromagnetic spectrum for cell coverage. An object can be allocated to one of $ N $ players. The value of owning the private good for player $ i $ is given by $ u_{i}\\left(x, \\theta_{i}\\right)=\\theta_{i} x_{i} $. The first-best allocation solves $$ \\max {\\left(x{1}, \\ldots x_{n}\\right) \\in{0,1}^{n}} \\sum_{i=1} \\theta_{i} x_{i} \\text { subject to } \\sum_{i} x_{i}=1 $$ Which results in allocating the good to the player $ i^{*} $ with the highest valuation: $ i^{*} \\in \\arg \\max x_i \\theta_{i}, $ and $$ x_{i}^{*}(\\theta)=\\left{\\begin{array}{cc} 1 \u0026 \\text { if } i=i^{*} \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$ The pivotal mechanism then has transfers $$ \\begin{aligned} m_{i}(\\hat{\\theta}) \u0026=\\sum_{j \\neq i} u_{j}\\left(x^{*}(\\hat{\\theta}), \\hat{\\theta}_{-i}\\right)-\\sum_{j \\neq i} u_{j}\\left(x_{-i}^{*}\\left(\\hat{\\theta}_{-i}\\right), \\hat{\\theta}\\right) \\ \u0026=\\left{\\begin{array}{cc} -\\left{\\max _{j \\neq i^{*}} \\hat{\\theta}_{j}\\right} 1 \u0026 \\text { if } i=i^{*} \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. \\end{aligned} $$ That is, every player $ i \\neq i^{*} $ is not pivotal and his presence does not affect the allocation. Therefore $ m_{i}(\\hat{\\theta})=0 $. Player $ i^{} $ is pivotal: without him, the object would go to the player with the second-highest valuation. The total surplus would be $ \\max _{j \\neq i^{}} \\theta_{j} $. This is the externality player $ i $ ' imposes on the others by being present, and how much he has to pay in the pivotal mechanism. Notice that this mechanism is identical to the second-price sealed-bid auction. 【Example】Consider a seller auctioning one item to 5 bidders with valuations $$ \\theta_{1}=20, \\theta_{2}=15, \\theta_{3}=12, \\theta_{4}=10, \\theta_{5}=6 $$ A VCG mechanism allocates the item to bidder 1 and bidder 1’s transfer is $$ \\begin{aligned} m_{1}(\\theta) \u0026=\\underbrace{\\sum_{j \\neq 1} u_{j}\\left(x\\left(\\theta_{1}, \\theta_{-1}\\right), \\theta_{j}\\right)}_{\\text {Value of all others when bidder } 1 \\text { is here }}-\\underbrace{\\sum_{j \\neq 1} u_{j}\\left(x_{-1}\\left(\\theta_{-1}\\right), \\theta_{j}\\right)}_{\\text { Value of all others when bidder 1 not here } } \\ \u0026=\\sum_{j=2}^{5} 0-\\left(15+\\sum_{j=3}^{5} 0\\right) \\ \u0026=-15 \\end{aligned} $$ Differences between the two terms is $ -15, $ which is the transfer to (i.e. payment from) bidder 1. Bidder 1 pays 15, which **equals to the second highest valuation (second-price auction)**. Consider the same 5 bidders with th","date":"0001-01-01","objectID":"/8.-mechanism-design/:4:4","tags":null,"title":"","uri":"/8.-mechanism-design/"},{"categories":null,"content":"A1 Optimization ","date":"0001-01-01","objectID":"/a1-optimization/:0:0","tags":null,"title":"","uri":"/a1-optimization/"},{"categories":null,"content":"1.1 Unconstrained Optimization Optimization problem $$ \\max _{x} f(x) $$ subject to $\\mathrm{x} \\in S$. where $f$ is the objective function, x is the choice variable, $S$ is the constraint set. The solution to the optimization problem $x^{}$ is called maximizer; $f\\left(x^{}\\right)$ is called the maximum (value) of the function $f$ subject to the constraint $\\mathrm{x} \\in S$ Local maximizer: the variable $x^{}$ is a local maximizer of the function $f$ subject to the constraint $\\mathrm{x} \\in S$ if there is a number $\\epsilon\u003e0$ such that $f(\\mathrm{x}) \\leq f\\left(\\mathrm{x}^{}\\right)$ for all $\\mathrm{x} \\in S$ for which the distance between $\\mathrm{x}$ and $\\mathrm{x}^{*}$ is at most $\\epsilon$ Global maximizer ","date":"0001-01-01","objectID":"/a1-optimization/:1:0","tags":null,"title":"","uri":"/a1-optimization/"},{"categories":null,"content":"1.1.1 Existence of solution Bounded set: the set $S$ is bounded if there exists a number $k$ such that the distance of every point in $S$ from the origin is at most $k$ Compact set: a closed and bounded set is a compact set. A continuous function on a compact set attains both a maximum and a minimum on the set. Necessary condition of interior solution Let $f$ be a differentiable function of $n$ variables defined on the set $S$. If the point x in the interior of $S$ is a local or global maximizer or minimizer of $f$ then $$ f_{i}(x)=0 \\text { for } i=1, \\ldots, n $$ Let $f$ be a function of $n$ variables with continuous partial derivatives of first and second order, defined on the set $S$. Suppose that $x^{*}$ is a stationary point of $f$ in the interior of $S$ (so that $f_{i}(\\mathbf{x})=0$ for all $i$ ) If $H\\left(\\mathbf{x}^{}\\right)$ is negative definite then $\\mathbf{x}^{}$ is a local maximizer If $x^{}$ is a local maximizer then $H\\left(x^{}\\right)$ is negative semidefinite If $H\\left(\\mathbf{x}^{}\\right)$ is positive definite then $\\mathbf{x}^{}$ is a local minimizer If $\\mathrm{x}^{}$ is a local minimizer then $H\\left(\\mathrm{x}^{}\\right)$ is positive semidefinite Saddle point: a stationary point that is neither a local maximizer nor a local minimizer is a saddle point. ","date":"0001-01-01","objectID":"/a1-optimization/:1:1","tags":null,"title":"","uri":"/a1-optimization/"},{"categories":null,"content":"1.1.2 Envelope theorem Let $f$ be a continuously differentiable function of $n+k$ variables. Define the function $f^{}$ of $k$ variables by $$ f^{}(\\mathrm{r})=\\max _{\\mathrm{x}} \\mathrm{f}(\\mathrm{x}, \\mathrm{r})=f\\left(\\mathrm{x}^{*}(\\mathrm{r}), \\mathrm{r}\\right) $$ where $x$ is an $n$ -vector and $r$ is a $k$ -vector. If the solution of the maximization problem is a continously differentiable function of $\\mathrm{r}$ then $$ f_{h}^{*}(\\mathrm{r})=f_{n+h}\\left(\\mathrm{x}^{*}(\\mathrm{r}), \\mathrm{r}\\right) $$ Instead of the total derivative, you only need to check the partial derivative w.r.t. the parameter of interest at the optimizing point. ","date":"0001-01-01","objectID":"/a1-optimization/:1:2","tags":null,"title":"","uri":"/a1-optimization/"},{"categories":null,"content":"1.2 Optimization with Equality Constraints Let $f$ and $g^{1}, \\ldots, g^{m}$ be continuously differentiable functions of $n$ variables defined on the set $S$, let $c_{j}$ for $j=1, \\ldots, m$ be numbers. Now we face the following problem $$ \\max _{x} f(x) \\text { subject to } g^{j}(x)=c_{j} \\text { for } j=1, \\ldots, m $$ ","date":"0001-01-01","objectID":"/a1-optimization/:2:0","tags":null,"title":"","uri":"/a1-optimization/"},{"categories":null,"content":".1.2.1 Lagrange method: constrained $\\Rightarrow$ unconstrained Transform the constrained problem into an unconstrained problem with $m$ more variables $$ \\max {\\mathbf{x}, \\lambda} \\mathscr{L}(\\mathbf{x}, \\lambda)=f(\\mathbf{x})-\\sum{j=1}^{m} \\lambda_{j}\\left(g^{j}(\\mathbf{x})-c_{j}\\right) $$ First and second order conditions Special cases where $f$ is concave (convex) and the $g^{\\prime}$ s are convex (concave) First order conditions $$ \\begin{array}{l} \\frac{\\partial \\mathscr{L}\\left(\\mathrm{x}^{}, \\lambda^{}\\right)}{\\partial x_{i}}=f_{i}\\left(\\mathrm{x}^{*}\\right)-\\left.\\sum_{j=1}^{m} \\lambda_{j}^{*} \\frac{\\partial g^{j}}{\\partial x_{i}}\\right|_{\\mathrm{x}^{*}}=0 \\text { for } i=1, \\ldots, n \\ \\frac{\\partial \\mathscr{L}\\left(\\mathrm{x}^{*}, \\lambda^{*}\\right)}{\\partial \\lambda_{j}}=c_{j}-g^{j}\\left(\\mathrm{x}^{*}\\right)=0 \\text { for } j=1, \\ldots, m \\end{array} $$ A special case with only one constraint $$ \\begin{aligned} \\nabla f\\left(x^{}\\right)\u0026=\\lambda^{} \\nabla g\\left(x^{}\\right) \\ g\\left(x^{}\\right)\u0026=c \\end{aligned} $$ ","date":"0001-01-01","objectID":"/a1-optimization/:2:1","tags":null,"title":"","uri":"/a1-optimization/"},{"categories":null,"content":"1.2.2 Lagrange multiplier $$ \\lambda^{}=\\frac{\\nabla f\\left(x^{}\\right)^{\\prime} \\Delta x}{\\nabla g\\left(x^{*}\\right)^{\\prime}\\Delta x} $$ The rate of change in the maximum over a change in the constraint. The value of the Lagrange multiplier on the $j$th constraint at the solution of the problem is equal to the rate of change in the maximal value of the objective function as the $j$th constraint is relaxed. e.g. Shadow price of scarce resource (internal value/imputed value) ","date":"0001-01-01","objectID":"/a1-optimization/:2:2","tags":null,"title":"","uri":"/a1-optimization/"},{"categories":null,"content":"1.2.3 Envelope theorem Let $f$ and $g^{j}$ for $j=1, \\ldots, m$ be continuously differentiable functions of $n+k$ variables. Define the function $f^{}$ of $k$ variables by $$ \\mathscr{L}^{}(\\mathrm{r})=\\max _{\\mathrm{x}} \\mathrm{f}(\\mathrm{x}, \\mathrm{r}) \\text { subject to } g^{\\prime}(\\mathrm{x}, \\mathrm{r})=0 \\text { for } j=1, \\ldots, m $$ where x is an $n$-vector and $\\mathrm{r}$ is a $k$-vector. Suppose that the solution of the maximization problem and the associated Lagrange multipliers $\\lambda_{1}, \\ldots, \\lambda_{m}$ are continuously differentiable functions of $r,$ then $$ f_{h}^{*}(\\mathrm{r})=\\mathscr{L}_{n+h}\\left(\\mathrm{x}^{*}(\\mathrm{r}), \\mathrm{r}\\right) \\text { for } h=1, \\ldots, k $$ where the function $\\mathscr{L}$ is defined by $\\mathscr{L}(\\mathrm{x}, \\mathrm{r})=f(\\mathrm{x}, \\mathrm{r})-\\sum_{j=1}^{m} \\lambda_{j}g^{j}\\left( \\mathrm{x}, \\mathrm{r}\\right)$ for every $(x, r)$ ","date":"0001-01-01","objectID":"/a1-optimization/:2:3","tags":null,"title":"","uri":"/a1-optimization/"},{"categories":null,"content":"1.3 Optimization with Inequality Constraints Optimization problem $$ \\max {x} f(x) \\text { subject to } g^{j}(x) \\leq c{j} \\text { for } j=1, \\ldots, m $$ Lagrange function $$ \\mathscr{L}(\\mathbf{x}, \\lambda)=f(\\mathbf{x})-\\sum_{j=1}^{m} \\lambda_{j}\\left(g^{j}(\\mathbf{x})-c_{j}\\right) $$ ","date":"0001-01-01","objectID":"/a1-optimization/:3:0","tags":null,"title":"","uri":"/a1-optimization/"},{"categories":null,"content":"1.3.1 Kuhn-Tucker conditions $(x, \\lambda)$ satisfies Kuhn-Tucker conditions for the problem if $L_{i}(x, \\lambda)=f_{i}\\left(x^{*}\\right)-\\left.\\sum_{j=1}^{m} \\lambda_{j}^{*} \\frac{\\partial g^{j}}{\\partial x_{i}}\\right|_{x^{*}}=0$ for $i=1, \\ldots, n$ $g^{j}(x)-c_{j}=0 \\text{ \u0026 } \\lambda_{j} \u003e 0$ or $g^{j}(\\mathbf{x})-c_{j}\u003c0 \\text{ \u0026 } \\lambda_{j}=0$ for $j=1, \\ldots, m$ The second set of conditions are equivalent to $$ \\lambda_{j} \\geq 0, g^{\\prime}(\\mathrm{x}) \\leq c_{j} \\text { and } \\lambda_{j}\\left(\\mathscr{f}^{j}(\\mathrm{x})-\\mathrm{c}_{j}\\right)=0 \\text { for } j=1, \\ldots, m $$ one of two inequalities must be binding - “complementary slackness condition”: If a Lagrange multiplier is positive, its associated constraint must be binding; if constraint is slack, its associated Lagrange multiplier must be zero. ","date":"0001-01-01","objectID":"/a1-optimization/:3:1","tags":null,"title":"","uri":"/a1-optimization/"},{"categories":null,"content":"Outline Game theory (MWG Chapter 7,8,9)： Game theory basic concepts, Nash equilibrium, strategic-form game, extensive-form game, Bayesian game. Information economics (MWG Chapter 13, 14) Incomplete information game, adverse selection, moral hazard. Mechanism design (MWG Chapter 23) Mechanism design theory, auction and revenue maximizing mechanism. (the most challenging part!) Social choice (MWG Chapter 21) Social choice theory, collective decision and welfare. 1. Basic Element The players, the rules, the outcomes, the payoffs ","date":"0001-01-01","objectID":"/chapter1.2.3./:0:1","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"1.1 Type Information about strategies and payoffs are complete: both prisoners know the available strategies and payoffs of the other. A game has complete information if the structure of the game tree (including the payoffs) is common knowledge among players. A game is one of perfect information if each information set contains a single decision node. Otherwise, it is a game of imperfect information. Simultaneous-move Games: Games where players choose actions simultaneously. Sequential-move Games: Games where players choose actions in a particular sequence. \r information sets: 信息集的集合是所有参与者决策结的分割 partition：完备事件组 Each partition element, i.e. one information set, contains one or some of this player’s decision nodes. Each information set represents a possible distinguishable circumstance under which a player is called upon to move. When the game has reached a non-singleton information set, the player being called upon to move is unable to differentiate between the decision nodes within this information set. The construction of “information set” must satisfy the following two restrictions: A player must have the same set of possible actions at every node within an information set. Perfect recall: a player (1) does not forget what she once knew, or (2) what she once have done. ","date":"0001-01-01","objectID":"/chapter1.2.3./:1:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"1.2 strategy The concept of a strategy is central to game theory. A strategy is a fully described behavioral disposition. A player’s strategy is a Complete Contingent Plan: 针对所有可能发生情况的行动方案 A strategy will always completely specify behavior, even at contingencies that are ruled out by the strategy itself. \r A winning strategy for the first mover: “After each round in which the other player moved his/her piece two cells forward, move its piece one cell forward. Otherwise, move its piece two cells forward.” Similarly, in any games with a multiple of 3 cells, the second mover has a win-for-sure strategy for the game: Second-mover advantage However, in any games with not-multiple-of-3 cells, the first mover has a win-for-sure strategy for the game: First-mover advantage ","date":"0001-01-01","objectID":"/chapter1.2.3./:2:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"1.3 Game tree / Extension Form consists of An initial node (in our example the node marked with Chance). Decision nodes (in our example the nodes at which player A or B faces strategy choices). Terminal nodes (the nodes with no succeeding decision nodes; associated with payoff vectors). An assignment of players (in our example Chance, A and B) to decision nodes. Branches that start at decision nodes and which indicate the decisions available to the player acting at that node. An assignment of payoffs to players at each terminal node. Extensive game form 1 A finite set of nodes X , a finite set of possible actions A , and a finite set of players {1, . . . , I}. 2 A function p : X → {X ∪ ∅} specifying a single immediate predecessor each node x. Initial node is denoted as x0. Successors are then $s(x) = p^{−1} (x)$; s(x) and p(x) are disjoint. Set of terminal nodes is T = {x ∈ X : s(x) = ∅}. X - T are called decision nodes. Note: p(x) must be a function, not a correspondence. This is to exclude graphs in which some nodes without common predecessor. A function α : $X - {x_0} → A$ 1 giving the action that leads to any non-initial node x from its immediate predecessor p(x); 2 satisfying the property that (same action cannot lead to two different nodes); $$ \\text { if } x^{\\prime}, x^{\\prime \\prime} \\in s(x) \\text { and } x^{\\prime} \\neq x^{\\prime \\prime}, \\text { then } \\alpha\\left(x^{\\prime}\\right) \\neq \\alpha\\left(x^{\\prime \\prime}\\right) $$ 3 the set of choices available at decision node x is (actions that lead from x to s(x)). $$ c(x)=\\left{a \\in \\mathscr{A}: a=\\alpha\\left(x^{\\prime}\\right) \\text { for some } x^{\\prime} \\in s(x)\\right) $$ 4 A collection of information sets H and a function $H : X - T → H $ assigning each decision node x to an information set H(x) ∈ H . The information sets H form a partition of X \\ T with the following two restrictions: 1 all decision nodes assigned to one information set have the same choices: c(x) = c(x 0 ) if H(x) = H(x 0 ) 2 perfect recall : a player (1) does not forget what she once knew, or (2) what she once have done. All choices available at an information set H: C(H) = {a ∈ A : a ∈ c(x) for x ∈ H}. 5 A function ι : H → {0, 1, . . . , I} assigning each information set to the player (including nature, player 0) who moves at the decision nodes in that set. Player i’s collection of information sets is $$ \\mathscr{H}_{i}={H \\in \\mathscr{H}: i=\\iota(H)} $$ 6 A function ρ : H0 × A → [0, 1] assigning probabilities to actions at information sets where nature moves satisfying ρ(H, a) = 0 if a ∈/ C(H) and $\\Sigma_{a \\in C(H)} \\rho(H, a)=1$ for all H ∈ H0 7 A collection of payoff functions u = {u1(·), . . . , uI(·)} assigning utilities to the players for each terminal node that can be reached, ui : T → R. ui is Bernoulli. Accordingly, a game in extensive form is specified by the collection $\\Gamma_{E}={\\mathscr{X}, \\mathscr{A}, l, p(\\cdot), \\alpha(\\cdot), \\mathscr{H}(\\cdot), \\iota(\\cdot), \\rho(\\cdot), u}$ ","date":"0001-01-01","objectID":"/chapter1.2.3./:3:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"1.4 Normal form ","date":"0001-01-01","objectID":"/chapter1.2.3./:4:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"strategy Let $\\mathscr{H}_{i}$ denote the collection of player i’s information sets, $\\mathscr{A} /$ the set of possible actions in the game, and $C(H) \\subset \\mathscr{A}$ the set of actions possible at information set $H .$ A strategy for player $i$ is a function $s_{i}: \\mathscr{H}_{i} \\rightarrow \\mathscr{A}$ such that $s_{i}(H) \\in C(H)$ for all $H \\in \\mathscr{H}_{l}$ Profile of strategies:$s=\\left(s_{1}, \\ldots, s_{l}\\right)$ for each player i ","date":"0001-01-01","objectID":"/chapter1.2.3./:4:1","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"normal form For a game with $/$ players, the normal form representation $\\Gamma_{N}$ specifies for each player i a set of strategies $S_{i}$ and a payoff function $u_{i}\\left(s_{1}, \\ldots, s_{i}\\right)$ giving the von Neumann-Morgenstern utility levels associated with the (possibly random) outcome arising from strategies $\\left(s_{1}, \\ldots, s_{l}\\right)$ Formally, $\\Gamma_{N}=\\left[l,\\left{S_{i}\\right},\\left{u_{i}(\\cdot)\\right}\\right]$ 每一个策略轮廓都直接指向一个收益，使用这种一一对应可以描述一个博弈，即博弈标准型。 混合策略：$\\sigma i:S_i \\rightarrow[0,1]$，assigns to each pure strategy $s_i \\in S_i$ a probability number σi(si) ≥ 0 which si will be played, where $\\sum{s_i∈S_i} σ_i(s_i) = 1$. \r 期望收益 $$ E_\\sigma[U_i(s)] = \\sum_{s \\in S}[\\sigma_1(s_1)\\times \\dots \\times \\sigma_i(s_i)\\times \\dots \\times \\sigma_I(s_I) ]u_i(s_1,\\dots,s_i\\ldots,s_I) $$ where $S = S_1 \\times\\dots\\times S_I$ 要求参与者的策略概率分布之间相互独立 从而标准型被表示为$\\Gamma_{N}=\\left[l,\\left{\\Delta (S_{i})\\right},\\left{u_{i}(\\cdot)\\right}\\right]$ \r 显然（注意需要满足前述对信息集的限制条件），行为策略总可以导致一个（或多个）与之结果等价的混合策略（概率的乘法法则）；反过来，混合策略也总可以找到一个（或多个）与之结果等价的行为策略。 结果等价：$O(\\sigma)$表示最终收益的概率分布等价，$\\sigma=(\\sigma_i)_{i \\in I}$ is the probability distribution over terminal nodes that results when each player i follows the precepts of $\\sigma_i$ . 2. Simultaneous Move Games ","date":"0001-01-01","objectID":"/chapter1.2.3./:4:2","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.1 严格占优策略 A strategy $s_{i} \\in S_{i}$ is a **strictly dominant strategy** for player $i$ in game $\\Gamma_{N}=\\left[l,\\left{S_{i}\\right},\\left{u_{i}(\\cdot)\\right}\\right]$ if **for all** $s_{i}^{\\prime} \\neq s_{i},$ we have $u_{i}\\left(s_{i}, s_{-i}\\right)\u003eu_{i}\\left(s_{i}^{\\prime}, s_{-i}\\right)$ for all $s_{-i} \\in S_{-i}$ 严优策略一定会被采用，好于其他所有策略 ","date":"0001-01-01","objectID":"/chapter1.2.3./:5:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.2 严劣策略 ","date":"0001-01-01","objectID":"/chapter1.2.3./:6:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.2.1 纯策略 A strategy $s_{i} \\in S_{i}$ is **strictly dominated** for player $i$ in game $\\Gamma_{N}=\\left[I,\\left{S_{i}\\right},\\left{u_{i}(\\cdot)\\right}\\right]$ if there **exists** another strategy $s_{i}^{\\prime} \\in S_{i}$ such that for all $s_{-i} \\in S_{-i}, u_{i}\\left(s_{i}^{\\prime}, s_{-i}\\right)\u003eu_{i}\\left(s_{i}, s_{-i}\\right)$. In this case, we say that strategy $s_{i}^{\\prime}$ strictly dominates $s_{i}$ 至少有一个策略好于该策略，严劣策略一定不会被选择。 ","date":"0001-01-01","objectID":"/chapter1.2.3./:6:1","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.2.2 混合策略 A strategy $\\sigma_{i} \\in \\Delta\\left(S_{i}\\right)$ is strictly dominated for player $i$ in a normal-form game $\\Gamma_{N}=\\left[I,\\left{\\Delta\\left(S_{i}\\right)\\right},\\left{u_{i}(\\cdot)\\right}\\right]$ if there exists another strategy $\\sigma_{i}^{\\prime} \\in \\Delta\\left(S_{i}\\right)$ such that for all $\\sigma_{-i} \\in \\prod_{j \\neq i} \\Delta\\left(S_{j}\\right), u_{i}\\left(\\sigma_{i}^{\\prime}, \\sigma_{-i}\\right)\u003eu_{i}\\left(\\sigma_{i}, \\sigma_{-i}\\right)$ $$\\begin{aligned} \u0026\\text { Note that } u_{i}\\left(\\sigma_{i}^{\\prime}, \\sigma_{-i}\\right)-u_{i}\\left(\\sigma_{i}, \\sigma_{-i}\\right)\\ \u0026=\\sum_{s_{-i} \\in S_{-i}}\\left[\\prod_{k \\neq i} \\sigma_{k}\\left(s_{k}\\right)\\right]\\left[u_{i}\\left(\\sigma_{i}^{\\prime}, s_{-i}\\right)-u_{i}\\left(\\sigma_{i}, s_{-i}\\right)\\right] \\end{aligned}$$ Iterated Deletion of Strictly Dominated Strategies \r A reasoning process, the logic of which depends on each player being rational, knowing his/her opponent being rational, knowing that his/her opponent knows that oneself being rational, …, and so on. ","date":"0001-01-01","objectID":"/chapter1.2.3./:6:2","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.3 劣策略 A strategy $s_{i} \\in S_{i}$ is **weakly dominated** for player $i$ in game $\\Gamma_{N}=\\left[I,\\left{S_{i}\\right},\\left{u_{i}(\\cdot)\\right}\\right]$ if there exists another strategy $s_{i}^{\\prime} \\in S_{i}$ such that for all $s_{-i} \\in S_{-i}, u_{i}\\left(s_{i}^{\\prime}, s_{-i}\\right) \\geq u_{i}\\left(s_{i}, s_{-i}\\right)$ **with strict inequality for some** $s_{-i}$. In this case, we say that strategy $s^{\\prime}_i$ weakly dominates $s_i$ . \r A strategy is a weakly dominant strategy if it weakly dominates every other strategy in Si if players believe that any strategies of their rivals will be played with some positive probabilities, a weakly dominated strategy can be dismissed. Order does not matter in iterated deletion of strictly dominated strategies, but it does matter for iterated deletion of weakly dominated strategies. An example.(因此并不建议采用) \r ","date":"0001-01-01","objectID":"/chapter1.2.3./:7:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.4 纳什均衡 In game $\\Gamma_{N}=\\left[I,\\left{\\Delta\\left(S_{i}\\right)\\right},\\left{u_{i}(\\cdot)\\right}\\right],$ strategy $\\sigma_{i}$ is a **best response** for player $i$ to his rivals' strategies $\\sigma_{-i}$ if $u_{i}\\left(\\sigma_{i}, \\sigma_{-i}\\right) \\geq u_{i}\\left(\\sigma_{i}^{\\prime}, \\sigma_{-i}\\right)$ for all $\\sigma_{i}^{\\prime} \\in \\Delta\\left(S_{i}\\right)$ Strategy $\\sigma_{i}$ is **never a best response** if there is no $\\sigma_{-i}$ for which $\\sigma_{i}$ is a best response. ","date":"0001-01-01","objectID":"/chapter1.2.3./:8:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"纯策略 A strategy profile $s=\\left(s_{1}, \\ldots, s_{l}\\right)$ constitutes a Nash equilibrium of game $\\Gamma_{N}=\\left[l,\\left{\\mathcal{S}_{i}\\right},\\left{u_{i}(\\cdot)\\right}\\right]$ if for every $i=1, \\ldots, l, u_{i}\\left(s_{i}, s_{-i}\\right) \\geq u_{i}\\left(s_{i}^{\\prime}, s_{-i}\\right)$ for all $s_{i}^{\\prime} \\in S_{i}$ Mutually best response: nobody has any incentive to deviate to another strategy 纳什均衡不一定是帕累托最优 Nash equilibrium outcome coincide with the set that survives iterated elimination of strictly dominated strategies pure strategy is played in a Nash equilibrium, it may not survive the iterative elimination of weakly dominated strategies Cell-by-cell inspection always works(划线法/Best Response法) ","date":"0001-01-01","objectID":"/chapter1.2.3./:8:1","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"混合策略 A mixed strategy profile $\\sigma=\\left(\\sigma_{1}, \\ldots, \\sigma_{l}\\right)$ constitutes a (mixed-strategy) Nash equilibrium of game $\\Gamma_{N}=\\left[I,\\left{\\Delta S_{i}\\right},\\left{u_{i}(\\cdot)\\right}\\right]$ if for every $i=1, \\ldots, I$ $u_{i}\\left(\\sigma_{i}, \\sigma_{-i}\\right) \\geq u_{i}\\left(\\sigma_{i}^{\\prime}, \\sigma_{-i}\\right)$ for all $\\sigma_{i}^{\\prime} \\in \\Delta S_{i}$ 纳什均衡的存在性, Nash (1950) Every game $\\Gamma_{N}=\\left[I,\\left{\\Delta\\left(S_{i}\\right)\\right},\\left{u_{i}(\\cdot)\\right}\\right]$ in which the sets $S_{1}, \\ldots, S_{I}$ have a finite number of elements has a mixed strategy Nash equilibrium. 纳什均衡的充要条件 Let $S_{i}^{+} \\subset S_{i}$ denote the set of pure strategies that player i plays with positive probability in mixed strategy profile $\\sigma=\\left(\\sigma_{1}, \\ldots, \\sigma_{l}\\right) .$ Strategy profile $\\sigma$ is a Nash equilibrium of game $\\Gamma_{N}=\\left[I,\\left{\\Delta\\left(S_{i}\\right)\\right},\\left{u_{i}(\\cdot)\\right}\\right]$ if and only if for all $i=1, \\ldots, I$ $u_{i}\\left(\\boldsymbol{s}_{i}, \\sigma_{-i}\\right)=u_{i}\\left(\\boldsymbol{s}_{i}^{\\prime}, \\sigma_{-i}\\right)$ for all $\\boldsymbol{s}_{i}, \\boldsymbol{s}_{i}^{\\prime} \\in S_{i}^{+}$ $u_{i}\\left(\\boldsymbol{s}_{i}, \\sigma_{-i}\\right) \\geq u_{i}\\left(\\boldsymbol{s}_{i}^{\\prime}, \\sigma_{-i}\\right)$ for all $s_{i} \\in S_{i}^{+}$ and all $s_{i}^{\\prime} \\notin S_{i}^{+}$ 条件1是求解混合策略纳什均衡的方法 \r 还可以通过画图求解 \r 定理 A strictly dominated pure strategy is not used with positive probability in any mixed strategy Nash equilibrium.（对劣策略不成立） \r \r \r ","date":"0001-01-01","objectID":"/chapter1.2.3./:8:2","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.5 连续博弈/无限策略博弈 Game Matrixes ⇒ Profit Functions Best response sets ⇒ Best-response functions Nash equilibrium: Mutual best responses ","date":"0001-01-01","objectID":"/chapter1.2.3./:9:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.5.1 古诺模型 A game where two firms (duopoly) compete in terms of the quantity sold (market share) of a homogeneous good is referred to as a Cournot game after the French economist who first studied it. Duopoly Two firms, i ∈ {1, 2}, produce the same good at the same marginal cost c. Let their outputs be q1 and q2. Let Q = q1 + q2 be the total output. The inverse demand function of the market is P(Q) = a − Q if Q \u003c a; otherwise P(Q) = 0. $$ \\begin{aligned} \\pi_{i}\\left(q_{i}, q_{j}\\right) \u0026=P(Q) \\cdot q_{i}-c \\cdot q_{i} \\ \u0026=\\left{\\begin{array}{ll} \\left(a-q_{1}-q_{2}-c\\right) \\cdot q_{i}, \u0026 \\text { if } q_{1}+q_{2}\u003ca \\ -c \\cdot q_{i} \u0026 \\text { otherwise } \\end{array}\\right. \\end{aligned} $$ First we study how firm 1 would respond given the output $q_{2}$ of firm 2. Firms 1 chooses $q_{1}$ to solve: $\\max _{q_{1}} \\pi_{1}\\left(q_{1}, q_{2}\\right)$ Solving the F.O.C. gives the best response function of firm 1: $$ b_{1}\\left(q_{2}\\right)=\\left{\\begin{array}{ll} \\frac{1}{2}\\left(a-c-q_{2}\\right), \u0026 \\text { if } q_{2}\u003ca-c \\ 0, \u0026 \\text { otherwise } \\end{array}\\right. $$ Similarly, firm 2 ’s best response function $b_{2}\\left(q_{1}\\right)$ takes the same form. Solving $q_{1}=b_{1}\\left(q_{2}\\right)$ and $q_{2}=b_{2}\\left(q_{1}\\right),$ we have $$ q_{1}^{*}=q_{2}^{*}=\\frac{a-c}{3} $$ **Once a collusion is formed**, each produces half of the monopolistic quantity q1 = q2 = Qm/2 and earns half of the monopolistic profit πm/2. Now the optimization for the collusion becomes: $$ \\max {Q}(a-Q) \\cdot Q-c \\cdot Q $$ F.O.C. gives $$ \\begin{aligned} \u0026 a-Q-c-Q=0 \\ \\Rightarrow \\quad Q{m}^{*}=\u0026 \\frac{a-c}{2} \\end{aligned} $$ Each firm produces half of it: $q_{1}^{m}=q_{2}^{m}=\\frac{a-c}{4}$ \r Oligopoly Now let’s generalize the number of the firms to $J$ Each firm has identical costs: $C\\left(q^{j}\\right)=c \\cdot q^{j}, c \\geq 0$ Let inverse market demand be $$ p=a-b \\cdot \\sum_{j=1}^{J} q^{j}, a\u003e0, b\u003e0 \\text { and } a\u003ec $$ The profit for firm $j$ is $$ \\Pi^{j}\\left(q^{1}, \\ldots, q^{J}\\right)=\\left(a-b \\cdot \\sum_{k=1}^{J} q^{k}\\right) \\cdot q^{j}-c \\cdot q^{j} $$ The best response of firm $j$ when firms $k \\neq j$ produce $q^{k}$. The profit function can be rewritten as $$ \\Pi^{j}\\left(q^{1}, \\ldots, q^{J}\\right)=\\left(a-b \\cdot q^{j}-b \\cdot \\sum_{k \\neq j} q^{k}\\right) \\cdot q^{j}-c \\cdot q^{j} $$ The first-order condition is $$ a-b q^{j}-b \\sum_{k \\neq j} q^{k}-b q^{j}-c=0 $$ This gives firm $j$ ’s best response function: $$ q^{j}=\\frac{a-c}{2 b}-\\frac{\\sum_{k \\neq j} q^{k}}{2} $$ Solving $J$ simultaneous equations with $J$ unknowns. A short-cut: Symmetric equilibrium: $q_{j}=q^{*}$ for all $j$ Then, we have $$ q^{*}=\\frac{a-c}{2 b}-\\frac{(J-1) q^{*}}{2} $$ Thus, we have $$ q^{*}=\\frac{a-c}{b(J+1)} $$ Insert back, the market price is $$ p^{*}=a-\\frac{J(a-c)}{J+1} $$ Remark: the Cournot price decreases (increases) as the number of firm increases (decreases) Consider the price-cost margin: $p^{*}-c=\\frac{a-c}{J+1}$ The price-cost margin is the highest when $J=1$ : a monopolistic market outcome. Also, $\\lim _{J \\rightarrow \\infty}\\left(p^{*}-c\\right)=0:$ a perfectly competitive market outcome. ","date":"0001-01-01","objectID":"/chapter1.2.3./:9:1","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.5.2 Bertrand competition In the Bertrand competition of duopoly, firms compete over prices instead of quantities of outputs. Usually the price is a continuous variable. Two firms $i, j,$ with identical marginal cost $c .$ The market demand function (assuming $a\u003ec$ ) $$ D(p)=\\left{\\begin{array}{ll} a-p, \u0026 \\text { if } p \\leq a \\ 0, \u0026 \\text { if } p\u003ea \\end{array}\\right. $$ The two firms' choices: $p_{i}, p_{j} \\in \\mathbb{R}$ Assume that the consumers only buy from the firm that offers lower price. Thus the demand for firm $i$ is: $$ q_{i}\\left(p_{i}, p_{j}\\right)=\\left{\\begin{array}{ll} a-p_{i}, \u0026 \\text { if } p_{i}\u003cp_{j} \\ \\frac{1}{2}\\left(a-p_{i}\\right), \u0026 \\text { if } p_{i}=p_{j} \\ 0, \u0026 \\text { if } p_{i}\u003ep_{j} \\end{array}\\right. $$ Notice the discontinuity in the demand function: If $p_{j}\u003ec,$ firm $i$ has incentive to undercut by $p_{j}-\\epsilon$ and wins the entire market. If firm $j$ charges $p_{j}=p_{i},$ each’s sale changes to half of the market. If firm $j$ undercuts the price to $p_{j}\u003cp_{i}$, firm $j$ can sell to the entire market while firm $i$ has 0 demand. Firm i’s profit is also a piecewise (discontinuous) function: $$ \\pi_{i}\\left(p_{i}, p_{j}\\right)=\\left{\\begin{array}{ll} \\left(p_{i}-c\\right)\\left(a-p_{i}\\right), \u0026 \\text { if } p_{i}\u003cp_{j} \\ \\frac{1}{2}\\left(p_{i}-c\\right)\\left(a-p_{i}\\right), \u0026 \\text { if } p_{i}=p_{j} \\ 0, \u0026 \\text { if } p_{i}\u003ep_{j} \\end{array}\\right. $$ Instead of taking the f.o.c. directly, let’s find the optimum manually this time (note that $(a+c) / 2$ is the maximizer of the quadratic function of $p_{i}$ ). Depending on $p_{i}$ and $c, 4$ cases (where the B.R. is set-valued): $$ B_{i}\\left(p_{j}\\right)=\\left{\\begin{array}{ll} \\left{p_{i} \\in \\mathbb{R}: p_{i}\u003ep_{j}\\right}, \u0026 \\text { if } p_{j}\u003cc \\ \\left{p_{i} \\in \\mathbb{R}: p_{i} \\geq p_{j}\\right}, \u0026 \\text { if } p_{j}=c \\ \\emptyset, \u0026 \\text { if } c\u003cp_{j} \\leq \\frac{a+c}{2} \\ \\frac{a+c}{2}, \u0026 \\text { if } p_{j}\u003e\\frac{a+c}{2} \\end{array}\\right. $$ Discontinuous functions with a “jump” of value may not have a maximum Best-Response Correspondences and Equilibrium \r ","date":"0001-01-01","objectID":"/chapter1.2.3./:9:2","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.5.3 Hotelling’s Model This is the Hotelling’s location game in both industrial organization theory and the electoral competition model in political science. Two parties: Left or Right. (Or two coffee shops: H and D.) The parties only differ in policy positions. (Two coffee shops only differ in locations – the price are the same.) x1, x2 ∈ [0, 1]. Voters (uniformly distributed over [0, 1]) will vote for the candidate whose policy position is closer. (Consumers will only go to wherever is nearby.) To illustrate the best responses and Nash equilibria, we need to look at best response correspondence instead of functions. Majority rule makes the candidates' winning probabilities discontinuous: “If Donald Trump attracts slightly more than $1 / 2$ of the voters, he wins the election with prob. 1. Hillary and Donald resolves a tie with a coin flip, each’s winning probability decreases to 0.5 Let’s derive the parties' best-responses case-by-case: $$ B_{1}\\left(x_{2}\\right)=\\left{\\begin{array}{ll} \\left(x_{2}, 1-x_{2}\\right), \u0026 \\text { if } x_{2}\u003c\\frac{1}{2} \\ \\frac{1}{2}, \u0026 \\text { if } x_{2}=\\frac{1}{2} \\ \\left(1-x_{2}, x_{2}\\right), \u0026 \\text { if } x_{2}\u003e\\frac{1}{2} \\end{array}\\right. $$ ","date":"0001-01-01","objectID":"/chapter1.2.3./:9:3","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.6 Incomplete Information and Bayesian Nash Equilibrium ","date":"0001-01-01","objectID":"/chapter1.2.3./:10:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.6.1 Incomplete Information When players do not know others’ payoffs, the game is of incomplete information. Harsanyi (1967-68) proposed an approach: Each player’s payoffs are determined by the realization of a random variable. The realization of the random variable is observed only by the player. We call it a player’s “type.” The ex-ante probability is assumed to be common knowledge among all the players. A Bayesian game as $\\left[l,\\left{S_{i}\\right},\\left{u_{i}(\\cdot)\\right}, \\Theta, F(\\cdot)\\right]$ Each player $i$ has a payoff function $u_{i}\\left(s_{i}, s_{-i}, \\theta_{i}\\right),$ where $\\theta_{i} \\in \\Theta_{i}$ is a random variable chosen by nature that is observed only by player $i$. Let $\\Theta=\\Theta_{1} \\times \\cdots \\times \\Theta_{I}$. The joint probability of the $\\theta_{i}$ ’s is given by $F\\left(\\theta_{1}, \\ldots, \\theta_{l}\\right),$ which is common knowledge. A pure strategy for player $i$ is a function $s_{i}\\left(\\theta_{i}\\right),$ a decision rule, specifying for each realization of his type a strategy choice. That is, $s_{i}: \\Theta_{i} \\rightarrow S_{i} .$ We denote the collection of player i’s pure strategies as $\\mathscr{S}_{i}$ (a functional space). Player i’s expected payoff given strategy profile $\\left(s_{1}(\\cdot), \\ldots, s_{l}(\\cdot)\\right)$ is given by $\\tilde{u}_{i}\\left(s_{1}(\\cdot), \\ldots, s_{l}(\\cdot)\\right) \\doteq E_{\\theta}\\left[u_{i}\\left(s_{1}(\\theta), \\ldots, s_{l}(\\theta), \\theta_{i}\\right)\\right]$ Continuous Strategies: Cournot Competition Revisited \r 转换成如下形式 \r \r \r ","date":"0001-01-01","objectID":"/chapter1.2.3./:10:1","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"2.6.2 Bayesian Nash equilibrium A pure strategy Bayesian Nash equilibrium in this Bayesian game is a triple of actions, one for the woman and one for each type of man, with the no-deviation property as the action of woman is optimal given the actions of each type of man and the woman’s belief about the man’s types; the action of each type of man is optimal, given the woman’s action choice. 注意一下两种等价定义的区别 A pure strategy Bayesian Nash equilibrium for the Bayesian game $\\left[I,\\left{S_{i}\\right},\\left{u_{i}(\\cdot)\\right}, \\Theta, F(\\cdot)\\right]$ is a profile of decision rules $\\left(s_{1}(\\cdot), \\ldots, s_{l}(\\cdot)\\right)$ that constitutes a Nash equilibrium of game $\\Gamma_{N}=\\left[I,\\left{\\mathscr{S}_{i}\\right},\\left{\\tilde{u}_{i}(\\cdot)\\right}\\right]$ That is, for every $i=1, \\ldots, l, \\tilde{u}_{i}\\left(s_{i}(\\cdot), s_{-i}(\\cdot)\\right) \\geq \\tilde{u}_{i}\\left(s_{i}^{\\prime}(\\cdot), s_{-i}(\\cdot)\\right)$ for all $s_{i}^{\\prime}(\\cdot) \\in \\mathscr{S}_{i}$ According to the definition earlier, the condition is equivalent to: $$ E_{\\theta}\\left[u_{i}\\left(s_{i}(\\theta), s_{-i}(\\theta), \\theta_{i}\\right)\\right]\u003eE_{\\theta}\\left[u_{i}\\left(s_{i}^{\\prime}(\\theta), s_{-i}(\\theta), \\theta_{i}\\right)\\right] $$ A profile of decision rules $\\left(s_{1}(\\cdot), \\ldots, s_{l}(\\cdot)\\right)$ is a Bayesian Nash equilibrium in Bayesian game $\\left.l,\\left{S_{i}\\right},\\left{u_{i}(\\cdot)\\right}, \\Theta, F(\\cdot)\\right]$ if and only if, for all $i$ and all $\\bar{\\theta}_{i} \\in \\Theta_{i}$ occurring with positive probability $E_{\\theta_{-1}\\left[U_{i}\\left(s_{i}\\left(\\bar{\\theta}_{i}\\right), s_{-i}\\left(\\theta_{-i}\\right), \\bar{\\theta}_{i}\\right) | \\bar{\\theta}_{i}\\right] \\geq E_{\\theta_{-1},}\\left[u_{i}\\left(s_{i}^{\\prime}, s_{-i}\\left(\\theta_{-i}\\right), \\bar{\\theta}_{i}\\right) | \\bar{\\theta}_{i}\\right] \\text { for all } s_{i}^{\\prime} \\in S_{i}}$ Q: 这两种定义的等价，是表明即使每个决策者不知道自己的type，也能够达到和知道自己type情况下相同的贝叶斯纳什均衡嘛？ Q2 \r 可能会漏掉，女人混合策略，男人纯策略的均衡情况 \r \r \r 3. Dynamic Games 标准型不能反映顺序信息。 ","date":"0001-01-01","objectID":"/chapter1.2.3./:10:2","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"3.1 Backward Induction (core) Generalized backward induction procedure: Start at the end of the game tree, and identify the Nash equilibria for each of the final subgames (i.e. those that have no other subgames nested within); Select one Nash equilibrium in each of these final subgames, and derive the reduced extensive form game in which these final subgames are replaced by the payoffs that result in these subgames when players use these equilibrium strategies; Repeat steps 1 and 2 for the reduced game. Continue the procedure until every move in $\\Gamma_{E}$ is determined. This collection of moves at the various information sets of $\\Gamma_{E}$ constitutes a profile of SPNE strategies. If multiple equilibria are never encountered in any step of this process, this profile of strategies is the unique SPNE. If multiple equilibria are encountered, the full set of SPNEs is identified by repeating the procedure for each possible equilibrium that could occur for the subgames in question. ","date":"0001-01-01","objectID":"/chapter1.2.3./:11:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"3.2 子博弈精炼纳什均衡 A subgame of an extensive form game $\\Gamma_E$ is a subset of the game having the following properties: It begins with an information set containing a single decision node, contains all the decision nodes that are successors of this node, and contains only these nodes. If decision node x is in the subgame, then any $x ^\\prime \\in H(x)$ is also in the subgame. The whole game is a subgame (of itself). Proper subgame (strict subset) 子博弈精炼纳什均衡(SPNE) A strategy profile σ in extensive form game $\\Gamma_E$ induces a Nash equilibrium in a particular subgame of $\\Gamma_E$ if the moves specified in σ for information sets within the subgame constitute a Nash equilibrium when this subgame is considered in isolation. A profile of strategies $\\sigma = (\\sigma_1, . . . , \\sigma_I)$ in an extensive form game $\\Gamma_E$ is a subgame perfect Nash equilibrium (SPNE) if it induces a Nash equilibrium in every subgame of $\\Gamma_E$. 存在性 Subgame perfection and backward induction: for finite games of perfect information, the set of SPNEs coincides with the set of Nash equilibria that can derived through backward induction. Every finite game of perfect information has a pure strategy subgame perfect Nash equilibrium. Furthermore, if no player has the same payoffs at any two terminal nodes, then there is a unique subgame perfect Nash equilibrium. \r ","date":"0001-01-01","objectID":"/chapter1.2.3./:12:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"3.3 Application ","date":"0001-01-01","objectID":"/chapter1.2.3./:13:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"3.3.1 Stackelberg Model Of Duopoly 动态的古诺模型 Firm 1 chooses a quantity $q_{1} \\geq 0$ Firm 2 observes $q_{1}$ and chooses a quantity $q_{2} \\geq 0$ The payoff to each firm is given by $$ \\Pi_{j}\\left(q_{i}, q_{j}\\right)=\\left(a-\\left(q_{j}+q_{i}\\right)\\right) q_{j}-c q_{j} $$ Firm 2 ’s best response function is $$ q_{2}=\\frac{a-c}{2}-\\frac{q_{1}}{2} $$ Look at firm 1’s problem. Firm 1 is to maximize by inserting the quantity $q_{2}$ into the profit function: $$ \\begin{aligned} \\Pi_{1}\\left(q_{1}, q_{2}\\right) \u0026=\\left(a-\\left(q_{1}+q_{2}\\right)\\right) q_{1}-c q_{1} \\ \u0026=\\left(a-\\left(q_{1}+\\frac{a-c}{2}-\\frac{q_{1}}{2}\\right)\\right) q_{1}-c q_{1} \\end{aligned} $$ Solve the maximization problem, we have $$ q^*_1 = \\frac{1}{2}(a-c),q^*_2 = \\frac{1}{4}(a-c) $$ ","date":"0001-01-01","objectID":"/chapter1.2.3./:13:1","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"3.4 Ultimatum Bargaining 最后通牒博弈 Two players, 1 and 2. Stage 1: Player 1 offers player 2 an amount of money x, $x \\in [0; 10]$. Stage 2: Player 2 chooses between “Accept” and “Reject” player 1’s offer. If the offered amount accepted, player 1 receives 10 - x while player 2 receives x. If rejected, then both receive 0. \r In an ultimatum game with continuous strategies described above, there exist infinitely many Nash equilibria. Any strategy profile involving player 2 adopting a “cutpoint strategy,” i.e. $\\exist \\bar{x}$ such that player 2 “accept if $x\\geq \\hat{x}$ and reject otherwise,” and player 1 offering an amount $\\hat{x}$ to player 2 is an Nash equilibrium of the game. The unique SPNE** strategies are player 1 chooses x = 0 while player 2 accept any offer amount. Alternating Offers Bargaining Player 1 begins in the first period by proposing to keep $x_{1}^{1}$. $M$ for herself and giving $\\left(1-x_{1}^{1}\\right) \\cdot M$ to Player 2 If Player 2 accepts, the deal is struck. If Player 2 rejects, another bargaining period is played. In period $2,$ Player 2 proposes to keep $y_{2}^{2} \\cdot M$ to himself, and giving $\\left(1-y_{2}^{2}\\right) \\cdot M$ to player 1 If Player 1 accepts, the deal is struck; otherwise, it is period 3 and Player 1 gets to make another proposal. Bargaining continues in this manner until a deal is struck or no agreement is reached when both players walk away with their disagreement values-a for Player 1 and $b$ for Player 2 A shrinking pie and decreasing gains from trade. Suppose there are 2 periods, Player 1 proposes a division of $M$ first, and Player 2 accepts/rejects. If accepted the proposal is implemented. If rejected, Player 2 gets to make a counter-proposal for how to split a reduced amount of pie $\\lambda M,$ where $0\u003c\\lambda\u003c1$ is the rate of decay. Player 1 can then accept or reject this final proposal. The bargaining game is then over with certainty. Example: suppose that $M=12$ dollar and $\\lambda=1 / 3, a=b=0 .$ What is the subgame perfect Nash equilibrium? Backward Induction Note that $M=12$ in period 1 but only 4 in period 2 Start from the second period. Player 2 knows he can get $4-\\epsilon(\\epsilon \\geq 0)$ if the game moves to the second period, because player 1 will prefer getting $\\epsilon$ to 0 at the end of period $2 .$ In the limit, player 1 will accept any second-period offer including $\\epsilon=0$ Anticipating this, Player 1 must offer Player 2 an amount $12-x \\geq 4$ in period $1,$ and keeping $x \\leq 8(x=8 \\text { when Player } 1$ maximizes) for herself. As Player 2 recognizes it as a weakly-better payoff than what could be gotten by waiting to period 2 , he accepts Player 1 ’s first period offer. Player 1 ’s first period offer to Player 2 thus equals the amount at stake at the start of the final round, $\\lambda \\cdot M$ ","date":"0001-01-01","objectID":"/chapter1.2.3./:13:2","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"3.5 Repeated Games ","date":"0001-01-01","objectID":"/chapter1.2.3./:14:0","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"3.5.1 Finitely Repeated Games A game being played repeated for a known and finite number of periods. let’s suppose the Prisoner’s dilemma will be played twice \r \r In sum, the only SPNE of this two-period repeated PD game is for both players to choose D in the first-period and choose D in the second period no matter which first-period outcome they observe. As long as there is a known, finite end, there will be no change in the equilibrium outcome of a game with a unique equilibrium. Besides PD game, this is also true for zero/constant-sum games. 【Proposition】The same argument can be generalized to finite repetition of any stage-game which has a unique equilibrium. Let $G$ be a finite static complete information game which has a unique Nash equilibrium. Suppose $G$ is played $T$ times ( $T$ periods) and before each stage the outcomes of preceding play are perfectly observable. Let the repeated game be denoted by $G(T)$. There is a unique subgame perfect Nash equilibrium for $G(T) :$ in which all players play the equilibrium action of the stage game, independent of the history of play that they observe. Rethink the Prisoner’s Dilemma: even if players play it multiple times, they still cannot do any better than both defect: This is because the play of future periods are fixed at the unique N.E. – No flexibility to generate credible future punishments for the non-cooperation of the current period. eg: credible future punishments \r Intuition: since the stage-game has two Nash equilibria, one can use the worse one as punishment.it is indeed a SPNE Summary If a simultaneous move game has a unique Nash equilibrium, then this equilibrium is also the unique subgame perfect Nash equilibrium of the finitely repeated game. If a simultaneous move game has multiple Nash equilibria, then there are many subgame perfect Nash equilibria of the finitely repeated game. Some of these involve the play of strategies that are collectively more profitable for players than the one-shot game Nash equilibria. ","date":"0001-01-01","objectID":"/chapter1.2.3./:14:1","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":"3.5.2 Infinitely Repeated Games A game being played infinitely without end. Since no one lives forever, we might call such games indefinitely repeated games. Possibly overlapping generations. e.g. the relationship with family members, friends, employer, or the community, who are expected to remain in our life for a while but of course, not forever. Discount factor, $\\delta = \\frac{1}{1+r} \\in [0: 1]$. $\\delta$ can also be interpreted as the probability that the two players will meet and play the game again tomorrow. Indefinitely repeated games are common. We often don’t know whether a relationship will continue in the future or not. 【Average Payoff】The average payoff of player i in the infinite game is $$ (1-\\delta) u_{i}=(1-\\delta) \\sum_{t=0}^{\\infty} \\delta^{t} \\pi_{t} $$ For example, if the players cooperate in every period, $\\pi_{t}=4$ for all $t$, we have $$ \\begin{aligned} (1-\\delta) u_{i} \u0026=(1-\\delta) \\sum_{t=0}^{\\infty} \\delta^{t} \\cdot 4 \\ \u0026=(1-\\delta) \\cdot \\frac{1}{1-\\delta} \\cdot 4=4 \\end{aligned} $$ 【Proposition(Cooperation)】If $\\delta$ is high enough, then there exists a subgame perfect Nash equilibrium such that $(C, C)$ is played in every period. Intuition: cooperation is possible now because the threat to play D is credible in each period, unlike in the finitely repeated games. One way to maintain cooperation is to use Trigger strategies: A player using a trigger strategy plays cooperatively as long as her rival(s) do so, but any defection on their part “triggers” a period of punishment, of specified length, in which she plays non-cooperatively in response. 3.5.2.1 Grim-trigger strategy A Grim-trigger strategy prescribes the following choices at each possible history: play C in the first period. In later periods, keep playing C if the previous outcome is (C,C); otherwise, switch to playing D forever. \r If both players use the grim-trigger strategy, it’s easy to check that both will cooperate forever as long as the discount factor $\\delta$ is sufficiently large. Check for deviations. Consider player $1 .$ Suppose player 1 is on the equilibrium path and does not deviate. Her total payoff starting from the current period is $$ 4+4 \\cdot \\delta+4 \\cdot \\delta^{2}+\\ldots=\\frac{4}{1-\\delta} $$ Now if player 1 deviates from the equilibrium path, i.e. she plays $D$ instead of $\\mathrm{C}$ in the current period. Then her total payoff is $$ 5+1 \\cdot \\delta+1 \\cdot \\delta^{2}+\\ldots=5+\\frac{\\delta}{1-\\delta} $$ To ensure no deviation we require player 1 ’s total payoff from the equilibrium path being greater than that from deviation: $$ \\begin{aligned} \\frac{4}{1-\\delta} \u0026 \\geq 5+\\frac{\\delta}{1-\\delta} \\ \u0026 \\Leftrightarrow \\delta \\geq \\frac{1}{4} \\end{aligned} $$ As long as $\\delta \\in(1 / 4,1)$ - player 1 is sufficiently patient-she will have no incentive to deviate from the equilibrium path, i.e. the cooperation phase. 3.5.2.2 Tit-for-Tat strategy There are “nicer” strategies that will also support (C,C) in every period as an SPNE outcome. A Tit-for-Tat strategy prescribes the following choices at each possible history: play $C$ in the first period. In later periods, keep playing $C$ if the opponent plays C. Switch to play D if the opponent plays D in the previous period. Return to playing $C$ if the opponent switch back to playing $C$; otherwise keep on playing $D$. Now if player 1 deviates from the equilibrium path, i.e. she plays D instead of $\\mathrm{C}$ in the current period. Then her total payoff is $$ 5+1 \\cdot \\delta+0 \\cdot \\delta^{2}+4 \\cdot \\delta^{3}+\\ldots=5+\\delta+\\frac{4 \\cdot \\delta^{3}}{1-\\delta} $$ The only different is the next three periods if the players back to the cooperation phase after 4 periods. To ensure no deviation we require player 1 ’s total payoff from the equilibrium path being greater than that from deviation: $$ \\begin{aligned} \\frac{4}{1-\\delta} \u0026 \\geq 5+\\delta+\\frac{4 \\cdot \\delta^{3}}{1-\\delta} \\ \u0026 \\Leftrightarrow \\delta \\geq \\frac{1}{4} \\end{aligned} $$ As long as $\\delta \\in(1 /","date":"0001-01-01","objectID":"/chapter1.2.3./:14:2","tags":null,"title":"","uri":"/chapter1.2.3./"},{"categories":null,"content":" 本篇主要来自于《高级公司金融》课堂所学，如有谬误，欢迎指正。同时参考了徐高老师的《金融经济学二十五讲》，微信搜索公众号PurePlay，后台回复金融经济学二十五讲即可获取高清电子版。 ","date":"0001-01-01","objectID":"/1.-fisherseparationtheorem/:0:0","tags":null,"title":"","uri":"/1.-fisherseparationtheorem/"},{"categories":null,"content":"目录 简介 证明 图解 评价 费雪 ","date":"0001-01-01","objectID":"/1.-fisherseparationtheorem/:1:0","tags":null,"title":"","uri":"/1.-fisherseparationtheorem/"},{"categories":null,"content":"简介 费雪分离定理(Fisher Separation Theorem)由经济学家欧文·费雪(Irving Fisher, 1867-1947)提出，该定理指出在完美资本市场(PCM)中，经济主体的投资决策与消费决策可以相互分离，也即投资决策与其消费偏好无关。 这意味着，公司不用考虑不同股东/投资者的消费偏好，只要投资净现值(NPV)大于0的项目即可。至于消费偏好，股东可以通过运转良好的资本市场予以满足。这从理论上证明了大型现代化公司存在的可能性，因此成为公司金融的奠基理论之一。 ","date":"0001-01-01","objectID":"/1.-fisherseparationtheorem/:2:0","tags":null,"title":"","uri":"/1.-fisherseparationtheorem/"},{"categories":null,"content":"证明 本部分为公式密集区，如引起不适请跳至图解部分 考虑一个仅有两期$t=0, 1$的效用最大化问题，假设只有一种消费品，消费者偏好可分离，即效用函数为 $$ u(c_0, c_1) = u(c_0)+\\beta u(c_1) $$ 其中$\\beta \\in [0,1]$代表未来效用的折现因子。 消费者初始的初始禀赋为$y_0$。在第0期可以进行投资$I_0$，投资的产出为$\\gamma \\Phi (I_0)$，其中$\\Phi^\\prime\u003e0,\\Phi^{\\prime\\prime}\u003c0,\\Phi(0)=0,\\Phi^{\\prime}(0) = +\\infty$。同时也可以进行储蓄$S_0$，市场利率为$r$。 效用最大化问题的数学表达式如下 $$ \\begin{align} \\operatorname{Max}{{C_0,C_1,I_0,S_0}} \\quad \u0026u(C_0)+\\beta u(C{1}) \u0026 \\ s.t. \\quad \u0026 C_{0}+I_{0}+S_{0} \\leqslant y_{0} \\tag{1}\\ \u0026 C_{1} \\leqslant \\gamma \\phi\\left(I_{0}\\right)+(1+r) S_{0} \\tag{2} \\end{align} $$ 通过$(1)+(2)/(1+r)$，将约束条件转化为 $$ s.t. \\quad C_{0}+\\frac{C_1}{1+r} \\leqslant y_{0}+\\frac{\\gamma \\phi\\left(I_{0}\\right)}{1+r}-I_{0} $$ 设定拉格朗日函数 $$ L=\\mu\\left(C_{0}\\right)+\\beta \\mu\\left(C_{1}\\right)+\\mu\\left[\\left(y_{0}+\\frac{\\gamma \\phi\\left(I_{0}\\right)}{1+r}-I_{0}\\right)-C_0+\\frac{C_{1}}{1+r}\\right] $$ 根据拉格朗日函数可以将效用最大化问题分解成两个子问题： 1）投资决策 投资决策的目标函数如下， $$ I_{0}^{*}=\\operatorname{argmax}_{I_{0}} \\frac{\\gamma \\phi\\left(I_{0}\\right)}{1+r}-I_{0} $$ 等式右边就是投资的NPV，为了最大化投资的总NPV，只要NPV为正的项目就应该被投资，这也证明了NPV法则的正确性。 根据一阶条件，可以发现，最优投资量的边际收益率=市场利率(机会成本)。 $$ FOC: \\quad \\gamma \\phi^{\\prime}{\\left(I_{0}\\right)}-1=r $$ 2）消费决策 在确定了最优投资$I_{0}^{*}$之后，通过以下最大化问题求出最优消费即可。 $$ \\begin{align} \\operatorname{Max}{{C_0,C_1}} \\quad \u0026\\mu\\left(C{0}\\right)+\\beta \\mu\\left(C_{1}\\right)\\ \\text {s.t.} \\quad \u0026 C_0+\\frac{C_1}{1+r} \\leqslant y_{0}+\\left(\\frac{\\gamma \\phi\\left(I_{0}^{*}\\right)}{1+r}-I_{0}^{*}\\right) \\end{align} $$ ","date":"0001-01-01","objectID":"/1.-fisherseparationtheorem/:3:0","tags":null,"title":"","uri":"/1.-fisherseparationtheorem/"},{"categories":null,"content":"图解 将上述问题反映在坐标系$(C_0,C_1)$中，点$A$即为最优投资决策点，根据投资决策问题的一阶条件，它是生产可能性曲线上斜率=市场利率$r$的点，确定了$A$点也就完成了投资决策。 接下来是消费决策，通过点$A$做所作的切线就是消费可能性曲线，切线上的点可以通过借贷的方式实现，事实上也就是消费者的预算约束线。预算约束线与效用无差异曲线的切点$B$即为最优的消费点。 \r ","date":"0001-01-01","objectID":"/1.-fisherseparationtheorem/:4:0","tags":null,"title":"","uri":"/1.-fisherseparationtheorem/"},{"categories":null,"content":"评价 费雪分离定理的关键假设在于完美资本市场(PCM)，包括无交易成本、信息完全对称、市场完全竞争等，是一个非常强的假设。例如，PCM中存贷利率必然相等，然而现实中往往不相等，而利率的不同会导致存款者与贷款者存在不同的最优投资决策，这时投资决策便不能与消费决策分离。 \r 当资本市场完善时，投资仅承担扩大财富的功能，而要满足消费者不同的跨期偏好，通过资本市场借贷即可实现任意的财富跨期转移；当资本市场不完善时，投资也需要承担一定的财富跨期转移功能，此时不同的跨期偏好就会导致不同的投资决策。 虽然PCM在现实中并不存在，但该定理仍然具有极其重要的理论意义： 公司只负责扩大财富，其唯一的标准即$NPV \\geq 0$； 公司价值等于投资项目的未来现金流的折现的加总； 将公司的股利政策视为公司给投资者带来的消费，那么股利政策与投融资决策相互独立； 发达的资本市场可以统一不同个体的投资决策，使更多人参与到投资中来。 ","date":"0001-01-01","objectID":"/1.-fisherseparationtheorem/:5:0","tags":null,"title":"","uri":"/1.-fisherseparationtheorem/"},{"categories":null,"content":"费雪 最后值得一提的是，费雪分离定理的提出者欧文·费雪(Irving Fisher, 1867-1947)。 费雪是20c初经济学界的“顶级流量”，是经济学领域的集大成者。费雪最著名的经济学贡献包括费雪效应(名义利率=实际利率+通胀率)，货币数量论($MV=PQ$)，他还对一般均衡理论、计量经济学也作出了很大的贡献。他是货币主义学派的先驱，被弗里德曼、托宾等称为**“20世纪美国最伟大的经济学家”**。 然而他错误地判断了20c30s年代的大萧条，导致其资产大幅缩水，名声和学术地位也受到了很大影响，但其仍在经济学史上留下了浓墨重彩的一笔。 ","date":"0001-01-01","objectID":"/1.-fisherseparationtheorem/:6:0","tags":null,"title":"","uri":"/1.-fisherseparationtheorem/"},{"categories":null,"content":"1. 实物投资的估值基础 【项目(Project)】能够产生未来现金流的实物投资。 【公司(Corporation)】项目的集合。几乎公司的任何决策都可以视为项目。 公司不同的价值取向：股东利益最大化，利益相关者利益最大化，社会价值最大化。 假定公司的目标为：股东利益最大化，等价于股票价值最大化。 问题：公司在投资决策过程中是否需要考虑不同股东的跨期偏好与风险偏好？不同项目是否有统一的决策准则？费雪分离定理解决了该问题。 ","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:0:0","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"1.1 费雪分离定理 ","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:1:0","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"1.1.1 简介 费雪分离定理(Fisher Separation Theorem)由经济学家欧文·费雪(Irving Fisher, 1867-1947)提出，该定理指出在完美资本市场(PCM)中，经济主体的投资决策与消费决策可以相互分离，也即投资决策与其消费偏好无关。 这意味着，公司不用考虑不同股东/投资者的消费偏好，只要投资净现值(NPV)大于0的项目即可。至于消费偏好，股东可以通过运转良好的资本市场予以满足。这从理论上证明了大型现代化公司存在的可能性，因此成为公司金融的奠基理论之一。 ","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:1:1","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"1.1.2 图解 考虑一个仅有两期$t=0, 1$的投资与消费决策问题。 Case 1：仅有投资项目没有银行 不投资的部分必须在第0期消费掉，投资收益作为第二期消费 此时产值等价于消费，生产可能性曲线与消费可能性曲线重合。 \r Case 2：既有投资项目又有银行 此时投资的部分可以储蓄下来作为第二期的消费。投资项目的作用是扩展消费可能性曲线。 \r 最优的投资决策点是生产可能性曲线与消费可能性曲线(预算约束线)的切点，最优投资额为$I^*$，与股东偏好无关。 \r 储蓄者(miser)的消费储蓄结构分析 \r 过度消费者(spender)的消费储蓄结构分析 \r $NPV$法则：投资$NPV \\ge 0$的项目。 \r 注意到其中三角形横轴产量/消费量即为对应纵轴产量/消费量的现值。 单个项目NPV分析 \r ","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:1:2","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"1.1.3 证明 考虑一个仅有两期$t=0, 1$的效用最大化问题，假设只有一种消费品，消费者偏好可分离，即效用函数为 $$ u(c_0, c_1) = u(c_0)+\\beta u(c_1) $$ 其中$\\beta \\in [0,1]$代表未来效用的折现因子。 消费者初始的初始禀赋为$y_0$。在第0期可以进行投资$I_0$，投资的产出为$\\gamma \\Phi (I_0)$，其中$\\Phi^\\prime\u003e0,\\Phi^{\\prime\\prime}\u003c0,\\Phi(0)=0,\\Phi^{\\prime}(0) = +\\infty$。 Case 1：仅有投资项目没有银行 效用最大化问题的数学表达如下 $$ \\begin{array}{l} Max \u0026 u(C_0)+\\beta u(C_1) \\ s.t.\u0026 C_{0}+I_{0} \\leqslant y_{0} \\ \u0026C_{1} \\leqslant \\gamma \\phi\\left(I_{0}\\right) \\end{array} $$ 将约束条件带入效用函数得到 $$ \\operatorname{Max}{\\left{I{0}\\right}} u\\left(y_{0}-I_{0}\\right)+\\beta u\\left(\\gamma \\phi\\left(I_{0}\\right)\\right) $$ 一阶条件： $$ FOC: -u^{\\prime}\\left(y_{0}-I_{0}\\right)+\\beta u^{\\prime}\\left(\\phi\\left(I_{0}\\right)\\right) \\gamma \\phi^{\\prime}\\left(I_{0}\\right)=0 $$ 移项后得到 $$ \\begin{align} \\frac{1}{\\beta} \\cdot \\frac{u^{\\prime}\\left(y_{0}-I_{0}\\right)}{u^{\\prime}\\left(\\gamma \\Phi \\left(I_{0}\\right)\\right)}=\\gamma \\phi^{\\prime}\\left(I_{0}\\right) \\tag{1.1} \\end{align} $$ 等式右边的含义是边际跨期替代率(marginal rate of intertemporal substitution, MRIS)。 进而可以求解得到，$I_0$是$\\gamma,y_0, \\beta$的函数，且函数$G(\\cdot)$与效用函数$u'(\\cdot)$有关，此时由于不存在可以借贷的资本市场，投资决策不独立于消费决策。 $$ I_{0}=G\\left(y_{0}, \\beta, \\gamma\\right) $$ 比较静态分析 利用式(1.1)结合反证法，可以证明$I_0$是$y_0, \\beta$的增函数，这是符合经济直觉的。$I_0$和$\\gamma$的关系不能确定，一方面投资回报率更大会更倾向于投资，但同样的投资可以带来更大的回报，因此没有必要投资那么多。 $$ \\begin{array}{l} y_{0} \\uparrow \\Rightarrow I_{0} \\uparrow \\ \\beta \\uparrow \\Rightarrow I_{0} \\uparrow \\ \\end{array} $$ Case 2：既有投资项目又有银行 效用最大化问题的数学表达式如下 $$ \\begin{align} \\operatorname{Max}{{C_0,C_1,I_0,S_0}} \\quad \u0026u(C_0)+\\beta u(C{1}) \u0026 \\s.t. \\quad \u0026 C_{0}+I_{0}+S_{0} \\leqslant y_{0} \\tag{1}\\\u0026 C_{1} \\leqslant \\gamma \\phi\\left(I_{0}\\right)+(1+r) S_{0} \\tag{2} \\end{align} $$ 通过$(1)+(2)/(1+r)$，将约束条件转化为 $$ s.t. \\quad C_{0}+\\frac{C_1}{1+r} \\leqslant y_{0}+\\frac{\\gamma \\phi\\left(I_{0}\\right)}{1+r}-I_{0} $$ 设定拉格朗日函数 $$ L=\\mu\\left(C_{0}\\right)+\\beta \\mu\\left(C_{1}\\right)+\\mu\\left[\\left(y_{0}+\\frac{\\gamma \\phi\\left(I_{0}\\right)}{1+r}-I_{0}\\right)-C_0+\\frac{C_{1}}{1+r}\\right] $$ 根据拉格朗日函数可以将效用最大化问题分解成两个子问题： 1）投资决策 投资决策的目标函数如下， $$ I_{0}^{*}=\\operatorname{argmax}_{I_{0}} \\frac{\\gamma \\phi\\left(I_{0}\\right)}{1+r}-I_{0} $$ 等式右边就是投资的NPV，为了最大化投资的总NPV，只要NPV为正的项目就应该被投资，这也证明了NPV法则的正确性。 根据一阶条件，可以发现，最优投资量的边际收益率=市场利率(机会成本)。 $$ FOC: \\quad \\gamma \\phi^{\\prime}{\\left(I_{0}\\right)}-1=r $$ 2）消费决策 在确定了最优投资$I_{0}^{*}$之后，通过以下最大化问题求出最优消费即可。 $$ \\begin{aligned} \\operatorname{Max}{{C_0,C_1}} \\quad \u0026\\mu\\left(C{0}\\right)+\\beta \\mu\\left(C_{1}\\right)\\ \\text {s.t.} \\quad \u0026 C_0+\\frac{C_1}{1+r} \\leqslant y_{0}+\\left(\\frac{\\gamma \\phi\\left(I_{0}^{*}\\right)}{1+r}-I_{0}^{*}\\right) \\end{aligned} $$ Case 3：考虑风险 Prove Fisher Separation Theorem in the case of one-period uncertainty. Here is the setup: There is a single consumption good. There are two dates of consumption, date 0 and date There are $N$ states of the world at date $1 .$ The probability that state $s$ hap pens is $\\pi s$, where their sum is $1 .$ Markets are complete: For each state $s$, there is an Arrow-Debreu security that costs $P s$ at date 0 to purchase and pays one unit of the consumption good at date 1 if and only if state $s$ occurs. The entrepreneur is an expected utility maximizer: $U(C 0)+\\beta E\\left[U\\left(C_{1}\\right)\\right]$ The entrepreneur has an endowment of $y_{0}$ at date $0,$ and no endowment at date $1 .$ The entrepreneur can invest at date 0 in a technology that pays $\\gamma(s) \\phi(I o)$ in state s at date 1 . where $I_{0}$ is the amount invested. Notice that $\\gamma$ is a function of s. The function satifies$\\phi^{\\prime}\u003e0, \\phi^{\\prime \\prime}\u003c0, \\phi(0)=0, \\phi^{\\prime}(0)=+\\infty$ Proof: Let $c_s$ denote consumption under state s in date 1. Consider utility maximization problem $$ \\begin{align} \\operatorname{Max} \\quad \u0026 U(C_0)+E[U(C_{1})]\u0026\\ s.t. \\quad \u0026 C_{0}+I_{0}+\\sum_{s=1}^{N} \\tau(s)P_s \\leqslant y_{0} \\quad \u0026(1) \\ \u0026 c_{s} \\leqslant \\gamma(s) \\phi\\left(I_{0}\\right)+ \\tau(s), \\text{ for } s=1,2,\\dots,N \\quad \u0026(2) \\end{align} $$ Take a summation of bugdet constrait (2), we have $$ \\sum_{s=1}^{N} P_s c_s \\leq \\sum_{s=1}^{N","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:1:3","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"1.1.4 评价 费雪分离定理：完美资本市场（借贷）的存在使得投资可以从消费问题中抽离出来（先做出投资决策，再根据投资结果决定消费） 理解：结合之前的图像，投资具有扩大财富与财富跨期转移的功能，当资本市场不完善时，投资需要承担一定的财富跨期转移功能，由于不同个体拥有不同的跨期偏好，可能导致不同的投资决策（无差异曲线与生产可能性曲线的交点）；当资本市场完善时，投资仅承担扩大财富的功能，财富转移的功能由资本市场承担，即使偏好不同，最终投资决策的均衡点也相同（$NPV \\geq 0$ 或者 $r_i = r$）。 关键假设：PCM（完美资本市场） No fraction：无交易成本，存贷利率相等。 存贷利率不相等时，对于存款者与贷款者存在不同的投资均衡点。 \r 信息对称：例如投资项目信息等 完全竞争：利率的接受者 虽然PCM在现实中并不存在，但该定理仍然具有极其重要的理论意义： 公司只负责扩大财富，其唯一的标准即$NPV \\geq 0$； 公司价值等于投资项目的未来现金流的折现的加总； 将公司的股利政策视为公司给投资者带来的消费，那么股利政策与投融资决策相互独立； 发达的资本市场可以统一不同个体的投资决策，使更多人参与到投资中来。 ","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:1:4","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"1.1.5 费雪 最后值得一提的是，费雪分离定理的提出者欧文·费雪(Irving Fisher, 1867-1947)。 费雪是20c初经济学界的“顶级流量”，是经济学领域的集大成者。费雪最著名的经济学贡献包括费雪效应(名义利率=实际利率+通胀率)，货币数量论($MV=PQ$)，他还对一般均衡理论、计量经济学也作出了很大的贡献。他是货币主义学派的先驱，被弗里德曼、托宾等称为**“20世纪美国最伟大的经济学家”**。 然而他错误地判断了20c30s年代的大萧条，导致其资产大幅缩水，名声和学术地位也受到了很大影响，但其仍在经济学史上留下了浓墨重彩的一笔。 ","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:1:5","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"1.2 NPV法则 项目的现金流： $$ \\underbrace{C_{0}}_{\\text {cost}} \\quad \\underbrace{C_{1} \\quad C_{2} \\quad C_{3} \\ldots C_{T}}_{\\text {cash flows}} $$ $$ \\mathrm{NPV}=\\underbrace{\\mathrm{PV}(\\text { future cash flows })}{\\text{Present (market) value of future cash flows} }-\\mathrm{C}{0} $$ ","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:2:0","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"Step 1. 计算自由现金流 计算时仅考虑项目带来的增量现金流 假定是一个全部以股权融资的公司，因此不考虑利息以及利息带来的税盾效应 计算公式： 公司自由现金流＝息税前利润－税金 + 折旧与摊销－资本支出－营运资本追加 股东自由现金流＝净利润 + 折旧与摊销－资本支出－营运资本追加 其中，净利润＝息税前利润－税金－利息 注意到上述税金中的税基是没有扣除利息的EBIT 更精确的公式如下 $$ \\mathrm{FCF}=\\mathrm{EBIT}^{*}(1-\\mathrm{t})+\\text { Depreciation }-\\mathrm{CAPX}-\\text { Change in NWC } $$ 公式的理解 现金流入有两个来源：主要来源是企业通过生产经营活动产生的利润；另一个来源是企业的折旧与摊销，这部分是作为企业的经营费用在利润中扣除的，但并没有实际的支付现金出去，所以在计算现金流时需要加回去。 现金流出：税金，资本支出（包括购置固定资产，无形资产及其他营业性资产的支出）和营运资本（存货、应收款项的增加而占用的资金等），最后剩余就是股东和债权人理论上能从企业提取的最大现金。 ","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:2:1","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"Step 2. 计算未来现金流的PV 思想(“Tracking Portfolio” Approach): 在金融市场找到与项目未来现金流相同的资产组合，该资产组合的市场价值等于项目的现值(Present Value, PV)。 本章仅考虑无风险项目，此时折现率(discount rate)为零息债券的到期收益率(yield to maturity, YTM)，项目的PV为 $$ P V=\\frac{C_{1}}{1+Y T M_{1}}+\\frac{C_{2}}{\\left(1+Y T M_{2}\\right)^{2}}+\\ldots \\ldots+\\frac{C_{T}}{\\left(1+Y T M_{T}\\right)^{T}} $$ \r ","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:2:2","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"1.3 IRR法则 内部收益率(Internal Rate of Return, IRR) ，使项目NPV为0的折现率。IRR可以类比为项目的投资回报率，便于理解，因此比NPV法则更加流行。 折现现金流是折现率$i$的函数 $$ DCF(i) = C_0 +\\frac{C_1}{1+i}+\\frac{C_2}{(1+i)^2}+ \\dots+ \\frac{C_T}{(1+i)^T} $$ ","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:3:0","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"1.3.1 IRR使用方法 首先计算现金机会成本(opportunity cost of cash, OCC)： 如果利率期限结构是水平线，即无风险利率不随时间变化，则直接将无风险利率作为OCC；项目有风险时需要加上风险溢价。 如果利率期限结构不是水平的，则需要计算hurdle rate：首先计算项目NPV，使折现现金流等于NPV的折现率即为hurdle rate，并将其作为OCC。 $$ \\begin{array}{c} DCF(i)=0 \\Rightarrow i = IRR \\ DCF(i)=N P V \\Rightarrow i=h \\ D C F(i)-C_{0}=PV \\Rightarrow i=h \\end{array} $$ 对于Late Cash-Flow stream (investing): 前期负现金流，后期正现金流。$DCF(i)$关于i递减，IRR大于OCC时接受该项目 对于Early Cash-Flow stream (financing): 前期正现金流，后期负现金流。$DCF(i)$关于i递增，IRR小于OCC时接受该项目(类比借钱，利率越低越好) \r ","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:3:1","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"1.3.2 局限性 IRR本身作为折现率并不符合现实 非传统项目现金流(e.g. 正负号多次变化)可能存在多个或者不存在IRR \r \r 互斥项目不能直接比较IRR \r \r 总结：IRR在实际中是常用的方法，但在非传统现金流中常常无法给出判断。$NPV\u003e0$才是衡量是否投资一个项目的最终准则。 ","date":"0001-01-01","objectID":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/:3:2","tags":null,"title":"","uri":"/1.-%E5%AE%9E%E7%89%A9%E6%8A%95%E8%B5%84%E7%9A%84%E4%BC%B0%E5%80%BC%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"2. 风险项目估值 思想：tracking portfolio Almost any real world project has risky future cash flows: $$ \\underbrace{C_{0}}_{\\text {cost }} \\underbrace{\\widetilde{C}_{1} \\tilde{C}_{3} \\ldots \\widetilde{C}_{\\text {T }}}_{\\text {Risky future cash flows }} $$ ","date":"0001-01-01","objectID":"/2.-%E9%A3%8E%E9%99%A9%E9%A1%B9%E7%9B%AE%E4%BC%B0%E5%80%BC/:1:0","tags":null,"title":"","uri":"/2.-%E9%A3%8E%E9%99%A9%E9%A1%B9%E7%9B%AE%E4%BC%B0%E5%80%BC/"},{"categories":null,"content":"2.1 利用远期估值 Main Insight: The forward price of, say, a commodity, represents the certainty equivalent of the future risky price Certainty Equivalent of a risky cash flow: Future risk-free cash flow that has the same present value as the risky cash flow 远期价格代表未来风险价格的确定性等价 $$ F_t = S_0(e^{rt}) $$ If “The risk in project cash flows stems from future price uncertainty of a particular good (e.g., an input or an output) There is a forward market for that good then the forward price can be used to find the PV of the CF Procedure: Calculate $C E(\\tilde{C}),$ the certainty equivalent of the risky cash flow Discount at the risk-free rate to find the PV: 其本质在于调整分资至无风险 $$ \\mathrm{PV}=\\frac{\\mathrm{E}\\left(\\tilde{\\mathrm{C}}{1}\\right)}{1+\\bar{r}{1}}+\\frac{\\mathrm{E}\\left(\\tilde{\\mathrm{C}}{2}\\right)}{\\left(1+\\overline{\\mathrm{r}}{2}\\right)^{2}}+\\frac{\\mathrm{E}\\left(\\tilde{\\mathrm{C}}{3}\\right)}{\\left(1+\\overline{\\mathrm{r}}{\\mathrm{j}}\\right)^{3}}+\\ldots \\ldots+\\frac{\\mathrm{E}\\left(\\tilde{\\mathrm{C}}{\\mathrm{T}}\\right)}{\\left(1+\\overline{\\mathrm{r}}{\\mathrm{r}}\\right)^{T}} $$ \r \r \r \r 但现实中往往难以完全复制，即使复制后也可能难以计算。 ","date":"0001-01-01","objectID":"/2.-%E9%A3%8E%E9%99%A9%E9%A1%B9%E7%9B%AE%E4%BC%B0%E5%80%BC/:1:1","tags":null,"title":"","uri":"/2.-%E9%A3%8E%E9%99%A9%E9%A1%B9%E7%9B%AE%E4%BC%B0%E5%80%BC/"},{"categories":null,"content":"2.2 风险调整的DCF $$ \\mathrm{PV}=\\frac{\\mathrm{E}\\left(\\tilde{\\mathrm{C}}{1}\\right)}{1+\\bar{r}{1}}+\\frac{\\mathrm{E}\\left(\\tilde{\\mathrm{C}}{2}\\right)}{\\left(1+\\overline{\\mathrm{r}}{2}\\right)^{2}}+\\frac{\\mathrm{E}\\left(\\tilde{\\mathrm{C}}{3}\\right)}{\\left(1+\\overline{\\mathrm{r}}{\\mathrm{j}}\\right)^{3}}+\\ldots \\ldots+\\frac{\\mathrm{E}\\left(\\tilde{\\mathrm{C}}{\\mathrm{T}}\\right)}{\\left(1+\\overline{\\mathrm{r}}{\\mathrm{r}}\\right)^{T}} $$ For now, consider only one future risky cash flow. Our objective is to find its PV: $$ \\frac{t=0}{P V} \\frac{t=1}{\\widetilde{C}} $$ Suppose CAPM is the model pricing financial assets. If the project was a traded asset, we would have $$ \\frac{\\widetilde{\\mathbf{C}}}{\\mathrm{PV}}-1=r_{f}+\\beta\\left(\\widetilde{r}_{M}-r_{f}\\right)+\\varepsilon $$ where $$ \\beta=\\frac{\\operatorname{cov}\\left(\\frac{\\widetilde{C}}{P V}, \\tilde{r}{M}\\right)}{\\operatorname{var}\\left(\\widetilde{r}{M}\\right)} $$ Measures the systematic risk of the project $\\cdot$ The CAPM pricing equation is: $$ \\frac{\\mathrm{E}(\\widetilde{\\mathrm{C}})}{\\mathrm{PV}}-1=r_{f}+\\beta\\left(\\bar{r}_{M}-r_{f}\\right) $$ Solving for PV, we get Risk-adjusted discounting using CAPM: $$ \\mathrm{PV}=\\underbrace{\\frac{\\mathrm{E}(\\widetilde{\\mathrm{C}})}{1+r_{f}+\\beta\\left(\\bar{r}_{M}-r_{f}\\right)}}_{\\text {Project’s cost of capital }} $$ Risk-adjusted discounting using APT: $$ \\mathrm{PV}=\\frac{\\mathrm{E}(\\widetilde{\\mathrm{C}})}{1+r_{f}+\\lambda_{1} \\beta_{1}+\\lambda_{2} \\beta_{2}+\\ldots+\\lambda_{k} \\beta_{k}} $$ Inputs necessary to implement DCF Expected cash flow (i.e., the average free cash flow) If CAPM is used: expected return on the market, and the return beta of the project If APT is used: factor premium for each factor, and the return sensitivity of the project to each factor The risk-free rate $\\beta$的估计 In practice “The Comparison Method” is commonly used to implement risk-adjusted discounting: –Use public financial information to estimate risk of the business. Procedure Identify a set of companies in the same business (i.e. in the same risk class) Use regression of companies’ traded equity returns on market (or factor) returns to obtain betas Adjust for taxes (we will examine tax adjustments later) Adjust for leverage Why do we adjust for leverage? • We want the beta of comparable businesses (i.e., project, or asset beta) • We observe the beta of one of the claims to those assets (the equity beta) • The amount of debt financing (leverage) affects the equity beta Hence: • We need to adjust the beta of the equity in order to recover the beta of the assets. Moderate leverage (risk-free debt): $$ \\boldsymbol{\\beta}{A}=\\frac{D}{D+E} 0+\\frac{E}{D+E} \\boldsymbol{\\beta}{E} \\Rightarrow \\boldsymbol{\\beta}{E}=\\left(1+\\frac{D}{E}\\right) \\boldsymbol{\\beta}{A} $$ High leverage (risky debt): $$ \\begin{aligned} \\beta_{A}=\\frac{D}{D+E} \\beta_{D}+\\frac{E}{D+E} \\beta_{E} \\Rightarrow \\beta_{E} \u0026=\\left(1+\\frac{D}{E}\\right) \\beta_{A}-\\left(\\frac{D}{E}\\right) \\beta_{D}\\ \u0026=\\beta_{A}+(\\beta_{A}-\\beta_{D}) \\cdot L \\ \\end{aligned} $$ \r 注意$\\beta_D$是L的增函数 \r \r \r d) Leverage does not change the cost of capital of figures Levering-up an investment makes it riskier, consequently its return should also increase \r Project Betas vs. Firm Betas • It is the risk of project cash flows (and hence project beta) that matters for valuation • In practice, most firms use their own (overall) cost of capital in evaluating projects • No formal method to adjust for project risk A different discount rate for each maturity? • Typically, the beta of a future cash flow will depend on its maturity • However, most firms use the same beta for discounting cash flows of all maturities • The resulting misvaluation may be severe, especially when the late cash flow pattern is pronounced Could use long- or short-term risk-free rate, and a corresponding long- or short-term beta ","date":"0001-01-01","objectID":"/2.-%E9%A3%8E%E9%99%A9%E9%A1%B9%E7%9B%AE%E4%BC%B0%E5%80%BC/:1:2","tags":null,"title":"","uri":"/2.-%E9%A3%8E%E9%99%A9%E9%A1%B9%E7%9B%AE%E4%BC%B0%E5%80%BC/"},{"categories":null,"content":"2.3 Ratio Comparison Method (Valuation by Multiples) Commonly used in real estate valuation and by investment banks (M\u0026As, IPOs) – Basic Insight: An asset should sell for $20 if it has twice the annual cash flow of a similar asset that recently sold for $10 – Implicit assumptions: • Project and comparison cash flows will grow at the same rate • Comparison prices are accurate – Relative valuation • Do not need to specify how actual prices are determined • Future expected cash flows and risk are implicit in the comparison price Procedure Choose comparable firms Use a large enough sample to average out firms’ idiosyncrasies and eliminate firms that have suffered “abnormal”events (i.e., takeovers) Choose bases for multiples Generic: Sales, Gross Profits, Operating Profits, Net Income, Book Values Industry Specific: Paid Miles Flown, # of restaurants, # of subscribers, circulation of a newspaper etc. Average across industry Calculate multiple for each comparable firm and then take the average Project bases for the valued firm Value the project (or the firm) \r The most popular version of ratio comparison approach Procedure: For a comparable business calculate $\\frac{P}{E}=k$ Estimate project’s first year expected earnings $E_{f}$ Compute the present value of future cash flows as $$ P V=E_{f} \\cdot k $$ Future earnings growth rates must be similar • What if current project earnings are negative? ​ – Start from the first year in which earnings are projected to be positive and discount the resulting price back ​ – Use other multiples • What if some comparison firms have negative earnings? ​ – May sum the values and the earnings separately and compute the average based on the aggregate value and aggregate earnings Leverage effects Leverage may increase P/E (growth firms) or reduce P/E (cash cows with declining earnings) For comparisons, we need to use unlevered $\\mathbf{P} / \\mathbf{E}:$ P/E ratio that would obtain if the comparison firm were all-equity financed $$ \\text { Observed (levered) } P / E=\\frac{\\text { Price }}{\\text { Earnings }}=\\frac{A-D}{X-r_{D} D} $$ $A=$ market value of the firm (i.e., debt + equity) $X=$ pre-interest earnings $D=$ debt value $r_{D}=\\operatorname{cost} \\text { of debt }$ Therefore, the unlevered P/E ratio is $\\boldsymbol{A} / \\boldsymbol{X}$ \r \r \r \r Pros Incorporates a lot of information in a simple way Incorporates market consensus about discount rates and growth rates Provides discipline in valuation process by ensuring that your valuation is in line with other valuations Cons Implicitly assumes all companies are alike in growth rates, cost of capital, and business composition. Hard to find true comps Difficult to incorporate firm specific information. Particularly problematic if operating changes are going to be implemented Accounting differences, particularly with earnings and equity-based measures. Multiples of FCF and EBITD may be preferable Provides little intuition on what the sources of value are – Why are prices high or low? In sum, when you have more than five minutes to value a project… DCF is a must! ","date":"0001-01-01","objectID":"/2.-%E9%A3%8E%E9%99%A9%E9%A1%B9%E7%9B%AE%E4%BC%B0%E5%80%BC/:1:3","tags":null,"title":"","uri":"/2.-%E9%A3%8E%E9%99%A9%E9%A1%B9%E7%9B%AE%E4%BC%B0%E5%80%BC/"},{"categories":null,"content":"2.4 Terminal Value Calculations In valuing long-lived projects or ongoing businesses, we cannot forecast every year of cash flows. Instead, forecast FCF until it is reasonable to think that the project or company is in “steady state” and estimate a “terminal value”. $$ \\begin{equation} \\text { Project Value }=\\sum_{t=0}^{N} \\frac{F C F_{t}}{(1+r)^{t}}+\\frac{\\text { Terminal Value }_{N}}{(1+r)^{N}} \\end{equation} $$ Typically, terminal value is calculated in one of three ways: 1. Terminal Value as Liquidation Value Unless liquidation is likely, this method tends to underestimate TV (useful as a lower bound) • Liquidation Value depends on Salvage Value and Net Working Capital recovered a) Salvage Value (SV): CF that the firm receives from liquidating its assets SV = Liquidation price – Liquidation costs b) Net Working Capital Does firm recoup NWC at project end? • If so, last $\\Delta $NWC = last NWC NWC’s actual value can differ from its book value • cannot recoup the A/R fully • inventory may sell above or below book value \r Using Multiples to get Terminal Value TV = k × MULTIPLE \r ","date":"0001-01-01","objectID":"/2.-%E9%A3%8E%E9%99%A9%E9%A1%B9%E7%9B%AE%E4%BC%B0%E5%80%BC/:1:4","tags":null,"title":"","uri":"/2.-%E9%A3%8E%E9%99%A9%E9%A1%B9%E7%9B%AE%E4%BC%B0%E5%80%BC/"},{"categories":null,"content":"3. 估值与税收 ","date":"0001-01-01","objectID":"/3.-%E4%BC%B0%E5%80%BC%E4%B8%8E%E7%A8%8E%E6%94%B6/:1:0","tags":null,"title":"","uri":"/3.-%E4%BC%B0%E5%80%BC%E4%B8%8E%E7%A8%8E%E6%94%B6/"},{"categories":null,"content":"3.1 税收对公司估值的影响 主要探讨资本结构变化引起的税收变化对估值的影响 3.1.1 现金流 税盾效应增加现金流 \r \r 注意：此处没有考虑利息支出，即仍假定为100%股权融资，之后再进行财务调整 3.1.2 风险 税盾影响系统性风险$ \\beta $ 债务额保持不变时 \r $$ \\beta_A = \\frac{\\beta_{A}}{1+\\frac{D}{E}} $$ 税盾可以减小系统性风险 $\\beta_{U A}$ reflects the systematic risk of the core business Using $\\beta_{U A}$ in the CAPM equation, we get $r_{U A}$ : the cost of capital for the project that would apply if the firm is **all equity financed** $r_{U A}=$ the rate that should be used for discounting FCF 债务水平动态变化时 \r \r 注意到，债务对风险的效应本质上是通过改变现金流发生。 ","date":"0001-01-01","objectID":"/3.-%E4%BC%B0%E5%80%BC%E4%B8%8E%E7%A8%8E%E6%94%B6/:1:1","tags":null,"title":"","uri":"/3.-%E4%BC%B0%E5%80%BC%E4%B8%8E%E7%A8%8E%E6%94%B6/"},{"categories":null,"content":"3.2 Adjusted Present Value (APV) Method Valuation by components Step 1: Separate cash flows into (a) FCF (as if the firm is all-equity financed) (b) Tax shields Step 2: Find the cost of capital (discount rate) for each component For FCF, use unlevered cost of capital For tax shields, you need a separate discount rate Step 3: Calculate the PV of each component and add these PVs to obtain the project PV Debt capacity: Marginal amount by which a firm can increase its debt when it adopts the project \r \r Risk-free rate will misvalue the tax shields for two reasons: a) The firm may not be able to take advantage of interest tax shields (future EBITs may fall below interest payments) b) Firm’s financing plans can be flexible: the firm may increase or decrease debt levels (and hence the tax shields) in the future Two possible solutions: a) Expected return of tax shield derived from the beta of the debt（例题） b) If there is uncertainty about the evolution of debt levels, use the derivatives valuation (i.e., real options) approach \r \r \r ","date":"0001-01-01","objectID":"/3.-%E4%BC%B0%E5%80%BC%E4%B8%8E%E7%A8%8E%E6%94%B6/:1:2","tags":null,"title":"","uri":"/3.-%E4%BC%B0%E5%80%BC%E4%B8%8E%E7%A8%8E%E6%94%B6/"},{"categories":null,"content":"3.3 Weighted Average Cost of Capital (WACC) Procedure Estimate a project’s FCF Discount FCF at a single rate (i.e., WACC) $$ W A C C=\\frac{E}{D+E} \\bar{r}{E}+\\frac{D}{D+E}\\left(1-T{C}\\right) \\bar{r}_{D} $$ 其中$\\bar{r}{E}$与$\\bar{r}{D}$并非实际成本而是机会成本 3.3.1 Cost of Equity Financing 根据公司过去的数据，或者类似公司的数据（需要进行杠杆调整） $$ \\beta_{\\text {equity}}^{\\text {comparable}} \\stackrel{\\text { unlever }}{\\longrightarrow} \\beta_{U A}^{\\text {comparable}}=\\beta_{U A}^{\\text {project}} \\stackrel{\\text { relever }}{\\longrightarrow} \\beta_{\\text {equity}}^{\\text {project}} $$ \r \r \r 3.3.2 Cost of Debt Financing Default-Free Debt: 使用YTM Risky Debt: 使用YTM会高估成本 注意到使用$r_f$会低估成本 In practice, most firms act conservatively and use YTM \r \r For risky debt Two alternative ways to get the true cost of debt: Use the beta of debt to calculate the expected return on debt Subtract expected losses from default and recalculate yields（例题） \r \r 3.3.3 Tax Rate \r \r \r \r 3.3.4 Weights on Equity and Debt 用项目WACC（例题）或者公司WACC都是不对的； To get D/(D+E): – Use project’s comparables: “Pure plays” in the same business as the project – Use firm’s D/(D+E) only if the project is similar to the firm’s other assets – Use intuitions from optimal determination of capital structure $\\bar{r}{E}$ and $\\bar{r}{D}$ are forward-looking expected returns, not the expected returns at the time the firm has raised financing Getting cheap financing does not justify investing in a bad project! One can adjust WACC for more than two sources of financing (e.g., preferred stock) $$ W A C C=\\frac{D}{V}(1-t) r_{D}+\\frac{E}{V} r_{E}+\\frac{P}{V} r_{P} $$ Project WACC and firm WACC are different If project risk differs significantly from the overall firm risk, using firm WACC based on the existing projects will give the wrong answer (see the next example) \r \r \r \r \r WACC不兼容其他方法，但可以简单计算，因此比较常用 APV兼容各种方法，但不常用，相比WACC更好 ","date":"0001-01-01","objectID":"/3.-%E4%BC%B0%E5%80%BC%E4%B8%8E%E7%A8%8E%E6%94%B6/:1:3","tags":null,"title":"","uri":"/3.-%E4%BC%B0%E5%80%BC%E4%B8%8E%E7%A8%8E%E6%94%B6/"},{"categories":null,"content":"4. Analysis of Capital Structure I: The MM Benchmark 此前介绍的专题为投资决策，接下来介绍公司的融资决策，融资决策的首要问题是确定债务融资与股权融资的比例，即资本结构。 【资本结构(Captial Structure)】公司不同资金来源的组合。 问题： 资本结构如何影响公司价值？ 最优资本结构如何确定？ 杠杆的不同计算方法 公式 含义 $\\frac{Debt}{Debt+MVEquity}$ 长期偿债能力 $\\frac{Debt}{\\text{Total Book Asset}}$ 历史融资比例 $\\frac{EBITD}{Interest}$ 利息支付能力 公司债由于存在不上市流通的部分，难以估值，通常采用账面价值，用账面价值作为市值(MV)的替代变量。 ","date":"0001-01-01","objectID":"/4.-capital-structure-i-mm-benchmark/:0:0","tags":null,"title":"","uri":"/4.-capital-structure-i-mm-benchmark/"},{"categories":null,"content":"4.1 Modigliani-Miller (M\u0026M) 理论框架 M\u0026M Theorem (Proposition I): Irrelevance of capital structure M\u0026M and the Cost of Capital (Proposition II) M\u0026M and the irrelevance of Distribution Policy（1961） 提出者 Franco Modigliani(1918), CMU, MIT（凯恩斯学派）, 1985 Nobel Prize(GE, 生命周期理论). Merton Miller(1923-2000), CMU,Chicago（芝加哥/自由主义学派）, 1990 Nonbel Prize(M\u0026M), 资本结构之父，现代公司金融之父。 学生Eugene Fama, 现代金融之父。 学生Myron Scholes, 1997 Nobel Prize, 期权定价之父，衍生品之父。 ","date":"0001-01-01","objectID":"/4.-capital-structure-i-mm-benchmark/:1:0","tags":null,"title":"","uri":"/4.-capital-structure-i-mm-benchmark/"},{"categories":null,"content":"4.1.1 M\u0026M Theorem (Proposition I) 【M\u0026M Theorem Ⅰ】假定： 公司总现金流不受资本结构影响 没有交易成本 没有套利机会 则公司总市值不受资本结构影响。 假设Ⅰ 在没有交易成本和套机机会的情况下，资本结构不影响公司的现金流； MM定理的另一种表述，在没有交易成本和套机机会的情况下，资本结构影响公司的价值当且仅当，资本结构影响公司的现金流。 该假定排除了以下情况： 税收 破产成本 Strategic effects: 竞争对手根据该公司资本结构调整战略 债务人对管理层的压力（战略调整等） 直观理解：将饼看作公司的未来现金流 \r 假设Ⅱ 无交易成本（证券发行费用，信息不对称），核心在于保证有一个complete \u0026 competitive市场 Markets are complete: For each state s, there is an Arrow-Debreu security that costs $P_s$ at date 0 to purchase and pays one unit of the consumption good at date 1 if and only if state s occurs. 公司发行的任何证券(任何现金流)都可以在市场上找到替代品(可以被完全复制)。 No value to “financial marketing”：单纯通过分割现金流(e.g.创造出市场偏好的现金流)是不可能获得收益的。 \r 假设Ⅲ 无套利，金融估值是理性的。 \r Proof: Suppose that the firm’s assets will generate a random cash flow $C$ next period (i.e., $C(s)$ in state $s$). Today the firm issues securities $K_{i}(s)$ against this cash flow $(i=1,2, \\dots, N$ is the index for different securities like equity, debt, etc.). Denote $C_i(s)$ as cash flow next period for security $i(i=1,2,\\ldots,N)$ in state $s$, then the firm’s cash flow next period $C(s)=\\sum_{i=1}^{N}C_i(s)$ for each state $s$. Complete Markets: For each future state of the world $s,$ there exists an Arrow-Debreu security (i.e., one that pays $$ 1$ iff $s$ occurs), trading at a market price of $P(s)$. By tracking portfolio using Arrow-Debreu security, the security $i$’s value $K_i=\\sum_{s=1}^{S}P(s)C_i(s)$. Then the total value of this firm is $$ \\begin{aligned} V \u0026= \\sum_{i=1}^{N}K_i \\ \u0026= \\sum_{i=1}^{N} \\sum_{s=1}^{S}\\left[P(s)C_i(s)\\right] \\ \u0026= \\sum_{s=1}^{S}\\sum_{i=1}^{N}\\left[P(s)C_i(s)\\right] \\ \u0026= \\sum_{s=1}^{S}\\left[P(s)\\sum_{i=1}^{N}C_i(s)\\right] \\ \u0026= \\sum_{s=1}^{S}\\left[P(s)C(s)\\right] \\end{aligned} $$ It means that the firm’s value is determined by it’s cash flow, independent of how it issues securities (M\u0026M 1). \r \r \r \r \r ","date":"0001-01-01","objectID":"/4.-capital-structure-i-mm-benchmark/:1:1","tags":null,"title":"","uri":"/4.-capital-structure-i-mm-benchmark/"},{"categories":null,"content":"4.1.2 M\u0026M and the cost of capital 【M\u0026M Theorem Ⅱ】在M\u0026MⅠ的假设下，WACC独立于资本结构。 证明： $$ \\begin{equation}P V=\\frac{E(C F)}{1+WACC}\\end{equation} $$ 由M\u0026MⅠ，$PV$不随资本结构变化；由假设Ⅰ，$CF$不随资本结构变化。因此$WACC$与资本结构无关。 1. 杠杆增加股权收益率 假设Ⅰ表明无税收，因此 $$ WACC = \\bar{r}^*=\\frac{D}{V}\\bar{r}{D}+\\frac{E}{V}\\bar{r}{E} $$ 从而股权收益率将随杠杆增大而增加(与上一章相符合) $$ \\begin{equation}\\bar{r}{E}=\\bar{r}^{}+\\left(\\bar{r}^{}-\\bar{r}{D}\\right) \\frac{D}{E}\\end{equation} $$ \r \r \r \r \r 2. 杠杆增加股权风险 The “asset beta” of firm $$ \\beta^{*}=\\frac{D}{V} \\beta_{D}+\\frac{E}{V} \\beta_{E} $$ Solve for $\\beta_{E}$ $$ \\beta_{E}=\\beta^{*}+\\left(\\beta^{*}-\\beta_{D}\\right) \\frac{D}{E} $$ 从而杠杆将增加股权的系统性风险(即使债务无风险)。 进一步通过CAPM模型可以将股权收益分解为business risk premium和financial risk premium $$ \\begin{aligned} \\bar{r}{E}\u0026=r{f}+\\left[\\boldsymbol{\\beta}^{}+\\left(\\boldsymbol{\\beta}^{}-\\boldsymbol{\\beta}{D}\\right) \\frac{D}{E}\\right]\\left(\\bar{r}{m}-r_{f}\\right)\\ \u0026=r_{f}+\\underbrace{\\boldsymbol{\\beta}^{*}\\left(\\bar{r}_{m}-r_{f}\\right)}_{\\text{business risk premium}} +\\underbrace{\\left(\\boldsymbol{\\beta}^{*}-\\boldsymbol{\\beta}_{D}\\right) \\frac{D}{E}\\left(\\bar{r}_{m}-r_{f}\\right)}_{\\text{financial risk premium}} \\end{aligned} $$ \r \r 不同融资结构对于投资者而言仅仅改变现金流的跨期结构，不改其现值/价值。这种改变可以通过资本市场实现，因此两种资本结构对投资者无差异。 3. 风险债务 M\u0026MⅡ对于有风险债务仍然成立。e.g.有可能破产的情况下M\u0026MⅡ仍成立，前提是无破产成本(假设Ⅱ)。 公司增发债务也不改变公司的价值，但是可能改变公司价值的分配：新债的风险大于旧债，如果新债和旧债有一样的优先级，股东获利，旧债权人吃亏(此种情况下旧债发行人事实上不完全理性)；如果新债低于旧债的优先级，则股东价值仍不变。(例题) \r \r \r \r ","date":"0001-01-01","objectID":"/4.-capital-structure-i-mm-benchmark/:1:2","tags":null,"title":"","uri":"/4.-capital-structure-i-mm-benchmark/"},{"categories":null,"content":"4.1.3 M\u0026M dividend irrelevance theorem(不要求掌握) 【定理】假定无税收，无交易成本，公司的投融资决策、经营策略固定，那么发放股利与股票回购对股权人无差异。 \r \r ","date":"0001-01-01","objectID":"/4.-capital-structure-i-mm-benchmark/:1:3","tags":null,"title":"","uri":"/4.-capital-structure-i-mm-benchmark/"},{"categories":null,"content":"4.2 理论意义 M\u0026M定理表明，若融资决策不影响投资决策，则融资不改变企业价值。同时指出，WACC与资本结构无关；EPS具有杠杆效应，并非越高越好。 If we know what does not matter, we may infer what does. 但现实中，由于税盾、破产成本等存在，融资决策仍然会影响公司价值。 ","date":"0001-01-01","objectID":"/4.-capital-structure-i-mm-benchmark/:2:0","tags":null,"title":"","uri":"/4.-capital-structure-i-mm-benchmark/"},{"categories":null,"content":"5. Capital Structure (II): Taxes and Financing Decisions ","date":"0001-01-01","objectID":"/5.-capital-structure-ii-taxes-and-financing-decisions/:0:0","tags":null,"title":"","uri":"/5.-capital-structure-ii-taxes-and-financing-decisions/"},{"categories":null,"content":"5.1 Beyond the M\u0026M Irrelevance Results Capital Structure and Distribution Policy Do Matter Evidence Corporations spend resources on their design Stock prices react dramatically to financing decisions On average, stock prices react positively to announcements by firms that they will distribute cash to shareholders they will increase their leverage On average, stock prices react negatively to announcements by firms that they will raise cash they will decrease their leverage ","date":"0001-01-01","objectID":"/5.-capital-structure-ii-taxes-and-financing-decisions/:1:0","tags":null,"title":"","uri":"/5.-capital-structure-ii-taxes-and-financing-decisions/"},{"categories":null,"content":"5.2 Trade-off Theory of Capital Structure In choosing their leverage ratios, firms trade off tax benefits of debt against bankruptcy and financial distress costs (财务困境) The optimal leverage ratio depends on firm characteristics: Characteristic Effect on Optimal Leverage Notes Profitability + 盈利能力强，降低破产概率/成本 Non-debt tax shields (depreciation, R\u0026D, etc) - 非债税盾多，债务的税盾价值降低 Tangibility of assets + 有形资产变卖收入高，降低破产概率/成本 Growth opportunities - 成长阶段现金流不稳定，增加破产概率/成本 Volatility of cash flows - 增加破产概率/成本 Size + 综合以上 \r ","date":"0001-01-01","objectID":"/5.-capital-structure-ii-taxes-and-financing-decisions/:2:0","tags":null,"title":"","uri":"/5.-capital-structure-ii-taxes-and-financing-decisions/"},{"categories":null,"content":"5.3 Taxes and Financing Decisions During the next few lectures, we will examine the costs and benefits of using debt. Today, we focus on how taxes affect the use of debt as well as other financing decisions Corporate tax shields are one of the main benefits of using debt financing Tax benefits diminish with the amount of debt used At low levels of debt, further borrowing generates large expected tax shield; at high leverage ratios, further borrowing generates little additional tax shields As we will discuss in future lectures, the opposite is true for the costs of debt (e.g., bankruptcy costs) In a recent survey, 45% of CEOs agree that taxes are an important determinant of capital structure choices. Relevant features of the U.S. tax code Interest and Dividends are taxed as ordinary income; Capital Gains are taxed effectively at a lower rate (can be deferred) Dividends (except for corporations) are tax disadvantaged relative to share repurchases. This difference has recently been reduced. Interest expense is tax deductible, dividends are not (classical tax system; in other countries imputation system). Corporations find it tax-favorable to own stock in other corporations: Corporations are taxed only on 30% of the dividend received (if they own less that 20% of the firm from which the dividend is received. See page 551 in the book for details) Other (non-debt) tax shields: Tax deductions for depreciation of PPE, R\u0026D expenses, past losses carried forward For investors, relevant cash flows are cash flows after taxes Need to consider four dimensions Corporate Taxes Tax Deductible Interest Expenses Personal Taxes State Taxes ","date":"0001-01-01","objectID":"/5.-capital-structure-ii-taxes-and-financing-decisions/:3:0","tags":null,"title":"","uri":"/5.-capital-structure-ii-taxes-and-financing-decisions/"},{"categories":null,"content":"5.3.1 Corporate Taxes \r \r \r \r Is This Important or Negligible? Suppose a firm with no debt undertakes a leveraged recapitalization, i.e., issues debt worth D and buys back equity with the proceeds Its value changes by: $$ \\frac{V_{L}}{V_{U}}=\\frac{V_{U}+P V(\\text {Tax Shield})}{V_{U}}=1+\\frac{T_{c} D}{V_{U}} $$ Thus, if $ T_{c}=35 % $: for $ \\mathrm{D}=20 % $, firm value increases by about $ 7 % $ for $ \\mathrm{D}=50 % $, it increases by about $ 17.5 % $ Bottom line: Tax shield of debt matters, potentially quite a bit. Caveats: Not all firms face full marginal tax rate Need to consider personal taxes too \r \r \r \r \r \r ","date":"0001-01-01","objectID":"/5.-capital-structure-ii-taxes-and-financing-decisions/:3:1","tags":null,"title":"","uri":"/5.-capital-structure-ii-taxes-and-financing-decisions/"},{"categories":null,"content":"5.3.2 Personal Tax Effects \r \r Therefore including personal taxes $$ V_{L}=V_U+g D $$ After considering personal taxes, three scenarios are possible $ g\u003e0 $ Leverage reduces taxes (and increases value) $ g\u003c0 $ Leverage increases taxes (and reduces value) $ g=0 $ Leverage does not affect value Remarks: Intuition: Suppose that you have $ 1 that you can label as “interest” or “equity return” for tax purposes. You should label it debt when: $$ \\left(1-T_{D}\\right)\u003e\\left(1-T_{c}\\right)\\left(1-T_{E}\\right) \\text{ or } g\u003e0 $$ How do these tax rates look like in the real world? since $ T_{D} $ and $ T_{E} $ are up to 38.6% and $ T_{C} $ is up to 35.0% For most cases $ \\left(1-T_{D}\\right)\u003e\\left(1-T_{C}\\right)\\left(1-T_{E}\\right), $ which implies $ g\u003e0 $ See examples next Without considering personal taxes, value of tax shields is estimated at 10% of firm value, considering personal taxes at 7% \r Relative Tax Advantage of Debt: $ \\left(1-T_{D}\\right)\u003e\\left(1-T_{C}\\right) \\times\\left(1-T_{E}\\right) $ Post-Bush I \r *Extreme assumption: No deferral of capital gains, 15% capital gains tax Post-Bush II: Some deferred capital gains \r *Assumption: Effective capital gains tax rate of 10% ","date":"0001-01-01","objectID":"/5.-capital-structure-ii-taxes-and-financing-decisions/:3:2","tags":null,"title":"","uri":"/5.-capital-structure-ii-taxes-and-financing-decisions/"},{"categories":null,"content":"6. Capital Structure (III): Financial Distress Costs ","date":"0001-01-01","objectID":"/6.-capital-structure-iii-financial-distress-costs/:0:0","tags":null,"title":"","uri":"/6.-capital-structure-iii-financial-distress-costs/"},{"categories":null,"content":"6.1 Financial Distress Costs So far, we have either excluded bankruptcy or have assumed costless bankruptcy; now, we introduce bankruptcy and financial distress costs Financial distress costs should be attributed only to the use of debt and should include costs that result from The event of bankruptcy The threat of bankruptcy Financial distress costs should not be attributed to: Misfortunes due to business risk (i.e., asset side of the balance sheet) Ask yourself: What additional reduction in total value can be attributed to the use of debt? ","date":"0001-01-01","objectID":"/6.-capital-structure-iii-financial-distress-costs/:1:0","tags":null,"title":"","uri":"/6.-capital-structure-iii-financial-distress-costs/"},{"categories":null,"content":"6.1.1 Types of Financial Distress Costs Direct Costs (a.k.a. bankruptcy costs) Related to the legal process involving the reorganization of the firm or the transfer of control to debt holders Costs that arise in the event of bankruptcy Indirect Costs Arise from the tendency of levered firms to engage in actions that are harmful to their debt holders and non-financial stakeholders Two types: Indirect costs related to debt holder-equity holder conflicts Indirect costs related to other stakeholders Remarks: Financial Distress cost = Bankruptcy cost, in broad sense direct cost Indirect cost Narrowly: Bankruptcy Cost = Direct cost = Direct Bankruptcy cost = Direct Financial Distress cost Financial Distress cost = Indirect cost = Indirect Bankruptcy cost = Indirect Financial Distress cost ","date":"0001-01-01","objectID":"/6.-capital-structure-iii-financial-distress-costs/:1:1","tags":null,"title":"","uri":"/6.-capital-structure-iii-financial-distress-costs/"},{"categories":null,"content":"6.1.2 How do financial distress costs affect the “pie” Helpful distinction between direct bankruptcy cost and indirect financial distress costs: (ignore taxes) \r ","date":"0001-01-01","objectID":"/6.-capital-structure-iii-financial-distress-costs/:1:2","tags":null,"title":"","uri":"/6.-capital-structure-iii-financial-distress-costs/"},{"categories":null,"content":"6.2 Direct Costs of Bankruptcy ","date":"0001-01-01","objectID":"/6.-capital-structure-iii-financial-distress-costs/:2:0","tags":null,"title":"","uri":"/6.-capital-structure-iii-financial-distress-costs/"},{"categories":null,"content":"6.2.1 Types of Direct Costs Time that management spends dealing with creditors, Average time in bankruptcy proceedings around 2 years for a typical case (both in Chapters 11 and 7) Bankruptcy Process: Legal Bankruptcy can occur as Liquidation/ ch7 Bankruptcy: Termination of the firm as a going concern (破产清算) Reorganization/ ch11 Bankruptcy: Option to keep as a going concern (重组，大部分是此种) Legal expenses, court and advisory fees How large are the direct costs? For large companies about 3.1% of total value on average For small firms estimated to be as large as 25-30% of total value However, direct costs are probably not so relevant Percentage of already diminished firm value Low present value: Multiply by probability of bankruptcy and then discount ","date":"0001-01-01","objectID":"/6.-capital-structure-iii-financial-distress-costs/:2:1","tags":null,"title":"","uri":"/6.-capital-structure-iii-financial-distress-costs/"},{"categories":null,"content":"6.2.2 Who bears the bankruptcy costs Seemingly debt holders (ex-post), but ultimately equity holders through default premium (ex-ante), since debt holders foresee a potential bankruptcy. At the time of the bankruptcy, debt holders bear bankruptcy costs. However, ultimately equity holders do so through the default premium Investors are aware of possible bankruptcy, so they “price-protect” themselves when buying securities issued by the firm (债权人根据破产成本提高相应的债券价格) Bankruptcy costs (direct and indirect) are initially borne by equity holders as residual claimants of the firm What are the bankruptcy costs born by equity holders? Increase in the required return on debt (to compensate for bankruptcy costs) Inability to obtain additional debt needed to continue operations Reduction in the return on assets: NPV \u003e 0 projects may not be undertaken \r \r ","date":"0001-01-01","objectID":"/6.-capital-structure-iii-financial-distress-costs/:2:2","tags":null,"title":"","uri":"/6.-capital-structure-iii-financial-distress-costs/"},{"categories":null,"content":"6.3 Debt Holder-Equity Holder Conflicts Shareholders of a levered firm have an incentive to invest (or pass-up on investments) to increase equity value without considering the reduction in debt value. Conflict between discounting CF to Equity Holders versus discounting Total Cash Flows. Two main DH-EH costs Debt overhang Asset substitution For this and future examples in this lecture, unless otherwise specified, we assume for simplicity: (1) Zero risk-free rate + (2) Zero risk premium + (3) No taxes. All arguments remain valid with: (1) Positive risk-free rate + (2) Positive risk premium + (3) Taxes Then, we have $ PV = E(CF)$. ","date":"0001-01-01","objectID":"/6.-capital-structure-iii-financial-distress-costs/:3:0","tags":null,"title":"","uri":"/6.-capital-structure-iii-financial-distress-costs/"},{"categories":null,"content":"6.3.1 The Asset Substitution Problem \r \r \r \r Remarks: Debt can generate an incentive for equity holders to take on excessive risk Equity is a call option on firm assets so “increasing volatility” may help equity holders \r This effect only arises if debt is “sufficiently risky” and never if it is completely safe (see above two example) \r As the next example shows, asset substitution can also generate credit rationing (inability to borrow at “any” rate/ 无论出多高的利息都借不到钱) \r \r ","date":"0001-01-01","objectID":"/6.-capital-structure-iii-financial-distress-costs/:3:1","tags":null,"title":"","uri":"/6.-capital-structure-iii-financial-distress-costs/"},{"categories":null,"content":"6.3.2 Debt Overhang/ 债务积压 【Definition】Debt overhang: tendency of equity holders to pass up positive NPV investments when the firm’s existing debt holders are expected to capture most of the benefits. Also known as the under-investment problem. \r \r Note that investment decision (NPV calculating) doesn’t take $ C_0 $ into account (sank cost) \r \r \r Remarks: Making the expense at t=1 is always a positive NPV decision But the initial debt financing can impede raising new capital and hence can make the firm pass up positive NPV investments Which security is considered to be issued at this point (t = 1) is irrelevant (i.e., debt overhang prevents issuing both new debt and new equity) Note that the original debt holders are compensated for the debt overhang problem by requiring debt to have a face value of $110 Notice how the pie shrinks: Had the project been equity financed the NPV would have been (10/11)(500 – 100 – 100) + (1/11)(150 – 100 – 100) = $268.18 With debt financing, due to the debt overhang problem (10/11)(500 – 100 – 100) + (1/11)(–100) = $263.63 What is the source of the problem? Can it be avoided? Seniority of the old debt contributes to the problem. However seniority may be desirable for other reasons. The problem would be solved if the original lender provided further financing (相比于不投资可以多收回50元的利息) But there may be a free-rider problem among original lenders (多个贷款人的情形) If the original debt is held by a diffuse group of lenders, it can be difficult to persuade them to invest additional resources Each individual lender would like to free ride on the decision of other lenders to bail out the firm \r \r \r \r \r Remarks: Debt overhang gives incentives to pass up internally financed positive NPV projects Covenants to protect creditors from this kind of behavior are quite common (e.g., relate cash disbursements to earnings and restrict them when earnings are low) Firms in financial distress cut dividends drastically Bond prices react negatively when firms announce large increases in their dividends \r \r \r \r ","date":"0001-01-01","objectID":"/6.-capital-structure-iii-financial-distress-costs/:3:2","tags":null,"title":"","uri":"/6.-capital-structure-iii-financial-distress-costs/"},{"categories":null,"content":"6.3.3 Mitigation of DH-EH conflicts Several practices and institutions can help to alleviate DH-EH conflicts a) Chapter 11 bankruptcy Debtor in Possession Financing Allows financing to fund investments necessary for operations Strip covenants and seniority from existing debt b) Reduced use of debt Conflicts arise only if debt is or can become risky All-equity financing eliminates the conflicts, but at the cost of losing tax shields and other benefits of debt c) Protective covenants Technical default – Violation of covenant Example of covenants: (more common in non-investment grade debt) Limitations in the issuance of additional debt Limitations in asset sales Limitations in cash payouts Reporting and disclosure Changes in management, control and ownership Covenants may also have costs: limit flexibility Covenants have become more stringent after the 80s Sometimes restrict takeovers and leveraged recapitalizations Bank debt tends to have more covenants and more stringent ones than public debt d) Bank and Privately Placed Debt Solve free-rider problem Better monitor (perhaps better control of asset substitution) However more costly Possible hold-up problems (if firm depends on a lender, this lender would have “too much” power in future renegotiations) e) Use of short-term debt instead of long-term debt Helps with Debt Overhang, Asset substitution or Reluctance to liquidate Can generate shortsightedness Increase direct costs of bankruptcy and transaction costs In some situations can generate “excessive” incentives to liquidate (see Massey-Ferguson case) f) Other possibilities Security design (e.g., convertibles) Managerial compensation ","date":"0001-01-01","objectID":"/6.-capital-structure-iii-financial-distress-costs/:3:3","tags":null,"title":"","uri":"/6.-capital-structure-iii-financial-distress-costs/"},{"categories":null,"content":"7. Corporate Governance ","date":"0001-01-01","objectID":"/7.-corporate-governance/:0:0","tags":null,"title":"","uri":"/7.-corporate-governance/"},{"categories":null,"content":"The Corporate Governance Question Why are ownership and control separated in modern corporations? Risk diversification: help shareholders diversify their portfolios (purchase stock of different Corporate) Expertise considerations: professional managerial expertise creates value To understand firms’ actions, need to take into account managerial incentives. Evidence: Managers do matter! Stock prices increase after announcement of election of a “star” CEO But also, sudden executive deaths (plane crashes or heart attacks) often accompanied by increases in firm stock prices! Largest increases for major conglomerates, whose founders built vast empires without returning much to investors. How do managers and shareholders differ? Shareholders want to maximize firm’s financial value while managers have additional objectives: To reduce idiosyncratic risk (to protect their non-diversifiable human capital) Career concerns (i.e., take actions that benefit their professional careers) To reduce effort (managers do not suffer the full consequences of effort reductions) To over-invest (empire building) To protect other constituencies (e.g., employees) Manager’s age, tenure in the firm, preferences (i.e., risk aversion), and wealth affect such differences Some examples a) “Entrenchment” Managers make irreversible investments to businesses in which they have particular expertise. Resistance to replacements and takeovers Fact: Less resistance if managers own substantial equity b) Making negative NPV investments to minimize risk Managerial tendency toward diversifying acquisitions that destroy shareholder value. c) Keeping leverage low to reduce bankruptcy risk (at the expense of possible tax savings) High personal cost of bankruptcy for managers e.g. risk of being fired d) Horizon problems Short-sighted investments by managers Fact: CEO compensation affected by proximity to retirement Example: Tyco-Kozlowski’s Party Dennis Kozlowski’s Sardinian birthday party cost Tyco an estimated $1 million Cost to Kozlowski depended on his equity stake in the company If he held 1% of outstanding shares, the party cost was only $10,000 from his perspective The remaining $990,000 would come from the other shareholders The CEO is effectively spending other people’s money. Clear moral hazard problem The “Corporate Governance” question: How should non-controlling shareholders organize the firm to better align managers’ incentives? External Mechanisms of Control Capital Structure Market for Corporate Control (Takeovers) Internal Mechanisms of Control Managerial Compensation Contracts Board of Directors (monitoring) / 董事会 Concentrated Ownership (institutional investors) Other Mechanisms Managerial Reputation Market Monitoring Model Setup Both principal and agent are risk-neutrality \u0026 limitied liability \r Risk-neutral principal: offers incentive contract $$ w \\in {w_R,w_0} $$ If $e=0$ is to be implemented, then $ w_0^*={0,0} $ (with $ \\bar{\\mu} = 0 $) If $e=1$ is to be implemented, then $ w_1^* = {w_R,w_0} $ (with $ \\bar{\\mu} = 0 $) $$ \\begin{aligned} \\min_{w_R,w_0} \u0026P_H w_R+(1-P_H)w_0 \\ s.t. \u0026w_R, w_0 \\geq 0 \\quad (PC) \\ \u0026P_Hw_R+(1-P_H)w_0-B \\geq P_Lw_R+(1-P_L)w_0 \\quad (IC) \\end{aligned} $$ According to IC, we have $$ \\Delta (w_R-w_0) \\geq B $$ with $w_0^* = 0$, we have $$ w_R \\geq \\frac{B}{\\Delta P } $$ which gives $ w_1^* = { \\frac{B}{\\Delta P },0}$. Remarks: Principal wants to induce $ e = 1 $, we have $$ \\begin{aligned} P_H(R-\\frac{B}{\\Delta P}) \\geq P_L R \\ \\underbrace{\\Delta P \\cdot R-B}_{NPV(e=1)} \\geq P_H\\frac{ B}{\\Delta P}-B=\\frac{P_L B}{\\Delta P} \u003e 0 \\end{aligned} $$ otherwise $ e = 0 $, it’s under effort / under investment If $e = 1 $, agent gets $P_H\\frac{ B}{\\Delta P}-B=\\frac{P_L B}{\\Delta P} \u003e 0$, which is called agent rent. Trade-off: incentives v.s. agent rent Traditional model: Incentives (有风险的薪水需要更高的溢价) v.s. insurance (risk averse agent) ","date":"0001-01-01","objectID":"/7.-corporate-governance/:1:0","tags":null,"title":"","uri":"/7.-corporate-governance/"},{"categories":null,"content":"经典回归模型到底在干嘛？ All models are wrong, but some are useful. – George Box 回归(Regression)最早由高尔顿（1886）提出，他发现，子辈的平均身高是其父辈平均身高以及他们所处族群平均身高的加权平均和，即身高具有均值回归的倾向。这个均值本质上是条件均值(给定父辈和种群平均身高，子代身高的均值)，经典回归模型事实上就是在估计条件均值。 在经典回归模型中，我们希望用解释变量(regressand)$X$的函数$g(X)$来预测被解释变量(regressor)$Y$。此时需要一个标准来测度$g(X)$与$Y$的接近程度，均方误(mean squared error, MSE)准则最常被使用，MSE是预测误差（预测值$g(X)$与目标$Y$之差）的平方的期望，表达式如下 $$ \\operatorname{MSE}(g)=E[Y-g(X)]^{2} = \\int\\int[y-g(x)]^2f_{XY}(x,y)\\mathrm{d} x\\mathrm{d} y $$ 其中，$f_{XY}(x,y)$是变量$X$和$Y$的联合概率分布。 显然，MSE越小，$g(X)$对$Y$的预测能力越强。因此现在的问题转换为，求解使MSE最小的函数$g(·)$，注意到MSE是函数$g(·)$的函数。 事实上，条件均值$E(Y|X)$就是使MSE最小的函数$g_0(X)$，可以用求微分和方差分解两种方法证明（证明见文末附录）。 需要注意的是，条件均值$E(Y|X)$是$X$而非$Y$的函数，例如在高尔顿的例子中，子代身高的条件均值，取决于父辈和种群的平均身高，也即父辈和种群的平均身高的函数。 MSE是衡量$g(X)$对$Y$的预测能力的准则之一，但非唯一准则。例如，平均绝对误差(mean absolute error, MAE)， $$ \\operatorname{MAE}(g)=E|Y-g(X)| $$ 此时，使MAE最小的函数$g(X)$是条件中位数，分位数回归采用的正是该准则。 相比MAE，MSE具有连续可导的优良性质。 此外，令$Y=E(Y | X)+\\varepsilon$，其中$\\varepsilon$被称为回归扰动项，则有 $$ \\begin{aligned} E(\\varepsilon | X) \u0026=E{[Y-E(Y | X)] | X} \\ \u0026=E(Y | X)-E\\left[g_{o}(X) | X\\right] \\ \u0026=E(Y | X)-g_{o}(X) \\ \u0026=0 \\end{aligned} $$ $E(\\varepsilon|X) = 0$意味着$\\varepsilon$不包含可用于预测$Y$的期望值的任何有关$X$的信息。换句话说，可用于预测$Y$的所有$X$的信息被包含在$E(Y|X)$中。 在很多经济问题中，一阶条件矩即条件均值也是关注的焦点。 基于以上诸多原因，回归等式被设定为$Y=E(Y | X)+\\varepsilon$，经典回归模型就是在估计$E(Y|X)$。常用的建模方法就是将$E(Y|X)$设定为某种有已知的函数形式，但包含少数未知参数，然后估计未知参数即可。 例如，线性回归模型假定 $$ E(Y|X)=\\beta_{0}+\\sum_{j=1}^{k} \\beta_{j} X_{j}, \\beta_{j} \\in \\mathbb{R} $$ 又如，Logistic回归模型假定 $$ E(Y|X)=\\frac{1}{1+\\exp (-\\beta_{0}-\\sum_{j=1}^{k} \\beta_{j} X_{j})} $$ 最终经典回归问题被转换为熟悉的参数估计。 ","date":"0001-01-01","objectID":"/1.-natureofregression/:0:0","tags":null,"title":"","uri":"/1.-natureofregression/"},{"categories":null,"content":"参考文献 [1] Francis, Galton. Regression Towards Mediocrity in Hereditary Stature[J]. The Journal of the Anthropological Institute of Great Britain and Ireland, 1886. [2] Hong Y. Advanced Econometrics, Higher Education Press, 2011:18-28. ","date":"0001-01-01","objectID":"/1.-natureofregression/:0:1","tags":null,"title":"","uri":"/1.-natureofregression/"},{"categories":null,"content":"附录 引理：重复期望法则(Law of Iterated Expectations, LIE)，对给定可测函数$G(X,Y)$，假设期望$E[G(X,Y)]$存在，则 $$ E[G(X, Y)]=E{E[G(X, Y) | X]} $$ 证明：仅考虑$\\left(Y,X^{\\prime}\\right)^{\\prime}$是连续随机向量的情形，有 $$ \\begin{aligned} E[G(X, Y)] \u0026=\\iint_{-\\infty}^{\\infty} G(x, y) f_{X Y}(x, y) \\mathrm{d} x \\mathrm{d} y \\ \u0026=\\iint_{-\\infty}^{\\infty} G(x, y) f_{Y | X}(y | x) f_{X}(x) \\mathrm{d} x \\mathrm{d} y \\ \u0026=\\int\\left[\\int_{-\\infty}^{\\infty} G(x, y) f_{Y | X}(y | x) \\mathrm{d} y\\right] f_{X}(x) \\mathrm{d} x \\ \u0026=\\int E[G(X, Y) | X=x] f_{X}(x) \\mathrm{d} x \\ \u0026=E{E[G(X, Y) | X]} \\end{aligned} $$ 定理：条件均值$E(Y|X)$是下列问题的最优解 $$ \\begin{aligned} E(Y | X) \u0026=\\arg \\min _{g \\in \\mathbb{F}} M S E(g) \\ \u0026=\\arg \\min _{g \\in \\mathbb{F}} E[Y-g(X)]^{2} \\end{aligned} $$ 其中$\\mathbb{F}$是所有可测和平方可积函数的集合，即 $$ \\mathbb{F}=\\left{g: \\mathbb{R}^{k+1} \\rightarrow \\mathbb{R} | \\int g^{2}(x) f_{X}(x) \\mathrm{d} x\u003c\\infty\\right} $$ 法一：方差分解 令$g_{0}(X) = E(Y | X)$，则 $$ \\begin{aligned} \\operatorname{MSE}(g) \u0026=E\\left[Y-g_{0}(X)+g_{0}(X)-g(X)\\right]^{2} \\ \u0026=E\\left[Y-g_{0}(X)\\right]^{2}+E\\left[g_{0}(X)-g(X)\\right]^{2}+2 E\\left{\\left[Y-g_{0}(X)\\right]\\left[g_{0}(X)-g(X)\\right]\\right} \\end{aligned} $$ 根据重复期望法则 $$ \\begin{aligned} E\\left{\\left[Y-g_{0}(X)\\right]\\left[g_{0}(X)-g(X)\\right]\\right} \u0026=E\\left{E\\left(\\left[Y-g_{0}(X)\\right]\\left[g_{0}(X)-g(X)\\right]|X\\right)\\right} \\ \u0026=E\\left{\\left[g_{0}(X)-g(X)\\right]E\\left(\\left[Y-g_{0}(X)\\right]|X\\right)\\right} \\ \u0026=E\\left{\\left[g_{0}(X)-g(X)\\right][E(Y|X)-g_{0}(X)]\\right} \\ \u0026=E\\left{\\left[g_{0}(X)-g(X)\\right]·0\\right} \\ \u0026=0 \\end{aligned} $$ $$ \\implies MSE(g) =E\\left[Y-g_{0}(X)\\right]^{2}+E\\left[g_{0}(X)-g(X)\\right]^{2} $$ $$ \\implies \\arg \\min _{g \\in \\mathbb{F}} M S E(g) = g_0(X) = E(Y|X) $$ 法二：求微分法 $$ \\operatorname{MSE}(g)=E[Y-g(X)]^{2} = \\int\\int[y-g(x)]^2f_{XY}(x,y)\\mathrm{d} x\\mathrm{d} y $$ 根据一阶条件，MSE对$g(X)$的导数为0 $$ \\frac{\\delta MSE(g)}{\\delta g(x)}=-2\\int[y-g(x)] f_{XY}(x,y) \\mathrm{d} y=0 $$ $$ \\implies \\int g(x)f_{XY}(x,y) \\mathrm{d}y = \\int yf_{XY}(x,y) \\mathrm{d}y $$ $$ \\implies g(x)\\int f_{XY}(x,y) \\mathrm{d}y = \\int yf_{XY}(x,y) \\mathrm{d}y $$ $$ \\implies g(x) f_X(x) = \\int yf_{XY}(x,y) \\mathrm{d}y $$ $$ \\implies g(x) = \\int y\\frac{f_{XY}(x,y)}{f_X(x)} \\mathrm{d}y $$ $$ \\implies g(x) = \\int yf_{Y|X}(y|x) \\mathrm{d}y=E(Y|X) $$ ","date":"0001-01-01","objectID":"/1.-natureofregression/:0:2","tags":null,"title":"","uri":"/1.-natureofregression/"},{"categories":null,"content":"3. IID随机样本 ","date":"0001-01-01","objectID":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/:0:0","tags":null,"title":"","uri":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/"},{"categories":null,"content":"3.1 渐进理论导论 LLN+CLT+Taloy Expansion可以推导出所有的渐进分布 ","date":"0001-01-01","objectID":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/:1:0","tags":null,"title":"","uri":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/"},{"categories":null,"content":"3.2 模型假设 \r \r 假设4.3弱于假设3.2 假设4.4 Q是正半定的+非奇异=Q是正定的 矩阵的各个元素是小于无穷的 对角元素小于无穷，则其他元素小于无穷。证： \r 柯西徐瓦兹不等式 \r 联系假设3.3(2): 4.4可以推出3.3(2) \r \r \r 假设4.5 \r 让其正定等价于让其非奇异 \r ","date":"0001-01-01","objectID":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/:2:0","tags":null,"title":"","uri":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/"},{"categories":null,"content":"3.3 OLS估计量的一致性 \r 证明： \r \r \r \r ","date":"0001-01-01","objectID":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/:3:0","tags":null,"title":"","uri":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/"},{"categories":null,"content":"3.4 OLS估计量的渐进分布 ","date":"0001-01-01","objectID":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/:4:0","tags":null,"title":"","uri":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/"},{"categories":null,"content":"3.4.1 渐进正态分布 \r \r \r \r \r ","date":"0001-01-01","objectID":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/:4:1","tags":null,"title":"","uri":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/"},{"categories":null,"content":"3.4.2 条件同方差的情形 \r \r \r 因此此时经典回归模型中的检验统计量仍然适用，应用其渐进分布 ","date":"0001-01-01","objectID":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/:4:2","tags":null,"title":"","uri":"/3.-iid%E9%9A%8F%E6%9C%BA%E6%A0%B7%E6%9C%AC/"},{"categories":null,"content":"6. Instrumental Variable Regression ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:0:0","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.1 Motivation So far, we’ve discussed what happens to OLS without the normality assumption. Asymptotic Theory with IID. Asymptotic Theory with Ergodic Stationary MDS Asymptotic Theory with Ergodic Stationary non-MDS. Next, we will discuss what happens to OLS if $ E\\left(\\varepsilon_{t} | X_{t}\\right) \\neq 0 $. When $ E\\left(\\varepsilon_{t} | X_{t}\\right) \\neq 0, $ OLS estimator is inconsistent! There are many cases with $ E\\left(\\varepsilon_{t} | X_{t}\\right) \\neq 0 $ Measurement Errors in variables. omitted Important Variables. Simultaneous Cases. If $ E\\left(\\varepsilon_{t} | X_{t}\\right) \\neq 0, $ what can we do? Give up OLS, and use alternative estimator; two-stage least squares (2SLS) estimator. 2SLS requires an instrumental variable (IV) satisfying certain conditions. Instrument relevance: $ \\operatorname{corr}\\left(Z_{t}, X_{t}\\right) \\neq 0 $ Instrument exogeneity: $ \\operatorname{corr}\\left(Z_{t}, \\varepsilon_{t}\\right)=0 $. We will investigate its statistical properties and construct hypothesis test statistics. When $ \\varepsilon_{t} $ is an MDS with conditional homoskedasticity When $ \\varepsilon_{t} $ is an MDS with conditional heteroskedasticity, When $ \\varepsilon_{t} $ is a non-MDS process. Note that the $ t $-test and $ F $-test obtained from the second stage regression estimation cannot be used even for large samples. Questions: When may the condition $ E\\left(\\varepsilon_{t} | X_{t}\\right)=0 $ fail? When $ E\\left(\\varepsilon_{t} | X_{t}\\right) \\neq 0, X_{t} $ is called ‘endogenous’, or $ X_{t} $ has the endogeneity problem. 【Example 6.2】Errors of Measurements in Dependent Variable: Now we consider a data generating process (DGP) given by $$ Y_{t}^{*}=\\beta_{0}^{\\circ}+\\beta_{1}^{\\circ} X_{t}+\\varepsilon_{t} $$ where $ X_{t} $ is the income, $ Y_{t}^{*} $ is the consumption, and $ \\left{\\varepsilon_{t}\\right} $ is i.i.d. $ \\left(0, \\sigma_{u}^{2}\\right) $ and is independent of $ \\left{X_{t}\\right} $. We don’t observe $ Y_{t}^{*}, $ but $ Y_{t}=Y_{t}^{*}+w_{t}, $ where $ \\left{w_{t}\\right} $ is i.i.d. $ \\left(0, \\sigma_{w}^{2}\\right) $ measurement errors independent of $ \\left{X_{t}\\right} $ and $ \\left{Y_{t}^{*}\\right} . $ Assume that $ \\left{w_{t}\\right} $ and $ \\left{\\varepsilon_{t}\\right} $ are mutually independent. Because we only observe $ \\left(X_{t}, Y_{t}\\right), $ we are forced to estimate the following model $$ Y_{t}=Y_{t}^{*}+w_{t}=\\beta_{0}^{o}+\\beta_{1}^{o} X_{t}+u_{t} $$ where $ u_{t}=w_{t}+\\varepsilon_{t} . $ since $ E\\left(X_{t} u_{t}\\right)=0, $ **OLS is consistent**! The measurement error in $ Y_{t} $ can be regarded as part of the true regression disturbance. It increases the asymptotic variance of $ \\sqrt{n}\\left(\\hat{\\beta}-\\beta^{\\circ}\\right), $ so renders the estimation of $ \\beta^{\\circ} $ less precise. 【Example 6.3】Errors in Expectations/ Regressors: Consider a linear regression model $$ Y_{t}=\\beta_{0}+\\beta_{1} X_{t}^{*}+\\varepsilon_{t} $$ where $ X_{t}^{*} $ is the economic agent’s conditional expectation of $ X_{t} $ at time $ t-1, $ and $ \\left{\\varepsilon_{t}\\right} $ is an $ I I D\\left(0, \\sigma^{2}\\right) $ sequence with $ E\\left(\\varepsilon_{t} | X_{t}^{*}\\right)=0 . $ The conditional expectation $ X_{t}^{*} $ is a latent variable. **One such example is the Phillips curve’s based inflation rate model in macroeconomics**. When the economic agent follows rational expectations, then $ X_{t}^{*}=E\\left(X_{t} | I_{t-1}\\right) $ and we have $$ X_{t}=X_{t}^{*}+v_{t} $$ where $$ E\\left(v_{t} | I_{t-1}\\right)=0 $$ where $ I_{t-1} $ is the information available to the economic agent at time $ t-1 . $ Assume that two error series $ \\left{\\varepsilon_{t}\\right} $ and $ \\left{v_{t}\\right} $ are independent of each other. Because we only observe $ \\left(X_{t}, Y_{t}\\right), $ we are forced to estimate the following model $$ Y_{t}=\\beta_{0}^{o}+\\beta_{1}^{o}\\left(X_{t}-v_{t}\\right)+\\varepsilon_{t}=\\beta_{0}^{o}+\\beta_{1}^{o} X_{t}+u_{t} $$ where $ u_{t}=\\varepsilon_{t}-\\beta_{1}^{\\circ} v_","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:1:0","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.2 Framework and Assumptions ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:2:0","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.2.1 Assumptions 【Assumption 7.1】Ergodic Stationarity: $ \\left{Y_{t}, X_{t}^{\\prime}, Z_{t}^{\\prime}\\right}_{t=1}^{\\prime n} $ is an ergodic stationary stochastic process, where $ X_{t} $ is a $ K \\times 1 $ vector, $ Z_{t} $ is a $ l \\times 1 $ vector, and $ l \\geq K $. Assumption 7.1 allows for IID observations and stationary time series observations. 【Assumption 7.2】Linearity: $$ Y_{t}=X_{t}^{\\prime} \\beta^{\\circ}+\\varepsilon_{t}, \\quad t=1, \\ldots, n $$ for some unknown parameter $ \\beta^{\\circ} $ and some unobservable disturbance $ \\varepsilon_{t} $. 【Assumption 7.3】Nonsingularity: The $ K \\times K $ matrix $$ Q_{x x}=E\\left(X_{t} X_{t}^{\\prime}\\right) $$ is nonsingular and finite. These three assumptions are similar to those in previous chapters. 【Assumption 7.4】IV Conditions: $ E\\left(\\varepsilon_{t} | X_t\\right) \\neq 0 $ $ E\\left(\\varepsilon_{t} | Z_{t}\\right)=0 $ The $ l \\times l $ matrix $$ Q_{z z}=E\\left(Z_{t} Z_{t}^{\\prime}\\right) $$ is finite and nonsingular, and the $ l \\times K $ matrix $$ Q_{z x}=E\\left(Z_{t} X_{t}^{\\prime}\\right) $$ is finite and of full rank (**with A. 7.1 rank=K**). 【Assumption 7.5】CLT: $$ n^{-1 / 2} \\sum_{t=1}^{n} Z_{t} \\varepsilon_{t} \\stackrel{d}{\\rightarrow} N(0, V) $$ for some $ K \\times K $ symmetric matrix $ V \\equiv \\operatorname{avar}\\left(n^{-1 / 2} \\sum_{t=1}^{n} Z_{t} \\varepsilon_{t}\\right) $ finite and nonsingular. Assumption 7.5 directly assumes that the CLT holds. This is often called a “high level assumption.” It covers three cases: IID, MDS and non-MDS for $ \\left{X_{t} \\varepsilon_{t}\\right} $. For an IID or MDS sequence $ \\left{Z_{t} \\varepsilon_{t}\\right}, $ we have $ V=\\operatorname{var}\\left(Z_{t} \\varepsilon_{t}\\right)=E\\left(Z_{t} Z_{t}^{\\prime} \\varepsilon_{t}^{2}\\right) $ For a non-MDS process $ \\left{Z_{t} \\varepsilon_{t}\\right} $, $ V=\\Sigma_{j=-\\infty}^{\\infty} \\operatorname{cov}\\left(Z_{t} \\varepsilon_{t} , Z_{t-j} \\varepsilon_{t-j}\\right) $ is a long-run variance-covariance matrix. ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:2:1","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.2.2 Instrumental Variable The random vector $ Z_{t} $ that satisfies assumption 7.4 is called instruments. The condition that $ l \\geq K $ in assumption 7.1 implies that **the number of instruments $ Z_{t} $ is larger than or at least equal to the number of regressors $ X_{t} $.** When $ E \\left(\\varepsilon_{t} | X_{t}\\right) \\neq 0, $ we usually (but not always) have $ E\\left(X_{t} \\varepsilon_{t}\\right) \\neq 0 . $ As a result, the OLS estimator is not consistent for $ \\beta^{\\circ} . $ Now suppose we have an instrument vector $ Z_{t} $ with $ E\\left(\\varepsilon_{t} | Z_{t}\\right)= 0 $, which implies $ E\\left(Z_{t} \\varepsilon_{t}\\right)=0 . $ Then we can first project $ X_{t} $ onto $ Z_{t} $ and then run a regression of $ Y_{t} $ on the projection. This will deliver consistent estimation of $ \\beta^{\\circ} $. Intuitively, IV is used when $ X_{t} $ is correlated to the error term. A valid IV induces changes in $ X_{t}, $ but has no direct effect on $ Y_{t} $. Question: How to choose instruments $ Z_{t} $ in practice? All exogenous variables in $ X_t $: First of all, one should analyze which explanatory variables in $ X_t $ are endogenous or exogenous. If an explanatory variable is exogenous, then this variable should be included in $ Z_t $, the set of instruments. For example, the constant term should always be included, because a constant is uncorrelated with any random variables. If $ k_{0} $ of $ K $ regressors are endogenous, at least $ k_{0} $ additional instruments should be included in $ Z_{t} $. Most importantly, **we should choose an IV which is closely related to $ X_{t} $.** As we will see below, the strength of the correlation between $ Z_{t} $ and $ X_{t} $ affects the magnitude of the asymptotic variance of the 2SLS estimator for $ \\beta_{0}, $ although it does not affect the consistency provided the correlation between $ Z_t $ and $ X_t $ is a non-zero constant. ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:2:2","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.3 Two-Stage Least Squares (2SLS) Estimation Question: Because $ E\\left(\\varepsilon_{t} | X_{t}\\right) \\neq 0, $ the OLS estimator $ \\hat{\\beta} $ is not consistent for $ \\beta^{\\circ} . $ How to obtain consistent estimators for $ \\beta^{\\circ} ? $ It should be pointed out that when $ E\\left(\\varepsilon_{t} | X_{t}\\right) \\neq 0, $ endogeneity arises due to various reasons including model misspecification. However, **it still makes sense to find out the expected marginal effect of explanatory variables $ X_{t} $ on the dependent variable $ Y_{t} $, even if the linear regression model is misspecified**. This requires consistent estimation of $ \\beta^{o} $. See For example, although Example 7.4 suffers from an omitted variable problem, one may be still interested in knowing the expected marginal effect of education on income. ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:3:0","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.3.1 Stage 1 Stage 1: Regress $ X_{t} $ on $ Z_{t} $ via $ \\mathrm{OLS} $ and save the predicted value $ \\hat{X}_{t} $. Here, the artificial linear regression model is $$ X_{t}=\\gamma^{\\prime} Z_{t}+v_{t}, \\quad t=1, \\ldots, n $$ where $ \\gamma $ is a $ I \\times K $ parameter matrix, and $ v_{t} $ is a $ K \\times 1 $ regression error. From the result in chapter 1, we have $ E\\left(Z_{t} v_{t}\\right)=0 $ if and only if $$ \\gamma=\\left[E\\left(Z_{t} Z_{t}^{\\prime}\\right)\\right]^{-1} E\\left(Z_{t} X_{t}^{\\prime}\\right) $$ The OLS estimator for $ \\gamma $ is $ \\hat{\\gamma}=\\left(Z^{\\prime} Z\\right)^{-1} Z^{\\prime} X, $ and the predicted value of $ X_{t} $ on $ Z_{t} $ is $ \\hat{X}_{t}=\\hat{\\gamma}^{\\prime} Z_{t} . $ In matrix form $$ \\mathbf{\\hat{X}}=\\mathbf{Z} \\hat{\\gamma}=\\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X} \\equiv P_Z \\mathbf{X} $$ where $ P_Z $ is symmetric and idempotent. ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:3:1","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.3.2 Stage 2 Stage 2: Use the predicted value $ \\hat{X}{t} $ as regressors for $ Y{t} . $ Regress $ Y_{t} $ on $ \\hat{X}_{t} $ and the resulting OLS estimator is called the 2SLS estimator, denoted as $ \\hat{\\beta}_{2 s l s} $. Question: Why use the fitted value $ \\hat{X}{t}=\\hat{\\gamma}^{\\prime} Z{t} $ as regressors? Because $ E\\left(Z_{t} \\varepsilon_{t}\\right)=0, $ the population projection $ \\gamma^{\\prime} Z_{t} $ is orthogonal to $ \\varepsilon $. In general, $ v_{t}=X_{t}-\\gamma^{\\prime} Z_{t}, $ which is orthogonal to $ Z_{t}, $ is correlated with $ \\varepsilon_{t} $ The stage 1 decomposes $ X_{t} $ into two components: $ \\gamma^{\\prime} Z_{t} $ and $ v_{t}, $ where $ \\gamma^{\\prime} Z_{t} $ is orthogonal to $ \\varepsilon_{t}, $ and $ v_{t} $ is correlated with $ \\varepsilon_{t} $. The regression model in the second stage can be written as $$ Y_{t}=\\hat{X}_{t}^{\\prime} \\beta^{\\circ}+\\tilde{\\varepsilon}_{t} $$ or in matrix form $$ Y=\\hat{X} \\beta^{\\circ}+\\tilde{\\varepsilon} $$ Note that the disturbance $ \\tilde{\\varepsilon}_{t} $ is not $ \\varepsilon_{t} $ because $ \\hat{X}_{t} $ is not $ X_{t} $. Using $ \\hat{X}=Z \\hat{\\gamma}=Z\\left(Z^{\\prime} Z\\right)^{-1} Z^{\\prime} X, $ we can write the $ 2 S L S $ estimator as follows: $$ \\begin{aligned} \\hat{\\beta}_{2 s l s} \u0026=\\left(\\hat{\\mathbf{X}}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} Y=\\left[(\\mathbf{Z} \\hat{\\gamma})^{\\prime}(\\mathbf{Z} \\hat{\\gamma})\\right]^{-1}(\\mathbf{Z} \\hat{\\gamma})^{\\prime} Y \\ \u0026=\\left{\\left[\\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X}\\right]^{\\prime}\\left[\\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X}\\right]\\right}^{-1}\\left[\\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X}\\right]^{\\prime} Y \\ \u0026=\\left[\\mathbf{X}^{\\prime} \\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X}\\right]^{-1} \\mathbf{X}^{\\prime} \\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} Y \\ \u0026=\\left[\\mathbf{X}^{\\prime} \\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X}\\right]^{-1} \\mathbf{X}^{\\prime} \\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} Y \\ \u0026=\\left[\\frac{\\mathbf{X}^{\\prime} \\mathbf{Z}}{n}\\left(\\frac{\\mathbf{Z}^{\\prime} \\mathbf{Z}}{n}\\right)^{-1} \\frac{\\mathbf{Z}^{\\prime} \\mathbf{X}}{n}\\right]^{-1} \\frac{\\mathbf{X}^{\\prime} \\mathbf{Z}}{n}\\left(\\frac{\\mathbf{Z}^{\\prime} \\mathbf{Z}}{n}\\right)^{-1} \\frac{\\mathbf{Z}^{\\prime} Y}{n} \\end{aligned} $$ or use the simple form $$ \\begin{aligned} \\hat{\\beta}{2sls}\u0026=\\left(\\hat{X}^{\\prime} \\hat{X}\\right)^{-1} \\hat{X}^{\\prime} Y \\ \u0026=\\left(X^{\\prime} \\mathbb{P}{z} X\\right)^{-1} X^{\\prime} \\mathbb{P}_{z} Y \\ \u0026=\\left[X^{\\prime} Z\\left(Z^{\\prime} Z\\right)^{-1} Z^{\\prime} X \\right]^{-1} X^{\\prime} Z\\left(Z^{\\prime} Z\\right)^{-1} Z^{\\prime} Y \\end{aligned} $$ Using the expression $ Y=\\mathrm{X} \\beta^{\\circ}+\\varepsilon $ from assumption 7.2 we have $$ \\begin{aligned} \\hat{\\beta}{2 s l s}-\\beta^{\\circ} \u0026=\\left[\\frac{\\mathbf{X}^{\\prime} \\mathbf{Z}}{n}\\left(\\frac{\\mathbf{Z}^{\\prime} \\mathbf{Z}}{n}\\right)^{-1} \\frac{\\mathbf{Z}^{\\prime} \\mathbf{X}}{n}\\right]^{-1} \\frac{\\mathbf{X}^{\\prime} \\mathbf{Z}}{n}\\left(\\frac{\\mathbf{Z}^{\\prime} \\mathbf{Z}}{n}\\right)^{-1} \\frac{\\mathbf{Z}^{\\prime} \\varepsilon}{n} \\ \u0026=\\left[\\begin{array}{c} \\hat{Q}{x z} \\hat{Q}{z z}^{-1} \\hat{Q}{z x} \\end{array}\\right]^{-1} \\hat{Q}{x z} \\hat{Q}{z z}^{-1} \\frac{Z^{\\prime} \\varepsilon}{n} \\end{aligned} $$ where $$ \\begin{aligned} \\hat{Q}{z z} \u0026=\\frac{\\mathbf{Z}^{\\prime} \\mathbf{Z}}{n}=n^{-1} \\sum{t=1}^{n} Z_{t} Z_{t}^{\\prime} \\ \\hat{Q}_{x z} \u0026=\\frac{\\mathbf{X}^{\\prime} \\mathbf{Z}}{n}=n^{-1} \\sum_{t=1}^{n} X_{t} Z_{t}^{\\prime} \\ Q_{z x} \u0026=\\frac{\\mathbf{Z}^{\\prime} \\mathbf{X}}{n}=n^{-1} \\sum_{t=1}^{n} Z_{t} X_{t}^{\\prime}=\\hat{Q}_{x z}^{","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:3:2","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.4 Consistency of 2SLS Question: What are the statistical properties of $ \\hat{\\beta}_{2 s l s} $? 【Theorem 7.1】Consistency of 2SLS: Under Assumptions 7.1-7.4, as $ n \\rightarrow \\infty $ $$ \\hat{\\beta}_{2 s l s} \\stackrel{P}{\\rightarrow} \\beta^{\\circ} $$ Proof: By the WLLN for e.s.p (ergodic stationary process), we have $$ \\begin{array}{c} \\hat{Q}{z z} \\stackrel{p}{\\rightarrow} Q{z z}, \\quad l \\times l \\ \\hat{Q}{x z} \\stackrel{p}{\\rightarrow} Q{x z}, \\quad K \\times l \\ \\frac{Z^{\\prime} \\varepsilon}{n} \\stackrel{p}{\\rightarrow} E\\left(Z_{t} \\varepsilon_{t}\\right)=0, \\quad l \\times 1 \\end{array} $$ Also, $ Q_{x z} Q_{z z}^{-1} Q_{z x} $ is a $ K \\times K $ symmetric and nonsingular matrix because $ Q_{x z} $ is of full rank, $ Q_{z z} $ is nonsingular, and $ l \\geq K $. It follows from continuity that $$ \\left[\\hat{Q}_{x z} \\hat{Q}_{z z}^{-1} \\hat{Q}_{z x}\\right]^{-1} \\stackrel{p}{\\rightarrow}\\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\\right]^{-1} $$ Consequently, we have $$ \\hat{\\beta}_{2 s l s}-\\beta^{\\circ} \\stackrel{P}{\\rightarrow}\\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\\right]^{-1} Q_{x z} Q_{z z}^{-1} \\cdot 0=0 $$ ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:4:0","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.5 Asymptotic Normality of 2SLS ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:5:0","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.5.1 Asymptotic distribution of 2SLS We now derive the asymptotic distribution of $ \\hat{\\beta}{2 \\text {sls}} . $ Write $$ \\sqrt{n}\\left(\\hat{\\beta}{2 s l s}-\\beta^{o}\\right)=\\left[\\hat{Q}{x z} \\hat{Q}{z z}^{-1} \\hat{Q}{z x}\\right]^{-1} \\hat{Q}{x z} \\hat{Q}{z z}^{-1} \\frac{z^{\\prime} \\varepsilon}{\\sqrt{n}}=\\hat{A} \\cdot \\frac{Z^{\\prime} \\varepsilon}{\\sqrt{n}} $$ where the $ K \\times l $ matrix $$ \\hat{A}=\\left[\\hat{Q}{x z} \\hat{Q}{z z}^{-1} \\hat{Q}{z x}\\right]^{-1} \\hat{Q}{x z} \\hat{Q}{z z}^{-1} $$ By the CLT assumption (A.7.5), we have $$ \\frac{Z^{\\prime} \\varepsilon}{\\sqrt{n}}=n^{-\\frac{1}{2}} \\sum_{t=1}^{n} Z_{t} \\varepsilon_{t} \\stackrel{d}{\\rightarrow} N(0, V) $$ where $ V $ is a finite and nonsingular $ I \\times I $ matrix. Then by the Slutsky theorem, we have $$ \\qquad \\sqrt{n}\\left(\\hat{\\beta}{2 s / s}-\\beta^{o}\\right) \\stackrel{d}{\\rightarrow}\\left(Q{x z} Q_{z z}^{-1} Q_{z x}\\right)^{-1} Q_{x z} Q_{z z}^{-1} \\cdot N(0, V) \\sim N\\left(0, A V A^{\\prime}\\right) $$ where $ A=\\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\\right)^{-1} Q_{x z} Q_{z z}^{-1} $. The asymptotic variance of $ \\sqrt{n}\\left(\\hat{\\beta}{2 s l s}-\\beta^{\\circ}\\right) $ $$ \\begin{aligned} \u0026 \\operatorname{avar}\\left(\\sqrt{n} \\hat{\\beta}{2 s l s}\\right) \\equiv \\Omega=A V A^{\\prime} \\ =\u0026\\left{\\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\\right]^{-1} Q_{x z} Q_{z z}^{-1}\\right} V\\left{\\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\\right]^{-1} Q_{x z} Q_{z z}^{-1}\\right}^{\\prime} \\ =\u0026\\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\\right]^{-1} Q_{x z} Q_{z z}^{-1} V Q_{z z}^{-1} Q_{z x}\\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\\right]^{-1} \\end{aligned} $$ 【Theorem 7.2】**Asymptotic Normality of 2SLS**: Under Assumptions 7.1-7.5, as $ n \\rightarrow \\infty $, $$ \\qquad \\sqrt{n}\\left(\\hat{\\beta}_{2 s l s}-\\beta^{\\circ}\\right) \\stackrel{d}{\\rightarrow} N(0, \\Omega) $$ The estimation of $ V $ depends on whether $ \\left{Z_{t} \\varepsilon_{t}\\right} $ is an MDS. We first consider the case where $ \\left{Z_{t} \\varepsilon_{t}\\right} $ is an MDS process. In this case, $ V=E\\left(Z_{t} Z_{t}^{\\prime} \\varepsilon_{t}^{2}\\right) $ and so we need not estimate the long-run variance-covariance matrix. ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:5:1","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.5.2 Form of Asymptotic Variance Now we discuss the form of $ \\Omega $. Note that IID case is the same as stationary ergodic MDS case, here we only discuss the latter case Case 1: $ \\left{Z_{t} \\varepsilon_{t}\\right} $ is a Stationary Ergodic MDS 【Assumption 7.6】MDS: $ \\left{Z_{t} \\varepsilon_{t}\\right} $ is an MDS; $ \\operatorname{var}\\left(Z_{t} \\varepsilon_{t}\\right)=E\\left(Z_{t} Z_{t}^{\\prime} \\varepsilon_{t}^{2}\\right) $ is finite and nonsingular. 【Corollary 7.3】Under Assumptions 7.1-7.4 and 7.6, we have as $ n \\rightarrow \\infty $ $$ \\sqrt{n}\\left(\\hat{\\beta}{2 s l s}-\\beta^{o}\\right) \\stackrel{d}{\\rightarrow} N(0, \\Omega) $$ where $ \\Omega $ is defined as above with $ V=E\\left(Z{t} Z_{t}^{\\prime} \\varepsilon_{t}^{2}\\right) $ Special Case: Conditional Homoskedasticity When $ \\left{Z_{t} \\varepsilon_{t}\\right} $ is an MDS with conditional homoskedasticity, the asymptotic variance $ \\Omega $ can be greatly simplified. 【Assumption 7.7】Conditional Homoskedasticity: $ E\\left(\\varepsilon_{t}^{2} | Z_{t}\\right)=\\sigma^{2} $. Under this assumption, by the law of iterated expectations, we obtain $$ V=E\\left(Z_{t} Z_{t}^{\\prime} \\varepsilon_{t}^{2}\\right)=E\\left[Z_{t} Z_{t}^{\\prime} E\\left(\\varepsilon_{t}^{2} | Z_{t}\\right)\\right]=\\sigma^{2} E\\left(Z_{t} Z_{t}^{\\prime}\\right)=\\sigma^{2} Q_{z z} $$ It follows that $$ \\begin{aligned} \\Omega \u0026=\\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\\right)^{-1} Q_{x z} Q_{z z}^{-1} \\sigma^{2} Q_{z z} Q_{z z}^{-1} Q_{z x}\\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\\right)^{-1} \\ \u0026=\\sigma^{2}\\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\\right)^{-1} \\end{aligned} $$ 【Corollary 7.4】**Asymptotic Normality of 2 SLS under MDS with Conditional Homoskedasticity**: Under Assumptions 7.1-7.4, 7.6 and 7.7, we have as $ n \\rightarrow \\infty $ $$ \\sqrt{n}\\left(\\hat{\\beta}_{2 s l s}-\\beta^{o}\\right) \\stackrel{d}{\\rightarrow} N(0, \\Omega) $$ where $$ \\Omega=\\sigma^{2}\\left[Q_{x z} Q_{z z}^{-1} Q_{z x}\\right]^{-1} $$ Case 2: $ \\left{Z_{t} \\varepsilon_{t}\\right} $ is a Stationary Ergodic non-MDS In this general case, we have $$ V \\equiv \\operatorname{avar}\\left(n^{-1 / 2} \\sum_{t=1}^{n} Z_{t} \\varepsilon_{t}\\right)=\\sum_{j=- \\infty}^{\\infty} \\Gamma(j) $$ where $ \\Gamma(j)=\\operatorname{cov}\\left(Z_{t} \\varepsilon_{t}, Z_{t-j} \\varepsilon_{t-j}\\right) $. We need to use a long-run variance-covariance matrix estimator for $ V $. When $ {Z_t \\varepsilon_t }$ is not a MDS, there is no need (and in fact there is no way) to consider conditional homoskedasticity and conditional heteroskedasticity separately. ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:5:2","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.6 Interpretation and Estimation of Asymptotic Variance of 2SLS Let us revisit the stage 2 regression model $$ Y_{t}=\\hat{X}_{t}^{\\prime} \\beta^{\\circ}+\\tilde{\\varepsilon}_{t} $$ where the regressor $$ \\hat{X}_{t}=\\hat{\\gamma}^{\\prime} Z_{t} $$ and the disturbance $ \\tilde{\\varepsilon}_{t}=Y_{t}-\\hat{X}_{t}^{\\prime} \\beta^{\\circ} $. Note that $ \\tilde{\\varepsilon}_{t} \\neq \\varepsilon_{t} $ because $ \\hat{X}_{t} \\neq X_{t} $. From A. 7.2, we have $$ \\tilde{\\varepsilon}{t}=Y{t}-\\hat{X}{t}^{\\prime} \\beta^{\\circ}=\\varepsilon{t}+\\left(X_{t}-\\hat{X}_{t}\\right)^{\\prime} \\beta^{\\circ}=\\varepsilon_{t}+\\hat{v}_{t}^{\\prime} \\beta^{\\circ} $$ where $ \\hat{v}_{t} \\equiv X_{t}-\\hat{X}_{t}=X_{t}-\\hat{\\gamma}^{\\prime} Z_{t} $. Since $ \\hat{v}{t} $ is the estimated residual from the stage 1 OLS regression $ \\mathrm{X}=\\mathrm{Z} \\gamma+v $ we have the following FOC holds: $$ \\mathbf{Z}^{\\prime}(\\mathbf{X}-\\hat{\\mathbf{X}})=\\mathbf{Z}^{\\prime} \\hat{v}=0 $$ It follows that the 2SLS estimator $$ \\begin{array}{l} \\hat{\\beta}{2 s l s}\u0026=\\left(\\hat{\\mathbf{X}}^{\\prime} \\hat{\\mathbf{X}}\\right)^{-1} \\hat{\\mathbf{X}}^{\\prime} Y \\ \u0026=\\left(\\hat{\\mathbf{X}}^{\\prime} \\hat{\\mathbf{X}}\\right)^{-1} \\hat{\\mathbf{X}}^{\\prime}\\left(\\hat{\\mathbf{X}} \\beta^{\\circ}+\\tilde{\\varepsilon}\\right) \\ \u0026=\\beta^{\\circ}+\\left(\\hat{\\mathbf{X}}^{\\prime} \\hat{\\mathbf{X}}\\right)^{-1} \\hat{\\mathbf{X}}^{\\prime}\\left[\\varepsilon+\\hat{v} \\beta^{\\circ}\\right] \\ \u0026=\\beta^{\\circ}+\\left(\\hat{\\mathbf{X}}^{\\prime} \\hat{\\mathbf{X}}\\right)^{-1} \\hat{\\mathbf{X}}^{\\prime} \\varepsilon \\end{array} $$ because $ \\hat{\\mathrm{X}}^{\\prime} \\hat{v}=0 $. Therefore, the asymptotic properties of $ \\hat{\\beta}{2SLS} $ are determined by $$ \\hat{\\beta}{2 s l s}-\\beta^{\\circ}=\\left(\\hat{\\mathbf{X}}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\varepsilon=\\left(\\frac{\\hat{\\mathbf{X}}^{\\prime} \\mathbf{X}}{n}\\right)^{-1} \\frac{\\hat{\\mathbf{X}}^{\\prime} \\varepsilon}{n} $$ The estimated residual $ \\hat{v}=\\mathbf{X}-\\hat{\\mathbf{X}} $ from the first stage has no impact on the statistical properties of $ \\hat{\\beta}_{2 s l s} $. We can proceed as if we were estimating $ Y=\\hat{\\mathbf{X}} \\beta^{\\circ}+\\varepsilon $ by OLS. Note that we have by the WLLN $$ \\hat{\\gamma}=\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X} \\stackrel{P}{\\rightarrow} Q_{z z}^{-1} Q_{z x}=\\gamma $$ We will consider the following artificial regression model $$ Y_{t}=\\tilde{X}_{t}^{\\prime} \\beta^{\\circ}+\\varepsilon_{t} $$ where $$ \\tilde{\\beta}=\\left(\\tilde{X}^{\\prime} \\tilde{X}\\right)^{-1} \\tilde{X}^{\\prime} Y $$ The asymptotic properties of $ \\hat{\\beta}_{2 s l s} $ are the same as those of the infeasible OLS estimator $ \\tilde{\\beta} $. This helps a lot in understanding the variance-covariance structure of $ \\hat{\\beta}_{2 s l s} $. It is just a convenient way to understand the nature of $ \\hat{\\beta}_{2 s l s} $. We now show that the asymptotic properties of $ \\hat{\\beta}_{2 s l s} $ are the same as the asymptotic properties of $ \\tilde{\\beta} $. For the asymptotic normality, observe that $$ \\sqrt{n}\\left(\\tilde{\\beta}-\\beta^{o}\\right)=\\hat{Q}{\\tilde{x} \\tilde{x}}^{-1} \\frac{\\tilde{X}^{\\prime} \\varepsilon}{\\sqrt{n}} \\stackrel{d}{\\rightarrow} Q{\\tilde{x} \\tilde{x}}^{-1} \\cdot N(0, \\tilde{V}) \\sim N\\left(0, Q_{\\tilde{x} \\tilde{x}}^{-1} \\tilde{V} Q_{\\tilde{x} \\tilde{x}}^{-1}\\right) $$ where $$ Q_{\\tilde{x} \\tilde{x}} \\equiv E\\left(\\tilde{X}_{t} \\tilde{X}_{t}^{\\prime}\\right), \\text { and } \\tilde{V} \\equiv \\operatorname{avar}\\left(n^{-1 / 2} \\sum_{t=1}^{n} \\tilde{X}_{t} \\varepsilon_{t}\\right) $$ **Case I**: MDS with Conditional Homoskedasticity Suppose $ \\left{\\tilde{X}{t} \\varepsilon{t}\\right} $ is MDS, and $ E\\left(\\varepsilon_{t}^{2} | \\tilde{X}_{t}\\right)=\\sigma^{2} $. Then we have $$ \\tilde{V}=E\\left(\\tilde{X}_{t} \\tilde{X}_{t}^{\\prime} \\varepsilon_{t}^{2}\\right)=\\sigma^{2} Q_{\\tilde{x}\\tilde{x}} $$ by the law of iterated expectations (LIE). It follows that $$ \\sqrt{n}\\left(\\tilde{\\beta}-\\beta^{o}\\r","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:6:0","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.7 Hypothesis Testing Case I: $ \\left{Z_{t} \\varepsilon_{t}\\right} $ is an MDS with Conditional Homoskedasticity We have $$ \\sqrt{n}\\left(\\hat{\\beta}{2 s l s}-\\beta^{o}\\right) \\stackrel{d}{\\rightarrow} N(0, \\sigma^{2}\\left[Q{x z} Q_{z z}^{-1} Q_{z x}\\right]^{-1}) $$ 【Theorem 7.8】**Hypothesis Testing**: Put $ \\hat{e} \\equiv Y-\\mathbf{X} \\hat{\\beta}_{2 s l s} . $ Then under Assumptions 7.1-7.4, 7.6 and 7.7, the Wald test statistic $$ \\hat{W}=\\frac{n\\left(R \\hat{\\beta}_{2 s l s}-r\\right)^{\\prime}\\left[R\\left(\\hat{\\mathbf{X}}^{\\prime} \\hat{\\mathbf{X}}\\right)^{-1} R^{\\prime}\\right]^{-1}\\left(R \\hat{\\beta}_{2 s l s}-r\\right)}{\\hat{e}^{\\prime} \\hat{e} /(n-K)} \\stackrel{d}{\\rightarrow} \\chi_{J}^{2} $$ as $ n \\rightarrow \\infty, $ under $ \\mathrm{H}_{0} $. Question: Is $ \\hat{W} / J $ the $ F $ -statistic from the second stage regression? No, because $ \\hat{e} $ is not the estimated residual from the second stage. Case II: $ \\left{Z_{t} \\varepsilon_{t}\\right} $ is a Stationary Ergodic MDS with Conditional Heteroskedasticity 【Theorem 7.9】Hypothesis Testing: Under Assumptions 7.1-7.4, 7.6 and 7.8, the Wald test statistic $$ \\hat{W} \\equiv n\\left(R \\hat{\\beta}{2 s l s}-r\\right)^{\\prime}\\left[R \\hat{Q}{\\hat{x} \\hat{x}}^{-1} \\hat{V}{\\hat{x} \\hat{x}} \\hat{Q}{\\hat{x} \\hat{x}}^{-1} R^{\\prime}\\right]^{-1}\\left(R \\hat{\\beta}{2 s l s}-r\\right) \\stackrel{d}{\\rightarrow} \\chi{J}^{2} $$ under $ \\mathrm{H}{0}, $ where $ \\hat{V}{\\hat{x} \\hat{x}}=n^{-1} \\Sigma_{t=1}^{n} \\hat{X}_{t} \\hat{X}_{t}^{\\prime} \\hat{e}_{t}^{2} $ and $ \\hat{e}_{t}=Y_{t}-X_{t}^{\\prime} \\hat{\\beta}_{2 s l s} $ Case III: $ \\left{Z_{t} \\varepsilon_{t}\\right} $ is a Stationary ergodic non-MDS 【Theorem 7.10】Hypothesis Testing: Under Assumptions 7.1-7.5 and 7.9, the Wald test statistic $$ \\hat{W}=n\\left(R \\hat{\\beta}{2 s l s}-r\\right)^{\\prime}\\left[R \\hat{Q}{\\hat{x} \\hat{x}}^{-1} \\hat{V}{\\hat{x} \\hat{x}} \\hat{Q}{\\hat{x} \\hat{x}}^{-1} R^{\\prime}\\right]^{-1}\\left(R \\hat{\\beta}{2 s l s}-r\\right) \\stackrel{d}{\\rightarrow} \\chi{J}^{2} $$ under $ \\mathrm{H}{0}, $ where $ \\hat{V}{\\hat{x} \\hat{x}}=\\hat{\\gamma}^{\\prime} \\hat{V} \\hat{\\gamma}, \\hat{\\gamma}=\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X} $ and $ \\hat{V} $ is a long-run variance-covariance estimator for $ V=\\Sigma_{j=-\\infty}^{\\infty} \\Gamma(j) $ with $ \\Gamma(j)= \\operatorname{cov}\\left(Z_{t} \\varepsilon_{t}, Z_{t-j} \\varepsilon_{t-j}\\right) $. ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:7:0","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.8 Hausman’s Test When there exists endogeneity so that $ E\\left(X_{t} \\varepsilon_{t}\\right) \\neq 0, $ the OLS estimator $ \\hat{\\beta} $ is inconsistent for $ \\beta^{\\circ} . $ Instead, $ \\hat{\\beta}_{2 s l s} $ should be used. In practice, one is not sure whether there exists endogeneity. The null hypothesis of interest is: $$ \\mathrm{H}{0}: E\\left(\\varepsilon{t} | X_{t}\\right)=0 $$ If this null hypothesis is rejected, one has to use the 2SLS estimator $ \\hat{\\beta}_{2 s l s} $ provided that one can find $ Z_{t} $ that satisfies A . 7.4. For simplicity, we impose the following conditions. 【Assumption 7.10】a) $ \\left{\\left(X_{t}^{\\prime}, Z_{t}^{\\prime}\\right)^{\\prime} \\varepsilon_{t}\\right} $ is an MDS process; and b) $ E\\left(\\varepsilon_{t}^{2} | X_{t}, Z_{t}\\right)=\\sigma^{2} $. The basic idea of Hausman’s test is under $ \\mathrm{H}{0}: E\\left(\\varepsilon{t} | X_{t}\\right)=0, $ both $ \\hat{\\beta}=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} Y $ and $ \\hat{\\beta}_{2 s l s} $ are consistent for $ \\beta^{\\circ} $. They converge to the same limit $ \\beta^{\\circ} $ but it can be shown that $ \\hat{\\beta} $ is an asymptotically efficient estimator while $ \\hat{\\beta}_{2 s l s} $ is not. Under the alternatives to $ H_{0}, \\hat{\\beta}_{2 s l s} $ remains to be consistent for $ \\beta^{\\circ} $ but $ \\hat{\\beta} $ is not. \r Hausman (1978) considers a test for $ H_{0} $ based on the difference between the two estimators: $$ \\hat{\\beta}_{2 s l s}-\\hat{\\beta} $$ which converges to zero under $ H_{0} $ but generally to a nonzero constant under $ H_{1} $. To construct Hausman’s (1978) test statistic, we need to derive the asymptotic distribution of $ \\hat{\\beta}_{2 s l s}-\\hat{\\beta} $. For this purpose, we first state a lemma. 【Lemma 7.11】Suppose $ \\hat{A} \\stackrel{p}{\\rightarrow} A $ and $ \\hat{B}=O_{P}(1) $. Then $ (\\hat{A}-A) \\hat{B} \\stackrel{p}{\\rightarrow} 0 $. We first consider the OLS $ \\hat{\\beta} $. Note that $$ \\sqrt{n}\\left(\\hat{\\beta}-\\beta^{\\circ}\\right)=\\hat{Q}{x x}^{-1} n^{-1 / 2} \\sum{t=1}^{n} X_{t} \\varepsilon_{t} $$ where $ \\hat{Q}_{x x}^{-1} \\stackrel{P}{\\rightarrow} Q_{x x}^{-1} $ and $$ n^{-1 / 2} \\sum_{t=1}^{n} X_{t} \\varepsilon_{t} \\stackrel{d}{\\rightarrow} N\\left(0, \\sigma^{2} Q_{x x}\\right) $$ as $ n \\rightarrow \\infty $. It follows that $ n^{-1 / 2} \\sum_{t=1}^{n} X_{t} \\varepsilon_{t}=O_{P}(1) $. And by Lemma 7.11, we have $$ \\sqrt{n}\\left(\\hat{\\beta}-\\beta^{o}\\right)=Q_{x x}^{-1} n^{-1 / 2} \\sum_{t=1}^{n} X_{t} \\varepsilon_{t}+o_{P}(1) $$ Similarly, we can obtain $$ \\sqrt{n}\\left(\\hat{\\beta}_{2 s l s}-\\beta^{o}\\right)=\\hat{A} n^{-1 / 2} \\sum_{t=1}^{n} Z_{t} \\varepsilon_{t}=A n^{-1 / 2} \\sum_{t=1}^{n} Z_{t} \\varepsilon_{t}+o_{P}(1) $$ where $$ \\hat{A}=\\left(\\hat{Q}_{x z} \\hat{Q}_{z z}^{-1} \\hat{Q}_{z x}\\right)^{-1} \\hat{Q}_{x z} \\hat{Q}_{z z} \\stackrel{p}{\\rightarrow} A=\\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\\right)^{-1} Q_{x z} Q_{z z}^{-1} $$ and $$ n^{-1 / 2} \\sum_{t=1}^{n} Z_{t} \\varepsilon_{t} \\stackrel{d}{\\rightarrow} N\\left(0, \\sigma^{2} Q_{z z}\\right) $$ It follows that $$ \\begin{aligned} \\sqrt{n}\\left(\\hat{\\beta}{2 s l s}-\\hat{\\beta}\\right)=\u0026 n^{-1 / 2} \\sum{t=1}^{n}\\left[\\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\\right)^{-1} Q_{x z} Q_{z z}^{-1} Z_{t}-Q_{x x}^{-1} X_{t}\\right] \\varepsilon_{t}+o_{P}(1) \\ \u0026 \\stackrel{d}{\\rightarrow} N\\left(0, \\sigma^{2}\\left(Q_{x z} Q_{z z}^{-1} Q_{z x}\\right)^{-1}-\\sigma^{2} Q_{x x}^{-1}\\right) \\end{aligned} $$ by the CLT for the stationary ergodic MDS process and Assumption 7.10 Therefore, under the null hypothesis $ \\mathrm{H}_{0}, $ the quadratic form $$ H=\\frac{n\\left(\\hat{\\beta}_{2 s l s}-\\hat{\\beta}\\right)^{\\prime}\\left[\\left(\\hat{Q}_{x z} \\hat{Q}_{z z}^{-1} \\hat{Q}_{z x}\\right)^{-1}-\\hat{Q}_{x x}^{-1}\\right]^{-1}\\left(\\hat{\\beta}_{2 s l s}-\\hat{\\beta}\\right)}{s^{2}} \\stackrel{d}{\\rightarrow} \\chi_{k}^{2} $$ as $ n \\rightarrow \\infty $ by the Slutsky theorem, where $ s^{2}=e^{\\prime} e / n $ is the residual variance estimator based on the OLS residual $ e=Y-X \\hat{\\beta} $. ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:8:0","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"More feasible way to do Hausman’s Test What if rank $ J\u003cK ? $ To see modified test, consider $$ \\begin{aligned} \\hat{\\beta}{2 s l s} \u0026=\\left(\\hat{\\mathbf{X}}^{\\prime} \\mathbf{X}\\right)^{-1} \\hat{\\mathbf{X}}^{\\prime} Y=\\left[(\\mathbf{Z} \\hat{\\gamma})^{\\prime}(\\mathbf{Z} \\hat{\\gamma})\\right]^{-1}(\\mathbf{Z} \\hat{\\gamma})^{\\prime} Y \\ \u0026=\\left{\\left[\\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X}\\right]^{\\prime}\\left[\\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X}\\right]\\right}^{-1}\\left[\\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X}\\right]^{\\prime} Y \\ \u0026=\\left[\\mathbf{X}^{\\prime} \\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X}\\right]^{-1} \\mathbf{X}^{\\prime} \\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} Y \\ \u0026=\\left[\\mathbf{X}^{\\prime} \\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\mathbf{X}\\right]^{-1} \\mathbf{X}^{\\prime} \\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} Y \\ \u0026=\\left[\\mathbf{X}^{\\prime} \\mathbf{P}{z} \\mathbf{X}\\right]^{-1} \\mathbf{X}^{\\prime} \\mathbf{P}{z} Y \\quad \\text { where } P{z} \\equiv \\mathbf{Z}\\left(\\mathbf{Z}^{\\prime} \\mathbf{Z}\\right)^{-1} \\mathbf{Z}^{\\prime} \\end{aligned} $$ Then the Hausman’s Test statistic becomes $$ H=\\frac{n\\left(\\hat{\\beta}{2 s l s}-\\hat{\\beta}\\right)^{\\prime}\\left[\\left(\\mathbf{X}^{\\prime} \\mathbf{P}{z} \\mathbf{X}\\right)^{-1}-\\mathbf{X}^{\\prime} \\mathbf{X}\\right]^{-1}\\left(\\hat{\\beta}{2 s l s}-\\hat{\\beta}\\right)}{s^{2}} \\stackrel{d}{\\rightarrow} \\chi{K}^{2} $$ Consider $$ \\begin{aligned} \\hat{\\beta}{2 s l s}-\\beta^{o} \u0026=\\left(\\mathbf{X}^{\\prime} \\mathbf{P}{z} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{P}{z} \\mathbf{Y}-\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{Y} \\ \u0026=\\left(\\mathbf{X}^{\\prime} \\mathbf{P}{z} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{P}{z} \\mathbf{Y}-\\left(\\mathbf{X}^{\\prime} \\mathbf{P}{z} \\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}^{\\prime} \\mathbf{P}{z} \\mathbf{X}\\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{Y} \\ \u0026=\\left(\\mathbf{X}^{\\prime} \\mathbf{P}{z} \\mathbf{X}\\right)^{-1}\\left{\\mathbf{X}^{\\prime} \\mathbf{P}{\\mathbf{z}}-\\left(\\mathbf{X}^{\\prime} \\mathbf{P}{\\mathbf{z}} \\mathbf{X}\\right)\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime}\\right} \\mathbf{Y} \\ \u0026=\\left(\\mathbf{X}^{\\prime} \\mathbf{P}{z} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{P}{\\mathbf{z}}\\left{\\mathbf{I}-\\mathbf{X}\\left(\\mathbf{X}^{\\prime} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime}\\right} \\mathbf{Y} \\ \u0026=\\left(\\mathbf{X}^{\\prime} \\mathbf{P}{z} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\prime} \\mathbf{P}{\\mathbf{z}} \\mathbf{M}{X} \\mathbf{Y} \\end{aligned} $$ since the term $ \\left(X^{\\prime} P{z} X\\right)^{-1} $ never goes to 0 , we can just test if $$ A \\equiv \\mathrm{X}^{\\prime} \\mathrm{P}{\\mathrm{z}} \\mathrm{M}{X} \\mathrm{Y} \\text { goes to } 0 $$ Let $ \\mathbf{X}^{\\prime} \\equiv\\left(X_{1}^{\\prime}, W^{\\prime}\\right) $ and $ \\mathbf{Z}^{\\prime} \\equiv\\left(Z_{1}^{\\prime}, W^{\\prime}\\right), $ where $ X_{1} $ are endogenous, $ Z_{1} $ are IV and $ W $ are exogenous. Then $$ \\mathbf{X}^{\\prime} \\mathbf{P}_{\\mathbf{z}} \\mathbf{M}_{X} \\mathbf{Y}=\\left[\\begin{array}{ll} \\mathbf{X}_{1}^{\\prime} \\mathbf{P}_{\\mathbf{z}} \\mathbf{M}_{X} \\mathbf{Y} \u0026 W^{\\prime} \\mathbf{P}_{\\mathbf{z}} \\mathbf{M}_{X} \\mathbf{Y} \\end{array}\\right] $$ where $ W^{\\prime} \\mathrm{P}_{z} \\mathrm{M}_{X} \\mathrm{Y} $ goes to zero. Thus, we need to check $ \\mathrm{X}{1}^{\\prime} \\mathrm{P}{\\mathrm{z}} \\mathrm{M}_{X} \\mathrm{Y}=0 . $ How? Consider $$ Y=X \\beta+P_{z} X_{1} \\delta+\\varepsilon $$ Apply OLS to this to have $$ \\hat{\\delta}=\\left(\\mathbf{X}_{1}^{\\prime} \\mathbf{P}_{\\mathbf{Z}} \\mathbf{","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:8:1","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.9 Examples ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:9:0","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.9.1 Hockey Tickets Suppose that we are interested in the impact of a hockey teams regular season wins on ticket sales in millions. For a sample of 28 teams in 1995, we estimates the following linear model: $$ Y_{i}=a+b X_{i}+u_{i} $$ where $ Y_{i} $ is the log of ticket sales in millions, and $ X_{i} $ is the number of regular season wins in 1995 Q1) Do you think we can get an unbiased estimate of b? omitted variable bias? or simultaneity? or measurement error in explanatory variables? If there is a ignored variable that might be correlated with both wins and ticket sales (You could make a hypothesis and then discuss the motivation), then OLS is unbiased due to the omitted variable bias. It’s also possible that not only might wins impact ticket sales, but ticket sales could impact wins. For example, if the owner relies on ticket sales to finance the acquisition of star players, and the presence of star players contributes to wins, higher ticket sales can lead to increased wins. $ \\Longrightarrow $ Simultaneity! Q2) One might think naively whether one can construct a simple test for whether $ X $ and $ u $ are correlated by doing the following: using OLS estimates of $ a $ and $ b $, estimate the error term: $$ \\widehat{u}{i}=Y{i}-\\widehat{a}-\\widehat{b} X_{i} $$ and then run $ \\widehat{u}_{i}=c+d X_{i}+v_{i} $ to see $ d=0 . $ Is this a good test procedure? No! If $ X $ and $ u $ are correlated, then $ O L S $ is inconsistent and $ \\widehat{u}_{i} $ is so for $ u $. And $ X \\widehat{u}=0 $ no matter the assumption is actually true in reality. ","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:9:1","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"6.9.2 Demand for Cigarettes One might think that high tax on cigarette consumption would discourage smoking so that improves people’s health condition (and save government health insurance spending). To investigate this hypothesis, consider the following model to estimate the elasticity of demand for cigarettes using annual data for the 48 contiguous U.S. states for 1985 through 1995 $$ Q_{i}=\\beta_{0}+\\beta_{1} P_{i}+\\epsilon_{i} $$ where $ P_{i} $ is the price, $ Q_{i} $ is the quantity of cigarettes, and $ \\epsilon_{i} $ is the error term. One may concern endogeneity due to simultaneity that is, as $ Q_{i} $ is causing $ P_{i} $ in the model (1), $ P_{i} $ is driving $ Q_{i} $ due to price-quantity equilibrium. $$ \\begin{aligned} P_{i} \u0026=\\gamma_{0}+\\gamma_{1} Q_{i}+u_{i} \\ P_{i} \u0026=\\gamma_{0}+\\gamma_{1}\\left(\\beta_{0}+\\beta_{1} P_{i}+\\epsilon_{i}\\right)+u_{i} \\ \u0026=\\frac{\\gamma_{0}+\\gamma_{1} \\beta_{0}}{1-\\gamma_{1} \\beta_{1}}+\\frac{\\gamma_{1} \\epsilon_{i}}{1-\\gamma_{1} \\beta_{1}}+\\frac{u_{i}}{1-\\gamma_{1} \\beta_{1}} \\end{aligned} $$ Clearly $ P_ i $ must be correlated with $ \\epsilon_{i} $ unless $ \\gamma_{1}=0 $ We found one instrumental variable, Sales Tax; which is the portion of the tax on cigarettes arising from the general sales tax, measured in dollars per pack (in real dollars, deflated by the Consumer Price Index). We apply 2 SLS and obtain the following estimated equations $$ \\begin{array}{l} \\hat{P}{i}\u0026= \u00264.62+\u00260.031 \\text { Sales Tax}i \\ \u0026 \u0026(0.03) \u0026(0.005)\\ \\hat{Q}{i}\u0026= \u00269.43- \u00261.14 \\hat{P}{i}\\ \u0026 \u0026(1.26) \u0026(0.37)\\ \\end{array} $$ Q1) (i) what are $ \\widehat{P}{i} $ and $ \\widehat{Q}{i} $ ? (i) in the first stage of 2 SLS, we regression $ P_{i} $ on the sole instrument and obtain the prediction $ \\widehat{P}_{i} $. Then, we use $ \\hat{P}_{i} $ as the independent variable in the 2nd stage of 2SLS. We regress $ Q_{i} $ on $ \\widehat{P}_{i} . \\widehat{Q}_{i} $ is the prediction of $ Q_{i} $ from the stage 2 regression. (ii) what is the IV estimate of b? The IV estimate of bl is the coefficient from the 2nd stage of 2SLS, which implies $ \\widehat{\\beta}_{1}=1.14 $. (iii) is $\\text{Sales Tax}_{i} $ a valid instrument? We need to check instrument relevance and instrument exogeneity. Given the result of the 1st stage, we note that Sales Taxi is quite significant as its associated t-stat is more than $ 6 $. This assures instrument relevance. On the other hand, we cannot statistically test instrument exogeneity in this case. For the sales tax to be exogenous, it must be uncorrelated with the error in the demand equation; that is, the sales tax must affect the demand for cigarettes only indirectly through the price. This seems plausible: General sales tax rates vary from state to state, because different states choose different mixes of sales, income, property, and other taxes to finance public undertakings. Those choices about public finance are driven by political considerations, not by factors related to the demand for cigarettes. Q2) Suppose that states levy special taxes that apply only to cigarettes and other tobacco products. These cigarette-specific taxes ($ \\text { Cig } \\text {Tax}_{i} $) constitute a possible second instrumental variable. Describe how the 2SLS process would change. The 1st stage of 2SLS will change. We now run $$ P_{i}=\\gamma_{0}+\\gamma_{1} \\text {Sales } \\operatorname{Tax}_{i}+\\gamma_{2} \\text { Cig} \\operatorname{Tax}_{i}+u_{i} $$ Then obtain the prediction $ \\widehat{P}_{i} $ from the above equation and use it as the new independent variable in the 2nd stage of 2SLS. The rest are the same. Q3) Using the instruments, we rerun the IV estimation and obtain the following results $$ \\widehat{Q}{i}=\\underset{(0.96)}{9.89}-\\underset{(0.25)}{1.28 P{i}} $$ Comparing with the IV results using one IV, comment on the change of standard error of the estimated price elasticity. The SE clearly gets smaller once an additional instrument is added. Because this estimate uses more information than the former equati","date":"0001-01-01","objectID":"/6.-instrumental-variables-regression/:9:2","tags":null,"title":"","uri":"/6.-instrumental-variables-regression/"},{"categories":null,"content":"1. General Regression Analysis \r 统计：计量经济学——推断统计；经济统计——描述统计。 核心逻辑$\\hat{\\beta} \\overset{p}{\\rightarrow} \\beta^ * \\overset{模型正确设定}{=} \\beta ^o$ 当且仅当模型正确设定时，$\\beta^*$可以被解释为边际效应 Hausman检验可以用来检验设否正确设定 当假定$u$服从正太分布时，$E(xu)=0$与$E(u|X)=0$等价 $$ \\operatorname{MSE}{T}(\\theta)=\\operatorname{Var}{\\theta}(T)+\\left[B_{T}(\\theta)\\right]^{2} $$ ","date":"0001-01-01","objectID":"/chapter12/:0:0","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"1.1 回归本质在估计$E(Y|X) $ 在经典回归模型中，我们希望用解释变量(regressand)$X$的函数$g(X)$来预测被解释变量(regressor)$Y$。此时需要一个标准来测度$g(X)$与$Y$的接近程度，均方误(mean squared error, MSE)准则最常被使用，MSE是预测误差（预测值$g(X)$与目标$Y$之差）的平方的期望，表达式如下 $$ \\operatorname{MSE}(g)=E[Y-g(X)]^{2} = \\int\\int[y-g(x)]^2f_{XY}(x,y)\\mathrm{d} x\\mathrm{d} y $$ 其中，$f_{XY}(x,y)$是变量$X$和$Y$的联合概率分布。 显然，MSE越小，$g(X)$对$Y$的预测能力越强。因此现在的问题转换为，求解使MSE最小的函数$g(·)$，注意到MSE是函数$g(·)$的函数。 事实上，条件均值$E(Y|X)$就是使MSE最小的函数$g_0(X)$，可以用求微分和方差分解两种方法证明（证明见文末附录）。 需要注意的是，条件均值$E(Y|X)$是$X$而非$Y$的函数。 MSE是衡量$g(X)$对$Y$的预测能力的准则之一，但非唯一准则。例如，平均绝对误差(mean absolute error, MAE)， $$ \\operatorname{MAE}(g)=E|Y-g(X)| $$ 此时，使MAE最小的函数$g(X)$是条件中位数，分位数回归采用的正是该准则。 相比MAE，MSE具有连续可导的优良性质。 令回归等式$Y=E(Y | X)+\\varepsilon$，其中$\\varepsilon$被称为回归扰动项，则有 $$ \\begin{aligned} E(\\varepsilon | X) \u0026=E{[Y-E(Y | X)] | X} \\ \u0026=E(Y | X)-E\\left[g_{o}(X) | X\\right] \\ \u0026=E(Y | X)-g_{o}(X) \\ \u0026=0 \\end{aligned} $$ $E(\\varepsilon|X) = 0$意味着$\\varepsilon$不包含可用于预测$Y$的期望值的任何有关$X$的信息。换句话说，可用于预测$Y$的所有$X$的信息被包含在$E(Y|X)$中。 进一步， $$ E(\\varepsilon)= E[E(\\varepsilon | X)]=0 $$ $\\varepsilon$与$X$正交 $$ \\begin{aligned} E(X \\varepsilon) \u0026=E[X E(\\varepsilon | X)] \\ \u0026=E(X \\cdot 0) \\ \u0026=0 \\end{aligned} $$ 事实上 ，$E[\\varepsilon h(X)]=0$ (对于任一的可测函数$h(·)$)与$E(\\varepsilon |X)=0$等价。这说明即使用非线性的模型，也无法改进预测效果。 ","date":"0001-01-01","objectID":"/chapter12/:0:1","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"附录 引理：重复期望法则(Law of Iterated Expectations, LIE)，对给定可测函数$G(X,Y)$，假设期望$E[G(X,Y)]$存在，则 $$ E[G(X, Y)]=E{E[G(X, Y) | X]} $$ 证明：仅考虑$\\left(Y,X^{\\prime}\\right)^{\\prime}$是连续随机向量的情形，有 $$ \\begin{aligned} E[G(X, Y)] \u0026=\\iint_{-\\infty}^{\\infty} G(x, y) f_{X Y}(x, y) \\mathrm{d} x \\mathrm{d} y \\ \u0026=\\iint_{-\\infty}^{\\infty} G(x, y) f_{Y | X}(y | x) f_{X}(x) \\mathrm{d} x \\mathrm{d} y \\ \u0026=\\int\\left[\\int_{-\\infty}^{\\infty} G(x, y) f_{Y | X}(y | x) \\mathrm{d} y\\right] f_{X}(x) \\mathrm{d} x \\ \u0026=\\int E[G(X, Y) | X=x] f_{X}(x) \\mathrm{d} x \\ \u0026=E{E[G(X, Y) | X]} \\end{aligned} $$ If $J$ contains more information than $I$, similarly, we have $$ E(Y|I) = E[E(Y|J)|I] $$ 定理：条件均值$E(Y|X)$是下列问题的最优解 $$ \\begin{aligned} E(Y | X) \u0026=\\arg \\min _{g \\in \\mathbb{F}} M S E(g) \\ \u0026=\\arg \\min _{g \\in \\mathbb{F}} E[Y-g(X)]^{2} \\end{aligned} $$ 其中$\\mathbb{F}$是所有可测和平方可积函数的集合，即 $$ \\mathbb{F}=\\left{g: \\mathbb{R}^{k+1} \\rightarrow \\mathbb{R} | \\int g^{2}(x) f_{X}(x) \\mathrm{d} x\u003c\\infty\\right} $$ 法一：方差分解 令$g_{0}(X) = E(Y | X)$，则 $$ \\begin{aligned} \\operatorname{MSE}(g) \u0026=E\\left[Y-g_{0}(X)+g_{0}(X)-g(X)\\right]^{2} \\ \u0026=E\\left[Y-g_{0}(X)\\right]^{2}+E\\left[g_{0}(X)-g(X)\\right]^{2}+2 E\\left{\\left[Y-g_{0}(X)\\right]\\left[g_{0}(X)-g(X)\\right]\\right} \\end{aligned} $$ 根据重复期望法则 $$ \\begin{aligned} E\\left{\\left[Y-g_{0}(X)\\right]\\left[g_{0}(X)-g(X)\\right]\\right} \u0026=E\\left{E\\left(\\left[Y-g_{0}(X)\\right]\\left[g_{0}(X)-g(X)\\right]|X\\right)\\right} \\ \u0026=E\\left{\\left[g_{0}(X)-g(X)\\right]E\\left(\\left[Y-g_{0}(X)\\right]|X\\right)\\right} \\ \u0026=E\\left{\\left[g_{0}(X)-g(X)\\right][E(Y|X)-g_{0}(X)]\\right} \\ \u0026=E\\left{\\left[g_{0}(X)-g(X)\\right]·0\\right} \\ \u0026=0 \\end{aligned} $$ $$ \\implies MSE(g) =E\\left[Y-g_{0}(X)\\right]^{2}+E\\left[g_{0}(X)-g(X)\\right]^{2} $$ $$ \\implies \\arg \\min _{g \\in \\mathbb{F}} M S E(g) = g_0(X) = E(Y|X) $$ 法二：求微分法 $$ \\operatorname{MSE}(g)=E[Y-g(X)]^{2} = \\int\\int[y-g(x)]^2f_{XY}(x,y)\\mathrm{d} x\\mathrm{d} y $$ 根据一阶条件，MSE对$g(X)$的导数为0 $$ \\frac{\\delta MSE(g)}{\\delta g(x)}=-2\\int[y-g(x)] f_{XY}(x,y) \\mathrm{d} y=0 $$ $$ \\implies \\int g(x)f_{XY}(x,y) \\mathrm{d}y = \\int yf_{XY}(x,y) \\mathrm{d}y $$ $$ \\implies g(x)\\int f_{XY}(x,y) \\mathrm{d}y = \\int yf_{XY}(x,y) \\mathrm{d}y $$ $$ \\implies g(x) f_X(x) = \\int yf_{XY}(x,y) \\mathrm{d}y $$ $$ \\implies g(x) = \\int y\\frac{f_{XY}(x,y)}{f_X(x)} \\mathrm{d}y $$ $$ \\implies g(x) = \\int yf_{Y|X}(y|x) \\mathrm{d}y=E(Y|X) $$ ","date":"0001-01-01","objectID":"/chapter12/:0:2","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"1.2 最优线性最小二乘估计$\\beta^* $ $X =(1,X_1,\\ldots,X_k )^{\\prime}$ $\\beta =(\\beta _0,\\beta _1,\\ldots,\\beta_k )^{\\prime}$ $$ \\min _{g \\in \\mathbf{A}} E[Y-g(X)]^{2}=\\min _{\\beta \\in \\mathbf{R}^{k+1}} E\\left(Y-X^{\\prime} \\beta\\right)^{2} $$ The key feature of A is that g(X) = X 0β is linear in β; not in X \r \r \r 最优最小二乘估计量，当且仅当$E(xu)=0$(一阶条件) why：soc Hessian matrix is positive definite provided E(XX ) is nonsingula 非奇异推出正定 \r \r ","date":"0001-01-01","objectID":"/chapter12/:0:3","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"1.3 模型正确设定 模型具有经济解释意义的前提的模型正确设定$E(u|x) = 0$ 模型正确设定： $$ Y=X^{\\prime} \\beta+u, \\beta \\in \\mathbb{R}^{k+1} $$ 如果存在某个参数值$\\beta ^o \\in \\mathbb{R}^{k+1}$，有 $$ E(Y|X) = X^{\\prime} \\beta^o $$ 则是对$E(Y|X)$的正确设定 若对任意$\\beta \\in \\mathbb{R}^{k+1}$ $$ E(Y|X) \\neq X^{\\prime} \\beta $$ 则模型设定错误。 如果模型正确设定，系数可以解释为期望边际效应 $$ \\beta^{o}=\\frac{\\mathrm{d} E(Y | X)}{\\mathrm{d} X} $$ 定理：如果模型正确设定$\\beta^ * \\overset{模型正确设定}{=} \\beta ^o$ \r \r $$ E(\\epsilon|X)=0 推出 E(\\epsilon X) = 0 $$ 2. 经典线性回归模型 经典的含义是：经典回归假设 $Z_t= { Y_t,X^{\\prime}t }^n{t=1}$是容量为n的样本，其中$Y_t$是一个标量，$X_t = (1,X_{1t}, \\ldots,X_{kt})^{\\prime}$ $$ \\begin{aligned} \\boldsymbol{Y} \u0026=\\left(Y_{1}, \\cdots, Y_{n}\\right)^{\\prime}, n \\times 1 \\ \\boldsymbol{\\varepsilon} \u0026=\\left(\\varepsilon_{1}, \\cdots, \\varepsilon_{n}\\right)^{\\prime}, n \\times 1 \\ \\boldsymbol{X} \u0026=\\left(X_{1}, \\cdots, X_{n}\\right)^{\\prime}, n \\times K \\end{aligned} $$ $$ \\boldsymbol{X} = \\begin{pmatrix} 1 \u0026 X_{11} \u0026 \\ldots \u0026 X_{k1} \\ 1 \u0026 X_{12} \u0026 \\ldots \u0026 X_{k2} \\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\vdots \\ 1 \u0026 X_{1n} \u0026 \\ldots \u0026 X_{kn} \\end{pmatrix} $$ 截面数据：样本之间相互独立（空间计量打破这一假设，假定样本之间存在地理关联） 时间序列数据：本质区别是样本之间存在相关性 面板数据 ","date":"0001-01-01","objectID":"/chapter12/:0:4","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.1 假定 假设3.1 线性 $$ Y_t=X^{\\prime}_t \\beta^o + \\varepsilon _t, \\quad t=1,\\ldots,n $$ 或者等价为 $$ \\boldsymbol{Y} = \\boldsymbol{X}\\beta ^o+ \\varepsilon $$ 该假设并不保证模型正确设定，模型正确设定当且仅当$E(\\varepsilon_t|X_t)=0$ \r \r 假设3.2 严格外生性 $$ E\\left(\\varepsilon_{t} | \\boldsymbol{X}\\right)=E\\left(\\varepsilon_{t} | X_{1}, \\cdots, X_{t}, \\cdots, X_{n}\\right)=0 \\quad t=1, \\cdots, n $$ 由重复期望法则$E(\\varepsilon _t| X_t)=0$，$E(\\varepsilon_t) = 0 $（小信息集=0，如果大信息集条件期望为0） 如果严格外生性成立，则 $$ E(\\boldsymbol{Y}|\\boldsymbol{X})=E(\\boldsymbol{X}\\beta^o|\\boldsymbol{X})+ E(\\boldsymbol{\\varepsilon} | \\boldsymbol{X})=\\boldsymbol{X}\\beta^o $$ 意味着模型被正确设定。 进一步，对任意$t,s \\in {1,\\dots,n}$ $$ \\begin{aligned} E\\left(X_{s} \\varepsilon_{t}\\right) \u0026=E\\left[E\\left(X_{s} \\varepsilon_{t} | \\boldsymbol{X}\\right)\\right] \\ \u0026=E\\left[X_{s} E\\left(\\varepsilon_{t} | \\boldsymbol{X}\\right)\\right] \\ \u0026=E\\left(X_{s} \\cdot 0\\right) \\ \u0026=\\mathbf{0} \\end{aligned} $$ 结合$E(\\varepsilon_t)=0$，有$cov(X_s,\\varepsilon_t)=0$。这排除了AR模型。 该假设的提出只是为了获得有限样本分布理论，对于大样本理论不需要严格外生性假设。 假设3.3 非奇异 \r 非奇异也可表述为最小特征值\u003e0，保证了无多重共线性 假设(2)保无近似多重共线性 $X^{\\prime}X$是对称阵，可以称为样本$X$的信息矩阵，因为它测度了$X$中的信息含量 假设3.4 球形误差方差 条件同方差 $$ E(\\varepsilon^2_t |\\boldsymbol{X})=\\sigma^2,t=1,\\dots,n $$ 条件无自相关 $$ E(\\varepsilon_t \\varepsilon_s |\\boldsymbol{X})=0,\\quad t \\neq s,t,s \\in {1,\\dots,n} $$ 假设3.2与3.4可以结合起来表述为 $$ E(\\boldsymbol{\\varepsilon} | \\boldsymbol{X})=\\mathbf{0}, \\boldsymbol{E}\\left(\\boldsymbol{\\varepsilon \\varepsilon}^{\\prime} | \\boldsymbol{X}\\right)=\\sigma^{2} \\boldsymbol{I} $$ 其中$\\boldsymbol{I} \\equiv \\boldsymbol{I}_{n}$ 是一个$\\boldsymbol{n} \\times \\boldsymbol{n}$单位阵 假设3.5 条件正态分布 $$ \\boldsymbol{\\varepsilon} | \\boldsymbol{X} \\sim N\\left(\\mathbf{0}, \\sigma^{2} \\boldsymbol{I}\\right) $$ 等价于$\\varepsilon_t \\sim IID \\quad N(0, \\sigma^2)$ 假设3.5可以推导出假设3.2和3.4（$E(\\boldsymbol{\\varepsilon} | \\boldsymbol{X})=\\mathbf{0}, \\boldsymbol{E}\\left(\\boldsymbol{\\varepsilon \\varepsilon}^{\\prime} | \\boldsymbol{X}\\right)=\\sigma^{2} \\boldsymbol{I}$），进一步 $$ f(\\boldsymbol{\\varepsilon} | \\boldsymbol{X})=\\frac{1}{(\\sqrt{2 \\pi \\sigma^{2}})^{n}} \\exp \\left(-\\frac{\\boldsymbol{\\varepsilon}^{\\prime} \\boldsymbol{\\varepsilon}}{2 \\sigma^{2}}\\right)=f(\\boldsymbol{\\varepsilon}) $$ \r 可以推导出，$\\boldsymbol{\\varepsilon}$ 独立于 $\\boldsymbol{X}$ 并且正态性+$Cov(\\varepsilon_t, \\varepsilon_s)=0, \\forall t \\neq s$可推导出$\\varepsilon_t$独立于$\\varepsilon_s$ 因此假设3.5还能推出$\\varepsilon_t, t=1,2,\\ldots$是IID的正态分布 综上，这是一个非常严格的假定。 ","date":"0001-01-01","objectID":"/chapter12/:1:0","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.2 普通最小二乘估计OLS ","date":"0001-01-01","objectID":"/chapter12/:2:0","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.2.1 OLS估计量的存在性 【OLS 估计量，ordinary least squares】定义线性回归模型$Y_t = X't \\beta + u_t$ 的残差平方和(sum of squared residuals, SSR)为 $$ \\begin{equation}\\begin{aligned} \\operatorname{SSR}(\\beta) \u0026 \\equiv(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta) \\ \u0026=\\sum{t=1}^{n}\\left(Y_{t}-X_{t}^{\\prime} \\beta\\right)^{2} \\end{aligned}\\end{equation} $$ 则OLS估计量是以下最优化问题的解 $$ \\begin{equation}\\hat{\\boldsymbol{\\beta}}=\\arg \\min _{\\boldsymbol{\\beta} \\in \\Bbb{R}^{K}} SSR(\\boldsymbol{\\beta})\\end{equation} $$ 【定理 2.1】在假设3.1和3.3(1)下，OLS估计量$\\hat{\\beta} $存在，并且 $$ \\begin{equation}\\begin{aligned} \\hat{\\beta} \u0026=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y} \\ \u0026=\\left(\\frac{1}{n} \\sum_{t=1}^{n} X_{t} X_{t}^{\\prime}\\right)^{-1} \\frac{1}{n} \\sum_{t=1}^{n} X_{t} Y_{t} \\end{aligned}\\end{equation} $$ 证明： $$ \\begin{equation}\\begin{aligned} \\frac{\\operatorname{d} SSR(\\beta)}{\\mathrm{d} \\beta} \u0026=\\frac{\\mathrm{d}}{\\mathrm{d} \\beta} \\sum_{t=1}^{n}\\left(Y_{t}-X_{t}^{\\prime} \\beta\\right)^{2} \\ \u0026=\\sum_{t=1}^{n} \\frac{\\partial\\left(Y_{t}-X_{t}^{\\prime} \\beta\\right)^{2}}{\\partial \\beta} \\ \u0026=\\sum_{t=1}^{n} 2\\left(Y_{t}-X_{t}^{\\prime} \\beta\\right) \\frac{\\partial\\left(Y_{t}-X_{t}^{\\prime} \\beta\\right)}{\\partial \\beta} \\ \u0026=-2 \\sum_{t=1}^{n} X_{t}\\left(Y_{t}-X_{t}^{\\prime} \\beta\\right) \\ \u0026=-2 \\boldsymbol{X}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)=\\bf{0} \\end{aligned}\\end{equation} $$ 结合假设3.1(1)非奇异（$\\lambda_{min}(X^{\\prime} X)\u003e0$）有 $$ \\begin{equation}\\hat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\end{equation} $$ 检查二阶条件，$K \\times K$ Hessian矩阵 $$ \\begin{aligned} \\frac{\\partial^{2} S S R(\\beta)}{\\partial \\beta \\partial \\beta^{\\prime}} \u0026=-2 \\sum_{t=1}^{n} \\frac{\\partial}{\\partial \\beta^{\\prime}}\\left[\\left(Y_{t}-X_{t}^{\\prime} \\beta\\right) X_{t}\\right] \\ \u0026=2 \\bf{X}^{\\prime} \\bf{X} \\end{aligned} $$ 根据假设3.1(1)，Hessian矩阵正定，从而$\\hat{\\beta}$是全局最优解。 ","date":"0001-01-01","objectID":"/chapter12/:2:1","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.2.2 $\\hat{\\beta}$与$ \\beta^* $ 不加证明地给出 $$ \\begin{aligned} SSR(\\beta)\u0026\\stackrel{p}{\\rightarrow}\\operatorname{MSE}(\\beta)=E[Y_t-X'_t\\beta]^{2} \\ \\hat{\\beta}\u0026\\stackrel{p}{\\rightarrow}\\beta^= \\left[E(\\boldsymbol{XX'})\\right]^{-1}E(\\boldsymbol{XY'}) \\end{aligned} $$ 联系第二章，$SSR(\\beta)$与$\\hat{\\beta}$分别是$MSE(\\beta)$与$ \\beta^ $的样本类似物(sample analogs) Whenever you have something unknown, always replace it with sample analogs. ","date":"0001-01-01","objectID":"/chapter12/:2:2","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.2.3 残差及其性质 $Y_i = X't\\hat{\\beta}$称为观测值$Y_t $的拟合值(fitted value)或预测值(predicted value)。另外，$e_t=Y_t - \\hat{Y}t$是观测值$Y_t $的估计残差(estimated residual)或预测误差(prediction error)。其中 $$ \\begin{aligned} \\mathbf{e}{t} \u0026=Y{t}-\\hat{Y}{t} \\ \u0026=\\left(X{t}^{\\prime} \\beta^{o}+\\varepsilon_{t}\\right)-X_{t}^{\\prime} \\hat{\\beta} \\ \u0026=\\varepsilon_{t}-X_{t}^{\\prime}\\left(\\hat{\\beta}-\\beta^{o}\\right) \\end{aligned} $$ 其中真实扰动项$ \\varepsilon_t $是不可避免的，而第二项$X'_t(\\hat{\\beta}-\\beta^o)$是估计误差：当样本容量越大(从而$\\hat{\\beta}$越靠近$\\beta^o$)，这一项将变得越小，乃至忽略不计。 prediction可以是对过去的预测，forecast是对将来的预测 注意到最小化问题$\\min {\\boldsymbol{\\beta} \\in \\Bbb{R}^{K}} SSR(\\boldsymbol{\\beta})$的一阶条件中， $$ \\boldsymbol{X}^{\\prime} \\boldsymbol{e}=\\sum{t=1}^{n} X_{t} e_{t}=\\mathbf{0} $$ 这意味着残差向量$\\boldsymbol{e}$与解释变量矩阵$\\bf{X}$正交(无论模型是否正确设定)。进一步，若$\\bf{X}_t$包含截距项，则$\\boldsymbol{X}^{\\prime} \\boldsymbol{e}$意味着$\\sum^n_{t=1}e^t=0$。 ","date":"0001-01-01","objectID":"/chapter12/:2:3","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.3 拟合优度与模型选择准则 ","date":"0001-01-01","objectID":"/chapter12/:3:0","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.3.1 拟合优度$ \\mathcal{R}^2 $ 残差波动越小，拟合优度越高 通过$\\boldsymbol{Y}=\\hat{\\boldsymbol{Y}}+\\boldsymbol{e}$， $$ \\begin{aligned} \\boldsymbol{Y}^{\\prime} \\boldsymbol{Y}\u0026=(\\hat{\\boldsymbol{Y}}+\\boldsymbol{e})'(\\hat{\\boldsymbol{Y}}+\\boldsymbol{e}) \\ \u0026=\\hat{\\boldsymbol{Y}}^{\\prime} \\hat{\\boldsymbol{Y}}+\\boldsymbol{e}^{\\prime} \\boldsymbol{e}+2 \\hat{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{e} \\ \u0026= \\hat{\\boldsymbol{Y}}\\boldsymbol{\\hat{Y}}+\\boldsymbol{e}^{\\prime} \\boldsymbol{e} \\end{aligned} $$ 其中 $$ \\hat{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{e}=(\\boldsymbol{X}\\hat{\\beta})^{\\prime}\\boldsymbol{e}=\\hat{\\beta}^{\\prime}\\boldsymbol{X}^{\\prime}\\boldsymbol{e}=\\hat{\\beta}^\\prime\\boldsymbol{0}=0 $$ 因此 $$ \\frac{\\hat{\\boldsymbol{Y}}^{\\prime} \\hat{\\boldsymbol{Y}}}{\\hat{\\boldsymbol{Y}}^{\\prime} \\hat{\\boldsymbol{Y}}}=\\frac{\\hat{\\boldsymbol{Y}}^{\\prime} \\hat{\\boldsymbol{Y}}}{\\hat{\\boldsymbol{Y}}^{\\prime} \\hat{\\boldsymbol{Y}}}-\\frac{\\boldsymbol{e}^{\\prime} \\boldsymbol{e}}{\\hat{\\boldsymbol{Y}}^{\\prime} \\hat{\\boldsymbol{Y}}}=1-\\frac{\\boldsymbol{e}^{\\prime} \\boldsymbol{e}}{\\hat{\\boldsymbol{Y}}^{\\prime} \\hat{\\boldsymbol{Y}}} $$ 【非中心化$\\mathcal{R}^{2}$】 非中心化多元相关系数平方$ R^2 $(uncentered squared multi-correlation coefficient)定义为 $$ \\mathcal{R}{\\mathrm{uc}}^{2}=\\frac{\\hat{\\boldsymbol{Y}}^{\\prime} \\hat{\\boldsymbol{Y}}}{\\boldsymbol{Y}^{\\prime} \\boldsymbol{Y}}=1-\\frac{\\boldsymbol{e}^{\\prime} \\boldsymbol{e}}{\\boldsymbol{Y}^{\\prime} \\boldsymbol{Y}} $$ $\\mathcal{R}{\\mathrm{uc}}^{2}$的含义是因变量$ {Y_t}$ 的样本二次型变动可以被预测值${\\hat{Y}t}$的样本二次型变动所预测的比例。根据定义，总有$0 \\le \\mathcal{R}{\\mathrm{uc}}^{2} \\le1$。 下面定义一个相近的指标， 称为中心化多元相关系数平方 (centered squared multi-correlation coefficiet)，通常简称为$ \\mathcal{R}^2 $。 【中心化$ \\mathcal{R}^2 $或决定系数(Coefficient of Determination)】决定系数定义为 $$ \\mathcal{R}^2 \\equiv 1-\\frac{\\sum_{t-1}^{n} e_{t}^{2}}{\\sum_{t=1}^{n}\\left(Y_{t}-\\bar{Y}\\right)^{2}} $$ 其中 $\\bar{Y}=n^{-1} \\sum_{t=1}^{n} Y_{t}$是样本均值。 当$X_{t}$包含截距项即$X_{0t}=1$时，可进行如下正交分解 $$ \\begin{aligned} \\sum_{t=1}^{n}\\left(Y_{t}-\\bar{Y}\\right)^{2} \u0026=\\sum_{t=1}^{n}\\left(\\hat{Y}_{t}-\\bar{Y}+Y_{t}-\\hat{Y}_{t}\\right)^{2} \\ \u0026=\\sum_{t=1}^{n}\\left(\\hat{Y}_{t}-Y\\right)^{2}+\\sum_{t=1}^{n} e_{t}^{2}+2 \\sum_{t=1}^{n}\\left(\\hat{Y}_{t}-Y\\right) e_{t} \\ \u0026=\\sum_{t=1}^{n}\\left(\\hat{Y}_{t}-\\bar{Y}\\right)^{2}+\\sum_{t=1}^{n} e_{t}^{2} \\end{aligned} $$ 其中交差项 $$ \\begin{aligned} \\sum_{t=1}^{n}\\left(\\hat{Y}_{t}-\\bar{Y}\\right) e_{t} \u0026=\\sum_{t=1}^{n} \\hat{Y}_{t} e_{t}-\\bar{Y} \\sum_{t=1}^{n} e_{t} \\ \u0026=\\hat{\\beta}^{\\prime} \\sum_{t=1}^{n} X_{t} e_{t}-\\bar{Y} \\sum_{t=1}^{n} e_{t} \\ \u0026=\\hat{\\beta}^{\\prime}\\left(\\mathrm{X}^{\\prime} e\\right)-\\bar{Y} \\sum_{t=1}^{n} e_{t} \\ \u0026=\\hat{\\beta}^{\\prime} \\cdot \\boldsymbol{0}-\\bar{Y} \\cdot 0 \\ \u0026=0 \\end{aligned} $$ 这里使用了OLS估计的一阶条件，即$\\boldsymbol{X}^{\\prime} \\boldsymbol{e}=0$ 和 $\\sum_{t=1}^{n} e_{t}=0$。从而 $$ \\begin{aligned} \\mathcal{R}^{2} \u0026 \\equiv 1-\\frac{e^{\\prime} e}{\\sum_{t=1}^{n}\\left(Y_{t}-\\bar{Y}\\right)^{2}} \\ \u0026=\\frac{\\sum_{t=1}^{n}\\left(Y_{t}-\\bar{Y}\\right)^{2}-\\sum_{t=1}^{n} e_{t}^{2}}{\\sum_{t=1}^{n}\\left(Y_{t}-\\bar{Y}\\right)^{2}} \\ \u0026=\\frac{\\sum_{t=1}^{n}\\left(\\hat{Y}_{t}-\\bar{Y}\\right)^{2}}{\\sum_{t=1}^{n}\\left(Y_{t}-\\bar{Y}\\right)^{2}} \\end{aligned} $$ 并且有 $$ 0 \\le \\mathcal{R}^2 \\le 1 $$ 反之，若$ X_t $不包含截距项，则 $$ \\begin{aligned} \\sum_{t=1}^{n}\\left(Y_{t}-\\bar{Y}\\right)^{2} \u0026=\\sum_{t=1}^{n}\\left(\\hat{Y}_{t}-\\bar{Y}\\right)^{2}+\\sum_{t=1}^{n} e_{t}^{2}+2 \\sum_{t=1}^{n}\\left(\\hat{Y}_{t}-\\bar{Y}\\right) e_{t} \\ \u0026 \\neq \\sum_{t=1}^{n}\\left(\\hat{Y}_{t}-\\bar{Y}\\right)^{2}+\\sum_{t=1}^{n} e_{t}^{2} \\end{aligned} $$ 在这种情况下，$\\mathcal{R}^2 $可能为负值。因为交叉项$2 \\sum_{t=1}^{n}\\left(\\hat{Y}_{t}-\\bar{Y}\\right) e_{t}$可能为负值。 【定理 2.2】当$ X_t $包含截距项时，$\\mathcal{R}^2$是${\\hat{Y}_t}$与${Y_i}$的样本方差的比值。 证明： ${\\hat{Y}t}$的样本均值为 $$ \\begin{aligned} \\bar{\\hat{Y}}\u0026=\\frac{1}{n} \\sum{t=1}^{n} \\hat{Y}{t}\\ \u0026=\\frac{1}{n} \\sum{i=1}^{n}\\left(Y_{t}-e_{t}\\right)\\ \u0026=\\bar{Y}-\\frac{1}{n} \\sum_{i=1}^{n} e_{t}\\ \u0026=\\bar{Y} \\end{aligned} $$ 其中$\\sum_{t=1}^{n} e_{t}=0$要求$ X_t $包含截距项。从而 $$ \\begin{aligned} \\mathcal{R}^{2} \u0026=\\frac{\\sum_{t=1}^{n}\\left(\\hat{Y}_{t}-\\bar{Y}\\right)^{2}}{\\sum_","date":"0001-01-01","objectID":"/chapter12/:3:1","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.3.2 AIC\u0026BIC \r ","date":"0001-01-01","objectID":"/chapter12/:3:2","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.4 OLS估计量的统计性质 \r \r 幂等矩阵的特征值只能是1或者0 M是残差的操作符e=MY = M$\\epsilon$ (2)是$ \\epsilon $的线性组合 (4)正太分布随鸡变量$ \\epsilon $的二次型服从卡方分布(SSR) \r (2)当$\\tau = (1,0,\\ldots,0)'$，则$\\tau’var(\\hat{\\beta}|X)\\tau=var(\\hat{\\beta_0})$ $MSE(\\hat{\\beta}_0|X)=var(\\hat{\\beta}_0|X)+Bias^2(\\hat{\\beta})\\rightarrow 0$, as $n \\rightarrow \\infty$ (3)+联合正态分布退出独立，用于推导假设检验的有限分布 \r \r \r 定理3.5 (4)(5)对于构建假设检验(t检验)有重要意义 \r 定理3.5(4)证明的一步 \r \r \r 直观理解是方差最小 高斯定理：严格外生性，球形方差，OLS是最优线性无偏估计量(BLUE) ","date":"0001-01-01","objectID":"/chapter12/:4:0","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.5 OLS估计量的抽样分布 引入假定3.5 \r \r ","date":"0001-01-01","objectID":"/chapter12/:5:0","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.6 残差方差估计量的分布 \r \r ","date":"0001-01-01","objectID":"/chapter12/:6:0","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.7 假设检验 \r J\u003eK可能会产生奇异矩阵 基本思想 \r \r ","date":"0001-01-01","objectID":"/chapter12/:7:0","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.7.1 J=1 \r \r \r 检验标准 \r \r ","date":"0001-01-01","objectID":"/chapter12/:7:1","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.7.2 J\u003e1 \r \r \r \r 更加常用的方法：本质上是LM检验 \r \r \r Wald Test \r ","date":"0001-01-01","objectID":"/chapter12/:7:2","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.8 重要应用 ","date":"0001-01-01","objectID":"/chapter12/:8:0","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.8.1 检验所有解释变量的联合显著性 \r \r e.g. 检验有效市场：任一系数不为零，则市场不是有效市场 ","date":"0001-01-01","objectID":"/chapter12/:8:1","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.8.3 检验遗漏变量 \r \r e.g. 格兰杰因果检验：预测力非因果性 Chow Test \r ","date":"0001-01-01","objectID":"/chapter12/:8:2","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"2.9 广义最小二乘估计GLS ","date":"0001-01-01","objectID":"/chapter12/:9:0","tags":null,"title":"","uri":"/chapter12/"},{"categories":null,"content":"6. Modern Portfolio Theory ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:0:0","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.1 Framework We can elaborate on our previous portfolio problem $$ \\max {a} E u\\left[Y{0}\\left(1+r_{f}\\right)+a\\left(\\tilde{r}-r_{f}\\right)\\right] $$ by considering $N\u003e1$ risky assets with returns $\\left(\\tilde{r}_{1}, \\tilde{r}_{2}, \\ldots, \\tilde{r}_{N}\\right)$ $$ \\begin{align*} \u0026 \\max _{a_{1}, a_2, \\ldots, a_{N}} E u\\left[Y_{0}\\left(1+r_{f}\\right)+\\sum_{i=1}^{N} a_{i}\\left(\\tilde{r}_{i}-r_{f}\\right)\\right] \\ =\u0026 \\max _{m_{1}, m_{2}, \\ldots, w_{N}} E u\\left[Y_{0}\\left(1+r_{f}\\right)+\\sum_{i=1}^{N} w_{i} Y_{0}\\left(\\tilde{r}_{i}-r_{f}\\right)\\right] \\ =\u0026 \\max _{m_{1}, w_{2}, \\ldots, w_{N}} E u\\left[Y_{0}\\left(1+\\tilde{r}_{P}\\right)\\right]=E u\\left(\\left(Y_{1}\\right)\\right) \\end{align*} $$ where $w_{i}=a_{i} / Y_{0}$ is the share of initial wealth allocated to each asset. $\\tilde{r}_{P}$ is the portfolio rate of return. Modern Portfolio Theory examines the solution to this problem assuming that investors have mean-variance utility, that is, assuming that investors' preferences can be represented by a trade-off between the mean (expected value) and variance of the $N$ asset returns. MPT was developed by Harry Markowitz (US, b.1927, Nobel Prize 1990 ) in the early 1950 s, the classic paper being his article “Portfolio Selection,” Journal of Finance Vol.7 (March 1952 ): pp. $77-91$. Assume that utility is provided by bundles of consumption goods, $u\\left(c_{1}, c_{2}, \\dots, c_{n}\\right)$ where the indexing is cross dates and states States of nature are mutually exclusive For each date and state of nature $(\\theta)$ there is a traditional budget constraint: $$ p_{1 \\theta} c_{1 \\theta}+p_{2 \\theta} c_{2 \\theta}+\\ldots+p_{m \\theta} c_{m \\theta} \\leq Y_{\\theta} $$ where the indexing runs across goods for a given state $\\theta ;$ $(i=1,2, \\ldots, m)$ correspond to the $m$ goods available in state of nature $\\theta$; the $m$ quantities $c_{i \\theta} ;$ and the $m$ prices $p_{i \\theta} $ $Y_{\\theta}$ is the “end of period” wealth level available in that same state ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:1:0","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.1.1 Three steps MPT summarizes an individual’s decision problem as being undertaken sequentially, in three steps: The Consumption-Savings Decision: how to split period zero income/wealth $Y_{0}$ between current consumption now $C_{0}$ and saving $S_{0}$ for consumption in the future where $C_{0}+S_{0}=Y_{0}$ The Portfolio Problem: choose assets in which to invest one’s savings so as to obtain a desired pattern of end-of-period wealth across the various states of nature; this means allocating $\\left(Y_{0}-C_{0}\\right)$ between a risk-free and $N$ risky assets The Consumption Choice: Given the realized state of nature and the wealth level obtained, the choice of consumption bundles to maximize the utility function $$ Y_{\\theta}=\\left(Y_{0}-C_{0}\\right)\\left[\\left(1+r_{f}\\right)+\\Sigma_{i=1}^{N} w_{i}\\left(r_{i \\theta}-r_{f}\\right)\\right] $$ ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:1:1","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.1.2 backward induction Starting from step 3: Step 3 is a standard microeconomic problem and its solution can be summarized by a Bernoulli utility function $u\\left(Y_{\\theta}\\right)$ representing the (maximum) level of utility that results from optimizing in step 3 given that the wealth available in state $\\theta$ is $Y_{\\theta}:$ $$ \\begin{align*} u\\left(Y_{\\theta}\\right) \\equiv \\operatorname{def} \\max _{\\left(c_{1} \\theta, \\ldots, c_{m \\theta}\\right)} \u0026u\\left(c_{1 \\theta}, \\ldots, c_{m \\theta}\\right)\\ s.t.\\quad \u0026p_{1 \\theta} c_{1 \\theta}+p_{2 \\theta} c_{2 \\theta}+\\ldots+p_{m \\theta} c_{m \\theta} \\leq Y_{\\theta} \\end{align*} $$ Step 2: Maximizing $E u\\left(Y_{\\theta}\\right)$ across all states of nature becomes the objective of step $$ \\max _{\\left(w_{1}, w_{2}, \\ldots, w_{N}\\right)} E u(\\tilde{Y})=\\Sigma_{\\theta} \\pi_{\\theta} u\\left(Y_{\\theta}\\right) $$ The end-of-period wealth can be written as $$ \\begin{aligned} \\tilde{Y} \u0026=\\left(Y_{0}-C_{0}\\right)\\left(1+\\tilde{r}_{P}\\right) \\ \\tilde{r}_{P} \u0026=r_{f}+\\Sigma_{i=1}^{N} w_{i}\\left(\\tilde{r}_{i}-r_{f}\\right) \\end{aligned} $$ Clearly an appropriate redefinition of the utility function leads to $$ \\max E u(\\tilde{Y})=\\max E u\\left[\\left(Y_{0}-C_{0}\\right)\\left(1+\\tilde{r}_{P}\\right)\\right]=\\operatorname{def} \\max E \\hat{u}(\\tilde{r_{P}}) $$ The level of investable wealth, $\\left(Y_{0}-C_{0}\\right)$, becomes a parameter of the “U-hat” representation. Finally, given the characteristics (e.g., expected return, standard deviation) of the optimally chosen portfolio, the optimal consumption and savings levels can be selected in step 1. From now on we work with utility functions defined on $\\tilde{r}{P}$. **This utility index can be further constrained to be a function of the mean and variance of the probability distribution of $r{P}$** This simplification can be accepted either as a working approximation or it may result from two further (alternative) hypotheses made within the expected utility framework. ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:1:2","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.2 Justifying Mean-Variance Utility The mean-variance utility hypothesis seemed natural at the time the MPT first appeared, and it retains some intuitive appeal today. But viewed in the context of more recent developments in financial economics, particularly the development of vN-M expected utility theory, it now looks a bit peculiar. The main justi\u000ccation for using a mean-variance approximation is its tractability Probability distributions are cumbersome to manipulate and difficult to estimate empirically Summarizing them by their fi\u000crst two moments is appealing A first question for us, therefore, is: Under what conditions will investors have preferences over the means and variances of asset returns? ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:2:0","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.2.1 approximate case In the approximate case, using a simple Taylor series approximation, one can also see that the mean and variance of an agent’s wealth distribution are critical to the determination of his expected utility for any distribution. If we start, as we did previously, by assuming an investor has preferences over terminal wealth $\\tilde{Y}$, potentially random because of randomness in the asset returns, described by a vN-M expected utility function $E[u(\\tilde{Y})]$ we can write $$ \\tilde{Y}=E(\\tilde{Y})+[\\tilde{Y}-E(\\tilde{Y})] $$ and interpret the portfolio problem as a trade-off between the expected payoff $$ E(\\tilde{Y}) $$ and the size of the “bet” $$ [\\tilde{Y}-E(\\tilde{Y})] $$ With this interpretation in mind, consider a second-order Taylor approximation of the Bernoulli utility function $u$ once the outcome $[\\tilde{Y}-E(\\tilde{Y})]$ of the bet is known: $$ u(\\tilde{Y}) \\approx u[E(\\tilde{Y})]+u^{\\prime}[E(\\tilde{Y})][\\tilde{Y}-E(\\tilde{Y})]+\\frac{1}{2} u^{\\prime \\prime}[E(\\tilde{Y})][\\tilde{Y}-E(\\tilde{Y})]^{2} $$ Now go back to the beginning of the period, before the outcome of the bet is known, and take expectations to obtain $$ E[u(\\tilde{Y})] \\approx u[E(\\tilde{Y})]+\\frac{1}{2} u^{\\prime \\prime}[E(\\tilde{Y})] \\sigma^{2}(\\tilde{Y}) $$ The right-hand side of this expression is in the desired form: if u is increasing, it rewards higher mean returns and if u is concave, it penalizes higher variance in returns. So one possible justification for mean-variance utility is to assume that the size of the portfolio bet $\\tilde{Y}− E(\\tilde{Y})$ is small enough to make this Taylor approximation a good one. But it isn’t safe to assume that portfolio bets are small. ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:2:1","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.2.2 Exact case Case 1: Quadratic Utility Function A second possibility is to assume that the Bernoulli utility function is quadratic, with $$ u(Y)=a+b Y+c Y^{2} $$ with $b\u003e0$ and $c\u003c0,$ Then $$ \\begin{aligned} u^{\\prime}(Y) \u0026=b+2 c Y \\ u^{\\prime \\prime}(Y) \u0026=2 c \\end{aligned} $$ so that $u^{\\prime \\prime \\prime}(Y)=0$ and all higher-order derivatives are zero as well. In this case, the second-order Taylor approximation holds exactly. Note, however, that for a quadratic utility function $$ R_{A}(Y)=-\\frac{u^{\\prime \\prime}(Y)}{u^{\\prime}(Y)}=-\\frac{2 c}{b+2 c Y} $$ which is increasing in $Y$. Hence, quadratic utility has the undesirable implication that the amount of wealth allocated to risky investments declines when wealth increases. Case 2: Normality Assumption 【Theorem】If $\\tilde{Y}$ is normally distributed, there exists a function $v$ such that $$ E u(\\tilde{Y})=v\\left(\\mu_{Y}, \\sigma_{Y}\\right) $$ Moreover, if $\\tilde{Y}$ is normally distributed and $u$ is increasing, then $v$ is increasing in $\\mu_{Y}$ $u$ is concave, then $v$ is decreasing in $\\sigma_{Y}$ $u$ is concave, then indifference curves defined over $\\mu_{Y}$ and $\\sigma_{Y}$ are convex \r Problems with the normality assumption: limited liability instruments such as stocks can pay at worst a negative return of $-100 %$ (complete loss of the investment) Returns on assets like options are highly non-normal. While the normal is perfectly symmetric about its mean, high-frequency returns are frequently skewed to the right and index returns appear skewed to the left $$ S(\\tilde{r_{i t}})=E\\left[\\frac{\\left(r_{i t}-\\mu_{i}\\right)^{3}}{\\sigma_{i}^{3}}\\right] $$ Sample high-frequency return distributions for many assets exhibit excess kurtosis or “fat tails” $$ K(\\tilde{r_{i}})=E\\left[\\frac{\\left(r_{i t}-\\mu_{i}\\right)^{4}}{\\sigma_{i}^{4}}\\right] $$ ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:2:2","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.3 $\\max E \\hat{u}(\\tilde{r}_{P})$ In a mean-variance (M-V) framework, an investor’s wants to maximize a function $u\\left(\\mu_{r}, \\sigma_{P}\\right)$. He likes expected return $\\left(\\mu_{r}\\right)$ and dislikes standard deviation $\\left(\\sigma_{P}\\right)$ Recall that portfolio $A$ is said to exhibit mean-variance dominance over portfolio B if either $$ \\begin{align*} \\mu_{A}\u003e\\mu_{B} \\text { and } \\sigma_{A} \\leq \\sigma_{B}\\ {\\mu_{A} \\geq \\mu_{B}} \\text { and } \\sigma_{A} \u003c \\sigma_{B} \\end{align*} $$ We can then define the **efficient frontier** as the set of all portfolios that are not mean-variance dominated by any other portfolio. By de\u000cnition, no (“rational”) mean-variance investor would choose to hold a portfolio not located on the efficient frontier. The shape of the efficient frontier is of primary interest; let us examine the efficient frontier in the two-asset case for a variety of possible asset return correlations. ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:3:0","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.3.1 Two Risky Assets Consider forming a portfolio from two risky assets: $$ \\begin{align*} \\tilde{r}_{1}, \\tilde{r}_{2}\u0026=\\text{random returns}\\ \\mu_{1}, \\mu_{2}\u0026=\\text{expected returns}\\ \\sigma_{1}, \\sigma_{2}\u0026=\\text{standard deviations} \\end{align*} $$ Assume $\\mu_{1}\u003c\\mu_{2}$ and $\\sigma_{1}\u003c\\sigma_{2}$ to create a trade-off between expected return and risk. If $w$ is the fraction of initial wealth allocated to asset 1 and $1-w$ is the fraction of initial wealth allocated to asset $2,$ the random return $\\tilde{r}{P}$ on the portfolio is $$ \\tilde{r}{P}=w \\tilde{r}{1}+(1-w) \\tilde{r}{2} $$ and the expected return $\\mu_{p}$ on the portfolio is $$ \\begin{aligned} \\mu_{P} \u0026=E\\left[w \\tilde{r}_{1}+(1-w) \\tilde{r}_{2}\\right] \\ \u0026=w E\\left(\\tilde{r}_{1}\\right)+(1-w) E\\left(\\tilde{r}_{2}\\right) \\ \u0026=w \\mu_{1}+(1-w) \\mu_{2} \\end{aligned} $$ The variance of the random portfolio return $$ \\begin{aligned} \\sigma_{P}^{2} \u0026=E\\left[\\left(\\tilde{r_{P}}-\\mu_{P}\\right)^{2}\\right] \\ \u0026=E\\left(\\left[w \\tilde{r}_{1}+(1-w) \\tilde{r}_{2}-w \\mu_{1}-(1-w) \\mu_{2}\\right]^{2}\\right) \\ \u0026=E\\left(\\left[w\\left(\\tilde{r}_{1}-\\mu_{1}\\right)+(1-w)\\left(\\tilde{r}_{2}-\\mu_{2}\\right)\\right]^{2}\\right) \\ \u0026=E[w^{2}\\left(\\tilde{r}_{1}-\\mu_{1}\\right)^{2}+(1-w)^{2}\\left(\\tilde{r}_{2}-\\mu_{2}\\right)^{2}\\ \u0026\\quad+2 w(1-w)\\left(\\tilde{r}_{1}-\\mu_{1}\\right)\\left(\\tilde{r}_{2}-\\mu_{2}\\right)] \\ \u0026=w^{2} E\\left[\\left(\\tilde{r}_{1}-\\mu_{1}\\right)^{2}\\right]+(1-w)^{2} E\\left[\\left(\\tilde{r}_{2}-\\mu_{2}\\right)^{2}\\right] \\ \u0026\\quad+2 w(1-w) E\\left[\\left(\\tilde{r}_{1}-\\mu_{1}\\right)\\left(\\tilde{r}_{2}-\\mu_{2}\\right)\\right] \\ \u0026=w^{2} \\sigma_{1}^{2}+(1-w)^{2} \\sigma_{2}^{2}+2 w(1-w) \\sigma_{12}\\ \u0026=w^{2} \\sigma_{1}^{2}+(1-w)^{2} \\sigma_{2}^{2}+2 w(1-w) \\sigma_{1} \\sigma_{2} \\rho_{12} \\end{aligned} $$ where $$ \\begin{aligned} \\sigma_{12}= \\text{the covariance between } \\tilde{r}_{1} \\text{ and } \\tilde{r}_{2} \\ \\rho_{12}= \\text{the correlation between } \\tilde{r}_{1} \\text{ and } \\tilde{r}_{2} \\end{aligned} $$ The source of the gains from diversi\u000cfication: the expected portfolio return is a weighted average of the expected returns on the individual asset returns, but the standard deviation of the portfolio return is not a weighted average of the standard deviations of the returns on the individual assets and can be reduced by choosing a mix of assets. Minimum Variance Portfolio(MVP) $$ \\sigma_{P}^{2}=w^{2} \\sigma_{1}^{2}+(1-w)^{2} \\sigma_{2}^{2}+2 w(1-w) \\sigma_{1} \\sigma_{2} \\rho_{12} $$ We can minimize the portfolio variance by setting the first derivative equal to zero: $$ \\frac{d \\sigma_{P}^{2}}{d w}=2 w \\sigma_{1}^{2}-2 \\sigma_{2}^{2}+2 w \\sigma_{2}^{2}+2 \\sigma_{12}-4 w \\sigma_{12}=0 $$ and solve for $w^{*}$ $$ w^{*}=\\frac{\\sigma_{2}^{2}-\\sigma_{12}}{\\sigma_{1}^{2}+\\sigma_{2}^{2}-2 \\sigma_{12}} $$ Case 1: $\\rho_{12}=1$ To see more specifically how this works, start with the case where $\\rho_{12}=1$ so that the individual asset returns are perfectly correlated. This is the one case in which there are **no gains from diversification**. With $\\rho_{12}=1$ $$ \\begin{aligned} \\mu_{P} \u0026=w \\mu_{1}+(1-w) \\mu_{2} \\ \\sigma_{P} \u0026=\\left[w^{2} \\sigma_{1}^{2}+(1-w)^{2} \\sigma_{2}^{2}+2 w(1-w) \\sigma_{1} \\sigma_{2} \\rho_{12}\\right]^{1 / 2} \\ \u0026=\\left[w^{2} \\sigma_{1}^{2}+(1-w)^{2} \\sigma_{2}^{2}+2 w(1-w) \\sigma_{1} \\sigma_{2}\\right]^{1 / 2} \\ \u0026=\\left(\\left[w \\sigma_{1}+(1-w) \\sigma_{2}\\right]^{2}\\right)^{1 / 2} \\ \u0026=w \\sigma_{1}+(1-w) \\sigma_{2} \\end{aligned} $$ In this special case, the standard deviation of the return on the portfolio is a weighted average of the standard deviations of the returns on the individual assets. \r To show that $P_{1} P_{2}$ is a straight line: no matter what percentage of wealth $w$ we choose to invest in $X$ the trade-off between expected value and standard deviation is constant. $$ \\begin{aligned} \\text { Slope } \u0026=\\frac{d \\mu_{P}}{d \\sigma_{P}} \\ \u0026=\\frac{d \\mu_{P} / d w}{d \\sigma_{P} / d w} \\ \u0026=\\frac{\\mu_{2}-\\mu_{1}}{\\sigma_{2}-\\sigma_{1}} \\end{aligned} $$ Case ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:3:1","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.3.2 Three Risky Assets With three assets, for example, an investor can choose $$ \\begin{align*} w_{1}\u0026= \\text{share of initial wealth allocated to asset 1}\\ w_{2}\u0026= \\text{share of initial wealth allocated to asset 2}\\ 1-w_{1}-w_{2}\u0026= \\text{share of initial wealth allocated to asset 3} \\end{align*} $$ Given the choices of $w_{1}$ and $w_{2}$ $$ \\begin{aligned} \\tilde{r}_{P} \u0026=w_{1} \\tilde{r}_{1}+w_{2} \\tilde{r}_{2}+\\left(1-w_{1}-w_{2}\\right) \\tilde{r}_{3} \\ \\mu p \u0026=w_{1} \\mu_{1}+w_{2} \\mu_{2}+\\left(1-w_{1}-w_{2}\\right) \\mu_{3} \\ \\sigma_{p}^{2} \u0026=w_{1}^{2} \\sigma_{1}^{2}+w_{2}^{2} \\sigma_{2}^{2}+\\left(1-w_{1}-w_{2}\\right)^{2} \\sigma_{3}^{2} \\ \u0026\\quad +2 w_{1} w_{2} \\sigma_{1} \\sigma_{2} \\rho_{12}+2 w_{1}\\left(1-w_{1}-w_{2}\\right) \\sigma_{1} \\sigma_{3} \\rho_{13} \\ \u0026\\quad +2 w_{2}\\left(1-w_{1}-w_{2}\\right) \\sigma_{2} \\sigma_{3} \\rho_{23} \\end{aligned} $$ First , Find MVF 注意到，2 资产的情况下不存在这一问题，其收益风险一一对应 In the case with two risky assets, the choice of w simultaneously determines $\\mu_p$and $\\sigma_p$. Our problem is to solve $$ \\begin{align*} \\min _{w_{1}, w_{2}}\\quad \u0026\\sigma_{P}^{2}\\ s.t.\\quad \u0026\\mu_{P}=\\bar{\\mu} \\end{align*} $$ for a given value of $\\bar{\\mu}$. But since we are more used to solving constrained maximization problems, consider the reformulated, but equivalent, problem: $$ \\begin{align*} \\max _{w_{1}, w_{2}}\\quad \u0026-\\sigma_{P}^{2}\\ s.t.\\quad \u0026\\mu_{P}=\\bar{\\mu} \\end{align*} $$ Set up the Lagrangian, using the expressions for $\\mu_{P}$ and $\\sigma_{P}$ derived previously: $$ \\begin{aligned} L \u0026=-{\\Large[}w_{1}^{2} \\sigma_{1}^{2}+w_{2}^{2} \\sigma_{2}^{2}+\\left(1-w_{1}-w_{2}\\right)^{2} \\sigma_{3}^{2}\\ \u0026\\quad+2 w_{1} w_{2} \\sigma_{1} \\sigma_{2} \\rho_{12}+2 w_{1}\\left(1-w_{1}-w_{2}\\right) \\sigma_{1} \\sigma_{3} \\rho_{13} \\ \u0026\\quad+2 w_{2}\\left(1-w_{1}-w_{2}\\right) \\sigma_{2} \\sigma_{3} \\rho_{23}{\\Large]} \\ \u0026\\quad+\\lambda\\left[w_{1} \\mu_{1}+w_{2} \\mu_{2}+\\left(1-w_{1}-w_{2}\\right) \\mu_{3}-\\bar{\\mu}\\right] \\end{aligned} $$ F.O.C. for $w_{1}$ $$ \\begin{aligned} 0 \u0026=-2 w_{1}^{*} \\sigma_{1}^{2}+2\\left(1-w_{1}^{*}-w_{2}^{*}\\right) \\sigma_{3}^{2} \\ \u0026-2 w_{2}^{*} \\sigma_{1} \\sigma_{2} \\rho_{12}-2\\left(1-w_{1}^{*}-w_{2}^{*}\\right) \\sigma_{1} \\sigma_{3} \\rho_{13} \\ \u0026+2 w_{1}^{*} \\sigma_{1} \\sigma_{3} \\rho_{13}+2 w_{2}^{*} \\sigma_{2} \\sigma_{3} \\rho_{23} \\ \u0026+\\lambda^{*} \\mu_{1}-\\lambda^{*} \\mu_{3} \\end{aligned} $$ F.O.C. for $w_{2}$ $$ 0=-2 w_{2}^{*} \\sigma_{2}^{2}+2\\left(1-w_{1}^{*}-w_{2}^{*}\\right) \\sigma_{3}^{2} $$ $$ \\begin{array}{l} -\\quad 2 w_{1}^{*} \\sigma_{1} \\sigma_{2} \\rho_{12}+2 w_{1}^{*} \\sigma_{1} \\sigma_{3} \\rho_{13} \\ -\\quad 2\\left(1-w_{1}^{*}-w_{2}^{*}\\right) \\sigma_{2} \\sigma_{3} \\rho_{23}+2 w_{2}^{*} \\sigma_{2} \\sigma_{3} \\rho_{23} \\ +\\quad \\lambda^{*} \\mu_{2}-\\lambda^{*} \\mu_{3} \\end{array} $$ B.C. $$ w_{1}^{*} \\mu_{1}+w_{2}^{*} \\mu_{2}+\\left(1-w_{1}^{*}-w_{2}^{*}\\right) \\mu_{3}=\\bar{\\mu} $$ The two first-order conditions and the constraint form a system of three equations in the three unknowns: $w_{1}^{*}, w_{2}^{*}$ and $\\lambda^{*}$. Moreover, the equations are linear in the unknowns $w_{1}^{*}, w_{2}^{*}$ and $\\lambda^{*}$ Given specific values for $\\mu_{1}, \\mu_{2}, \\mu_{3}, \\sigma_{1}, \\sigma_{2}, \\sigma_{3}, \\rho_{12}, \\rho_{13}, \\rho_{23},$ and $\\bar{\\mu}$ they can be solved quite easily. \r ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:3:2","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.3.3 N Risky Assets Organize the portfolio shares and expected returns into a vectors: $$ w=\\left[\\begin{array}{c} w_{1} \\ w_{2} \\ \\vdots \\ w_{N} \\end{array}\\right] \\quad \\text { and } \\quad \\mu=\\left[\\begin{array}{c} \\mu_{1} \\ \\mu_{2} \\ \\vdots \\ \\mu_{N} \\end{array}\\right] $$ where $$ w_{1}+w_{2}+\\ldots+w_{N}=1 $$ Meanwhile, the variances and covariances can be organized into a matrix - a collection of rows and columns: $$ \\Sigma=\\left[\\begin{array}{lccc} \\sigma_{1}^{2} \u0026 \\sigma_{1} \\sigma_{2} \\rho_{12} \u0026 \\dots \u0026 \\sigma_{1} \\sigma_{N} \\rho_{1 N} \\ \\sigma_{1} \\sigma_{2} \\rho_{12} \u0026 \\sigma_{2}^{2} \u0026 \\dots \u0026 \\sigma_{2} \\sigma_{N} \\rho_{2 N} \\ \u0026 \\cdot \u0026 \\dots \u0026 \\cdot \\ \\sigma_{1} \\sigma_{N} \\rho_{1 N} \u0026 \\sigma_{2} \\sigma_{N} \\rho_{2 N} \u0026 \\dots \u0026 \\sigma_{N}^{2} \\end{array}\\right] $$ Using the rules from linear algebra for multiplying vectors and matrices, the expected return on any portfolio with shares in the vector $w$ is $$ \\mu^{\\prime} w $$ and the variance of the random return on the portfolio is $$ w^{\\prime} \\Sigma w $$ Hence, the problem of minimizing the variance for a given mean can be written compactly as $$ \\max _{w}-w^{\\prime} \\Sigma w \\quad \\text { s.t. } \\quad \\mu^{\\prime} w=\\bar{\\mu} \\quad \\text { and } \\quad I^{\\prime} w=1 $$ where $I$ is a vector of $N$ ones. Problems of this form are called quadratic programming problems and can be solved very quickly on a computer even when the number of assets $N$ is large. We can also add more constraints, such as $w_{i} \\geq 0,$ ruling out short sales. The minimum variance frontier traces out the minimized variance or standard deviation for each required mean return. Adding assets shifts the minimum variance frontier to the left, as opportunities for diversi\u000ccation are enhanced(大多数情况下并非包络线而是变得更好). \r But portfolio A exhibits mean-variance dominance over portfolio B, since it offers a higher expected return with the same standard deviation. \r Define the efficient frontier as the set of all portfolios that are not mean-variance dominated by any other portfolio. The efficient frontier extends only along the top arm of the minimum variance frontier. \r ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:3:3","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.3.4 N risky asset and 1 risk-free asset So far, however, our analysis has assumed that there are only risky assets. An additional, quite striking, result emerges when we add a risk free asset to the mix. This implication was first noted by James Tobin (US, 1918-2002, Nobel Prize 1981) in his paper “Liquidity Preference as Behavior Towards Risk,\" Review of Economic Studies Vol.25 (February 1958): pp.65-86. Risk free asset: money market mutual fund (MMMF), T-bill, deposit rate. It is usually assumed that the rate of return on the risk-free asset is equal to the borrowing and lending rate in the economy. Consider, therefore, the larger portfolio formed when an investor allocates the fraction $w$ of his or her initial wealth to a to a portfolio of risky assets and the remaining fraction $1-w$ to a risk free asset with return $r_{f}$ If the risky part of this portfolio has random return $\\tilde{r},$ expected return $\\mu_{r}=E(\\tilde{r})$ and variance $\\sigma_{r}^{2}=E\\left[\\left(\\tilde{r}-\\mu_{r}\\right)^{2}\\right]$ then the larger portfolio has random return $\\tilde{r}_{P}=w \\tilde{r}+(1-w) r_{f}$ with expected return $$ \\mu_{P}=E\\left[w \\tilde{r}+(1-w) r_{f}\\right]=w \\mu_{r}+(1-w) r_{f} $$ and variance $$ \\begin{aligned} \\sigma_{P}^{2} \u0026=E\\left[\\left(\\tilde{r}_{p}-\\mu_{P}\\right)^{2}\\right] \\ \u0026=E\\left[w \\tilde{r}+(1-w) r_{f}-w \\mu_{r}-(1-w) r_{f}\\right]^{2} \\ \u0026=E\\left[w\\left(\\tilde{r}-\\mu_{r}\\right)\\right]^{2}=w^{2} \\sigma_{r}^{2} \\end{aligned} $$ The expression for the portfolio’s variance $$ \\sigma_{P}^{2}=w^{2} \\sigma_{r}^{2} $$ implies $$ \\sigma_{P}=w \\sigma_{r} $$ Hence $$ w=\\frac{\\sigma_{P}}{\\sigma_{r}} $$ Hence, with $\\sigma_{r}$ given, a larger share of wealth $w$ allocated to risky assets is associated with a higher standard deviation $\\sigma_{P}$ for the larger portfolio. Then, we have $$ \\begin{aligned} \\mu_{P} \u0026=\\frac{\\sigma_{P}}{\\sigma_{r}} \\mu_{r}+\\left(1-\\frac{\\sigma_{P}}{\\sigma_{r}}\\right) r_{f} \\ \u0026=r_{f}+\\left(\\frac{\\mu_{r}-r_{f}}{\\sigma_{r}}\\right) \\sigma_{P} \\end{aligned} $$ It shows that for portfolios of risky and riskless assets: o The relationship between $\\sigma_{P}$ and $\\mu_{P}$ is linear. o The slope of the linear relationship is given by the **Sharpe ratio**, defined here as the “expected excess return” offered by the risky components of the portfolio divided by the standard deviation of the return on that risky component: $$ \\frac{\\mu_{r}-r_{f}}{\\sigma_{r}} $$ Note that the tangency portfolio T can be identified as the portfolio along the efficient frontier of risky assets that has the highest Sharpe ratio. All investors with mean-variance utility will prefer some combination of the risk free asset and risky portfolio T to any other portfolio. 【Theorem】With N risky assets and a risk-free one, the efficient frontier is a straight line. \r ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:3:4","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.3.5 Optimal Portfolio Choice The optimal portfolio is naturally de\u000cned as that portfolio maximizing the investor’s (mean-variance) utility; That portfolio for which he is able to reach the highest indifference curve in MV space; Indifference Curve in MV Space Recall that either of two sets of assumptions will imply that indifference curves in this $ \\mu-\\sigma $ diagram slope upward and are convex: Investors have vN-M expected utility with quadratic Bernoulli utility functions Asset returns are normally distributed and investors have vN-M expected utility with increasing and concave Bernoulli utility functions ??? Such curves will be increasing and convex from the origin; They are increasing because additional risk needs to be compensated by higher means; They are convex if and only if the investor is characterized by increasing absolute risk aversion (IARA), which is the case under MV preferences, as we have claimed. Criterion Each indifference curve maps out all combinations of risk and return that provide us with the same utility. The slope of indifference curve indicates the marginal rate of substitution(MRS) between our preference for risk and return, which is subjective. The effcient frontier shows the tradeoff between risk and return, the slope of which indicates the marginal rate of transformation(MRT) offered by MVF. An important feature of the optimal portfolio that we choose to maximize our utility is that the subjective MRS is exactly equal to the objectively determined MRT between risk and return. Without Risk-free Asset Investor B is less risk averse than investor A. Different investors face the same assessment of the return and risk offered by risky assets, they may hold different portfolios. But all optimal portfolios are along the efficient frontier. Thus, the mean-variance utility hypothesis built into Modern Portfolio Theory implies that all investors choose optimal portfolios along the efficient frontier. \r With Risk-free Asset With $ MRS_A = \\frac{\\Delta \\mu}{\\Delta \\sigma}\u003e MRS_B $, investor B is less risk averse than investor A. But both choose same combination of the “tangency portfolio” T and the risk free asset. \r 【 Theorem】Any risk averse investor, independently of her risk aversion, will diversify between a risky (tangency portfolio) fund and the riskless asset. 【two-fund theorem/ separation theorem】all investors will invest in the same two funds, the risk-free asset on the one hand, and the risky portfolio (T) identifi\u000ced by the tangency point. \r Equity mutual fund managers can all focus on building the unique portfolio that lies along the efficient frontier of risky assets and has the highest Sharpe ratio. Each individual investor can then tailor his or her own portfolio by choosing the combination of the riskless assets and the risky mutual fund that best suits his or her own aversion to risk. ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:3:5","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.4 Pros and Cons of MPT ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:4:0","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.4.1 Cons First, basic assumptions may not hold: either utility must be quadratic or asset returns must be normal to support mean-variance utility hypothesis. Second, the estimation or “calibration” of the model’s parameters. With $N$ risky assets, the vector $\\mu$ of expected returns contains $N$ elements and the matrix $\\Sigma$ of variances and covariances contains $N(N+1) / 2$ unique elements. When $N=100,$ for example, there are $100+(100 \\times 101) / 2=5150$ parameters to estimate! And to use data from the past to estimate these parameters, one has to assume that past averages and correlations are a reliable guide to the future. ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:4:1","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.4.2 Pros On the other hand, the MPT teaches us a very important lesson about how individual assets with imperfectly, and especially negatively, correlated returns can be combined into a diversified portfolio to reduce risk. And the MPT’s separation theorem suggests that a retirement savings plan that allows participants to choose between a money market mutual fund and a well-diversified equity fund is fully optimal under certain circumstances and perhaps close enough to optimal more generally. Finally, our first equilibrium model of asset pricing, the Capital Asset Pricing Model, builds directly on the foundations provided by Modern Portfolio Theory. ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:4:2","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6.5 Practice \r \r \r ","date":"0001-01-01","objectID":"/5.-modern-portfolio-theory/:5:0","tags":null,"title":"","uri":"/5.-modern-portfolio-theory/"},{"categories":null,"content":"6. Capital Asset Pricing Model ","date":"0001-01-01","objectID":"/6.-capm/:0:0","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.1 MPT and the CAPM The Capital Asset Pricing Model builds directly on Modern Portfolio Theory. It was developed in the mid-1960s by William Sharpe (US, b.1934, Nobel Prize 1990), John Lintner (US, 1916-1983), and Jan Mossin (Norway, 1936-1987). William Sharpe, “Capital Asset Prices: A Theory of Market Equilibrium Under Conditions of Risk,” Journal of Finance Vol.19 (September 1964): pp.425-442. John Lintner, “The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets,” Review of Economics and Statistics Vol.47 (February 1965 ): pp. 13-37. Jan Mossin, “Equilibrium in a Capital Asset Market,” Econometrica Vol.34 (October 1966): pp.768-783. Assumptions of the Capital Asset Pricing Model Investors are risk-averse individuals who maximize the expected utility of their end-of-period wealth Investors are price-takers and have homogeneous expectations about asset returns that have a joint normal distribution There exists a risk-free asset such that investors may borrow or lend unlimited amounts at the risk-free rate. The quantities of assets are fixed. All assets are marketable and perfectly divisible. Asset markets are frictionless and information is costless and simultaneously available to all investors. There are no market imperfections such as taxes, regulations, or restrictions on short selling. But whereas Modern Portfolio Theory is a theory describing the demand for financial assets, the Capital Asset Pricing Model is a theory describing equilibrium in financial markets. By making an additional assumption, that supply equals demand in financial markets, the CAPM yields additional implications about the pricing of financial assets and risky cash flows. Like MPT, the CAPM assumes that investors have mean-variance utility and hence that either investors have quadratic Bernoulli utility functions or that the random returns on risky assets are normally distributed. Thus, some of the same caveats that apply to MPT also apply to the CAPM. For example, one might hesitate before applying the CAPM to price options. The traditional CAPM also assumes that there is a risk free asset as well as a potentially large collection of risky assets. Under these circumstances, as we’ve seen, all investors will hold some combination of the riskless asset and the tangency portfolio: the efficient portfolio of risky assets with the highest Sharpe ratio. But the CAPM goes further than the MPT by imposing an equilibrium condition. Because there is no demand for risky financial assets except to the extent that they comprise the tangency portfolio, and because, in equilibrium, the supply of financial assets must equal demand, the market portfolio consisting of all existing financial assets must coincide with the tangency portfolio. In equilibrium, that is, “everyone” must “own the market.” ","date":"0001-01-01","objectID":"/6.-capm/:1:0","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.2 CAPM ","date":"0001-01-01","objectID":"/6.-capm/:2:0","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.2.1 CML In the CAPM, equilibrium in financial markets requires the demand for risky assets |the tangency portfolio| to coincide with the supply of financial assets|the market portfolio. The CAPM’s first implication is immediate: the market portfolio is efficient. The line originating at $\\left(0, r_{f}\\right)$ and running through $\\left(\\sigma_{M}, E\\left(r_{M}\\right)\\right)$ is called the capital market line (CML). \r Endowed with point $A,$ we always have two choices available when there is a capital market: moving along the MVF or moving along CML by borrowing and lending. First move to B where MRS=MRT of the MVF, and $U_1$ increases to $U_2$; Then we can better off by moving to $M$ and borrowing to reach $C$, utility increases to $U_{3}$. \r In equilibrium, the MRS is the same for all individuals, regardless of their subjective attitude to risk. Hence, it also follows that all individually optimal portfolios are located along the CML and are formed as combinations of the risk free asset and the market portfolio. \r Recall that the trade-off between the standard deviation and expected return of any portfolio combining the riskless asset and the tangency portfolio is described by the linear relationship $$ E\\left(\\tilde{r}{\\mathrm{P}}\\right)=r{f}+\\left[\\frac{E\\left(\\tilde{r}{T}\\right)-r{f}}{\\sigma_{T}}\\right] \\sigma_{P} $$ since the CAPM implies that the tangency and market portfolios coincide, the formula for the Capital Market Line is likewise $$ E\\left(\\tilde{r}_{P}\\right)=r_{f}+\\left[\\frac{E\\left(\\tilde{r}_{M}\\right)-r_{f}}{\\sigma_{M}}\\right] \\sigma_{P} $$ And since all individually optimal portfolios are located along the CML, it implies that the market portfolio’s Sharpe ratio $$ \\frac{E{(\\tilde{r_M})}-r_f}{\\sigma_M} $$ measures the equilibrium price of risk: the expected return that each investor gives up when he or she adjusts his or her total portfolio to reduce risk. ","date":"0001-01-01","objectID":"/6.-capm/:2:1","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.2.2 Covariance Consider making an equally weighted portfolio of $n$ assets, i.e., with all $w_{j}=1 / n .$ Assume that among the rates of return, one has the maximum variance, $\\sigma_{\\max }^{2} .$ $\\lim {n \\rightarrow \\infty} \\sigma{p}^{2}=\\bar{\\sigma}_{i j}$ Observe that $$ \\sigma_{p}^{2}=\\sum_{i=1}^{n} \\sum_{j=1}^{n} W_{i} W_{j} \\sigma_{i j} $$ An equally weighted portfolio has $$ \\sigma_{p}^{2}=\\frac{1}{n^{2}} \\Sigma_{i=1}^{n} \\Sigma_{j=1}^{n} \\sigma_{i j}=\\frac{1}{n^{2}} \\Sigma_{i=1}^{n} \\sigma_{i}^{2}+\\frac{1}{n^{2}} \\Sigma_{i=1}^{n} \\Sigma_{j \\neq i} \\sigma_{i j} $$ Observe that the first term satisfies $$ \\frac{1}{n^{2}} \\Sigma_{i=1}^{n} \\sigma_{i}^{2}\u003c\\frac{1}{n^{2}} * n * \\sigma_{\\max }^{2} \\rightarrow 0 \\Leftarrow n \\rightarrow \\infty $$ The second term satisfies $$ \\frac{1}{n^{2}} \\Sigma_{i=1}^{n} \\Sigma_{j \\neq i} \\sigma_{i j}=\\frac{n^{2}-n}{n^{2}} \\bar{\\sigma}_{i j} \\rightarrow \\bar{\\sigma}_{i j} \\Leftarrow n \\rightarrow \\infty $$ the average covariance between rates of return. Thus $$ \\lim {n \\rightarrow \\infty} \\sigma{p}^{2}=\\bar{\\sigma}{i j} $$ 2. $\\lim {n \\rightarrow \\infty} \\frac{\\partial \\sigma{p}^{2}}{\\partial w{i}}=2 \\bar{\\sigma}_{i j}$ With $$ \\sigma_{p}^{2}=\\sum_{i=1}^{N} \\sum_{j=1}^{N} w_{i} w_{j} \\sigma_{i j}=\\sum_{i=1}^{N} w_{i}^{2} \\sigma_{i}^{2}+\\sum_{i \\neq j} w_{i} w_{j} \\sigma_{i j} $$ Take derivative, $$ \\frac{\\partial \\sigma_{p}^{2}}{\\partial w_{i}}=2 w_{i} \\sigma_{i}^{2}+2 \\Sigma_{j\u003ei} w_{j} \\sigma_{i j} $$ Evaluated where all $w_{i}=1 / n,$ this becomes $$ 2 \\frac{\\sigma_{i}^{2}}{n}+2 \\frac{n-1}{n} \\bar{\\sigma}_{i j} \\rightarrow 2 \\bar{\\sigma}_{i j} \\Leftarrow n \\rightarrow \\infty $$ the average covariance between rates of return, and $$ \\lim _{n \\rightarrow \\infty} \\frac{\\partial \\sigma_{p}^{2}}{\\partial w_{i}}=2 \\bar{\\sigma}_{i j} $$ ","date":"0001-01-01","objectID":"/6.-capm/:2:2","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.2.3 Deriving the CAPM Consider an equilibrium, everyone holds combination of risk free asset and market portfolio. Next, let’s consider an arbitrary asset - “asset $j^{\\prime \\prime}-$ with random return $\\tilde{r}{j},$ expected return $E\\left(\\tilde{r}{j}\\right),$ and standard deviation $\\sigma_{j} .$ (Possible, even though $M$ already contains $j$.)) MPT would take $E\\left(\\tilde{r}{j}\\right)$ and $\\sigma{j}$ as “data” - that is, as given. The CAPM again goes further and asks: if asset $j$ is to be demanded by investors with mean-variance utility, what restrictions must $E\\left(\\tilde{r}{j}\\right)$ and $\\sigma{j}$ satisfy? To answer this question, consider an investor who takes the portion of his or her initial wealth that he or she allocates to risky assets and divides it further: using the fraction $w$ to purchase asset $j$ and the remaining fraction $1-w$ to buy the market portfolio. Note that since the market portfolio already includes some of asset $j$ choosing $w\u003e0$ really means that the investor “overweights” asset $j$ in his or her own portfolio. Conversely, choosing $w\u003c0$ means that the investor “underweights” asset $j$ in his or her own portfolio. Based on our previous analysis, we know that this investor’s portfol of risky assets now has random return $$ \\tilde{r}{P}=w \\tilde{r}{j}+(1-w) \\tilde{r}{M} $$ expected return $$ E\\left(\\tilde{r}{P}\\right)=w E\\left(\\tilde{r}{j}\\right)+(1-w) E\\left(\\tilde{r}{M}\\right) $$ and variance $$ \\sigma_{P}^{2}=w^{2} \\sigma_{j}^{2}+(1-w)^{2} \\sigma_{M}^{2}+2 w(1-w) \\sigma_{j M} $$ where $\\sigma_{j M}$ is the covariance between $\\tilde{r}_{j}$ and $\\tilde{r}_{M} .$ We can use these formulas to trace out how $\\sigma_{P}$ and $E\\left(\\tilde{r}_{P}\\right)$ vary as $w$ changes. The red curve traces out how $\\sigma_{P}$ and $E\\left(\\tilde{r}_{P}\\right)$ vary as $w$ changes, that is, as asset $j$ gets underweighted or overweighted relative to the market portfolio. The red curve passes through $M,$ since when $w=0$ the new portfolio coincides with the market portfolio. For all other values of $w$, however, the red curve must lie below the CML. \r Otherwise, a portfolio along the CML would be dominated in mean-variance by the new portfolio. Financial markets would no longer be in equilibrium, since some investors would no longer be willing to hold the market portfolio. \r Together, these observations imply that the red curve must be tangent to the CML at point M. Key point: Tangent means equal in slope. We already know that the slope of the Capital Market Line is $$ \\frac{E\\left(\\tilde{r}{M}\\right)-r{f}}{\\sigma_{M}} $$ But what is the slope of the red curve? Let $f\\left(\\sigma_{P}\\right)$ be the function defined by $E\\left(\\tilde{r}_{P}\\right)=f\\left(\\sigma_{P}\\right)$ and therefore describing the red curve. Next, define the functions $g(w)$ and $h(w)$ by $$ \\begin{aligned} g(w) \u0026=E\\left(\\tilde{r}{P}\\right)=w E\\left(\\tilde{r}{j}\\right)+(1-w) E\\left(r_{M}\\right) \\ h(w) \u0026=\\sigma_{P}=\\left[w^{2} \\sigma_{j}^{2}+(1-w)^{2} \\sigma_{M}^{2}+2 w(1-w) \\sigma_{j M}\\right]^{1 / 2} \\end{aligned} $$ Substitute into $$ E\\left(\\tilde{r}_{P}\\right)=f\\left(\\sigma_{P}\\right) $$ then $$ g(w)=f(h(w)) $$ and use the chain rule to compute $$ g^{\\prime}(w)=f^{\\prime}(h(w)) h^{\\prime}(w)=f^{\\prime}\\left(\\sigma_{P}\\right) h^{\\prime}(w) $$ and get $$ f^{\\prime}\\left(\\sigma_{P}\\right)=\\frac{g^{\\prime}(w)}{h^{\\prime}(w)} $$ and compute $g^{\\prime}(w)$ and $h^{\\prime}(w)$ from the formulas we know $$ g(w)=w E\\left(\\tilde{r}_{j}\\right)+(1-w) E\\left(r_{M}^{\\prime}\\right) $$ implies $$ g^{\\prime}(w)=E\\left(\\tilde{r}_{j}\\right)-E\\left(\\tilde{r}_{M}\\right) $$ $$ h(w)=\\left[w^{2} \\sigma_{j}^{2}+(1-w)^{2} \\sigma_{M}^{2}+2 w(1-w) \\sigma_{j M}\\right]^{1 / 2} $$ implies $$ \\begin{aligned} h^{\\prime}(w)\u0026=\\frac{1}{2}\\left(\\frac{2 w \\sigma_{j}^{2}-2(1-w) \\sigma_{M}^{2}+2(1-2 w) \\sigma_{j M}}{\\left[w^{2} \\sigma_{j}^{2}+(1-w)^{2} \\sigma_{M}^{2}+2 w(1-w) \\sigma_{j M}\\right]^{1 / 2}}\\right)\\ \u0026=\\frac{w \\sigma_{j}^{2}-(1-w) \\sigma_{M}^{2}+(","date":"0001-01-01","objectID":"/6.-capm/:2:3","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.2.4 CML V.S. SML Capital Market Line Efficient set given 1 risk free and $n$ risky assets. Relevant for choice between alternative portfolios. Drawn in $\\left(\\sigma_{p}, \\mu_{p}\\right)$ diagram. ray starting at $\\left(0, r_{f}\\right)$ in that diagram. Security market line Location of all traded assets in equilibrium. Also location of any portfolio of these assets. Not relevant for choice between assets which are already traded, so that equilibrium prices are observable. But relevant if equilibrium price at $t=0$ is unknown. Drawn in $\\left(\\beta_{j}, \\mu_{j}\\right)$ diagram. A line through $\\left(0, r_{f}\\right)$ in that diagram With $$ \\begin{aligned} \u0026\\sigma_{i M}=\\rho_{iM} \\cdot \\sigma_{i} \\sigma_{M}\\ \u0026\\beta_{i}=\\frac{\\sigma_{i M}}{\\sigma_{M}^{2}}=\\frac{\\rho_{iM} \\cdot \\sigma_{i} \\sigma_{x}}{\\sigma_{m}^{2}}=\\frac{\\rho_{iM} \\cdot \\sigma_{i}}{\\sigma_{M}} \\end{aligned} $$ Rewrite CML \u0026 SML $$ \\begin{aligned} \u0026S M L: \\quad E\\left(\\tilde{r}{i}\\right)=r{f}+\\frac{E\\left(\\tilde{r}{M}\\right)-r{f}}{\\sigma_{M}} \\rho_{i M} \\cdot \\sigma_{i}\\ \u0026C M L: \\quad E\\left(\\tilde{r}_{i}\\right)=r_{f}+\\frac{E\\left(\\tilde{r_M}\\right)-r_{f}}{\\sigma_{M}}\\sigma_i \\end{aligned} $$ Thus an asset lies on both CML \u0026 SML i.f.f. $\\rho_{iM}=1$. While any asset lies a SML. Therefore, a asset lies on CML i.f.f. it’s market portfolio or the combination of market portfolio \u0026 risk-free asset. ","date":"0001-01-01","objectID":"/6.-capm/:2:4","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.2.5 Interpretation $$ E\\left(\\tilde{r}{j}\\right)=r{f}+\\frac{\\sigma_{j M}}{\\sigma_{M}^{2}}\\left[E\\left(r_{M}^{\\prime}\\right)-r_{f}\\right] $$ The expected rate of return on any asset depends on only one characteristic of that asset, namely its rate of return’s covariance with the rate of return on the market portfolio. The expected rate of return is equal to the risk free interest rate plus a term which depends on a measure of risk. (Higher risk means higher expected rate of return.) The relevant measure of risk is the asset’s beta. This is multiplied with the expected excess rate of return on the market portfolio. Risk measure depends on covariance because the covariance determines how much that asset will contribute to the risk of the agent’s portfolio. This is true for any agent, since all hold the same risky portfolio. There are two complementary ways of interpreting this result. Both bring us back to the theme of diversification emphasized by MPT. Both take us a step further, by emphasizing as well the idea of aggregate risk, which cannot be “diversified away,” and idiosyncratic risk, which can be diversified away. Aggregate risk \u0026 idiosyncratic risk The first approach uses the CAPM equation in its original form $$ E\\left(\\tilde{r}{j}\\right)=r{f}+\\frac{\\sigma_{j M}}{\\sigma_{M}^{2}}\\left[E\\left(\\tilde{r}_{M}\\right)-r_{f}\\right] $$ together with the definition of correlation, which implies $$ \\rho_{j M}=\\frac{\\sigma_{j M}}{\\sigma_{j} \\sigma_{M}} $$ Then the CAPM relationship $$ E\\left(\\tilde{r}_{j}\\right)=r_{f}+\\left[\\frac{E\\left(\\tilde{r}_{M}\\right)-r_{f}}{\\sigma_{M}}\\right] \\rho_{j M} \\sigma_{j} $$ The term inside brackets is the equilibrium price of risk. And since the correlation lies between -1 and 1 , the term $\\rho_{j M} \\sigma_{j}$ satisfying $$ \\rho_{j M} \\sigma_{j} \\leq \\sigma_{j} $$ represents the “portion” of the total risk $\\sigma_{j}$ in asset $j$ that is correlated with the market return. \r The idiosyncratic risk in asset j , that is, the portion that is uncorrelated with the market return, can be diversified away by holding the market portfolio. Since this risk can be freely shed through diversification, it is not “priced.” Hence, according to the CAPM, risk in asset j is priced only to the extent that it takes the form of aggregate risk that, because it is correlated with the market portfolio, cannot be diversified away. Thus, according to the CAPM: Only assets with random returns that are positively correlated with the market return earn expected returns above the risk free rate. They must, in order to induce investors to take on more aggregate risk. Assets with returns that are uncorrelated with the market return have expected returns equal to the risk free rate, since their risk can be completely diversified away. Assets with negative betas——that is, with random returns that are negatively correlated with the market return—— have expected returns below the risk free rate! For these assets, $E(\\tilde{r_j}) - r_f \u003c 0$ is like an “insurance premium” that investors will pay in order to insulate themselves from aggregate risk. Statistical interpretation The second approach to interpreting the CAPM uses $$ E\\left(\\tilde{r}{j}\\right)=r{f}+\\beta_{j}\\left[E\\left(\\tilde{r}_{M}\\right)-r_{f}\\right] $$ together with the definition of $$ \\beta_{j}=\\frac{\\sigma_{j M}}{\\sigma_{M}^{2}} $$ Consider a statistical regression of the random return $\\tilde{r}_{j}$ on asset $j$ on a constant and the market return $\\tilde{r}_{M}$ $$ \\tilde{r}_{j}=\\alpha+\\beta_{j} \\tilde{r_n}+\\varepsilon_{j} $$ This regression breaks the variance of $\\tilde{r}_{j}$ down into two “orthogonal” (uncorrelated) components: The component $\\beta_{j} r_{M}$ that is systematically related to variation in the market return. The component $\\varepsilon_{j}$ that is not. The slope coefficient in a linear regression is $$ \\beta_{j}=\\frac{\\sigma_{j M}}{\\sigma_{M}^{2}} $$ the same “beta” as in the CAPM! But this is not an accident: to the contrary, it restates t","date":"0001-01-01","objectID":"/6.-capm/:2:5","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.3 Application of CAPM ","date":"0001-01-01","objectID":"/6.-capm/:3:0","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.3.1 Valuing Risky Cash Flows Let $\\tilde{C}{t+1}$ denote a random payoff to be received at time $t+1$ (“one period from now” and let $P{t}^{C}$ denote its price at time $t$ (“today.\") If $\\tilde{C}{t+1}$ was known in advance, that is, if the payoff were riskless, we could find its value by discounting it at the risk free rate: $$ P{t}^{C}=\\frac{\\tilde{C}{t+1}}{1+r{f}} $$ But when $\\tilde{C}{t+1}$ is truly random, we need to find its expected value $E\\left(\\tilde{C}{t+1}\\right)$ and then “penalize” it for its riskiness either by discounting at a higher rate $$ P_{t}^{C}=\\frac{E\\left(\\tilde{C}_{t+1}\\right)}{1+r_{f}+\\psi} $$ or by reducing its value more directly $$ P_{t}^{C}=\\frac{E\\left(\\tilde{C}_{t+1}\\right)-\\Psi}{1+r_{f}} $$ The CAPM can help us identify the appropriate risk premium $\\psi$ or $\\Psi$. risk premium $\\psi$ Our previous analysis suggests that, broadly speaking, the risk premium implied by the CAPM will somehow depend on the extent to which the random payoff $\\tilde{C}_{t+1}$ is correlated with the return on the market portfolio. To apply the CAPM to this valuation problem, we can start by observing that with price $P_{t}^{C}$ today and random payoff $\\tilde{C}_{t+1}$ one period from now, the return on this asset or investment project is defined by $$ 1+\\tilde{r}_{C}=\\frac{\\tilde{C}_{t+1}}{P_{t}^{C}} $$ or $$ \\tilde{r}_{C}=\\frac{\\tilde{C}_{t+1}-P_{t}^{C}}{P_{t}^{C}} $$ where the notation $\\tilde{r}_{C}$ emphasizes that this return, like the future cash flow itself, is risky. Now the CAPM implies that the expected return $E\\left(\\tilde{r}{C}\\right)$ must satisfy $$ E\\left(\\tilde{r}{\\mathrm{C}}\\right)=r_{f}+\\beta_{C}\\left[E\\left(r_{M}^{\\prime}\\right)-r_{f}\\right] $$ where the project’s beta depends on the covariance of its return with the market return: $$ \\beta_{C}=\\frac{\\sigma_{C M}}{\\sigma_{M}^{2}} $$ This is what takes skill: with an existing asset, one can use data on the past correlation between its return and the market return to estimate beta. With a totally new project that is just being planned, a combination of experience, creativity, and hard work is often needed to choose the right value for $\\beta_{C}$. But once a value for $\\beta_{C}$ is determined, we can use $$ E\\left(\\tilde{r}_{C}\\right)=r_{f}+\\beta_{C}\\left[E\\left(\\tilde{r}_{M}\\right)-r_{f}\\right] $$ together with the definition of the return itself $$ \\tilde{r}_{C}=\\frac{\\tilde{C}_{t+1}}{P_{t}^{C}}-1 $$ to write $$ E\\left(\\frac{\\tilde{C}_{t+1}}{P_{t}^{C}}-1\\right)=r_{f}+\\beta_{C}\\left[E\\left(r_{M}\\right)-r_{f}\\right] $$ which implies $$ \\begin{aligned} \\left(\\frac{1}{P_{t}^{C}}\\right) E\\left(\\tilde{C}_{t+1}\\right) \u0026=1+r_{f}+\\beta_{C}\\left[E\\left(r_{M}\\right)-r_{f}\\right] \\ P_{t}^{C} \u0026=\\frac{E\\left(\\tilde{C}_{t+1}\\right)}{1+r_{f}+\\beta_{C}\\left[E\\left(r_{M}\\right)-r_{f}\\right]} \\end{aligned} $$ Right-hand side is “expected present value”, with Risk-adjusted discount rate(RADR). Risk-adjustment again depends on $\\left(E\\left(\\tilde{r}_{M}\\right)-r_{f}\\right) / \\sigma_{M}^{2}$ and covariance. $$ E\\left(\\tilde{r}_{C}\\right)=r_{f}+\\beta_{C}\\left[E\\left(\\tilde{r}_{M}\\right)-r_{f}\\right] $$ the CAPM implies a risk premium of $$ \\psi=\\beta_{C}\\left[E\\left(\\tilde{r_{M}}\\right)-r_{f}\\right] $$ depends critically on the covariance between the return on the risky project and the return on the market portfolio. risk premium $\\Psi$ Alternatively, $$ \\left(\\frac{1}{P_{t}^{C}}\\right) E\\left(\\tilde{C}_{t+1}\\right)=1+r_{f}+\\beta_{C}\\left[E\\left(\\tilde{r}_{M}\\right)-r_{f}\\right] $$ can be rewritten as $$ P_{t}^{C}=\\frac{E\\left(\\tilde{C}_{t+1}\\right)-P_{t}^{C} \\beta_{C}\\left[E\\left(\\tilde{r_{M}}\\right)-r_{f}\\right]}{1+r_{f}} $$ indicating that the CAPM also implies $$ \\Psi=P_{t}^{C} \\beta_{C}\\left[E\\left(\\tilde{r_{M}}\\right)-r_{f}\\right] $$ which, again as expected, depends critically on the covariance between the return on the risky project and the return on the market portfolio. $$ P_{t}^{C}=\\frac{E\\left(\\tilde{C}_{t+1}\\right)-P_{t}^{C} \\beta_{C}\\left[E\\left(\\ti","date":"0001-01-01","objectID":"/6.-capm/:3:1","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.4 Pros and Cons of CAPM ","date":"0001-01-01","objectID":"/6.-capm/:4:0","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.4.1 empirical shortcomings An enormous literature is devoted to empirically testing the CAPM’s implications. Although results are mixed, studies have shown that when individual portfolios are ranked according to their betas, expected returns tend to line up as suggested by the theory. A famous article that presents results along these lines is by Eugene Fama (Nobel Prize 2013) and James MacBeth, “Risk, Return, and Equilibrium,” Journal of Political Economy Vol.81 (May-June 1973),pp.607-636. Early work on the MPT, the CAPM, and econometric tests of the efficient markets hypothesis and the CAPM is discussed extensively in Eugene Fama’s 1976 textbook, Foundations of Finance. More recent evidence against the CAPM’s implications is presented by Eugene Fama and Kenneth French, “Common Risk Factors in the Returns on Stocks and Bonds,” Journal of Financial Economics Vol.33 (February 1993): pp.3-56. This paper shows that equity shares in small firms and in firms with high book (accounting) to market value have expected returns that differ strongly from what is predicted by the CAPM alone. Quite a bit of recent research has been directed towards understanding the source of these “anomalies”. ","date":"0001-01-01","objectID":"/6.-capm/:4:1","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.4.2 Pros Despite some empirical shortcomings, however, the CAPM quite usefully deepens our understanding of the gains from diversification. Related, the CAPM alerts us to the important distinction between idiosyncratic risk, which can be diversified away, and aggregate risk, which cannot. Like MPT, the CAPM must rely on one of the two strong assumptions��either quadratic utility or normally-distributed returns|that justify mean-variance utility. And while the CAPM is an equilibrium theory of asset pricing, it stops short of linking asset returns to underlying economic fundamentals. ","date":"0001-01-01","objectID":"/6.-capm/:4:2","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"6.4.3 APT \u0026 ADT These last two points motivate our interest in other asset pricing theories, which are less restrictive in their assumptions and/or draw closer connections between asset prices and the economy as a whole. Arbitrage Pricing Theory, to which we will turn our attention next, yields many of the same implications as the CAPM, but requires less restrictive assumptions about preferences and the distribution of asset returns. The equilibrium version of Arrow-Debreu theory draws links between asset prices and the economy that are only implicit in the CAPM. ","date":"0001-01-01","objectID":"/6.-capm/:4:3","tags":null,"title":"","uri":"/6.-capm/"},{"categories":null,"content":"7. Arbitrage Pricing Theory ##7.1 Overview The Arbitrage Pricing Theory (APT) was developed by Stephen Ross(US, b.1944) in the mid-1970s. Stephen Ross, “The Arbitrage Theory of Capital Asset Pricing,” Journal of Economic Theory Vol.13 (December 1976): pp.341-360. The idea behind factor models is to summarize all relevant economic variables in a set of indicators that are the main factors explaining the security markets returns. The assumption is that few such factors are sufficient to characterize the aggregate component of securities returns, and any remaining uncertainty in security returns can be attributed to firm-specific (or idiosyncratic) shocks. ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:0:0","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.1.1 APT \u0026 CAPM The APT bears a close resemblance to the CAPM. In fact, a special case of the APT implies a relationship between expected returns on arbitrary, but well-diversified, portfolios and the return on the market portfolio that is identical to the relationship implied by the CAPM. But the APT allows for a more flexible, or general, depiction of aggregate risk than does the CAPM. And, as its name suggests,the APT is a no-arbitrage theory of asset pricing, that does not require the strong assumptions imposed by the CAPM as an equilibrium theory. The weakness of the APT——the cost, if you will, of its added flexibility and generality——is that it applies only to “well-diversified portfol3ios” and “most” individual securities: there is no guarantee that it applies to all individual assets. ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:0:1","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.2 The Single-Factor Model The simplest version of the APT is also the one that can best be thought of as the “no-arbitrage” variant of the CAPM. There is single factor that is sufficient to capture the aggregate component of the ex-post return on individual asset $j$ as follows: $$ \\tilde{r}{j}=E\\left(\\tilde{r}{j}\\right)+\\beta_{j} \\tilde{f}+\\varepsilon_{j} $$ where $E(\\tilde{f})=0, E\\left(\\varepsilon_{j}\\right)=0, \\operatorname{Cov}\\left(\\tilde{f}, \\varepsilon_{j}\\right)=0$ and $\\operatorname{Cov}\\left(\\varepsilon_{j}, \\varepsilon_{k}\\right)=0$ for all $j=1,2, \\ldots, J$ and $k=1,2, \\ldots, J$ with $k \\neq j$. Deviations from the expected return on security are explained by on unexpected movement in the common factor $\\tilde{f}$ (aggregate/systemic shock) $\\varepsilon_{j}$ idiosyncratic (or asset-specific) shock ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:1:0","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.2.1 The Market Model If we assert that the common component of unanticipated movements in individual security returns is explained by unanticipated movements in the return on a broad index of securities, such as the S\u0026P500 index, then we have the market model, which is a variant of the single-factor model according to which the random return $\\tilde{r}{j}$ on each asset $j=1,2, \\ldots, J$ is related to the random return $\\tilde{r}{M}$ on the market portfolio via $$ \\tilde{r}{j}=\\alpha{j}+\\beta_{j}\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right]+\\varepsilon_{j} $$ where $E\\left(\\varepsilon_{j}\\right)=0, \\operatorname{Cov}\\left(\\tilde{r}_{M}, \\varepsilon_{j}\\right)=0$ and $\\operatorname{Cov}\\left(\\varepsilon_{j}, \\varepsilon_{k}\\right)=0$ for all $j=1,2, \\ldots, J$ and $k=1,2, \\ldots, J$ with $k \\neq j$. The market model is called a single-factor model since it breaks the random return on each asset down into two uncorrelated or “orthogonal” components: $\\beta_{j}\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right]$ which depends on the model’s single aggregate variable or “factor,” in this case the market return. $\\varepsilon_{j}$ which is a purely idiosyncratic effect. The parameter’s $ \\alpha_j $ and j can be estimated through a statistical regression of each asset’s return on the market return, imposing the additional statistical assumption that the regression error $ \\varepsilon_j $ is uncorrelated across assets as well as with the market portfolio. As we will see, it is partly this added assumption that $ \\varepsilon_j $ is purely idiosyncratic that gives the APT its \\bite.\" Before moving on to see how the APT makes use of the market model, let’s take note of two other purposes that the market model can serve. Estimation of $\\beta$ First, since $$ \\tilde{r}{j}=\\alpha{j}+\\beta_{j}\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right]+\\varepsilon_{j} $$ can be written as $$ \\tilde{r}_{j}=\\bar{\\alpha}_{j}+\\beta_{j} \\tilde{r}_{M}+\\varepsilon_{j} $$ where $$ \\bar{\\alpha}_{j}=\\alpha_{j}-\\beta_{j} E\\left(\\tilde{r}_{M}\\right) $$ the regressions from the market model yield slope coefficients $$ \\beta_{j}=\\frac{\\operatorname{Cov}\\left(\\tilde{r}_{j}, \\tilde{r}_{M}\\right)}{\\sigma_{M}^{2}} $$ that can be viewed as estimates of the CAPM beta for asset $j$ Second, since $$ \\tilde{r}{j}=\\alpha{j}+\\beta_{j}\\left[\\tilde{r_{M}}-E\\left(\\tilde{r}_{M}\\right)\\right]+\\varepsilon_{j} $$ implies $$ \\begin{aligned} E\\left(\\tilde{r}_{j}\\right) \u0026=\\alpha_{j} \\ \\sigma_{j}^{2} \u0026=E\\left(\\left[\\tilde{r}_{j}-E\\left(\\tilde{r}_{j}\\right)\\right]^{2}\\right) \\ \u0026=E\\left(\\left[\\beta_{j}\\left[r_{\\tilde{M}}-E\\left(\\tilde{r}_{M}\\right)\\right]+\\varepsilon_{j}\\right]^{2}\\right) \\ \u0026=\\beta_{j}^{2} E\\left(\\left[r_{M}-E\\left(\\tilde{r}_{M}\\right)\\right]^{2}\\right)+2 \\beta_{j} E\\left(\\left[r_{M}^{2}-E\\left(r_{M}^{2}\\right)\\right] \\varepsilon_{j}\\right)+E\\left(\\varepsilon_{j}^{2}\\right) \\ \u0026=\\beta_{j}^{2} \\sigma_{M}^{2}+2 \\beta_{j} \\operatorname{Cov}\\left(\\tilde{r}_{M}, \\varepsilon_{j}\\right)+\\sigma_{\\varepsilon_{j}}^{2} \\ \u0026=\\beta_{j}^{2} \\sigma_{M}^{2}+\\sigma_{\\varepsilon_{j}}^{2} \\end{aligned} $$ Similarly, the market model implies that for any two assets $j$ and $k \\neq j$. $$ \\begin{aligned} \\operatorname{cov}\\left(\\tilde{r}_{j}, \\tilde{r}_{k}\\right) \u0026=E\\left(\\left[\\tilde{r}_{j}-E\\left(\\tilde{r}_{j}\\right)\\right]\\left[\\tilde{r}_{k}-E\\left(\\tilde{r}_{k}\\right)\\right]\\right) \\ \u0026=E\\left(\\left[\\beta_{j}\\left[r_{M}-E(r \\tilde{M})\\right]+\\varepsilon_{j}\\right]\\left[\\beta_{k}\\left[r_{M}-E\\left(r_{M}\\right)\\right]+\\varepsilon_{k}\\right]\\right) \\ \u0026=\\beta_{j} \\beta_{k} E\\left(\\left[r_{M}-E\\left(r_{M}\\right)\\right]^{2}\\right)+\\beta_{j} E\\left(\\left[r_{M}-E\\left(r_{M}\\right)\\right] \\varepsilon_{k}\\right) \\ \u0026+\\beta_{k} E\\left(\\left[r_{M}-E\\left(r_{M}\\right)\\right] \\varepsilon_{j}\\right)+E\\left(\\varepsilon_{j} \\varepsilon_{k}\\right) \\ \u0026=\\beta_{j} \\beta_{k} \\sigma_{M}^{2}+\\beta_{j} \\operatorname{cov}\\left(r_{M}, \\varepsilon_{k}\\right)+\\beta_{k} \\operatorname{cov}\\left(r_{M}, \\v","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:1:1","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.3 The Assumptions of APT The APT starts by assuming that asset returns are governed by a factor model such as the market model. To derive implications for expected returns, the APT makes two additional assumptions: There are enough individual assets to create many well-diversified portfolios. Investors act to eliminate all arbitrage opportunities across all well-diversified portfolios(No Arbitrage Conditions). ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:2:0","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.3.1 Well diversified portfolios Two Assets Case Before moving all the way to well-diversified portfolios, let’s consider an “only somewhat” diversified portfolio. Consider, in particular, a portfolio consisting of only two assets, $j$ and k, both of which have returns described by the market model. Let $w$ be the share of the portfolio allocated to asset $j$ and $1-w$ the share allocated to asset $k$. $$ \\begin{aligned} \\tilde{r}{j} \u0026=E\\left(\\tilde{r}{j}\\right)+\\beta_{j}\\left[r_{M}-E\\left(r_{M}^{-}\\right)\\right]+\\varepsilon_{j} \\ \\tilde{r}_{k} \u0026=E\\left(\\tilde{r}_{k}\\right)+\\beta_{k}\\left[\\tilde{r}_{M}-E\\left(r_{M}\\right)\\right]+\\varepsilon_{k} \\end{aligned} $$ imply that the return $\\tilde{r}_{w}$ on the portfolio is $$ \\begin{array}{l} \\tilde{r_{w}}\u0026=w E\\left(\\tilde{r}_{j}\\right)+(1-w) E\\left(\\tilde{r}_{k}\\right)\\ \u0026+w \\beta_{j}\\left[\\tilde{r_{M}}-E\\left(\\tilde{r_{M}}\\right)\\right]+(1-w) \\beta_{k}\\left[\\tilde{r_{M}}-E\\left(\\tilde{r_{M}}\\right)\\right] \\ \u0026+w \\varepsilon_{j}+(1-w) \\varepsilon_{k} \\ \u0026=E\\left(\\tilde{r}_{w}\\right)+\\beta_{w}\\left[\\tilde{r_{M}}-E\\left(\\tilde{r_{M}}\\right)\\right]+\\varepsilon_{w} \\end{array} $$ The first implication is that the expected return on the portfolio is just a weighted average of the expected returns on the individual assets, with weights corresponding to those in the portfolio itself: $$ E(\\tilde{r_{w}})=w E\\left(\\tilde{r}_{j}\\right)+(1-w) E\\left(\\tilde{r}_{k}\\right) $$ The second implication is that the portfolio’s beta is the same weighted average of the betas of the individual assets: $$ \\beta_{w}=w \\beta_{j}+(1-w) \\beta_{k} $$ The third implication is subtle but very important. We can see that the idiosyncratic component of the portfolio’s return is a weighted average of the idiosyncratic components of the individual asset returns: $$ \\varepsilon_{w}=w \\varepsilon_{j}+(1-w) \\varepsilon_{k} $$ But the variance of the idiosyncratic component of the portfolio’s return is not a weighted average of the variances of the idiosyncratic components of the individual asset returns: $$ \\begin{aligned} \\sigma_{\\varepsilon_{w}}^{2} \u0026=E\\left(\\varepsilon_{w}^{2}\\right)=E\\left(\\left[w \\varepsilon_{j}+(1-w) \\varepsilon_{k}\\right]^{2}\\right) \\ \u0026=w^{2} E\\left(\\varepsilon_{j}^{2}\\right)+2 w(1-w) E\\left(\\varepsilon_{j} \\varepsilon_{k}\\right)+(1-w)^{2} E\\left(\\varepsilon_{k}^{2}\\right) \\ \u0026=w^{2} \\sigma_{\\varepsilon_{j}}^{2}+(1-w)^{2} \\sigma_{\\varepsilon_{k}}^{2} \\end{aligned} $$ For example, if the individual returns have idiosyncratic components with equal variances $$ \\sigma_{\\varepsilon_{j}}^{2}=\\sigma_{\\varepsilon_{k}}^{2}=\\sigma^{2} $$ and the portfolio gives equal weight to the two assets $w=1-w=1 / 2,$ then $$ \\sigma_{\\varepsilon_{w}}^{2}=(1 / 2)^{2} \\sigma^{2}+(1 / 2)^{2} \\sigma^{2}=(1 / 2) \\sigma^{2} $$ Even with just two assets, the portfolio’s idiosyncratic risk is cut in half. Of course, this is just a special case of the gains from diversification exploited by MPT. But the APT pushes the idea to its logical and mathematical limits. N Assets Case Let’s head in the same direction, by considering a portfolio with a large number $N$ of individual assets. Let $w_{i}, i=1,2, \\ldots, N,$ denote the share of the portfolio allocated to asset $i$. And consider, in particular, the “equal weighted” case, where $$ w_{i}=1 / N $$ for all $i=1,2, \\ldots, N$ Since each of the individual asset returns are generated by the market model, the return on this equal-weighted portfolio will be $$ \\tilde{r_{w}}=E(\\tilde{r_{w}})+\\beta_{w}\\left[r_{M}^{\\tilde{r}}-E\\left(r_{M}^{\\prime}\\right)\\right]+\\varepsilon_{w} $$ where the expected return on the equal-weighted portfolio is just the average of the expected returns on the individual assets: $$ E\\left(\\tilde{r}_{w}\\right)=\\Sigma_{i=1}^{N} w_{i} E\\left(\\tilde{r}_{i}\\right)=(1 / N) \\Sigma_{i=1}^{N} E\\left(\\tilde{r}_{i}\\right) $$ the equal-weighted portfolio’s beta is just the average of the individual assets' betas: $$ \\beta_{w}=\\Sigma_{i=1}^{N} w_{i} \\beta_{i}=(1 / N) \\Sigm","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:2:1","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.4 Proof APT with Single-Factor Model ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:3:0","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.4.1 No-arbitrage conditions An arbitrage opportunity exists if an investor can earn pro\u000cts without exposing him or herself to risk and making any net investment. In a well functioning capital market such opportunities should not exist since market participants would instantaneously take advantage of them and thus move prices so that they disappear. Law of one price: two assets which are equivalent in all economically relevant aspects must have the same market price. No-arbitrage conditions such as this one underpin much of modern finance theory. Let us turn to the implications of no-arbitrage in the context of the APT. ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:3:1","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.4.2 Same Beta: Proposition 1 Our first no-arbitrage argument applies to well-diversified portfolios with the same betas. 【Proposition (1)】The absence of arbitrage opportunities requires well-diversified portfolios with the same betas to have the same expected returns. Proof: To see why this proposition must be true, consider two well-diversified portfolios, one with $$ \\tilde{r}{w}^{1}=E\\left(\\tilde{r}{w}\\right)+\\beta_{w}\\left[r_{M}^{\\prime}-E\\left(r_{M}^{\\prime}\\right)\\right] $$ and the other with $$ \\tilde{r}_{w}^{2}=E\\left(\\tilde{r}_{w}^{\\prime}\\right)+\\Delta+\\beta_{w}\\left[r_{M}^{2}-E\\left(r_{M}^{\\prime}\\right)\\right] $$ where $\\triangle\u003e0 .$ These portfolios have the same beta, but the second has a higher expected return. $$ \\begin{array}{c} \\tilde{r}_{w}^{1}=E\\left(\\tilde{r}_{w}\\right)+\\beta_{w}\\left[r_{M}^{\\tau}-E\\left(r_{M}^{\\prime}\\right)\\right] \\ \\tilde{r}_{w}^{2}=E\\left(r_{w}^{\\prime}\\right)+\\Delta+\\beta_{w}\\left[r_{M}^{\\prime}-E\\left(r_{M}^{\\prime}\\right)\\right] \\end{array} $$ Now consider a strategy of taking a “long position” worth $x$ in portfolio 2 and a “short position” worth $-x$ in portfolio 1. This strategy is self-financing, in that it requires “no money down” at $t=0$ But the strategy yields a payoff at $t=1$ of $$ \\begin{aligned} x\\left(1+\\tilde{r}{w}^{2}\\right)-x\\left(1+\\tilde{r}{w}^{1}\\right) \u0026=x E\\left(\\tilde{r}{w}\\right)+x \\Delta+x \\beta{w}\\left[r \\tilde{m}-E\\left(\\tilde{r}{M}\\right)\\right] \\ \u0026-x E\\left(\\tilde{r}{w}\\right)-x \\beta_{w}\\left[r_{M}^{2}-E\\left(r_{M}\\right)\\right] \\ \u0026=x \\Delta\u003e0 \\end{aligned} $$ Thus, $\\Delta\u003e0$ is inconsistent with the absence of arbitrage opportunities. Similarly, $\\Delta\u003c0$ is also inconsistent with the absence of arbitrage opportunities. Then, $\\Delta = 0$ ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:3:2","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.4.3 Different beta: Proposition 2 Suppose that the risk-free rate is 4% and that we have the following situation for well diversified portfolios $ A $ and $ C: $ $$ \\begin{array}{|l|c|c|} \\hline portfolio \u0026 beta \u0026 expected return \\ \\hline \\mathrm{A} \u0026 1 \u0026 0.14 \\ \\mathrm{C} \u0026 0.5 \u0026 0.08 \\ \\hline \\end{array} $$ Is this consistent with no arbitrage? Figure: Returns as a function of the systematic factor \r The expected return on $ C $ means that there is an arbitrage opportunity. We can construct a portfolio that has the same beta as $ C $ and a higher return, which we have already seen violates no arbitrage. $ \\mathrm{D} $ is such an arbitrage portfolio, it is a portfolio invested half in the risk-free asset (with $ \\beta=0 $ ) and half in the well diversified portfolio A. Thus $ \\beta_{D}=0.5 * 0+0.5 * 1=0.5 $ and $$ E\\left(\\tilde{r}_{D}\\right)=0.5 r_{f}+0.5 E\\left(\\tilde{r}_{A}\\right)=0.09 $$ Hence, **no-arbitrage implies that well-diversified portfolios with different factor loadings must have expected returns proportional to their betas**. This means that they plot on a straight line in the expected return - beta diagram. 【Proposition (2)】The absence of arbitrage opportunities requires the expected returns on all well-diversified portfolios to satisfy $$ \\qquad E\\left(\\tilde{r}{w}\\right)=\\lambda{0}+\\lambda_{1} \\beta_{w} $$ for constants $\\lambda_{0}$ and $\\lambda_{1}$ that are the same for all portfolios. \r The APT implies that expected returns on well-diversified portfolios line up along $E\\left(\\tilde{r}{w}\\right)=\\lambda{0}+\\lambda_{1} \\beta_{w}$. Proof: Consider, therefore, a third well-diversified portfolio, with $$ \\tilde{r}{w}^{3}=E\\left(\\tilde{r}{w}^{3}\\right)+\\beta_{w}\\left[r_{M}^{\\prime}-E\\left(\\tilde{r}_{M}\\right)\\right] $$ and $\\beta_{w}$ different from zero and one. We need to show that $$ E\\left(\\tilde{r}_{w}^{3}\\right)=\\lambda_{0}+\\lambda_{1} \\beta_{w} $$ so, paralleling the argument from before, suppose instead that $$ E\\left(\\tilde{r}_{w}^{3}\\right)=\\lambda_{0}+\\lambda_{1} \\beta_{w}+\\Delta $$ where $\\triangle\u003e0$ \r If $E\\left(\\tilde{r}{w}^{3}\\right)$ lies above the line $\\lambda{0}+\\lambda_{1} \\beta_{w},$ then a strategy that takes a long position in portfolio 3 and short positions in portfolios 1 and 2 will constitute an arbitrage opportunity. $$ \\begin{aligned} \\tilde{r}_{w}^{1} \u0026=\\lambda_{0} \\ \\tilde{r}_{w}^{2} \u0026=\\lambda_{0}+\\lambda_{1}+\\left[r_{M}-E\\left(\\tilde{r}_{M}\\right)\\right] \\ \\tilde{r}_{w}^{3} \u0026=\\lambda_{0}+\\lambda_{1} \\beta_{w}+\\Delta+\\beta_{w}\\left[r_{M}^{2}-E\\left(r_{M}^{*}\\right)\\right] \\end{aligned} $$ The no-arbitrage argument requires two steps. First, form a fourth well-diversified portfolio from the first two, by allocating the shares $1-\\beta_{w}$ to portfolio 1 and $\\beta_{w}$ to portfolio $2 .$ This portfolio has $$ \\begin{aligned} \\tilde{r}_{w}^{4} \u0026=\\left(1-\\beta_{w}\\right) \\tilde{r}_{w}^{1}+\\beta_{w} \\tilde{r}_{w}^{2} \\ \u0026=\\left(1-\\beta_{w}\\right) \\lambda_{0}+\\beta_{w}\\left(\\lambda_{0}+\\lambda_{1}\\right)+\\beta_{w}\\left[\\tilde{r}_{M}-E\\left(r_{M}^{2}\\right)\\right] \\ \u0026=\\lambda_{0}+\\lambda_{1} \\beta_{w}+\\beta_{w}\\left[r_{M}-E\\left(r_{M}\\right)\\right] \\end{aligned} $$ Second, observe that portfolio 4 has the same beta, but a lower expected return, than portfolio $3 .$ Proposition 1 implies that this is inconsistent with the absence of arbitrage. Since portfolios 3 and 4 have the same beta, a strategy that allocates x to portfolio 3 and -x to portfolio 4 is self-financing at t = 0 and yields a payoff $x\\Delta \u003e 0$ at t = 1. This confirms that $\\Delta \u003e 0$ is inconsistent with the absence of arbitrage. Similarly, $\\Delta \u003c0$ is also inconsistent with the absence of arbitrage. Hence, if $$ \\begin{aligned} \\tilde{r}{w}^{1} \u0026=\\lambda{0} \\ \\tilde{r}{w}^{2} \u0026=\\lambda{0}+\\lambda_{1}+\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right] \\ \\tilde{r}_{w}^{3} \u0026=\\lambda_{0}+\\lambda_{1} \\beta_{w}+\\Delta+\\beta_{w}\\left[r_{M}^{\\prime}-E\\left(r_{M}^{\\prime}\\right)\\right] \\end{aligned} $$ then $\\triangl","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:3:3","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.4.4 Interpretation Interpretation of $\\lambda_0$ To interpret $\\lambda_{0}$, consider a well-diversified portfolio, call it portfolio 1 with $\\beta_{w}=0,$ so that $$ \\tilde{r}_{w}^{1}=E\\left(\\tilde{r}_{w}^{1}\\right)+0 \\times\\left[r_{M}^{\\prime}-E\\left(r_{M}^{1}\\right)\\right]=E\\left(\\tilde{r}_{w}^{1}\\right) $$ But if the return on portfolio 1 always equals its own expected return, either: Portfolio 1 is a well-diversified portfolio that forms a “synthetic” risk-free asset or portfolio 1 consists entirely of risk-free assets. Either way, its return must equal the risk-free rate: $$ \\tilde{r}{w}^{1}=E\\left(\\tilde{r}{w}^{1}\\right)=r_{f} $$ But proposition 2 requires the expected return on portfolio 1 to equal $\\lambda_{0}$ as well: $$ E\\left(\\tilde{r}_{w}^{1}\\right)=\\lambda_{0}+\\lambda_{1} \\times 0=\\lambda_{0} $$ Together with $$ E\\left(\\tilde{r}_{w}^{1}\\right)=r_{f} $$ this observation tells us that $$ \\lambda_{0}=r_{f} $$ Interpretation of $\\lambda_1$ Next, consider a second well-diversified portfolio, call it portfolio 2 with $\\beta_{w}=1,$ so that $$ \\tilde{r}_{w}^{2}=E\\left(\\tilde{r}_{w}^{2}\\right)+1 \\times\\left[\\tilde{r}_{M}-E\\left(r_{M}\\right)\\right] $$ Portfolio 2 is well diversified and has the same beta as the market portfolio, so it must also have the same expected return as the market portfolio( by proposition 1 ). Hence, $$ \\begin{aligned} \\tilde{r}_{w}^{2} \u0026=E\\left(\\tilde{r}_{w}^{2}\\right)+\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right] \\ \u0026=E\\left(\\tilde{r}_{M}\\right)+\\left[r_{M}^{\\prime}-E\\left(\\tilde{r}_{M}\\right)\\right] \\ \u0026=\\tilde{r}_{M} \\end{aligned} $$ Thus either Portfolio 2 is a well-diversified portfolio that always has the same return as the market portfolio or portfolio 2 actually is the market portfolio In any case, proposition 2 requires the expected return on portfolio 2 to be given by $$ E\\left(\\tilde{r}{w}^{2}\\right)=\\lambda{0}+\\lambda_{1} \\times 1=\\lambda_{0}+\\lambda_{1} $$ Together with $\\tilde{r}_{w}^{2}=\\tilde{r}_{M}$ and $\\lambda_{0}=r_{f}$, we have $$ \\lambda_{1}=E\\left(\\tilde{r}_{M}\\right)-r_{f} $$ Since $\\lambda_{0}=r_{f}$ and $\\lambda_{1}=E\\left(r_{M}^{\\prime}\\right)-r_{f},$ the line $E\\left(r_{w}^{\\prime}\\right)=\\lambda_{0}+\\lambda_{1} \\beta_{w}$ is the CAPM’s security market line. $$ E\\left(\\tilde{r}_{w}\\right)=\\lambda_{0}+\\lambda_{1} \\beta_{w}=r_{f}+\\beta_{w}\\left[E\\left(\\tilde{r}_{M}\\right)-r_{f}\\right] $$ This is the same relationship implied by the CAPM. But the APT derives this relationship without assuming that utility is quadratic and without assuming that returns are normally distributed (although the returns do have to behave in accordance with the market model). On the other hand, the APT holds only for well-diversified portfolios, not necessarily for individual assets. Hence, the APT’s added generality comes at a cost, in terms of less widely-applicable results. One might argue, as well, that the APT must apply “most” individual securities, since if a large number of individual assets violated the APT relationship, it would be possible to construct a well-diversified portfolio of those assets and arbitrage away the higher or lower expected returns. ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:3:4","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.5 Proof APT with Multifactor Models Assumption: well-diversified portfolios no-arbitrage conditions Consider a simplified version of the Fama-French model with two factor return on the market portfolio the return on a portfolio that takes long positions in “value” stocks (small, overlooked, or old-fashioned companies) and short positions in “growth” stocks (bigger, more popular, or newer companies) It implies that the return on each individual asset $j=1,2, \\ldots, J$ is $$ \\tilde{r}{j}=E\\left(\\tilde{r}{j}\\right)+\\beta_{j, m}\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right]+\\beta_{j, v}\\left[\\tilde{r}_{V}-E\\left(\\tilde{r}_{V}\\right)\\right] $$ ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:4:0","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.5.1 Same Beta: Proposition 3 【Proposition (3)】The absence of arbitrage opportunities requires well-diversified portfolios with identical betas on both factors to have the same expected returns. Proof: Consider two well-diversified portfolios, one with $$ \\tilde{r}{w}^{1}=E\\left(\\tilde{r}{w}\\right)+\\beta_{w, m}\\left[r_{M}^{\\prime}-E\\left(\\tilde{r_{M}}\\right)\\right]+\\beta_{w, v}\\left[r_{v}-E\\left(\\tilde{r}_{v}\\right)\\right] $$ and the other with $$ \\tilde{r}_{w}^{2}=E\\left(\\tilde{r}_{w}\\right)+\\Delta+\\beta_{w, m}\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right]+\\beta_{w, v}\\left[\\tilde{r}_{v}-E\\left(\\tilde{r}_{v}\\right)\\right] $$ where $\\triangle\u003e0 .$ Now consider a strategy of taking a long position worth $x$ in portfolio 2 and a short position worth $-x$ in portfolio 1 This strategy is self-financing at $t=0$ but yields a $t=1$ payoff of $$ x\\left(1+\\tilde{r}{w}^{2}\\right)-x\\left(1+\\tilde{r}{w}^{1}\\right)=x \\Delta\u003e0 $$ Hence, the absence of arbitrage opportunities is inconsistent with $\\Delta\u003e0$. Similarly, the absence of arbitrage opportunities is inconsistent with $\\Delta\u003c 0$. Thus, $\\Delta =0$. Proposition 3 is proved. ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:4:1","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.5.2 Different Beta: Proposition 4 【Proposition (4)】The absence of arbitrage opportunities requires the expected returns on all well-diversified portfolios to satisfy $$ E\\left(\\tilde{r}{w}\\right)=\\lambda{0}+\\lambda_{m} \\beta_{w, m}+\\lambda_{v} \\beta_{w, v} $$ for constants $\\lambda_{0}, \\lambda_{m}, \\lambda_{v}$ that are the same for all portfolios. Proof: Considering three portfolios, all well-diversified. The first has $\\beta_{w, m}=\\beta_{w, v}=0,$ risk-free portfolio $$ \\tilde{r}_{w}^{1}=E\\left(\\tilde{r}_{w}^{1}\\right) $$ the second has $\\beta_{w, m}=1$ and $\\beta_{w, v}=0,$ market portfolio $$ \\tilde{r}_{w}^{2}=E\\left(\\tilde{r}_{w}^{2}\\right)+\\left[r_{M}^{2}-E\\left(\\tilde{r}_{M}\\right)\\right] $$ and the third has $\\beta_{w, m}=0$ and $\\beta_{w, v}=1,$ value portfolio $$ \\tilde{r}_{w}^{3}=E\\left(\\tilde{r}_{w}^{3}\\right)+\\left[\\tilde{r}_{v}-E\\left(\\tilde{r}_{v}\\right)\\right] $$ Next, select $$ \\begin{aligned} \\lambda_{0}\u0026=E\\left(\\tilde{r}_{w}^{1}\\right)\\ \\lambda_{m} \u0026=E\\left(\\tilde{r}_{w}^{2}\\right)-E\\left(\\tilde{r}_{w}^{1}\\right) \\ \\lambda_{v} \u0026=E\\left(\\tilde{r}_{w}^{3}\\right)-E\\left(\\tilde{r}_{w}^{1}\\right) \\end{aligned} $$ Since portfolio 1 has $\\beta_{w, m}=\\beta_{w, v}=0$ $$ E\\left(\\tilde{r}{w}^{1}\\right)=\\lambda{0}+\\lambda_{m} \\times 0+\\lambda_{v} \\times 0=\\lambda_{0} $$ Since portfolio 2 has $\\beta_{w, m}=1$ and $\\beta_{w, v}=0$ $$ E\\left(\\tilde{r}{w}^{2}\\right)=\\lambda{0}+\\lambda_{m} \\times 1+\\lambda_{v} \\times 0=E\\left(\\tilde{r}_{w}^{1}\\right)+E\\left(\\tilde{r}_{w}^{2}\\right)-E\\left(\\tilde{r}_{w}^{1}\\right) $$ Since portfolio 3 has $\\beta_{w, m}=0$ and $\\beta_{w, v}=1$ $$ E\\left(\\tilde{r}{w}^{3}\\right)=\\lambda{0}+\\lambda_{m} \\times 0+\\lambda_{v} \\times 1=E\\left(\\tilde{r}_{w}^{1}\\right)+E\\left(\\tilde{r}_{w}^{3}\\right)-E\\left(\\tilde{r}_{w}^{1}\\right) $$ Next, let’s consider a fourth and fifth well-diversified portfolios. The fourth has $$ \\tilde{r}_{w}^{4}=E\\left(\\tilde{r}_{w}^{4}\\right)+\\beta_{w, m}\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right]+\\beta_{w, v}\\left[\\tilde{r}_{V}-E\\left(\\tilde{r}_{V}\\right)\\right] $$ and features a configuration of $\\beta_{w, m}$ and $\\beta_{w, v}$ that differs from portfolios 1 through 3. Assuming that $$ E\\left(\\tilde{r}{w}^{4}\\right)=\\lambda{0}+\\lambda_{m} \\beta_{w, m}+\\lambda_{v} \\beta_{w, v}+\\Delta $$ where $\\Delta \\neq 0$ The fifth well-diversified portfolio has the same configuration of $\\beta_{w, m}$ and $\\beta_{w, v}$ but is built as a portfolio of the first three portfolios: Allocate share $\\beta_{w, m}$ to portfolio 2 “loads” exclusively on the market portfolio Allocate the share $\\beta_{w, v}$ to portfolio 3 loads exclusively on the “value-to-growth” portfolio Allocate the remaining share $1-\\beta_{w, m}-\\beta_{w, v}$ to portfolio 1 (free from systematic risk) to avoid any additional exposure to risk. With $$ \\begin{aligned} \\tilde{r}{w}^{1} \u0026=E\\left(\\tilde{r}{w}^{1}\\right)=\\lambda_{0} \\ \\tilde{r}_{w}^{2} \u0026=E\\left(\\tilde{r}_{w}^{2}\\right)+\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right]=\\lambda_{0}+\\lambda_{m}+\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right] \\ \\tilde{r}_{w}^{3} \u0026=E\\left(\\tilde{r}_{w}^{3}\\right)+\\left[\\tilde{r}_{v}-E\\left(\\tilde{r}_{v}\\right)\\right]=\\lambda_{0}+\\lambda_{v}+\\left[\\tilde{r}_{v}-E\\left(\\tilde{r}_{v}\\right)\\right] \\end{aligned} $$ We have Portfolio 5 $$ \\begin{aligned} \\tilde{r}_{w}^{5} \u0026=\\lambda_{0}+\\lambda_{m} \\beta_{w, m}+\\lambda_{v} \\beta_{w, v} \\ \u0026+\\beta_{w, m}\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right]+\\beta_{w, v}\\left[\\tilde{r}_{v}-E\\left(\\tilde{r}_{v}\\right)\\right] \\end{aligned} $$ Now we have two well-diversified portfolios, portfolio 4 with $$ \\begin{aligned} \\tilde{r}_{w}^{4} \u0026=\\lambda_{0}+\\lambda_{m} \\beta_{w, m}+\\lambda_{v} \\beta_{w, v}+\\Delta \\ \u0026+\\beta_{w, m}\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right]+\\beta_{w, v}\\left[\\tilde{r}_{v}-E\\left(\\tilde{r}_{v}\\right)\\right] \\end{aligned} $$ and portfolio 5 with $$ \\begin{aligned} \\tilde{r}_{w}^{5}\u0026=\\lambda_{0}+\\lambda_{m} \\beta_{w, m}+\\lambda_{v}","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:4:2","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.5.3 Interpretation Once again, we can build more intuition for this result by interpreting the coefficients $\\lambda_{0}, \\lambda_{m}, \\lambda_{v}$ $\\tilde{r}{w}^{1}=E\\left(\\tilde{r}{w}^{1}\\right)=\\lambda_{0}$ implies that $$ \\lambda_{0}=r_{f} $$ $\\lambda_{0}$ equals the risk-free rate. Since the market portfolio has $\\beta_{w, m}=1$ and $\\beta_{w, v}=0,$ proposition 4 implies $$ E\\left(\\tilde{r}_{w}\\right)=\\lambda_{0}+\\lambda_{m} \\times 1+\\lambda_{v} \\times 0=\\lambda_{0}+\\lambda_{m}=r_{f}+\\lambda_{m} $$ or $$ \\lambda_{m}=E\\left(\\tilde{r}_{M}\\right)-r_{f} $$ $\\lambda_{m}$ is the risk premium on the market portfolio. Since the value-to-growth portfolio has $\\beta_{w, m}=0$ and $\\beta_{w, v}=1$ proposition 4 implies $$ \\begin{array}{c} E\\left(\\tilde{r}_{w}\\right)=\\lambda_{0}+\\lambda_{m} \\times 0+\\lambda_{v} \\times 1=\\lambda_{0}+\\lambda_{v}=r_{f}+\\lambda_{v} \\ \\lambda_{v}=E\\left(\\tilde{r}_{v}\\right)-r_{f} \\end{array} $$ $\\lambda_{v}$ is the risk premium on the value portfolio. Hence $$ E\\left(\\tilde{r}{w}\\right)=\\lambda{0}+\\lambda_{m} \\beta_{w, m}+\\lambda_{v} \\beta_{w, v} $$ is equivalent to $$ E\\left(\\tilde{r}_{w}\\right)=r_{f}+\\beta_{w, m}\\left[E\\left(\\tilde{r}_{M}\\right)-r_{f}\\right]+\\beta_{w, v}\\left[E\\left(\\tilde{r}_{v}\\right)-r_{f}\\right] $$ Drawing a first analogy to the CAPM, the APT implies a multidimensional version of the security market line that reflects multiple sources of systematic risk. Hence, the APT can be considered an extension of the CAPM that allows for multiple sources of systematic risk, albeit one that applies to well-diversified portfolios but not necessarily to all individual assets. ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:4:3","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.6 The APT The multi-factor model assumes that the rate of return on any security is a linear function of $k$ factors: $$ \\tilde{r}{j}=E\\left(\\tilde{r}{j}\\right)+\\beta_{j 1} \\tilde{f}_{1}+\\ldots+\\beta_{j k} \\tilde{f}_{k}+\\tilde{\\varepsilon}_{j} $$ where $\\beta_{j k}$ is the sensitivity of the $j$th asset’s returns to the $k$ th factor; $\\tilde{f}_{k}$ is the mean zero $k$ th factor common to the returns of all assets; $\\tilde{\\varepsilon}_{j}$ is a random zero mean noise term for the $j$ th asset. The APT is derived under the assumption of perfectly competitive and frictionless capital markets. Individuals are assumed to have homogeneous beliefs that the random returns for the set of assets are governed by the linear k-factor model: $$ \\tilde{r}{j}=E\\left(\\tilde{r}{j}\\right)+\\beta_{j 1} \\tilde{f}_{1}+\\ldots+\\beta_{j k} \\tilde{f}_{k}+\\tilde{\\varepsilon}_{j} $$ The APT requires that the number of assets under consideration $N$ be much larger than the number of factors $k,$ and that the noise term $\\tilde{\\varepsilon}_{j}$ be the idiosyncratic risk component for the $j$th asset. It must be independent of all factors and all error terms for other assets. ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:5:0","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.6.1 How to construct an arbitrage portfolio An arbitrage portfolio in equilibrium satisfies: using no wealth having no risk must earn no return on average Let $\\omega_{i}$ be the change in the dollar amount invested in $i$th asset as a percentage of an individual’s total invested wealth. To form an arbitrage portfolio is to sell some assets and use the proceeds to buy others, i.e., $$ \\Sigma_{i=1}^{N} \\omega_{i}=0 $$ If there are $N$ assets in the arbitrage portfolio, the additional portfolio return gained is $$ \\begin{aligned} \\tilde{r}_{p} \u0026=\\sum_{i=1}^{N} \\omega \\tilde{r}_{i} \\ \u0026=\\sum_{i=1}^{N} \\omega_{i} E\\left(\\tilde{r}_{i}\\right)+\\Sigma_{i=1}^{N} \\omega_{i} \\beta_{i 1} \\tilde{f}_{1}+\\ldots+\\Sigma_{i=1}^{N} \\omega_{i} \\beta_{i k} \\tilde{f}_{k}+\\Sigma_{i=1}^{N} \\omega_{i} \\tilde{\\varepsilon}_{i} \\end{aligned} $$ To obtain a riskless arbitrage portfolio it is necessary to eliminate both idiosyncratic risk and systematic risk: one portfolio with the following conditions should do selecting percentage changes in investment ratios $\\omega_{i}$ to be small diversifying across a large number of assets choosing changes $\\omega_{i}$ so that for each factor $k$ the weighted sum of the systematic risk component $\\beta_{k}$ is zero. $$ \\begin{aligned} \u0026\\omega_{i} \\approx 1 / N \u0026\\quad \\mathrm{N} \\text{ is a large number} \\ \u0026\\Sigma_{i=1}^{N} \\omega_{i} \\beta_{i k}=0 \u0026\\quad \\text{for each factor} \\end{aligned} $$ Because $\\varepsilon_{i}$ are independent, by law of large numbers it is guaranteed that a weighted average of them will approach zero in the limit as $\\mathrm{N}$ becomes large. Thus we are now left with $$ \\tilde{r}{p}=\\Sigma{i=1}^{N} \\omega_{i} E\\left(\\tilde{r}_{i}\\right)+\\Sigma_{i=1}^{N} \\omega_{i} \\beta_{i 1} \\tilde{f}_{1}+\\ldots+\\Sigma_{i=1}^{N} \\omega_{i} \\beta_{i k} \\tilde{f}_{k} $$ since we have chosen the weighted average of the systematic risk components for each factor to be zero $\\Sigma \\omega_{i} \\beta_{i k}=0,$ thus the systemic risk is eliminated and the return on our arbitrage portfolio becomes a constant $$ \\tilde{r}_{p}=\\Sigma_{i=1}^{N} \\omega_{i} E\\left(\\tilde{r}_{i}\\right) $$ Since the arbitrage portfolio has no risk and requires no new wealth. If the return on the arbitrage portfolio were not zero, it would be possible to achieve an infinite return with no capital requirements and no risk. Such an opportunity is impossible if the market is to be in equilibrium. In fact, if the individual arbitrageur is in equilibrium, the return on any and all arbitrage portfolios must be zero: $$ \\tilde{r}{p}=\\Sigma{i=1}^{N} \\omega_{i} E\\left(\\tilde{r}_{i}\\right)=0 $$ ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:5:1","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.6.2 The arbitrage pricing line Consider a projection on factor $k$. In equilibrium, all assets must fall on the arbitrage pricing line. \r Because the arbitrage pricing relationship is linear, we can use the slope-intercept definition of a straight line and rewrite $$ E\\left(\\tilde{r}{i}\\right)=r{f}+\\left(\\bar{\\delta}{k}-r{f}\\right) \\beta_{i k} $$ where $\\bar{\\delta}_{k}$ is the expected return on a portfolio with unit sensitivity to the $k$ th factor and zero sensitivity to all other factors. \r $\\lambda_{k}$ represents the risk premium, in equilibrium, for the $k$ th factor. It is the difference between (1) the expectation of a portfolio that has unit response to the $k$ th factor and zero response to the other factors and (2) the risk-free rate $r_{f}$ : $$ \\lambda_{k}=\\bar{\\delta}_{k}-r_{f} $$ Hence, the arbitrage pricing theory can be rewritten as $$ E\\left(\\tilde{r}_{i}\\right)-r_{f}=\\left(\\bar{\\delta}_{1}-r_{f}\\right) \\beta_{i 1}+\\ldots+\\left(\\bar{\\delta}_{k}-r_{f}\\right) \\beta_{i k} $$ If we explain it as a linear regression equation (assuming that the vectors of returns have a joint normal distribution and that the factors have been linearly transformed so that their transformed vectors are orthonormal) then the coefficients $\\beta_{i k}$ are defined in exactly the same $\\beta$ in $\\mathrm{CAPM}$, $$ \\beta_{i k}=\\frac{\\operatorname{cov}\\left(r_{i}, \\delta_{k}\\right)}{\\operatorname{var}\\left(\\delta_{k}\\right)} $$ So the CAPM is seen to be a special case of the APT (where asset returns are assumed to be joint normal). ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:5:2","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.6.3 APT V.S. CAPM APT is much more robust than the CAPM for several reasons: APT makes no assumptions about the distribution of asset returns APT makes no strong assumptions about individuals' utility functions APT allows the equilibrium returns of assets to be dependent on many factors, instead of only one(like beta). APT yields a statement about the relative pricing of any subset of assets; hence one need not measure the entire universe of assets in order to test the theory There is no special role for the market portfolio in APT, whereas CAPM requires that the market portfolio be efficient APT is easily extended to a multi-period framework(Ross 1976) ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:5:3","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.6.4 Graphic Example The factor loadings for two factors, say changes in unanticipated real output and changes in unanticipated inflation, are plotted on the axes. Consider a projection on both factor, \r We have, \r The origin represents the risk-free rate that is the rate of return received when an asset has zero beta on both factors. Points along the diagonal dashed lines have equal expected return but not the same risk; e.g., all points along the line OJ have an expected rate of return equal to the risk-free rate but are not riskless portfolios. Suppose the arbitrage price model $$ E\\left(\\tilde{r}{i}\\right)-r{f}=\\left(\\bar{\\delta}{1}-r{f}\\right) \\beta_{i 1}+\\left(\\bar{\\delta}_{2}-r_{f}\\right) \\beta_{i 2} $$ with estimated values $r_{f}=10 %, \\bar{\\delta}_{1}=20 %$ and $\\bar{\\delta}_{1}=15 % .$ Thus the vertical intercept is $\\frac{E\\left(\\tilde{r}_{i}\\right)-r_{f}}{\\delta_{1}-r_{f}}$ and the slope of the equal return line is $\\frac{\\overline{\\delta_{2}}-r_{f}}{\\delta_{1}-r_{f}}$. Suppose that we know that a CAPM efficient market portfolio has been chosen, with expected return $30 %$ and $\\beta_{i 1}=1.5, \\beta_{i 2}=1 .$ The market portfolio is at point $I$. The security market line indicated by $OI$. The CAPM measures risk only in one dimension. If we are told that the portfolio’s CAPM beta is 0.5, then it will be the point P. If people are in fact sensitive to more than one type of risk when choosing portfolios of equal return, the APT is superior to the CAPM because CAPM is unidimensional in risk. It is perfectly reasonable that portfolio P' might be preferred to P by some investors because it has the same return as portfolio P but a preferable combination of sensitivities to the underlying factors. For example, a public pension fund manager might not care much about the sensitivity of the value of the fund to industrial production, but might be very concerned about hedging against unexpected changes in inflation. ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:5:4","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.7 Looking For Factors One shortcoming of the multifactor APT is that it gives no guidance concerning the determination of the relevant risk factors or their risk premiums. Two principles guide us when we specify a reasonable list of factors. factors are returns on the so-called factor-mimicking portfolios, or factors that seem likely to be important risk factors; that is, factors that concern investors sufficiently that they will demand meaningful risk premiums to bear exposure to those sources of risk. a limited number of systematic factors with considerable ability to explain security returns; typically macroeconomic, business-cycle-related factors (e.g., the growth rate of GDP) ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:6:0","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.7.1 Models with Factor-Mimicking Portfolios The Fama-French (FF) Three-Factor Model specifies macroeconomic factors as candidates for relevant sources of systematic risk uses firm characteristics that seem on empirical grounds to proxy for exposure to systematic risk. As noted previously, evidence against the CAPM is presented by Eugene Fama and Kenneth French, “Common Risk Factors in the Returns on Stocks and Bonds,” Journal of Financial Economics Vol.33 (February 1993): pp.3-56 In particular, Fama and French find that returns on a portfolio that takes long positions in small firms and short positions in large firms and a portfolio that takes long positions in firms with high book value relative to market value and short positions in firms with low book-to-market value are as important as the overall market return in explaining expected returns on individual stocks. Thus, Fama and French suggest replacing the market model with a three-factor model in which the random return on each individual asset $j=1,2, \\ldots, J$ is governed by $$ \\begin{aligned} \\tilde{r}{j} \u0026=\\alpha{j}+\\beta_{j, m}\\left[\\tilde{r}_{M}-E\\left(\\tilde{r}_{M}\\right)\\right] \\ \u0026+\\beta_{j, s}\\left[\\tilde{r}_{S M B}-E\\left(\\tilde{r}_{S M B}\\right)\\right]+\\beta_{j, h}\\left[\\tilde{r}_{H M L}-E\\left(\\tilde{r}_{H M L}\\right)\\right]+\\varepsilon_{j} \\end{aligned} $$ where $\\tilde{r}_{S M B}$ is the return on the “small-minus-big” portfolio, i.e., the return of a portfolio of small stocks in excess of the return on a portfolio of large stocks. $\\tilde{r}_{H M L}$ is the return on the “high-minus-low” book-to-market value portfolio, i.e., the return of a portfolio of stocks with a high book-to-market ratio in excess of the return on a portfolio of stocks with a low book-to-market ratio. $\\beta_{j, m}, \\beta_{j, s}$ and $\\beta_{j, h}$ measure the exposure (correlation) of the return on asset $j$ to each of these sources of risk, the idiosyncratic term $\\varepsilon_{j}$ has the same properties as in the market model. For the Fama-French model, the same type of no-arbitrage arguments that led us to propositions 3 and 4 imply that the expected return on any well-diversified portfolio is $$ \\begin{aligned} E(\\tilde{r_{w}}) \u0026=r_{f}+\\beta_{w, m}\\left[E(\\tilde{r_{M}})-r_{f}\\right] \\ \u0026+\\beta_{w, s}\\left[E\\left(\\tilde{r}_{S M B}\\right)-r_{f}\\right]+\\beta_{w, h}\\left[E\\left(\\tilde{r}_{H M L}\\right)-r_{f}\\right] \\end{aligned} $$ where $\\tilde{r}_{M}, \\tilde{r}_{S M B}$ and $\\tilde{r}_{H M L}$ are returns on the tracking portfolios for the market, small-minus-big, and high-minus-low book-to-market portfolios. This version of the APT implies that a portfolio will have a higher expected returns when its own return is Positively correlated with the market return (consistent with the CAPM) Positive correlated with the SMB and/or HML return (inconsistent with the CAPM). Correlation with SMB and/or HML may be an indicator of financial vulnerability, over and above macroeconomic risk. ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:6:1","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.7.2 Macroeconomic Factor Model A famous paper by Chen, Roll and Ross, takes us even further away from the CAPM, by constructing a multifactor version of the APT in which a set of macroeconomic variables measure alternative sources of systematic risk. Nai-Fu Chen, Richard Roll, and Stephen Ross, “Economic Forces and the Stock Market,” Journal of Business Vol.59 (July 1986): 383-403. They experiment with a variety of specifications before settling on a five-factor macroeconomic model. In Chen, Roll, and Ross' multifactor model, the random return on each individual asset $j=1,2, \\ldots, J$ is governed by $$ \\begin{aligned} \\tilde{r}{j} \u0026=\\alpha{j}+\\beta_{j, I P} \\tilde{IP}+\\beta_{j, U I} \\tilde{UI} \\ \u0026+\\beta_{j, E I} \\tilde{EI} +\\beta_{j, T S} \\tilde{TS} +\\beta_{j, R P} \\tilde{RP}+\\varepsilon_{j} \\end{aligned} $$ where $\\tilde{P}=%$ change in industrial production $\\tilde{U} I=%$ change in unexpected inflation $\\tilde{E} I=%$ change in expected inflation $\\tilde{T} S=a$ “term structure” variable defined as excess return of long-term government bonds over short-term interest rates $\\tilde{R P}=\\mathrm{a}$ “risk premium” variable defined as excess return of low grade bonds over high grade bonds All of the factors are expressed as deviations from their expected values, so that $\\alpha_{j}$ continues to measure the expected return on asset $j$. The idiosyncratic term $\\varepsilon_{j}$ has the same properties as in the market model. For the Chen-Roll-Ross model, no-arbitrage arguments imply that the expected return on any well-diversified portfolio is $$ \\begin{aligned} E\\left(\\tilde{r}{w}\\right) \u0026=r{f}+\\beta_{w, I P}\\left[E\\left(\\tilde{r}_{I P}\\right)-r_{f}\\right] \\ \u0026+\\beta_{w, U I}\\left[E\\left(\\tilde{r}_{U I}\\right)-r_{f}\\right]+\\beta_{w, E I}\\left[E\\left(\\tilde{r}_{E I}\\right)-r_{f}\\right] \\ \u0026+\\beta_{w, T S}\\left[E\\left(\\tilde{r}_{T S}\\right)-r_{f}\\right]+\\beta_{w, R P}\\left[E\\left(\\tilde{r}_{R P}\\right)-r_{f}\\right] \\end{aligned} $$ where $\\tilde{r}_{I P}, \\tilde{r}_{U I}, \\tilde{r}_{E l}, \\tilde{r}_{T S}$ and $\\tilde{r}_{R P}$ are the returns on tracking portfolios for the macroeconomic factors. This version of the APT implies that a portfolio will have a higher expected return when its own return is Positively correlated with IP and/or RP, which are high in good times Negatively correlated with UI (especially) and/or EI and/or TS in bad times. The TS and RP variables may once again indicate a role for financial vulnerability. ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:6:2","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"7.8 The APT and Risk Premia Since the APT’s formula for expected returns applies to well-diversified portfolios and not necessarily to individual assets, one should take care in using it to value risky cash flows from individual investment projects. Still, partly for the sake of completeness and also to see how we can extend our previous valuation exercise using the CAPM to allow for the APT’s multiple sources of systematic risk, let’s go ahead and see how it works. The valuation problem involves attaching a price $P_{0}^{C}$ today $(\\text { at } t=0)$ to a risky cash-flow $\\tilde{C}_{1}$ received in the future (at $t=1$ ). After taking the expected value $E\\left(\\tilde{C}{1}\\right)$ of the cash flow, we want to find the appropriate risk premium $\\psi$ to add to the risk-free rate $r{f}$ so that $$ P_{0}^{C}=\\frac{E\\left(\\tilde{C}_{1}\\right)}{1+r_{f}+\\psi} $$ provides an accurate assessment of the project’s value today. Since the APT, like the CAPM, is cast in terms of returns, we can start by computing the random return on the project as $$ \\begin{aligned} 1+\\tilde{r}{C} \u0026=\\frac{\\tilde{C}{1}}{P_{0}^{C}} \\end{aligned} $$ or $$ \\tilde{r}{C}= \\frac{\\tilde{C}{1}}{P_{0}^{C}}-1=\\frac{\\tilde{C}_{1}-P_{0}^{C}}{P_{0}^{C}} $$ Next, we need to choose a factor model to use with the APT. Let’s use a simpler, two-factor version of the Chen-Roll-Ross macroeconomic model where the return on each asset $i$ is given by $$ \\tilde{r}{i}=E\\left(\\tilde{r}{i}\\right)+\\beta_{i, I P} \\tilde{I P}+\\beta_{i, I N F} \\tilde{N} F+\\varepsilon_{i} $$ where $\\tilde{P}$ is industrial production and $\\tilde{N} F$ is inflation. Here is where another advantage of the APT becomes apparent: It might be easier to estimate (or guess!) how the return on a project will vary with macroeconomic output and inflation then to estimate how it will vary with the return on the market portfolio. Boldly setting aside the distinction between well-diversified portfolios and individual assets or cash flows, our two-factor macroeconomic model and the APT imply that the expected return on the project should be $$ E\\left(\\tilde{r}{C}\\right)=r{f}+\\beta_{c, I P}\\left[E\\left(\\tilde{r}_{I P}\\right)-r_{f}\\right]+\\beta_{c, I N F}\\left[E\\left(\\tilde{r}_{I N F}\\right)-r_{f}\\right] $$ where $\\tilde{r}_{I P}$ and $\\tilde{r}_{I N F}$ are returns on the tracking or mimicking portfolios for industrial production and inflation. Finally, we have $$ E\\left(\\frac{\\tilde{C}{1}}{P{0}^{C}}-1\\right)=r_{f}+\\beta_{c, I P}\\left[E\\left(\\tilde{r}_{I P}\\right)-r_{f}\\right]+\\beta_{c, | N F}\\left[E\\left(\\tilde{r}_{I N F}\\right)-r_{f}\\right] $$ This implies $$ P_{0}^{C}=\\frac{E\\left(\\tilde{C}_{1}\\right)}{1+r_{f}+\\beta_{c, I P}\\left[E\\left(\\tilde{r}_{I P}\\right)-r_{f}\\right]+\\beta_{c, | N F}\\left[E\\left(\\tilde{r}_{I N F}\\right)-r_{f}\\right]} $$ or, more simply, $$ P_{0}^{C}=\\frac{E\\left(\\tilde{C}_{1}\\right)}{1+r_{f}+\\psi} $$ where the risk premium $$ \\psi=\\beta_{c, I P}\\left[E\\left(\\tilde{r}_{IP}\\right)-r_{f}\\right]+\\beta_{c, I N F}\\left[E\\left(\\tilde{r}_{IN F}\\right)-r_{f}\\right] $$ compensates for the project’s exposure to the risk of recession (falling IP) or inflation (rising INF). ","date":"0001-01-01","objectID":"/7.-arbitrage-pricing-theory/:7:0","tags":null,"title":"","uri":"/7.-arbitrage-pricing-theory/"},{"categories":null,"content":"8. Arrow-Debreu Pricing: Equilibrium The Arrow-Debreu framework was developed in the 1950s and 1960s by Kenneth Arrow (US, b.1932, Nobel Prize 1972) and Gerard Debreu (France, 1921-2004, Nobel Prize 1983), some key references being: Gerard Debreu, Theory of Value: An Axiomatic Analysis of Economic Equilibrium, New Haven: Yale University Press, 1959. Kenneth Arrow, “The Role of Securities in the Optimal Allocation of Risk-Bearing,” Review of Economic Studies Vol.31 (April 1964): pp.91-96. ","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:0:0","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.1 The Arrow-Debreu Economy General Equilibrium Supply = Demand With production (if desired) Static but Multi-period No restrictions on preferences The most general of the theories we shall consider. ","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:1:0","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.1.1 A-D security Two dates: $ t=0 $ and $ t=1, N $ possible states at $ t=1, $ with probabilities $ \\pi_{i}, i=1,2, \\ldots, N $ 【Definition】An Arrow-Debreu security at state $ i $ is the following payoff $$ \\left(\\begin{array}{c} 0 \\ \\vdots \\ 1 \\ \\vdots \\ 0 \\end{array}\\right) \\longleftrightarrow \\text { state } i $$ That is, an Arrow-Debreu security at $ i $ pays 1 unit of time consumption good if state $ i $ happens and nothing at the rest of states. Let $ q_{i} $ be the price of Arrow-Debreu security at $ i $. That is, $ q_{i} $ is the number of units of consumption good at time 0. $ q_{i} $ also is called the state price of $ i $ since Arrow-Debreu security at state $ i $ pays 1 unit of consumption good only at state $ i $. \r There will have two umbrellas contingent on the weather in tomorrow. $$ \\begin{array}{l} \\text {Umbrella } 1=\\left(\\begin{array}{l} 1 \\ 0 \\end{array}\\right) \\ \\text {Umbrella } 2=\\left(\\begin{array}{l} 0 \\ 1 \\end{array}\\right) \\end{array} $$ That is, Umbrella 1: The umbrella to be delivered in tomorrow if it will rain tomorrow; and Umbrella 2: The umbrella to be delivered in tomorrow if it will be sunshine tomorrow. The price for Umbrella 1 is the price of rain tomorrow and the price for Umbrella 2 is the price of sunshine tomorrow. ","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:1:1","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.1.2 A-D market 【Definition】Complete market: you could buy any time consumption bundle from the existing markets if you have enough time 0 consumption good. The agent can buy whatever he wants on time 0’s markets if he has enough time 0 consumption goods. In other words, the agent can buy any time 1 consumption plan if he has enough time 0 consumption goods; and The agent can sell whatever he has on markets to transform it into time 0 consumption goods. In other words, the agent can sell his time 1 endowment on markets and get his total income represented by time 0 consumption good Suppose there is only one riskless financial asset whose payoffs at time 1 are 1 at both states. The market is not complete. Given the AD security price vector $ q $ and an agent’s endowment $ w=\\left(w^{0} ; w^{1}, \\ldots w^{N}\\right), $ what does the agent have? Since the markets are complete, he wants to sell all his time 1 endowments on time 0 ’s financial markets to transform them into time 0 ’s wealth (good). Time 1 endowment $ w=\\left(w^{1}, \\ldots w^{N}\\right) $ is equivalent to having a portfolio $ \\theta=w^{T}=\\left(w^{1}, \\ldots w^{N}\\right)^{T} $ Thus, the agent has total time 0 wealth $$ W=w^{0}+\\Sigma_{i=1}^{N} q^{i} w^{i} $$ What does he want to buy for time 1’s consumption bundle and time 0’s consumption good? Suppose he is going to buy consumption bundle $ c=\\left(c^{0} ; c^{1}, \\ldots, c^{N}\\right) $ since the markets are complete, he can buy these as long as he has enough wealth at time 0 similarly, time 1 consumption plan $ C^{1} $ is equivalent to buying a portfolio $ \\theta=\\left(c^{1}, \\ldots c^{N}\\right)^{T} $ Therefore, its total cost of this consumption bundle is $$ c^{0}+\\Sigma_{i=1}^{N} q^{i} c^{i} $$ Thus, the agent’s feasible budget set is $$ w^{0}+\\Sigma_{i=1}^{N} q^{i} w^{i} \\geq c^{0}+\\Sigma_{i=1}^{N} q^{i} c^{i} $$ ","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:1:2","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.1.3 A-D economy Key features of our version of the A-D economy Two dates: $ t=0 $ (today, when assets are purchased) and $ t=1 $ (the future, when payoffs are received). This can (and will) be generalized. N possible states at $ t=1 $, with probabilities $ \\pi_{i}, i=1,2, \\ldots, N $ One perishable (non-storable) good at each date (more goods can be added and the possibility of storage introduced, at the cost of more notational complexity) Individuals initially receive goods as endowments (but production could be introduced) $ K $ investors, $ j=1,2, \\ldots, K, $ who may differ in their preferences and endowments Agent' Utility Maximization Problem Let $ w_{j}^{0}= $ agent j’s endowment at $ t=0 $ $ w_{j}^{i}= $ agent j’s endowment in state i at $ \\mathrm{t}=1 $ $ c_{j}^{0}= $ agent j’s consumption at $ t=0 $ $ c_{j}^{i}= $ agent j’s consumption in state i at $ \\mathrm{t}=1 $ Arrow-Debreu securities or state-contingent claims, is the only traded securities: security i priced $ q^{i} $ promises one unit of consumption if state $ i $ occurs and nothing otherwise. Use consumption at $ t=0 $ as the “numeraire/标尺”, that is, the goods in terms of which all other prices are quoted. Let $ q^{i} $ be the price at $ t=0, $ measured in units of $ t=0 $ consumption, of a contingent claim that pays off one unit of consumption in a particular state $ i $ at $ t=1 $ and zero otherwise. Buying state-contingent claims is the only way for a consumer to secure purchasing power at a future date-state( the good is perishable.) For simplicity, assume that each investor first uses the contingent claims market to sell off his or her endowments at $ t=0 $ and in each state at $ t=1, $ then uses the same markets to buy back consumption at $ t=0 $ and in each state at $ t=1 $ Then we won’t need additional notation to keep track of purchases and sales of contingent claims: purchases coincide with consumption and sales with endowments Investor $ j $ in our economy therefore faces the budget constraint $$ w_{j}^{0}+\\sum_{i=1}^{N} q^{i} w_{j}^{i} \\geq c_{j}^{0}+\\sum_{i=1}^{N} q^{i} c_{j}^{i} $$ Note that we can always go back and compute net sales $ w_{j}^{0}-c_{j}^{0} $ and $ w_{j}^{i}-c_{j}^{i} $ or purchase $ c_{j}^{0}-w_{j}^{0} $ and $ c_{j}^{i}-w_{j}^{i} $ for all $ i=1,2, \\ldots, N $ of contingent claims if there turn out to be of interest. Investors are allowed to have different utility functions, and in the most general A-D model need not even have expected utility functions $$ u_{j}\\left(c_{j}^{0}, c_{j}^{1}, c_{j}^{2}, \\ldots, c_{j}^{N}\\right) $$ But to obtain sharper results, we will a**ssume that all investors maximize vN-M expected utility functions, but are allowed to have different Bernoulli utility functions** reflecting possibly different attitudes towards risk. Thus, investor $ j $ chooses $ c_{j}^{0} $ and $ c_{j}^{i} $ for all $ i=1,2, \\ldots, N $ to maximize $$ u_{j}\\left(c_{j}^{0}\\right)+\\beta E\\left[u_{j}\\left(\\tilde{c}_{j}\\right)\\right]=u_{j}\\left(c_{j}^{0}\\right)+\\beta \\sum_{i=1}^{N} \\pi_{i} u_{j}\\left(c_{j}\\right) \\quad (1) $$ where the discount factor $ \\beta $ is again a measure of patience, subject to the budget constraint $$ w_{j}^{0}+\\Sigma_{i=1}^{N} q^{i} w_{j}^{i} \\geq c_{j}^{0}+\\sum_{i=1}^{N} q^{i} c_{j}^{i} \\quad (2) $$ Note, again, Arrow and Debreu’s **key insight**: the mathematical structure of this investor’s problem is identical to the problem faced by a consumer who must divide his or her income up into amounts to be spent on apples, bananas, oranges, etc. Extending our version of the model to include more than two periods would require even more notation(!). But, both conceptually and mathematically, that extension simply amounts to introducing more goods: pears, pineapples, etc. In the A-D economy, as in microeconomics more generally, each individual investor operating in perfectly competitive markets takes prices as given and sees him or herself as being able to purchase as much or as little of each good at thos","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:1:3","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.2 Competitive Equilibrium and Pareto Optimum ","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:2:0","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.2.1 Agent' Utility Maximization Problem Let’s focus on the first requirement for a CE: investor $ j $ chooses $ c_{j}^{0} $ and $ c_{j}^{i} $ for all $ i=1,2, \\ldots, N $ to maximize $$ u_{j}\\left(c_{j}^{0}\\right)+\\beta \\sum_{i=1}^{N} \\pi_{i} u_{j}\\left(c_{j}\\right) $$ subject to the budget constraint $$ w_{j}^{0}+\\sum_{i=1}^{N} q^{i} w_{j}^{i} \\geq c_{j}^{0}+\\sum_{i=1}^{N} q^{i} c_{j}^{i} $$ The Lagrangian for the investor’s problem $$ L=u_{j}\\left(c_{j}^{0}\\right)+\\beta \\sum_{i=1}^{N} \\pi_{i} u_{j}\\left(c_{j}\\right)+\\lambda_{j}\\left(w_{j}^{0}+\\sum_{i=1}^{N} q^{i} w_{j}^{i}-c_{j}^{0}-\\sum_{i=1}^{N} q^{i} c_{j}^{i}\\right) $$ So the first-order condition(F.O.C) for all $ i=1,2, \\ldots, N $ $$ \\begin{aligned} u_{j}^{\\prime}\\left(c_{j}^{0}\\right)-\\lambda_{j} \u0026=0 \\ \\beta \\pi_{i} u_{j}^{\\prime}\\left(c_{j}^{\\prime}\\right)-\\lambda_{j} q^{i} \u0026=0 \\end{aligned} $$ It implies that $$ \\beta \\pi_{i} u_{j}^{\\prime}\\left(c_{j}^{i}\\right)=u_{j}^{\\prime}\\left(c_{j}^{0}\\right) q^{i} $$ or $$ q^{i}=\\frac{\\beta \\pi_{i} u_{j}^{\\prime}\\left(c_{j}^{i}\\right)}{u_{j}^{\\prime}\\left(c_{j}^{0}\\right)}=\\frac{M U_{j}^{i}}{M U_{j}^{0}} $$ The investor, as an actor in this economy, takes the price $ q^{i} $ as given and uses it to choose $ c_{j}^{0} $ and $ c_{j}^{i} $ optimally. But we, as observers of this economy, can use this optimality condition to see what the investor’s choices of $ c_{j}^{0} $ and $ c_{j}^{i} $ tell us about the contingent claim price $ q^{i} $ and, by extension, about asset prices more broadly. $$ q^{i}=\\frac{\\beta \\pi_{i} u_{j}^{\\prime}\\left(c_{j}^{i}\\right)}{u_{j}^{\\prime}\\left(c_{j}^{0}\\right)} \\quad \\text { for all } i=1,2, \\dots, N $$ The price $ q^{i} $ tends to be higher when: $ \\beta $ is larger, indicating that investors are more patient. $ \\pi_{i} $ is larger, indicating that state $ i $ is more likely. $ u_{j}^{\\prime}\\left(c_{j}^{i}\\right) $ is larger or $ u_{j}^{\\prime}\\left(c_{j}^{0}\\right) $ is smaller The price $ q^{i} $ tends to be higher when $ u_{j}^{\\prime}\\left(c_{j}^{i}\\right) $ is larger or $ u_{j}^{\\prime}\\left(c_{j}^{0}\\right) $ is smaller. If $ u_{j} $ is concave, that is, if investor $ j $ is risk averse, then a larger value of $ u_{j}^{\\prime}\\left(c_{j}^{i}\\right) $ corresponds to a smaller value of $ c_{j}^{i} $ and a smaller value of $ u_{j}^{\\prime}\\left(c_{j}^{0}\\right) $ corresponds to a larger value of $ c_{j}^{0} $. That is, $ q^{i} $ is higher if investor j’s consumption falls between $ t=0 $ and state $ i $ at $ t=1 $ And this same condition must hold for all investors in the economy. Hence, $ q^{i} $ is higher if everyone expects consumption to fall in state $ i $. $$ q^{i}=\\frac{\\beta \\pi_{i} u_{j}^{\\prime}\\left(c_{j}^{i}\\right)}{u_{j}^{\\prime}\\left(c_{j}^{0}\\right)} \\quad \\text { for all } i=1,2, \\dots, N $$ When is consumption most likely to fall for everyone in the economy? During a recession. Hence, the A-D model associates a high contingent claim price $ q^{i} $ with a recession, drawing an explicit link between asset prices and the rest of the economy that is, at best, implicit in the CAPM. This highlights an important, but somewhat subtle, distinction: the CAPM describes the behavior of asset returns, while the A-D model describes the behavior of asset prices. Asset return and asset prices To more directly compare the implications of these two models, let’s translate the A-D model’s implications for prices into implications for asset returns instead. In the A-D model, a contingent claim that costs $ q^{i} $ at $ t=0 $ pays off one unit of consumption in state $ i $ at $ t=1 $ Hence, the return on this asset between $ t=0 $ and state $ i $ at $ t=1 $ is $$ 1+r^{i}=\\frac{1}{q^{i}} \\quad \\text { or } \\quad r^{i}=\\frac{1}{q^{i}}-1 $$ A high price $ q^{i} $ corresponds to a low return $ r^{i} $. Hence, the A-D model associates recessions with low asset returns. Consistent with CAPM intuition, investors are willing to accept a low return on a contingent claim that pays off during a recessio","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:2:1","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.2.2 Numerical Example To derive some additional implications of the A-D model, let’s assume now that all investors have a logarithmic Bernoulli utility function $$ u_{j}(c)=\\ln (c) $$ so that $ u_{j}^{\\prime}(c)=1 / c $ for all $ j=1,2, \\ldots, K $ and that the common discount factor is $ \\beta=0.9 $ But let’s assume, as well, that investors differ in terms of their endowments. More specifically, suppose that the economy consists of two types of investors-type 1 and type 2-in equal numbers. Now we can consider a “representative” of each type: $ j=1 $ and $ j=2 $. Suppose for simplicity that there are only two possible states, $ i=1 $ and $ i=2, $ at $ t=1 $ which occur with probabilities $ \\pi_{1}=2 / 3 $ and $ \\pi_{2}=1 / 3 $. \r In this economy, there are three sets of requirements for a CE Type 1 investors take prices as given and choose consumptions optimally. Type 2 investors take prices as given and choose consumptions optimally. All markets clear. Let’s consider what each set of requirements implies. Step 1: Type 1 investor The representative type 1 investor takes $ q^{1} $ and $ q^{2} $ as given, chooses $ c_{1}^{0}, c_{1}^{1} $ and $ c_{1}^{2} $ to maximize $$ \\ln \\left(c_{1}^{0}\\right)+0.9\\left[(2 / 3) \\ln \\left(c_{1}^{1}\\right)+(1 / 3) \\ln \\left(c_{1}^{2}\\right)\\right] $$ subject to $$ 8+20 q^{1}+13 q^{2} \\geq c_{1}^{0}+q^{1} c_{1}^{1}+q^{2} c_{1}^{2} $$ Set up the Lagrangian $$ \\begin{array}{l} L=\\ln \\left(c_{1}^{0}\\right)+0.9\\left[(2 / 3) \\ln \\left(c_{1}^{1}\\right)+(1 / 3) \\ln \\left(c_{1}^{2}\\right)\\right] \\ \\quad+\\lambda_{1}\\left[8+20 q^{1}+13 q^{2}-c_{1}^{0}-q^{1} c_{1}^{1}-q^{2} c_{1}^{2}\\right] \\end{array} $$ So the $ F . O . C . $ $$ \\begin{aligned} \\frac{1}{c_{1}^{0}} \u0026=\\lambda_{1} \\ 0.9(2 / 3) \\frac{1}{c_{1}^{1}} \u0026=\\lambda_{1} q^{1} \\ 0.9(1 / 3) \\frac{1}{c_{1}^{2}} \u0026=\\lambda_{1} q^{2} \\end{aligned} $$ Use the first equation to eliminate $ \\lambda_{1} $ in the other two equations. $$ 0.9(2 / 3) \\frac{c_{1}^{0}}{c_{1}^{1}}=q^{1} $$ $$ 0.9(1 / 3) \\frac{c_{1}^{0}}{c_{1}^{2}}=q^{2} $$ If we were only interested in solving the individual investor’s problem, we could use the two equations and the budget constraint to solve for $ c_{1}^{0}, c_{1}^{1} $ and $ c_{1}^{2} $ in terms of $ q^{1} $ and $ q^{2} $. Step 2: Type 2 investor The representative type 2 investor takes $ q^{1} $ and $ q^{2} $ as given, chooses $ c_{2}^{0}, c_{2}^{1} $ and $ c_{2}^{2} $ to maximize $$ \\ln \\left(c_{2}^{0}\\right)+0.9\\left[(2 / 3) \\ln \\left(c_{2}^{1}\\right)+(1 / 3) \\ln \\left(c_{2}^{2}\\right)\\right] $$ subject to $$ 12+5 q^{1}+2 q^{2} \\geq c_{2}^{0}+q^{1} c_{2}^{1}+q^{2} c_{2}^{2} $$ Similarly, we could then use two equations and the budget constraint to solve for $ c_{2}^{0}, c_{2}^{1} $ and $ c_{2}^{2} $ in terms of $ q^{1} $ and $ q^{2} $ Step 3: Market Clearing Since there are equal numbers of each investor type, market clearing requires $$ \\begin{array}{l} w_{1}^{0}+w_{2}^{0}=20=c_{1}^{0}+c_{2}^{0} \\ w_{1}^{1}+w_{2}^{1}=25=c_{1}^{1}+c_{2}^{0} \\ w_{1}^{2}+w_{2}^{2}=15=c_{1}^{2}+c_{2}^{2} \\end{array} $$ You might at this point be worried. We seem to have 9 equations that must be satisfied: two first-order conditions and one budget constraint for each of the two representative investors and three market-clearing conditions. But we have only 8 “unknowns” to solve for: $ c_{1}^{0}, c_{1}^{1}, c_{1}^{2}, c_{2}^{0}, c_{2}^{1}, c_{2}^{2}, q^{1} $ and $ q^{2} $ But there is actually no problem: What we have discovered, instead, is a special case of Walras' Law. Walras' Law says that in an economy with $ K $ consumers and $ M $ markets, if All $ K $ consumers' budget constraints are satisfied and $ M-1 $ markets are in equilibrium then the $ M $th market must be in equilibrium as well Walras' Law implies that we really have only 8 unknowns in 8 equations. Having the same number of equations as unknowns doesn’t guarantee that there will be a solution or that a solution, if it exists, will be unique. But it does suggest that a unique solutio","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:2:2","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.2.3 Welfare Theorems of Economics The A-D economy is one in which the two welfare theorems of economics hold: the resource allocation from a competitive equilibrium is Pareto optimal, and any Pareto optimal resource allocation can be supported in a competitive equilibrium. To see this, let’s continue to work with the special case with only two types of investors and two possible states at $ t=1 $. But let’s generalize slightly, by assuming that both types of investors have a Bernoulli utility function of the more general form $$ u\\left(c_{j}^{0}\\right)+\\beta\\left[\\pi_{1} u\\left(c_{j}^{1}\\right)+\\pi_{2} u\\left(c_{j}^{2}\\right)\\right] $$ where $ \\pi_{1} $ is the probability of state 1 and $ \\pi_{2}=1-\\pi_{1} $ is the probability of state 1, and by letting $ w^{0}, w^{1}, $ and $ w^{2} $ denote the aggregate endowments at $ t=0 $ and the two states at $ t=1 $. Social Planner’s Problem Now consider a social planner, who divides the aggregate endowments up into amounts allocated to each of the two representative investors subject to the resource constraints $$ \\begin{array}{l} w^{0} \\geq c_{1}^{0}+c_{2}^{0} \\ w^{1} \\geq c_{1}^{1}+c_{2}^{1} \\ w^{2} \\geq c_{1}^{2}+c_{2}^{2} \\end{array} $$ in order to maximize a weighted sum of their expected utilities $$ \\begin{aligned} \\theta\\left(u\\left(c_{1}^{0}\\right)+\\beta\\left[\\pi_{1} u\\left(c_{1}^{1}\\right)+\\pi_{2} u\\left(c_{1}^{2}\\right)\\right]\\right) \\ +(1-\\theta)\\left(u\\left(c_{2}^{0}\\right)+\\beta\\left[\\pi_{1} u\\left(c_{2}^{1}\\right)+\\pi_{2} u\\left(c_{2}^{2}\\right)\\right]\\right) \\end{aligned} $$ The Lagrangian for this social planner’s problem $$ \\begin{aligned} L \u0026=\\theta\\left(u\\left(c_{1}^{0}\\right)+\\beta\\left[\\pi_{1} u\\left(c_{1}^{1}\\right)+\\pi_{2} u\\left(c_{1}^{2}\\right)\\right]\\right) \\ \u0026+(1-\\theta)\\left(u\\left(c_{2}^{0}\\right)+\\beta\\left[\\pi_{1} u\\left(c_{2}^{1}\\right)+\\pi_{2} u\\left(c_{2}^{2}\\right)\\right]\\right) \\ \u0026+\\lambda^{0}\\left(w^{0}-c_{1}^{0}-c_{2}^{0}\\right)+\\lambda^{1}\\left(w^{1}-c_{1}^{1}-c_{2}^{1}\\right)+\\lambda^{2}\\left(w^{2}-c_{1}^{2}-c_{2}^{2}\\right) \\end{aligned} $$ the first-order conditions $$ \\begin{aligned} \\theta u^{\\prime}\\left(c_{1}^{0}\\right)=\\lambda^{0} \u0026 \\text { and }(1-\\theta) u^{\\prime}\\left(c_{2}^{0}\\right)=\\lambda^{0} \\ \\theta \\beta \\pi_{1} u^{\\prime}\\left(c_{1}^{1}\\right)=\\lambda^{1} \u0026 \\text { and }(1-\\theta) \\beta \\pi_{1} u^{\\prime}\\left(c_{2}^{1}\\right)=\\lambda^{1} \\ \\theta \\beta \\pi_{2} u^{\\prime}\\left(c_{1}^{2}\\right)=\\lambda^{2} \u0026 \\text { and }(1-\\theta) \\beta \\pi_{2} u^{\\prime}\\left(c_{2}^{2}\\right)=\\lambda^{2} \\end{aligned} $$ can be combined to obtain $$ \\begin{aligned} \\frac{\\beta \\pi_{1} u^{\\prime}\\left(c_{1}^{1}\\right)}{u^{\\prime}\\left(c_{1}^{0}\\right)} \u0026=\\frac{\\beta \\pi_{1} u^{\\prime}\\left(c_{2}^{1}\\right)}{u^{\\prime}\\left(c_{2}^{0}\\right)} \\ \\frac{\\beta \\pi_{2} u^{\\prime}\\left(c_{1}^{2}\\right)}{u^{\\prime}\\left(c_{1}^{0}\\right)} \u0026=\\frac{\\beta \\pi_{2} u^{\\prime}\\left(c_{2}^{2}\\right)}{u^{\\prime}\\left(c_{2}^{0}\\right)} \\end{aligned} $$ The social planner equates marginal rates of substitution between $ t=0 $ and each state at $ t=1 $ across the two agent types. \r In any competitive equilibrium, however, both agent types will equate the same marginal rates of substitution to the contingent claim prices: $$ \\begin{aligned} \\frac{\\beta \\pi_{1} u^{\\prime}\\left(c_{1}^{1}\\right)}{u^{\\prime}\\left(c_{1}^{0}\\right)} \u0026=q^{1}=\\frac{\\beta \\pi_{1} u^{\\prime}\\left(c_{2}^{1}\\right)}{u^{\\prime}\\left(c_{2}^{0}\\right)} \\ \\frac{\\beta \\pi_{2} u^{\\prime}\\left(c_{1}^{2}\\right)}{u^{\\prime}\\left(c_{1}^{0}\\right)} \u0026=q^{2}=\\frac{\\beta \\pi_{2} u^{\\prime}\\left(c_{2}^{2}\\right)}{u^{\\prime}\\left(c_{2}^{0}\\right)} \\end{aligned} $$ Hence, in the A-D economy as in the Edgeworth box: First Welfare Theorem of Economics: The resource allocation from a competitive equilibrium is Pareto optimal Second Welfare Theorem of Economics: A Pareto optimal resource allocation can be supported in a competitive equilibrium. Numerical Example, continued In our numerical example with log utili","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:2:3","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.3 Risk Sharing We can use this approach to study how investors optimally share risk, keeping in mind that in the competitive equilibrium of an A-D economy they will use financial markets to do so. Now consider the same social planner’s problem as before, the first-order conditions $$ \\begin{aligned} \\theta u^{\\prime}\\left(c_{1}^{0}\\right)=\\lambda^{0} \u0026 \\text { and } \\quad(1-\\theta) u^{\\prime}\\left(c_{2}^{0}\\right)=\\lambda^{0} \\ \\theta \\beta \\pi_{1} u^{\\prime}\\left(c_{1}^{1}\\right)=\\lambda^{1} \u0026 \\text { and } \\quad(1-\\theta) \\beta \\pi_{1} u^{\\prime}\\left(c_{2}^{1}\\right)=\\lambda^{1} \\ \\theta \\beta \\pi_{2} u^{\\prime}\\left(c_{1}^{2}\\right)=\\lambda^{2} \u0026 \\text { and } \\quad (1-\\theta) \\beta \\pi_{2} u^{\\prime}\\left(c_{2}^{2}\\right)=\\lambda^{2} \\end{aligned} $$ Let’s focus on the last two of these optimality conditions, since they show how the investors share risk across states 1 and 2 at $ t=1 $ $$ \\begin{array}{l} \\theta \\beta \\pi_{1} u^{\\prime}\\left(c_{1}^{1}\\right)=(1-\\theta) \\beta \\pi_{1} u^{\\prime}\\left(c_{2}^{1}\\right) \\ \\theta \\beta \\pi_{2} u^{\\prime}\\left(c_{1}^{2}\\right)=(1-\\theta) \\beta \\pi_{2} u^{\\prime}\\left(c_{2}^{2}\\right) \\end{array} $$ Using the aggregate resource constraints, imply $$ \\begin{array}{l} \\theta u^{\\prime}\\left(c_{1}^{1}\\right)=(1-\\theta) u^{\\prime}\\left(w^{1}-c_{1}^{1}\\right) \\ \\theta u^{\\prime}\\left(c_{1}^{2}\\right)=(1-\\theta) u^{\\prime}\\left(w^{2}-c_{1}^{2}\\right) \\end{array} $$ since $ w^{1}=c_{1}^{1}+c_{2}^{1} $ and $ w^{2}=c_{1}^{2}+c_{2}^{2} $ Case 1: $ w^{1}=w^{2} $ Note, first, that when $ w^{1}=w^{2}, $ so that there is no aggregate risk, these two equations are exactly the same. In this case, we must have $ c_{1}^{1}=c_{1}^{2} $ and $ c_{2}^{1}=c_{2}^{2} . $ For each type of agent, consumption in state 1 is the same as consumption in state 2. With $ w^{1}=w^{2}, $ we have $ c_{1}^{1}=c_{1}^{2} $ and $ c_{2}^{1}=c_{2}^{2} . $ For each type of agent consumption in state 1 is the same as consumption in state 2. The two welfare theorems then imply that in any competitive equilibrium without aggregate risk, agents will use financial markets to diversify away all idiosyncratic risk in their individual endowments. This result holds true even when individual investors differ in their risk aversion. Case 2: $ w^{1} \\neq w^{2} $ $$ \\begin{array}{l} \\theta u^{\\prime}\\left(c_{1}^{1}\\right)=(1-\\theta) u^{\\prime}\\left(w^{1}-c_{1}^{1}\\right) \\ \\theta u^{\\prime}\\left(c_{1}^{2}\\right)=(1-\\theta) u^{\\prime}\\left(w^{2}-c_{1}^{2}\\right) \\end{array} $$ Next, suppose that $ w^{1} \\neq w^{2}, $ so that there is aggregate risk. In this case, the two equations are different. At least one of the two types of investors will have consumption that differs across states 1 and 2 at $ t=1 . $ Aggregate risk, by definition, cannot be diversified away. An interesting special case arises when the two investor types have Bernoulli utility functions of the same CRRA form so that $$ u_{j}(c)=\\frac{c^{1-\\gamma}-1}{1-\\gamma} $$ with $ \\gamma\u003e0 $ for $ j=1,2, $ and $ u_{j}^{\\prime}(c)=c^{-\\gamma} $ When investors have identical CRRA utility functions $$ \\begin{array}{l} \\theta u^{\\prime}\\left(c_{1}^{1}\\right)=(1-\\theta) u^{\\prime}\\left(w^{1}-c_{1}^{1}\\right) \\ \\theta u^{\\prime}\\left(c_{1}^{2}\\right)=(1-\\theta) u^{\\prime}\\left(w^{2}-c_{1}^{2}\\right) \\end{array} $$ specialize to $$ \\begin{array}{l} \\theta\\left(c_{1}^{1}\\right)^{-\\gamma}=(1-\\theta)\\left(w^{1}-c_{1}^{1}\\right)^{-\\gamma} \\ \\theta\\left(c_{1}^{2}\\right)^{-\\gamma}=(1-\\theta)\\left(w^{2}-c_{1}^{2}\\right)^{-\\gamma} \\end{array} $$ implies $$ \\begin{array}{l} \\theta^{1 / \\gamma}\\left(w^{1}-c_{1}^{1}\\right)=(1-\\theta)^{1 / \\gamma} c_{1}^{1} \\ \\theta^{1 / \\gamma}\\left(w^{2}-c_{1}^{2}\\right)=(1-\\theta)^{1 / \\gamma} c_{1}^{2} \\end{array} $$ can be solved for $$ \\begin{aligned} \u0026c_{1}^{1}=w^{1} \\frac{\\theta^{1 / \\gamma}}{\\theta^{1 / \\gamma}+(1-\\theta)^{1 / \\gamma}} \\quad \\text{ and } \\quad c_{1}^{2}=w^{2} \\frac{\\theta^{1 / \\gamma}}{\\theta^{1 / \\gamma}+(1-\\theta)^{1 / \\gamma}} \\ \u0026c","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:3:0","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.4 Summary and Comparisons ","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:4:0","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.4.1 Summary The A-D model is an explicit equilibrium model of asset prices. Through the equilibrium condition $$ q^{i}=\\frac{\\beta \\pi_{i} u_{j}^{\\prime}\\left(c_{j}^{i}\\right)}{u_{j}^{\\prime}\\left(c_{j}^{0}\\right)} $$ which must hold for all states $ i=1,2, \\ldots, N $ and all investors $ j=1,2, \\ldots, K, $ the A-D model links asset prices to aggregate un-diversifiable risk in the economy as a whole. The A-D model’s generality has been both a strength and a weakness. It is a strength, because the A-D model makes no special assumptions about investors' preferences or the distribution of asset returns. But it is also a weakness, since it is difficult to see, at least at first, how it can be applied to think about assets that are actually traded in financial markets, like stocks, bonds, and options ","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:4:1","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.4.2 Arrow-Debreu v.s. CAPM Since MPT and the CAPM were developed around the same time, it is useful to consider how each of these two approaches brings economic analysis to bear on the problem of pricing risky assets and cash flows. Markowitz, Sharpe, Lintner, and Mossin put $ \\sigma^2 $ on one axis and $ \\mu $ on the other. This allowed them to make more rapid progress, deriving important results for portfolio management and asset pricing. But the resulting theory proved very difficult to generalize: it requires either quadratic utility or normally distributed returns. \r Arrow and Debreu put consumption in the good state on one axis and consumption in the bad state on the other. Their theory requires none of the restrictive assumptions that underly the CAPM. But it took much longer to recognize its implications for asset pricing. \r In the end, both approaches proved to be incredibly valuable. Markowitz, Sharpe, Arrow, and Debreu all won Nobel Prizes. Advantages of the Arrow-Debreu approach: Investors do not have to have quadratic utility, or even expected utility functions. Returns do not need to be normally distributed The theory draws very explicit links between asset prices and the rest of the economy. ","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:4:2","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.4.3 APT and the A-D model The APT can also be compared to the no-arbitrage variant of Arrow-Debreu theory. The APT replaces the A-D model’s abstract description of “states of the world” with a more practical, or empirically-motivated depiction of the underlying sources of aggregate risk and then prices portfolios of assets based on their exposures to those sources of risk. ","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:4:3","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"8.4.4 Further progress Further progress with the A-D model can be made along two dimensions: The Consumption Capital Asset Pricing Model (CCAPM) is a special case of the A-D model that adds further assumptions in order to get more specific results and to draw deeper links between the A-D model and the traditional CAPM. The A-D model can also be used as a no-arbitrage theory of asset pricing. Through this approach, we can use existing assets to make inferences about what contingent claim prices should be, then use those contingent claim prices to price other assets as bundles of contingent claims, including assets like options with obviously non-normally distributed returns. The inflation-adjusted growth rate of the Standard and Poor’s index of stock prices is very volatile. \r So while real (inflation-adjusted) consumption growth does seem to be related to changes in stock prices \r stock prices seem much too volatile relative to consumption, compared to what the equilibrium version of Arrow-Debreu theory( consumption is a function of securities price) would predict. \r ","date":"0001-01-01","objectID":"/8.-arrow-debreu-pricing-equilibrium/:4:4","tags":null,"title":"","uri":"/8.-arrow-debreu-pricing-equilibrium/"},{"categories":null,"content":"9. Arrow-Debreu Pricing: No-Arbitrage We have already used the Arrow-Debreu model as an equilibrium theory of asset pricing. But the model also works as a no-arbitrage theory, in which (1) contingent claims prices are inferred from existing asset prices and then (2) used to price other assets and risky cash flows. The framework cleverly and usefully sidesteps the important, but as yet unresolved, problem of making assumptions about investors' utility functions or the distribution of consumption and/ or asset returns. Two innovative papers along these lines appeared in the late 1970 s, around the same time that Robert Lucas was working out the details of the CCAPM. Merton Miller won the Nobel Prize in 1990 . Douglas Breeden and Robert Litzenberger, “Prices of State-Contingent Claims Implicit in Option Prices,” Journal of Business Vol.51 (October 1978 ): pp.621-651. Rolf Banz and Merton Miller, “Prices for State-Contingent Claims: Some Estimates and Applications,” Journal of Business Vol.51 (October 1978 ): pp.653-672. ","date":"0001-01-01","objectID":"/9.-arrow-debreu-pricing-no-arbitrage/:0:0","tags":null,"title":"","uri":"/9.-arrow-debreu-pricing-no-arbitrage/"},{"categories":null,"content":"9.1 The Value Additivity Theorem Before moving on to consider the more realistic case with uncertainty in full detail, it is useful to consider a result that is interesting in its own right, but also useful in underscoring one of the most important lessons that we drew from our analysis of the CAPM, that only aggregate risk is reflected in asset prices and returns. To illustrate the result, consider once more the case where there are two dates, $ t=0 $ and $ t=1, $ and $ i=1,2, \\ldots, N $ possible states at $ t=1 $. And consider two complex securities, $ j=1 $ and $ j=2, $ with prices $ p_{1}^{A} $ and $ p_{2}^{A} $ at $ t=0 $ and payoffs $ z_{1}^{i} $ and $ z_{2}^{i} $ in each state $ i=1,2, \\ldots, N $ at $ t=1 $. 【Theorem】The Value Additivity Theorem says that if there is a third asset, $ j=3, $ with payoffs that are, in every state, equal to the same linear combination of the payoffs provided by assets $ j=1 $ and $ j=2, $ so that $$ z_{3}^{i}=\\alpha z_{1}^{i}+\\beta z_{2}^{i} \\quad \\text { for all } i=1,2, \\dots, N $$ then the price of asset $ j=3 $ must be the same linear combination of the prices of assets $ j=1 $ and $ j=2, $ so that $$ p_{3}^{A}=\\alpha p_{1}^{A}+\\beta p_{2}^{B} $$ Proof: The Value Additivity Theorem follows from a familiar no-arbitrage argument. since $$ z_{3}^{i}=\\alpha z_{1}^{i}+\\beta z_{2}^{i} \\quad \\text { for all } i=1,2, \\dots, N $$ the payoffs from asset $ j=3 $ can be reproduced by the portfolio formed from $ \\alpha $ units of asset $ j=1 $ and $ \\beta $ units of asset $ j=2 . $ The cost of this portfolio is $$ \\alpha p_{1}^{A}+\\beta p_{2}^{B} $$ So if $ p_{3}^{A}\u003e\\alpha p_{1}^{A}+\\beta p_{2}^{B}, $ it would be profitable to sell asset $ j=3 $ and buy the portfolio, and if $ p_{3}^{A}\u003c\\alpha p_{1}^{A}+\\beta p_{2}^{B}, $ it would be profitable to buy asset $ j=3 $ and sell the portfolio. ","date":"0001-01-01","objectID":"/9.-arrow-debreu-pricing-no-arbitrage/:1:0","tags":null,"title":"","uri":"/9.-arrow-debreu-pricing-no-arbitrage/"},{"categories":null,"content":"Perfectly Negatively Correlated Case As an interesting and revealing special case, suppose that the payoffs on assets $ j=1 $ and $ j=2 $ are perfectly negatively correlated, so that $$ \\alpha z_{1}^{i}+\\beta z_{2}^{i}=Z \\quad \\text { for all } i=1,2, \\dots, N $$ where $ Z $ is constant across all states $ i=1,2, \\ldots, N $. In this case, asset $ j=3 $ is risk free, and so the return to holding asset $ j=3 $ must equal the risk-free rate. But how could the two risky assets, $ j=1 $ and $ j=2, $ provide higher rates of return while yielding the risk-free rate in combination? The answer is that they cannot. since the payoffs from assets $ j=1 $ and $ j=2 $ are perfectly negatively correlated, those payoffs contain only idiosyncratic risk. And, as we have seen from the CAPM, idiosyncratic risk is not priced. Note once again that none of the arguments we’ve worked through so far has made any reference at all to preferences or distributions of consumption and/or returns. That is the appeal of the no-arbitrage approach: it requires none of the special assumptions that cause problems for the CAPM. ","date":"0001-01-01","objectID":"/9.-arrow-debreu-pricing-no-arbitrage/:1:1","tags":null,"title":"","uri":"/9.-arrow-debreu-pricing-no-arbitrage/"},{"categories":null,"content":"9.2 Market Completeness and Complex Securities At the beginning of the term, we briefly discussed the concept of market completeness. We can now define and discuss the concept in more detail. 【Definition】Financial markets are complete if, for each possible state of each future date, there exists a market for a contingent claim that pays off one unit of consumption in that date-state combination and zero otherwise. Of course, we do not see pure contingent claims being traded in any financial market. But “synthetic” contingent claims can often be constructed from portfolios of complex securities that make payoffs in more than one state-date combination like those we do see traded. Typically, however, this requires that markets be complete. ","date":"0001-01-01","objectID":"/9.-arrow-debreu-pricing-no-arbitrage/:2:0","tags":null,"title":"","uri":"/9.-arrow-debreu-pricing-no-arbitrage/"},{"categories":null,"content":"9.2.1 Example To see how this works, let’s begin with an example in which markets are complete, and use contingent claims prices to price a complex asset. Suppose that there are two dates, today $ (t=0) $ and “next period” $ (t=1) . $ And suppose there are three possible states, $ i=1,2,3, $ at $ t=1 $. Financial markets are complete, therefore, if contingent claims are traded at $ t=0 $ for all three states at $ t=1 $. Assume, therefore, that we observe the three contingent claims prices: $ q^{1}=0.60, q^{2}=0.20, $ and $ q^{3}=0.15 $. Consider a complex security (security $ j=1 $ ) with payoff $ z_{1}^{1}=3 $ in state $ 1, $ payoff $ z_{1}^{2}=2 $ in state $ 2, $ and payoff $ z_{1}^{3}=0 $ in state 3 $$ \\begin{array}{l} q^{1}=0.60, q^{2}=0.20, q^{3}=0.15 \\ z_{1}^{1}=3, z_{1}^{2}=2, z_{1}^{3}=0 \\end{array} $$ Since the payoffs provided by the complex security can be replicated by a portfolio consisting of 3 contingent claims for state 1 and 2 contingent claims for state 2, the price of the complex security must equal $ 0.60 \\times 3+0.20 \\times 2=2.20 . $ The general formula is $$ p_{1}^{A}=q^{1} z_{1}^{1}+q^{2} z_{1}^{2}+q^{3} z_{1}^{3} $$ In this first example, we did not “need” the contingent claim for state 3. But suppose we want to consider a second complex security (security $ j=2) $ with payoff $ z_{2}^{1}=1 $ in state $ 1, $ payoff $ z_{2}^{2}=1 $ in state $ 2, $ and payoff $ z_{2}^{3}=1 $ in state 3 $$ \\begin{array}{l} q^{1}=0.60, q^{2}=0.20, q^{3}=0.15 \\ z_{2}^{1}=1, z_{2}^{2}=1, z_{2}^{3}=1 \\end{array} $$ since the payoffs provided by the complex security can be replicated by a portfolio consisting of 1 contingent claim for state 1, 1 contingent claim for state 2, and 1 contingent claim for state 3, the price of the complex security must equal $$ p_{2}^{A}=q^{1} z_{2}^{1}+q^{2} z_{2}^{2}+q^{3} z_{2}^{3}=0.6+0.2+0.15=0.95 $$ To price assets $ j=1 $ and $ j=2, $ we need all three contingent claims. Partly for practice but also to set the stage for the next step in our analysis, consider a third complex security $ (j=3), $ with payoff $ z_{3}^{1}=2 $ in state 1, payoff $ z_{3}^{2}=0 $ in state $ 2, $ and payoff $ z_{3}^{3}=2 $ in state 3 $$ \\begin{aligned} q^{1} \u0026=0.60, q^{2}=0.20, q^{3}=0.15 \\ z_{3}^{1} \u0026=2, z_{3}^{2}=0, z_{3}^{3}=2 \\ p_{3}^{A} \u0026=q^{1} z_{3}^{1}+q^{2} z_{3}^{2}+q^{3} z_{3}^{3}=0.6 * 2+0.15 * 2=1.5 \\end{aligned} $$ Now, let’s turn the problem around. Suppose we observe the payoffs and prices of the three complex securities: \r Can we use these data to infer the prices of the three contingent claims and thereby con\u000crm that markets are complete, even without explicit markets for the contingent claims? Consider a portfolio consisting of $ w_{1}^{1} $ “units” of asset 1, $ w_{1}^{2} $ units of asset 2, and $ w_{1}^{3} $ units of asset 3. This portfolio has payoffs: $$ \\begin{aligned} w_{1}^{1} z_{1}^{1}+w_{1}^{2} z_{2}^{1}+w_{1}^{3} z_{3}^{1}\u0026=3 w_{1}^{1}+w_{1}^{2}+2 w_{1}^{3} \u0026 \\text { in state 1} \\ w_{1}^{1} z_{1}^{2}+w_{1}^{2} z_{2}^{2}+w_{1}^{3} z_{3}^{2}\u0026=2 w_{1}^{1}+w_{1}^{2} \u0026\\text { in state 2} \\ w_{1}^{1} z_{1}^{3}+w_{1}^{2} z_{2}^{3}+w_{1}^{3} z_{3}^{3}\u0026=w_{1}^{2}+2 w_{1}^{3} \u0026\\text { in state 3} \\end{aligned} $$ The portfolio has payoffs: $$ \\begin{aligned} \u00263 w_{1}^{1}+w_{1}^{2}+2 w_{1}^{3} \u0026 \\text { in state } 1 \\ \u0026 2 w_{1}^{1}+w_{1}^{2} \u0026 \\text { in state } 2 \\ \u0026 w_{1}^{2}+2 w_{1}^{3} \u0026 \\text { in state } 3 \\end{aligned} $$ Let’s use this portfolio “synthesize” a contingent claim for state 1. We need: $$ \\begin{aligned} 3 w_{1}^{1}+w_{1}^{2}+2 w_{1}^{3} \u0026=1 \\ 2 w_{1}^{1}+w_{1}^{2} \u0026=0 \\ w_{1}^{2}+2 w_{1}^{3} \u0026=0 \\end{aligned} $$ We have a system of three linear equations in three unknowns: $$ \\begin{aligned} 3 w_{1}^{1}+w_{1}^{2}+2 w_{1}^{3} \u0026=1 \\ 2 w_{1}^{1}+w_{1}^{2} \u0026=0 \\ w_{1}^{2}+2 w_{1}^{3} \u0026=0 \\end{aligned} $$ The solution has $$ \\begin{array}{l} w_{1}^{1}=1 / 3 \\ w_{1}^{2}=-2 / 3 \\ w_{1}^{3}=1 / 3 \\end{array} $$ Buying a portfolio consisting of $ 1 / 3 $ unit of asset 1, $ -2 / 3 $ un","date":"0001-01-01","objectID":"/9.-arrow-debreu-pricing-no-arbitrage/:2:1","tags":null,"title":"","uri":"/9.-arrow-debreu-pricing-no-arbitrage/"},{"categories":null,"content":"9.2.2 Market Completeness Condition These examples are illustrative of two more general results. 【Proposition 1】If markets are complete, any complex security or cash flow stream can be replicated as a portfolio of contingent claims. This proposition underlies our ability to price complex assets using contingent claims prices. To work in reverse, and construct contingent claims from complex assets, it was important that we had three complex assets: one for each state at $ t=1 . $ Otherwise, we would have had more equations (states) than unknowns (amounts of each asset to buy). More generally, if there are $ N $ states at $ t=1 $, we would need $ M=N $ different complex assets to have the same number of equations as unknowns. Suppose, however, that in our example the third asset had payoffs $ z_{3}^{1}=2, z_{3}^{2}=2 $ and $ z_{3}^{3}=2 $ instead of $ z_{3}^{1}=2, z_{3}^{2}=0 $ and $ z_{3}^{3}=2 $. A technical problem would arise, because although there are three complex assets, buying one unit of asset 3 yields exactly the same payoffs as buying two units of asset $ 2, $ with $ z_{2}^{1}=1, z_{2}^{2}=1 $ and $ z_{2}^{3}=1 $. In effect, we would have had only two “linearly independent” assets: again, in our systems, we would have had more equations (states) than unknowns (amounts of each asset to buy). More generally, the “linear independence” requirement means that it should not be possible to replicate exactly the payoffs from one complex security using a portfolio of the other complex securities. But subject to this caveat, we have our second general result. 【Proposition (2)】If $ M=N, $ where $ M $ is the number of complex securities and $ N $ the number of states, and if all of the $ M $ complex securities have linearly independent payoffs, then (i) it is possible to infer the prices of contingent claims from the prices of the complex securities and (ii) markets are complete. Once we are able to infer contingent claims prices from the prices of existing complex securities, we can use no-arbitrage arguments to price other complex securities and risky cash flows. In particular, the value at $ t=0 $ of a project that returns $ X^{i} $ in state $ i=1,2, \\ldots N $ at $ t=1 $ can be calculated as $$ P^{A}=\\sum_{i=1}^{N} q^{i} X^{i} $$ This first round of examples shows how the Arrow-Debreu “friction” of complete markets for contingent claims is not so fanciful after all. We can now turn our attention to the practical question: which real-world assets are most convenient to use in solving for contingent claims prices. In cases without uncertainty, interest rates on bonds of different terms to maturity will work. In cases with uncertainty, options prices are especially valuable. ","date":"0001-01-01","objectID":"/9.-arrow-debreu-pricing-no-arbitrage/:2:2","tags":null,"title":"","uri":"/9.-arrow-debreu-pricing-no-arbitrage/"},{"categories":null,"content":"9.3 Using the Term Structure of Interest Rates In multi-period problems without uncertainty, discount bonds of different maturities are contingent claims. A discount bond with $ T $ years to maturity costs $ P(T) $ today and pays off one dollar $ T $ years from now. Under certainty, this bond has the same payoff as a contingent claim that returns one dollar in the one possible state $ T $ years from now. If we know the bond’s price $ P(T), $ we also know the contingent claims price $ q(T) $ The term structure of interest rates is the family of interest rates, $ r(1), r(2), r(3), \\ldots, $ on risk-free discount bonds with terms $ t=1,2,3, \\ldots $ to maturity. Since the interest rate on a $ T $ -year discount bond is defined by $$ P(T)=\\frac{1}{[1+r(T)]^{T}} $$ there is a direct connection between the term structure of interest rates and the contingent claims prices in cases without uncertainty. This insight also tells us that if we want to use the Arrow-Debreu approach to value a multi-period project with certain cash flows $ X(1), X(2), \\ldots, X(T) $ over the next $ T $ years, we can simply discount those future cash flows using the term structure of interest rates: $$ \\begin{aligned} P^{A} \u0026=q(1) X(1)+q(2) X(2)+\\ldots+q(T) X(T) \\ \u0026=P(1) X(1)+P(2) X(2)+\\ldots+P(T) X(T) \\ \u0026=\\frac{X(1)}{1+r(1)}+\\frac{X(2)}{[1+r(2)]^{2}}+\\ldots+\\frac{X(T)}{[1+r(T)]^{T}} \\end{aligned} $$ As we discussed previously, the US Treasury issues bills with maturities less than one year that are structured as discount bonds. Longer-term US Treasury bonds make regular interest (“coupon”) payments. But the US Treasury allows financial institutions to break these bonds down into portfolios of separately-traded discount bonds, called STRIPS (Separate Trading of Registered Interest and Principal of Securities). Even without STRIPS, however, it is possible to construct “synthetic” discount bonds from portfolios of coupon bonds. To see how, consider two coupon bonds, with the same term to maturity but different coupon payments. A five-year bond with 1000 face (or par) value and 5 percent annual coupon rate makes payments of 50 each year, every year, for the next five years and then returns 1000 at the end of five years. A five-year bond with 1000 face (or par) value and 10 percent annual coupon rate makes payments of 100 each year, every year, for the next five years and then returns 1000 at the end of five years. The second bond sells at a higher price today, in light of its higher coupon rate. Consider the strategy of buying two of the bonds with the 5 percent coupon and selling one bond with the 10 percent coupon, in order the “cancel out” the interest payments. \r The portfolio of bonds costs 600 today and pays 1000 at the end of five years. These cash flows are identical to those provided by a discount bond that costs 600 today and pays 1000 five years from now. We can compute the 5 -year interest rate in the term structure using $$ 600=\\frac{1000}{[1+r(5)]^{5}} \\Rightarrow r(5)=0.1076 $$ and the associated contingent claim price as $$ q(5)=\\frac{1}{[1+r(5)]^{5}}=0.60 $$ Of course, only rarely will it be appropriate to assume there is no uncertainty when pricing assets or cash flows. In those circumstances, however, we can use information in the term structure of interest rates or, equivalently, in the prices of coupon bonds to infer contingent claims prices. ","date":"0001-01-01","objectID":"/9.-arrow-debreu-pricing-no-arbitrage/:3:0","tags":null,"title":"","uri":"/9.-arrow-debreu-pricing-no-arbitrage/"},{"categories":null,"content":"9.4 Using Options ","date":"0001-01-01","objectID":"/9.-arrow-debreu-pricing-no-arbitrage/:4:0","tags":null,"title":"","uri":"/9.-arrow-debreu-pricing-no-arbitrage/"},{"categories":null,"content":"9.4.1 To Complete the Market Consider an example with two periods, $ t=0 $ and $ t=1 $, and three possible states, $ i=1,2,3 $, at $ t=1 $ Suppose only one asset is traded: a complex security with payoff $ P_{s}^{1}=1 $ in state $ 1, P_{s}^{2}=2 $ in state $ 2, $ and $ P_{s}^{3}=3 $ in state 3. Think of this asset as a stock, so that the payoff $ P_{s}^{i} $ is the share price, obtained by selling the stock at $ t=1 $. A call option is a contract that gives the buyer the right, but not the obligation, to purchase a share of stock at the strike price $ K $ at $ t=1 $ or, more generally, on or before some expiration date $ T $. At $ t=1, $ the call is said to be in the money if the actual share price is above the strike price and out of the money if the actual share price is below the strike price. At $ t=1, $ the option will have value only if it is in the money. But at $ t=0, $ the option will have value even if there is only a probability of it being in the money at $ t=1 $. With the stock as the only traded asset, and three states at $ t=1 $ financial markets are incomplete. Suppose, however, that we introduce two options on the stock: one with strike price $ K=1 $ and the other with strike price $ K=2 $ Let $ C_{1}^{i}(S, 1) $ denote the payoffs generated by the call option on the stock $ S $ with strike price $ K=1 $ with expiration date at $ T=1 $ in each state $ i=1,2,3 $. Let $ C_{1}^{i}(S, 2) $ denote the payoffs generated by the call option on the stock $ S $ with strike price $ K=2 $ with expiration date at $ T=1 $ in each state $ i=1,2,3 $. We can use information on the stock price to determine the payoffs from the two call options: \r Now we can ask if the addition of the two options make markets complete. The answer is yes: The option with strike price 2 is a contingent claim for state 3. Buying one call with strike price 1 and selling (“writing”) 2 calls with strike price 2 creates a contingent claim for state 2. Buying one share of the stock, selling 2 calls with strike price 1 and buying one call with strike price 2 creates a contingent claim for state 1 Of course, it would also be possible to “complete the market” with two other assets, so long as their payoffs are linearly independent. But options are an obvious choice, since they are related to the assets that are already traded. And since they often give rise to a structure of payoffs across the complex assets that makes solving for contingent claims prices relatively easy. Unfortunately, it is not always possible to complete the market using a single stock and a set of call options on that stock (linearly independent need to be satisfied). To see why, suppose that instead of having $ P_{s}^{1}=1, P_{s}^{2}=2, $ and $ P_{s}^{3}=3, $ the stock in our example had $ P_{s}^{1}=2, P_{s}^{2}=2, $ and $ P_{s}^{3}=3 $. Since its price does not differ across states 1 and $ 2, $ it won’t be possible to use this stock and the associated options to obtain complete markets. Recomputing the payoffs from options with strike prices $ K=1 $ and $ K=2 $: \r In this case, a portfolio formed by buying two calls with strike price $ K=1 $ and writing one call with strike price $ K=2 $ yields the same payoffs as the stock itself. Markets are not complete. Fortunately, this problem can often be sidestepped by first forming a portfolio of stocks and then introducing a set of call options on the entire portfolio. Suppose we have two stocks, $ s=1 $ and $ s=2 $, with payoffs: \r Stock 1 does not “distinguish” between states 1 and 2 , and stock 2 does not distinguish between states 2 and 3. \r But consider the payoffs provided by a portfolio $ P $ consisting of one share of stock 1 and one share of stock 2. Now the payoffs from the portfolio differ across all states. Let’s add call options on the portfolio with strike prices $ K=2 $ and $ K=3 $. \r since the payoffs from the portfolio and the two options on the portfolio are linearly independent, markets are complete. You can confirm that:","date":"0001-01-01","objectID":"/9.-arrow-debreu-pricing-no-arbitrage/:4:1","tags":null,"title":"","uri":"/9.-arrow-debreu-pricing-no-arbitrage/"},{"categories":null,"content":"9.4.2 To Infer Contingent Claims Prices We can now combine the message of our last proposition with another basic lesson we learned from studying the CAPM to get a practical idea as to how to infer contingent claims prices under uncertainty in the real world. Proposition 3 indicates that it is helpful to start with a portfolio with a payoff pattern that distinguishes among all future states. The CAPM suggests that this portfolio is likely to be the market portfolio, since returns on the market portfolio will reflect all underlying sources of aggregate risk. So let’s see how we can use the market portfolio and associated call options to infer contingent claims prices. Let $ P_{s}^{i} $ now denote the value of a share in the market portfolio in each state $ i=1,2, \\ldots, N $ at $ t=1 $. To streamline the notation, rearrange the labeling of the states as necessary so that those with higher indices $ i $ correspond to more favorable outcomes for the stock market: $$ P_{s}^{1}\u003cP_{s}^{2}\u003c\\ldots\u003cP_{s}^{N} $$ and assume, as well, that there is a constant increment $ \\delta\u003e0 $ by which stock prices rise across states, so that $$ P_{s}^{i+1}=P_{s}^{i}+\\delta \\quad \\text { for all } i=1,2, \\dots, N-1 $$ This last assumption involves an approximation if the value of the market portfolio can vary continuously, but the approximation can be made arbitrarily good by choosing $ \\mathrm{N} $ sufficiently large and $ \\delta $ sufficiently small. Now for any particular state $ i, $ construct a portfolio by Buying one call with strike price $ P_{s}^{i-1}=P_{s}^{i}-\\delta $ Writing two calls with strike price $ P_{s}^{i} $ Buying one call with strike price $ P_{s}^{i+1}=P_{s}^{i}+\\delta $ Now let’s see how the payoffs on the portfolio of options depends on the stock price at $ t=1 $: \r This portfolio pays off $ \\delta $ in state $ i $ and zero otherwise. Hence, it is equivalent to a portfolio of $ \\delta $ contingent claims for state $ i $. Letting $ V_{0}(S, K) $ denote the price (value) at $ t=0 $ of the call option on the stock with strike price $ K, $ the cost of assembling this portfolio of options is: $$ V_{p}^{0}=V_{0}\\left(S, P_{s}^{i}+\\delta\\right)+V_{0}\\left(S, P_{s}^{i}-\\delta\\right)-2 V_{0}\\left(S, P_{s}^{i}\\right) $$ And since this portfolio of options is equivalent to a portfolio of $ \\delta $ contingent claims for state $ i, $ we can now infer that the price of a contingent claim for state $ i $ is $$ q^{i}=\\frac{1}{\\delta}\\left[V_{0}\\left(S, P_{s}^{i}+\\delta\\right)+V_{0}\\left(S, P_{s}^{i}-\\delta\\right)-2 V_{0}\\left(S, P_{s}^{i}\\right)\\right] $$ Note that we don’t have to actually buy the options if all we want to do is to price the contingent claim, we simply need to observe the option prices. And if the options we need are not actually traded, we can use an options pricing formula to figure out what their prices should be. ","date":"0001-01-01","objectID":"/9.-arrow-debreu-pricing-no-arbitrage/:4:2","tags":null,"title":"","uri":"/9.-arrow-debreu-pricing-no-arbitrage/"},{"categories":null,"content":"9.4.3 Black-Scholes Option Pricing We have now seen how we can use options prices together with no-arbitrage arguments to make inferences about contingent claims prices. More sophisticated no-arbitrage arguments constructed by Fischer Black (US, 1938-1995), Myron Scholes (Canada/US, b.1941, Nobel Prize 1997 ), and Robert Merton (US, b.1944, Nobel Prize 1997 ) showed how options prices could be inferred from assumptions about and observations on the underlying stock price. Their papers were both published in 1973. Fischer Black and Myron Scholes, “The Pricing of Options and Corporate Liabilities,” Journal of Political Economy Vol.81 (May-June 1973 ): pp.637-654. Robert Merton, “Theory of Rational Option Pricing,” The Bell Journal of Economics and Management Science Vol.4 (Spring 1973): pp.141-183. To see how the theory works, continue to assume a simple two-period structure, with $ t=0 $ and $ t=1, $ and assume as well, that there are only two states, $ i=G $ and $ i=B, $ at $ t=1 . $ Let $ P_{0}= $ price of the stock at $ t=0 $ $ P^{G}=$ price of the stock in state $ i=G $ at $ t=1 $ $ P^{B}= $ price of the stock in state $ i=B $ at $ t=1 $ $ r_{f}= $ risk-free interest rate between $ \\mathrm{t}=0 $ and $ \\mathrm{t}=1 $ $ \\pi= $ probability of the good state $ i=G $ at $ t=1 $ The stock price is $ P_{0} $ at $ t=0 $ and either $ P^{G} $ or $ P^{B} $ at $ t=1 $ \r Now consider a call option on the stock with strike price $ K $. Let $ V_{0}= $ price (value) of the call at $ t=0 $ $ C^{G}= $ payoff generated by the call in state $ i=G $ at $ t=1 $ $ C^{B}= $ payoff generated by the call in state $ i=B $ at $ t=1 $ Assume, for now, that the call is in the money in both states at $ t=1 $. Then: $$ \\qquad C^{G}=P^{G}-K \\quad \\text { and } \\quad C^{B}=P^{B}-K $$ We can easily compute the expected payoff as $ \\pi\\left(P^{G}-K\\right)+(1-\\pi)\\left(P^{B}-K\\right), $ but the option price $ V_{0} $ must still be corrected for risk. One of the key insights that underlies the Black-Scholes formula is that we don’t need to make any specific assumptions about preferences or the nature of aggregate risk to price the option. Instead, we can use a no-arbitrage argument that: Replicates the option’s payoffs using a portfolio of the stock and a risk-free bond. Recognizes that risk is already priced into the stock. We want to construct a portfolio consisting of $ S $ shares of the stock and $ \\mathrm{B} $ bonds that replicates the payoffs from the option in both states at $ t=1 $ $$ \\begin{aligned} S P^{G}+B\\left(1+r_{f}\\right) \u0026=P^{G}-K \\ S P^{B}+B\\left(1+r_{f}\\right) \u0026=P^{B}-K \\end{aligned} $$ \r This is a set of two linear equations in the two unknowns: $ S $ and $ B $. The solution is $$ S=1 \\quad \\text { and } \\quad B=-\\frac{K}{1+r_{f}} $$ since the stock costs $ P_{0} $ and the bond costs 1 , the cost of this portfolio at $ t=0 $ is $$ P_{0}-\\frac{K}{1+r_{f}} $$ This means that the price of the option must also be $$ V_{0}=P_{0}-\\frac{K}{1+r_{f}} $$ Note that the price of the option is not equal to the expected value of its payoff, for the same reason that the stock price $ P_{0} $ will not be the expected value of its payoff: both prices need to be adjusted for risk. Yet, our clever use of no-arbitrage arguments allows us to price the option without making any assumptions about risk and, for that matter, without even having to know the probabilities $ \\pi $ and $ 1-\\pi $ of the two states at $ t=1 . $ All of this information has already been incorporated into the stock price $ P_{0} $. For future reference, let’s write the solution for the option price in this first case as $$ V_{0}=N_{1} P_{0}-N_{2} \\frac{K}{1+r_{f}} $$ where, in this case, $$ N_{1}=1 \\quad \\text { and } \\quad N_{2}=1 $$ Next, let’s consider the case in which the call is in the money in the good state and out of the money in the bad state at $ t=1 $ $$ C^{G}=P^{G}-K \\quad \\text { and } \\quad C^{B}=0 $$ Again we want to construct a portfolio consisting of $ S $ shares of the st","date":"0001-01-01","objectID":"/9.-arrow-debreu-pricing-no-arbitrage/:4:3","tags":null,"title":"","uri":"/9.-arrow-debreu-pricing-no-arbitrage/"},{"categories":null,"content":"1. Valuation of Risky Cash Flow ","date":"0001-01-01","objectID":"/part1-4/:0:0","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"1.1 Inter-temporal Optimization Static optimization is to optimally allocate spending across different goods at a point of time (e.g. maximize utility from consuming apples and bananas today subject to current budget constraint). Intertemporal optimization is to optimally allocate spending over time (e.g. maximize utility from consuming apples today and next year subject to intertemporal budget constraint). ","date":"0001-01-01","objectID":"/part1-4/:1:0","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"1.1.1 Time Dimension Intertemporal utility function For simplicity, economists typically assume separable, discounting and concave utility function in 2 periods $$ u\\left(C_{0}\\right)+\\beta u\\left(C_{1}\\right) $$ where $C_{0}$ and $C_{1}$ are consumptions today and next year A concave utility implies convex indifference curves with slope called intertemporal marginal rate of substitution (how much to give up today for next year) $u$ is concave utility function which implies consumption smoothing preference. \r $\\beta\u003c1$ is the discount factor, a measure of patience Intertemporal budget constraint Suppose $$ \\begin{aligned} Y_{0}\u0026=\\text { income today } \\ Y_{1}\u0026=\\text { income next year }\\ S\u0026= \\text{amount saved(or borrowed if negative) today}\\ r\u0026= \\text{interest rate} \\end{aligned} $$ Today $Y_{0} \\geq C_{0}+S$. Next year, $Y_{1}+(1+r) S \\geq C_{1}$. Thus the life-time budget constraint is $$ Y_{0}+\\frac{Y_{1}}{1+r} \\geq C_{0}+\\frac{C_{1}}{1+r} $$ The present value of income must be sufficient to cover the presen value of consumption over the two periods! \r The price of consumption today relative to the price of consumption next year is given by $$ \\frac{p_{0}}{p_{1}}=1+r $$ The slope of the intertemporal budget constraint is $-(1+r)$. Optimal Allocation At the optimum, the intertemporal marginal rate of substitution equals the slope of the budget constraint $$ \\frac{u'(C_0)}{\\beta u'(C_1)}=1+r $$ ","date":"0001-01-01","objectID":"/part1-4/:1:1","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"1.1.2 Risk Dimension To add/model the element of risk, let’s follow Arrow-Debreu by imagining that there are two possible outcomes: $$ \\begin{aligned} Y_{0} \u0026=\\text { income today } \\ Y_{1}^{G} \u0026=\\text { income next year under good state } \\end{aligned} $$ $Y_{1}^{B}=$ income next year under bad state where assumption $Y_{1}^{G}\u003eY_{1}^{B}$ makes good state good and $\\pi=$ probability of the good state $1-\\pi=$ probability of the bad state How to represent preferences over risky alternatives? Under uncertainty, the consumer maximizes expected utility $u\\left(C_{0}\\right)+\\beta u\\left(C_{1}\\right)$ which equivalent to $$ u\\left(C_{0}\\right)+\\beta\\left[\\pi u\\left(C_{1}^{G}\\right)+(1-\\pi) u\\left(C_{1}^{B}\\right)\\right] $$ We can see that concavity of the function u, which in the standard micro case represents a preference for diversity, represents a preference for smoothness in consumption over time and across states in the future–the consumer is risk averse in the sense that he does not want consumption in the bad state to be too much different from consumption in the good state. $$ Y_0 + \\underbrace{q_G Y^G_1 + q_B Y^B_1}{\\text{PV of future income}} ≥ C_0 + \\underbrace{q_G C^G_1 + q_B C^B_1}{\\text{PV of future consumption}} $$ \r \r Value a bond that pays its $100 coupon at the end of each year for 3-years, and its par value of $1,000 in 3-years You have discovered three pure discount bonds (each with a $1,000 par value) that mature in 1, 2, and 3 years, and that are trading at $960, $890, and $810 respectively \r 2. Making Choices in Risky Situations 1 Criteria for Choice Over Risky Prospects State-by-state Dominance Mean-variance Dominance Sharpe Ratio 2 Preferences and Utility Functions Preferences Preferences and Utility Functions 3 Expected Utility Functions 4 The Expected Utility Theorem Lottery Assumptions The Expected Utility Theorem 5 The Allais Paradox ","date":"0001-01-01","objectID":"/part1-4/:1:2","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"Criteria for Choice Over Risky Prospects “risk” refers to uncertainty about the future cash flows provided by a financial asset. state-by-state dominance over investments 1 and 2, because it pays as much in all states and strictly more in at least one state. Any investor who prefers more to less (nonsatiated in consumption) would always choose investment 3 above the others. mean-variance dominance over investment 2, since it offers a higher expected return with lower variance. Mean-variance dominance neither implies nor is implied by state-by-state dominance. In probability theory, if a random variable X can take on n possible values, X1; X2; … ; Xn, with probabilities p1; p2; …; pn, then the expected value of X is E(X) = p1X1 + p2X2 + … + pnXn the variance of X is σ 2 (X) = p1[X1 − E(X)]2 + p2[X2 − E(X)]2 + … + pn[Xn − E(X)]2 and the standard deviation of X is σ(X) = σ 2 (X) . In a mean-variance (M-V) framework, an investor’s wants to maximize a function $u\\left(\\mu_{r}, \\sigma_{P}\\right)$ She likes expected return $\\left(\\mu_{r}\\right)$ and dislikes standard deviation $\\left(\\sigma_{P}\\right)$ Recall that portfolio $A$ is said to exhibit mean-variance dominance over portfolio B if either $$ \\frac{\\mu_{A}\u003e\\mu_{B}}{\\mu_{A} \\geq \\mu_{B}} \\text { and } \\sigma_{A} \\leq \\sigma_{B} $$ Mean-variance dominance neither implies nor is implied by state-by-state dominance. Mean-variance Dominance can be expressed in the form of a criterion for selecting investments of equal magnitude For investments of the same Er, choose the one with lower σ For investments of the same σ, choose the one with greatest Er William Sharpe (US, b.1934, Nobel Prize 1990) suggested that in these circumstances, it can help to compare the two assets’ Sharpe ratios, defined as E(r)/σ(r). But using the Sharpe ratio to choose between assets means assuming that investors “weight” the mean and standard deviation equally, in the sense that a doubling of σ(r) is adequately compensated by a doubling of E(r). Investors who are more or less averse to risk will disagree. Summary State-by-state dominance is the most robust criterion, but often cannot be applied. Mean-variance dominance is more widely-applicable, but can sometimes be misleading and cannot always be applied. The Sharpe ratio can always be applied, but requires a very specific assumption about consumer attitudes towards risk. We need a more careful and comprehensive approach to comparing random cash flows. ","date":"0001-01-01","objectID":"/part1-4/:1:3","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"Preferences and Utility Functions rational: A1完备性、A2传递性 A.3. The preference relation is assumed to be continuous: if an and bn are two sequences of bundles such that an → a, bn → b and an \u0017 bn for all n, then a \u0017 b.Very small changes in consumption bundles cannot lead to large changes in preferences over those bundles. 理性是效用函数存在的必要非充分条件 理性+连续是连续效用函数存在的充分条件 \r An ordinal utility function describing a consumer’s preferences over two goods can be written as u(x, y), the same preferences could be expressed as another utility function that is an increasing transformation of u: g(x, y) = f (u(x, y)). Utility functions g and u give rise to identical indifference curve mappings. A cardinal utility function that preserves preference orderings uniquely up to positive affine transformations. Two utility indices are related by an affine transformation, i.e. u and v satisfies a relationship of the form v(x) = au(x) + b, where a and b are constants. ","date":"0001-01-01","objectID":"/part1-4/:1:4","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"不确定性下的效用函数：伯努利效用函数，VNM效用函数 期望效用函数 \r ","date":"0001-01-01","objectID":"/part1-4/:1:5","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"彩票 多结果彩票可以被看成复合彩票 因此彩票不失一般性 ","date":"0001-01-01","objectID":"/part1-4/:1:6","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"期望效用定理 \r \r By Theorem 3.1, we already know that (C:2) and (C:3) are su\u000ecient to guarantee the existence of a utility function U over lotteries and, by (C:1a), de\u000cned both on lotteries and speci\u000cc payo\u000bs received with certainty as well. \r \r \r 假设 \r \r 首先证明U(X)存在，然后证明关于概率线性 \r \r \r The concept of a Bernoulli utility function is ordinal （单调变换保序） the concept of vN-M utility function is cardinal!In this sense, the vN-M utility function that represents any given preference relation is not unique.（线性变换保序） ","date":"0001-01-01","objectID":"/part1-4/:1:7","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"独立性假设的悖论：Allais parodox The Allais paradox suggests that feelings about probabilities may not always be “linear,” but linearity in the probabilities is precisely what defines vN-M utility functions. 行为金融批判：独立性（效用函数对概率不一定线性），风 险厌恶（某些初始财富，有可能变得风险偏好） 3. Measuring Risk and Risk Aversion ","date":"0001-01-01","objectID":"/part1-4/:1:8","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"3.1 Measuring Risk Aversion If U[E(W )] \u003c E[U(W )] or the utility function is strictly convex, then the individual is a risk lover; If U[E(W )] = E[U(W )] or the utility function is linear, then the individual is risk neutral; If U[E(W )] \u003e E[U(W )] or the utility function is strictly concave, then the individual is risk averse; coeffcient of risk aversion \u0026 Probability premium coeffcient of absolute risk aversion $$ R_{A}(Y)=-\\frac{u^{\\prime \\prime}(Y)}{u^{\\prime}(Y)} $$ coeffcient of relative risk aversion $$ R_{R}(Y)=-\\frac{Y u^{\\prime \\prime}(Y)}{u^{\\prime}(Y)} $$ To interpret the two measures of risk aversion, it is helpful to recall from calculus the theorem stated by Brook Taylor (England, $1685-1731$ ), regarding the approximation of a function $f$ using its derivatives: the “first-order” approximation $$ f(x+a)=f(x)+f^{\\prime}(x) a+o(a) $$ and the “second-order” approximation $$ f(x+a)=f(x)+f^{\\prime}(x) a+\\frac{1}{2} f^{\\prime \\prime}(x) a^{2}+o\\left(a^{2}\\right) $$ \r 注意上述等式与概率溢价的联系 Probability premium $\\pi(w, \\epsilon, u)$ $$ L=(0.5+\\pi \\circ w+\\epsilon, 0.5-\\pi \\circ w-\\epsilon) $$ Find the $\\pi$ to make $C E(L)=w,$ that is $$ \\begin{aligned} \u0026 U(L)=u(w)=(0.5+\\pi) u(w+\\epsilon)+(0.5-\\pi) u(w-\\epsilon) \\ \\Rightarrow \\quad \u0026 \\pi(w, \\epsilon ; u) \\end{aligned} $$ 风险溢价(Risk premium)与确定性等价 Jensen’s Inequality Let $g$ be a concave function and $\\tilde{x}$ be a random variable. Then $$ g[E(\\tilde{x})] \\geq E[g(\\tilde{x})] $$ Furthermore, if $g$ is strictly concave and the probability that $\\tilde{x} \\neq E(\\tilde{x})$ is greater than zero, the inequality is strict. An implication of Jensen’s inequality is that the maximum riskless payoff that a risk-averse investor is willing to exchange for the asset with random payoff $\\tilde{Z}$, called the certainty equivalent for that asset, will always be less than $E(\\tilde{Z})$ The difference between the higher expected value $E(\\tilde{Z})$ and the smaller certainty equivalent $C E(\\tilde{Z})$ can then be used to define the positive risk premium $\\Pi(\\tilde{Z})$ for the asset: $$ \\Pi(\\tilde{Z})=E(\\tilde{Z})-C E(\\tilde{Z}) $$ Mathematically, the certainty equivalent $C E(\\tilde{Z})$ and risk premium $\\Pi(\\tilde{Z})$ for an asset with random payoff $\\tilde{Z}$ with expected value $E(\\tilde{Z})$ are defined by $$ E(u(Y+\\tilde{Z}))=u[Y+C E(\\tilde{Z})] $$ “the expected utility from buying the asset equals the utility from getting the certainty equivalent for sure,” and $$ E(u(Y+\\tilde{Z}))=u[Y+E(\\tilde{Z})-\\Pi(\\tilde{Z})] $$ assume for simplicity that $E(\\tilde{Z})=0$ and consider a second-order Taylor approximation to $E(u(Y+\\tilde{Z}))$ $$ \\begin{aligned} E(u(Y+\\tilde{Z})) \u0026 \\approx E[u(Y)]+E\\left[u^{\\prime}(Y) \\tilde{Z}\\right]+E\\left[\\frac{1}{2} u^{\\prime \\prime}(Y) \\tilde{Z}^{2}\\right] \\ \u0026=u(Y)+u^{\\prime}(Y) E(\\tilde{Z})+\\frac{1}{2} u^{\\prime \\prime}(Y) E\\left[\\tilde{Z}^{2}\\right] \\ \u0026=u(Y)+\\frac{1}{2} \\sigma_{\\tilde{Z}}^{2} u^{\\prime \\prime}(Y) \\end{aligned} $$ since $Y$ is not random. Consider a first-order Taylor approximation to $u[Y+E(\\tilde{Z})-\\Pi(\\tilde{Z})]$ $$ \\begin{aligned} u[Y+E(\\tilde{Z})-\\Pi(\\tilde{Z})] \u0026=u[Y-\\Pi(\\tilde{Z})] \\ \u0026 \\approx u(Y)-u^{\\prime}(Y) \\Pi(\\tilde{Z}) \\end{aligned} $$ Hence, with $E(\\tilde{Z})=0,$ the equation defining the risk premium $E(u(Y+\\tilde{z}))=u[Y+E(\\tilde{Z})-\\Pi(\\tilde{Z})]$ $$ \\begin{aligned} E(u(Y+\\tilde{Z})) \u0026 \\approx u(Y)+\\frac{1}{2} \\sigma_{\\bar{Z}}^{2} u^{\\prime \\prime}(Y) \\ u[Y+E(\\tilde{Z})-\\Pi(\\tilde{Z})] \u0026 \\approx u(Y)-u^{\\prime}(Y) \\Pi(\\tilde{Z}) \\end{aligned} $$ imply $$ \\Pi(\\tilde{Z}) \\approx \\frac{1}{2} \\sigma_{\\bar{Z}}^{2} R_{A}(Y) $$ (上述证明过程经常考到) \r under the simplifying assumption that $E(\\tilde{Z})=0,$ it also applies when $E(\\tilde{Z}) \\neq 0,$ except that the level of income must be adjusted from $Y$ to $Y+E(\\tilde{Z})$ to take into account the nonzero expected return: $$ \\Pi(\\tilde{Z}) \\approx \\frac{1}{2} \\sigma_{\\tilde{Z}}^{2} R_{A}(Y+E(\\tilde{Z})) $$ 当保险仅仅补偿不确定性至$E(\\tilde{Z})$时， $$ u(Y+E\\tilde{Z}-Fee) \\longleftright","date":"0001-01-01","objectID":"/part1-4/:1:9","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"3.2 Stochastic Dominance SBSD implies FSD implies SSD First-order stochastic dominance First-order stochastic dominance(FSD) Let $F_{A}(\\tilde{x})$ and $F_{B}(\\tilde{x})$ be two cumulative probability distributions for random payoffs in $[a, b] .$ We say that $F_{A}(\\tilde{x}) F S D F_{B}(\\tilde{x})$ if and only if: $$ F_{A}(x) \\leq F_{B}(x), \\forall x $$ that is, $F_{2}(x) \\text{ FSD } F_{1}(x),$ if and only if $$ E\\left[u\\left(Z_{2}\\right)\\right] \\geq E\\left[u\\left(Z_{1}\\right)\\right] $$ **for any nondecreasing Bernoulli utility function** $u$ state-by-state dominance implies first-order stochastic dominance but first-order stochastic dominance does not necessarily imply state-by-state dominance. But first-order stochastic dominance remains quite a strong condition. Since an asset that displays first-order stochastic dominance over all others will be preferred by any investor with vN-M utility who prefers higher payoffs to lower payoffs, the price of such an asset is likely to be bid up until the dominance goes away. Second-order stochastic dominance second-order stochastic dominance(SSD) Let $F_{A}(\\tilde{x})$ and $F_{B}(\\tilde{x})$ be two cumulative probability distributions for random payoffs in $[a, b] .$ We say that $F_{A}(\\tilde{x}) \\text{ SSD } F_{B}(\\tilde{x})$ if and only if for any $x:$ $$ \\int_{-\\infty}^{x}\\left[F_{B}(t)-F_{A}(t)\\right] d t \\geq 0 $$ (with strict inequality for some meaningful interval of values of $t$ ). \r that is, asset 4 displays second-order stochastic dominance over asset $3,$ if and only if $$ E\\left[u\\left(Z_{4}\\right)\\right] \\geq E\\left[u\\left(Z_{3}\\right)\\right] $$ for any nondecreasing and concave Bernoulli utility function $u$ Second-order stochastic dominance is a weaker condition than first-order stochastic dominance, in that first-order stochastic dominance implies second-order stochastic dominance but second-order stochastic dominance does not necessarily imply first-order stochastic dominance. But second-order stochastic dominance remains a strong condition. Since an asset that displays second-order stochastic dominance over all others will be preferred by any risk-averse investor with vN-M utility, the price of such an asset is likely to be bid up until the dominance goes away. ","date":"0001-01-01","objectID":"/part1-4/:1:10","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"3.3 Mean Preserving Spreads It is also useful, therefore, to consider an alternative criterion that focuses entirely on the standard deviation of a random payoff, as a measure of the riskiness of the corresponding asset, holding the mean or expected value fixed. Graphically, a mean preserving spread takes probability from the center of a distribution and shifts it to the tails. \r Mathematically, one way of producing a mean preserving spread is to take one random variable $X_{1}$ and defining a second, $X_{2}$, by adding “noise” in the form of a third, zero-mean random variable $Z$ : $$ X_{2}=X_{1}+Z $$ with $E(Z)=0$ Let $X_{1}$ and $X_{2}$, with $E\\left(X_{1}\\right)=E\\left(X_{2}\\right),$ be random payoffs on two assets. Then the following two statements are equivalent: (i) $X_{2}=X_{1}+Z$ for some random variable $Z$ with $E(Z)=0$ (ii) asset 1 displays second-order stochastic dominance (SSD) over asset 2 4. 风险厌恶与投资决策 ","date":"0001-01-01","objectID":"/part1-4/:1:11","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"4.1 投资组合配置 风险厌恶投资者决策：风险资产与无风险资产 $$ \\begin{aligned} Y_{0}\u0026= \\text{initial wealth} \\ a\u0026= \\text{amount allocated to stocks} \\ \\tilde{r}\u0026= \\text{random return on stocks}\\ r_{f} \u0026=\\text {risk - free return} \\ \\tilde{Y}_{1} \u0026=\\text { final wealth} \\ \\tilde{Y}_{1} \u0026=\\left(1+r_{f}\\right)\\left(Y_{0}-a\\right)+a(1+\\tilde{r}) \\ \u0026=Y_{0}\\left(1+r_{f}\\right)+a\\left(\\tilde{r}-r_{f}\\right) \\end{aligned} $$ Objective Function The investor chooses a to maximize expected utility: $$ \\max _{a} E\\left[u\\left(\\tilde{Y}_{1}\\right)\\right]=\\max _{a} E u\\left[Y_{0}\\left(1+r_{f}\\right)+a\\left(\\tilde{r}-r_{f}\\right)\\right] $$ The first-order condition(F.O.C.) is $$ E\\left[u^{\\prime}\\left[Y_{0}\\left(1+r_{f}\\right)+a^{*}\\left(\\tilde{r}-r_{f}\\right)\\right]\\left(\\tilde{r}-r_{f}\\right)\\right]=0 $$ Note: we are allowing the investor to sell stocks short $\\left(a^{*}\u003c0\\right)$ or to buy stocks on margin $\\left(a^{*}\u003eY_{0}\\right)$ if he or she desires. **Theorem 5.1** If the Bernoulli utility function $u$ is increasing and concave, then $\\bullet a^{*}\u003e0$ if and only if $E(\\tilde{r})\u003er_{f}$ $\\circ a^{*}=0$ if and only if $E(\\tilde{r})=r_{f}$ $\\circ a^{*}\u003c0$ if and only if $E(\\tilde{r})\u003cr_{f}$ Thus, a risk-averse investor will always allocate at least some funds to the stock market if the expected return on stocks exceeds the risk-free rate. 定理证明 To prove the theorem, let $$ W(a)=E\\left[u^{\\prime}\\left[Y_{0}\\left(1+r_{f}\\right)+a\\left(\\tilde{r}-r_{f}\\right)\\right]\\left(\\tilde{r}-r_{f}\\right)\\right] $$ so that the investor’s first-order condition can be written more compactly as $$ W\\left(a^{*}\\right)=0 $$ it follows that $$ W^{\\prime}(a)=E\\left[u^{\\prime \\prime}\\left[Y_{0}\\left(1+r_{f}\\right)+a\\left(\\tilde{r}-r_{f}\\right)\\right]\\left(\\tilde{r}-r_{f}\\right)^{2}\\right]\u003c0 $$ since $u$ is concave. This means that $W$ is a decreasing function of Finally, with $$ \\begin{aligned} W(a) \u0026=E\\left[u^{\\prime}\\left[Y_{0}\\left(1+r_{f}\\right)+a\\left(\\tilde{r}-r_{f}\\right)\\right]\\left(\\tilde{r}-r_{f}\\right)\\right] \\ W(0) \u0026=E\\left[u^{\\prime}\\left[Y_{0}\\left(1+r_{f}\\right)\\right]\\left(\\tilde{r}-r_{f}\\right)\\right] \\ \u0026=u^{\\prime}\\left[Y_{0}\\left(1+r_{f}\\right)\\right] E\\left(\\tilde{r}-r_{f}\\right) \\ \u0026=u^{\\prime}\\left[Y_{0}\\left(1+r_{f}\\right)\\right]\\left[E(\\tilde{r})-r_{f}\\right] \\end{aligned} $$ since $u$ is increasing, this means that $W(0)$ has the same sign as $$ E(\\tilde{r})-r_{f} $$ We now know that: $\\bullet W(a)$ is a decreasing function $\\bullet W(0)$ has the same sign as $E(\\tilde{r})-r_{f}$ $\\bullet W\\left(a^{*}\\right)=0$ roof $=(\\tilde{r})-r_{f}\u003e0$ implies $W(0)\u003e0,$ and since $W$ is decreasing, $\\mathcal{W}\\left(a^{*}\\right)=0$ implies $a^{*}\u003e0 .$ since $W(0)$ has the same sign as $E(\\tilde{r})-r_{f}, E(\\tilde{r})-r_{f}\u003e0$ 风险中性情况 Before moving on, return to the general problem $$ \\max {a} E u\\left[Y{0}\\left(1+r_{f}\\right)+a\\left(\\tilde{r}-r_{f}\\right)\\right] $$ but assume now that the investor is risk-neutral, with $$ u(Y)=\\alpha Y+\\beta $$ and $\\alpha\u003e0,$ so that more wealth is preferred to less. The risk-neutral investor solves $$ \\begin{aligned} \u0026 \\max _{a} \\in \\alpha\\left[Y_{0}\\left(1+r_{f}\\right)+a\\left(\\tilde{r}-r_{f}\\right)\\right]+\\beta \\ =\u0026 \\max _{a} \\alpha Y_{0}\\left(1+r_{f}\\right)+a\\left[E(\\tilde{r})-r_{f}\\right]+\\beta \\end{aligned} $$ So long as $E(\\tilde{r})-r_{f}\u003e0,$ the risk-neutral investor will choose $a^{*}$ to be as large as possible, borrowing as much as he or she is allowed to in order to buy more stocks on margin. ","date":"0001-01-01","objectID":"/part1-4/:1:12","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"4.2 风险厌恶与投资组合配置 The following result was proven by Kenneth Arrow in “The Theory of Risk Aversion,” published in the 1971 volume Essays in the Theory of Risk-Bearing and reprinted in 1983 in volume 3 of the Collected Papers of Kenneth $J$. Arrow (Harvard University Press). 投资者之间比较 Theorem 5.2* Consider two investors, $i=1$ and $i=2,$ and suppose that for all wealth levels $Y$, $$ \\begin{array}{l}R_{A}^{1}(Y)\u003eR_{A}^{2}(Y) \\ \\text { where } R_{A}^{i}(Y) \\text { is investor i’s coefficient of absolute risk aversion. Then } \\ a_{1}^{*}(Y)\u003ca_{2}^{*}(Y)\\end{array} $$ where $a_{i}^{*}(Y)$ is amount allocated by investor i to stocks when he or she has initial wealth $Y$. 其中绝对风险偏好也可以替换为相对风险偏好 注意到：DARA+CRRA是比较合理的风险偏好结构 投资者风险投资随收入变化 ARA与投资额 Theorem 5.4 Let $a^{*}\\left(Y_{0}\\right)$ be the solution to $$ \\max _{a} E u\\left[Y_{0}\\left(1+r_{f}\\right)+a\\left(\\tilde{r}-r_{f}\\right)\\right] $$ If $u(Y)$ is such that (a) $R_{A}^{\\prime}(Y)\u003c0$ then $\\frac{d a^{*}\\left(Y_{0}\\right)}{d Y_{0}}\u003e0$ (b) $R_{A}^{\\prime}(Y)=0$ then $\\frac{d a^{*}\\left(Y_{0}\\right)}{d Y_{0}}=0$ (c) $R_{A}^{\\prime}(Y)\u003e0$ then $\\frac{d a^{*}\\left(Y_{0}\\right)}{d Y_{0}}\u003c0$ RRA与投资弹性 Define the elasticity of the function $a^{*}\\left(Y_{0}\\right)$ as $$ \\eta=\\frac{d\\left(\\ln a^{*}\\left(Y_{0}\\right)\\right)}{d\\left(\\ln Y_{0}\\right)}=\\frac{Y_{0}}{a^{*}\\left(Y_{0}\\right)} \\frac{d\\left(a^{*}\\left(Y_{0}\\right)\\right)}{d Y_{0}} $$ The elasticity measures the percentage change in $a^{*}\\left(Y_{0}\\right)$ brought about by a percentage-point change in $Y_{0}$ Theorem 5.5 Let $a^{*}\\left(Y_{0}\\right)$ be the solution to $$ \\max _{a} E u\\left[Y_{0}\\left(1+r_{f}\\right)+a\\left(\\tilde{r}-r_{f}\\right)\\right] $$ If $u(Y)$ is such that $$ \\begin{array}{l} \\text { (a) } R_{R}^{\\prime}(Y)\u003c0 \\text { then } \\eta\u003e1 \\ \\text { (b) } R_{R}^{\\prime}(Y)=0 \\text { then } \\eta=1 \\ \\text { (c) } R_{R}^{\\prime}(Y)\u003e0 \\text { then } \\eta\u003c1 \\end{array} $$ Note that with constant relative risk aversion, $a^∗$ rises proportionally with wealth. Two additional The theorem confirms what we know about CRRA utility: it implies that $a^{*}$ rises proportionally with $Y_{0}$. But the theorem extends the results to the cases of decreasing and increasing relative risk aversion. Portfolios, Risk Aversion, and Wealth With CRRA $$ \\frac{a^{*}}{Y_{0}}=K $$ where $$ K=\\frac{\\left(1+r_{f}\\right)\\left(\\left[\\pi\\left(r_{G}-r_{f}\\right)\\right]^{1 / \\gamma}-\\left[(1-\\pi)\\left(r_{f}-r_{B}\\right)\\right]^{1 / \\gamma}\\right)}{\\left(r_{G}-r_{f}\\right)\\left[(1-\\pi)\\left(r_{f}-r_{B}\\right)\\right]^{1 / \\gamma}-\\left(r_{B}-r_{f}\\right)\\left[\\pi\\left(r_{G}-r_{f}\\right)\\right]^{1 / \\gamma}} $$ Hence $$ \\begin{aligned} \\ln \\left(a^{*}\\left(Y_{0}\\right)\\right) \u0026=\\ln (K)+\\ln \\left(Y_{0}\\right) \\ \\eta \u0026=\\frac{d\\left(\\ln a^{*}\\left(Y_{0}\\right)\\right)}{d\\left(\\ln Y_{0}\\right)}=1 \\end{aligned} $$ ","date":"0001-01-01","objectID":"/part1-4/:1:13","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"4.3 风险厌恶与储蓄行为 Suppose there are two periods, $t=0$ and $t=1,$ and let $Y_{0}=$ initial wealth $s=$ amount saved in period $\\mathrm{t}=0$ $c_{0}=Y_{0}-s=$ amount consumed in period $t=0$ $\\tilde{R}=1+\\tilde{r}=$ random, gross return on savings $\\bar{c}_{1}=s \\tilde{R}=$ amount consumed in period $\\mathrm{t}=1$ Suppose also that the investor has vN-M expected utility over consumption during periods $t=0$ and $t=1$ given by $$ u\\left(c_{0}\\right)+\\beta E\\left[u\\left(\\tilde{c}_{1}\\right)\\right]=u\\left(Y_{0}-s\\right)+\\beta E[u(s \\tilde{R})] $$ where the discount factor $\\beta$ is a measure of patience Risk Aversion and Saving Behavior The solution to the investor’s saving problem $$ \\max _{s} u\\left(Y_{0}-s\\right)+\\beta E[u(s \\tilde{R})] $$ FOC $$ \\underbrace{u^{\\prime}\\left(Y_{0}-s^{*}\\right)}_{MU_1}=\\underbrace{\\beta E\\left[u^{\\prime}\\left(s^{*} \\tilde{R}\\right) \\tilde{R}\\right]}_{MU_2} $$ 必考 当储蓄收入$\\tilde{R}$的不确定性上升时： in the form of a mean preserving spread in the distribution of R˜ . Intuitively, one might expect there to be two offsetting effects: 替代效应 the riskier return will make saving less attractive and thereby reduce s ∗ ; 预防性储蓄 The riskier return might lead to “precautionary saving” in order to cushion period t = 1 consumption against the possibility of a bad output and thereby increase s ∗ . let $$ g(\\tilde{R}) = u^{\\prime}\\left(s^{*} \\tilde{R}\\right) \\tilde{R} $$ Jensen’s inequality will imply that after a mean preserving spread the distribution of $\\tilde{R}$ in this expectation will fall if g is concave and rise if g is convex. The product and chain rules for differentiation imply $$ \\begin{aligned} g^{\\prime}(\\tilde{R}) \u0026=u^{\\prime \\prime}\\left(s^{} \\tilde{R}\\right) s^{} \\tilde{R}+u^{\\prime}\\left(s^{} \\tilde{R}\\right) \\ g^{\\prime \\prime}(\\tilde{R}) \u0026=u^{\\prime \\prime \\prime}\\left(s^{} \\tilde{R}\\right)\\left(s^{}\\right)^{2} \\tilde{R}+u^{\\prime \\prime}\\left(s^{} \\tilde{R}\\right) s^{}+u^{\\prime \\prime}\\left(s^{} \\tilde{R}\\right) s^{} \\ \u0026=u^{\\prime \\prime \\prime}\\left(s^{} \\tilde{R}\\right)\\left(s^{}\\right)^{2} \\tilde{R}+2 u^{\\prime \\prime}\\left(s^{} \\tilde{R}\\right) s^{} \\end{aligned} $$ implies that $g^{\\prime \\prime}(\\tilde{R})$ has the same sign as $$ u^{\\prime \\prime \\prime}\\left(s^{} \\tilde{R}\\right) s^{} \\tilde{R}+2 u^{\\prime \\prime}\\left(s^{} \\tilde{R}\\right) $$ To understand precautionary saving behavior, the concept of prudence is defined by Miles Kimball, “Precautionary Saving in the Small and in the Large,” Econometrica Vol.58 (January 1990): pp.53-73. Kimball defines the coefficient of absolute prudence as $$ P_{A}(Y)=-\\frac{u^{\\prime \\prime \\prime}(Y)}{u^{\\prime \\prime}(Y)} $$ and the coefficient of relative prudence as $$ P_{R}(Y)=-\\frac{Y u^{\\prime \\prime \\prime}(Y)}{u^{\\prime \\prime}(Y)} $$ thereby extending the analogous measures of absolute and relative risk aversion. Risk Aversion and Saving Behavior $g^{\\prime \\prime}(\\tilde{R})$ has the same sign as $$ \\begin{array}{l} \\qquad u^{\\prime \\prime \\prime}\\left(s^{*} \\tilde{R}\\right) s^{*} \\tilde{R}+2 u^{\\prime \\prime}\\left(s^{*} \\tilde{R}\\right) \\ r^{\\prime \\prime \\prime}(Y) Y+2 u^{\\prime \\prime}(Y)=u^{\\prime \\prime}(Y)\\left[\\frac{Y u^{\\prime \\prime \\prime}(Y)}{u^{\\prime \\prime}(Y)}+2\\right]=u^{\\prime \\prime}(Y)\\left[2-P_{R}(Y)\\right] \\end{array} $$ $g^{\\prime \\prime}(\\tilde{R})$ is positive if $2\u003cP_{R}(Y)$ $g^{\\prime \\prime}(\\tilde{R})$ is negative if $2\u003eP_{R}(Y)$ Kimball defines the coefficient of absolute prudence as $$ P_{A}(Y)=-\\frac{u^{\\prime \\prime \\prime}(Y)}{u^{\\prime \\prime}(Y)} $$ and the coefficient of relative prudence as $$ P_{R}(Y)=-\\frac{Y u^{\\prime \\prime \\prime}(Y)}{u^{\\prime \\prime}(Y)} $$ thereby extending the analogous measures of absolute and relative risk aversion. Risk Aversion and Saving Behavior $g^{\\prime \\prime}(\\tilde{R})$ has the same sign as $$ \\begin{array}{l} \\qquad u^{\\prime \\prime \\prime}\\left(s^{*} \\tilde{R}\\right) s^{*} \\tilde{R}+2 u^{\\prime \\prime}\\left(s^{*} \\tilde{R}\\right) \\ u^{\\prime \\prime \\prime}(Y) Y+2 u^{\\prime","date":"0001-01-01","objectID":"/part1-4/:1:14","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"4.4 风险厌恶与prudence (补充) 下面三者等价 证明 \r \r \r ","date":"0001-01-01","objectID":"/part1-4/:1:15","tags":null,"title":"","uri":"/part1-4/"},{"categories":null,"content":"2. Foundation of Probability Theory [toc] ","date":"0001-01-01","objectID":"/2.-foundation-of-probability-theory/:0:0","tags":null,"title":"","uri":"/2.-foundation-of-probability-theory/"},{"categories":null,"content":"2.1 Random Experiments Definition (Random Experiment): A random experiment is a process that generates at least two possible outcomes. One and only one outcome will occur, but there is uncertainty associated with which one will occur. Two essential elements of a random experiment: The set of all possible outcomes——sample space; The likelihood with which each outcome will occur——probability function. Fundamental axioms of modern econometrics: An economic system can be viewed as a random experiment governed by some probability distribution or probability law. Any economic phenomena (often in form of data) can be viewed as an outcome of this random experiment. ","date":"0001-01-01","objectID":"/2.-foundation-of-probability-theory/:1:0","tags":null,"title":"","uri":"/2.-foundation-of-probability-theory/"},{"categories":null,"content":"2.2 Basic Concepts of Probability Definition (Sample Space): The possible outcomes of the random experiment are called basic outcomes, and the set of all basic outcomes is called the sample space, denoted by $ S $. When an experiment is performed, the realization of the experiment is one outcome in the sample space. Example: Finite sample space Tossing a coin $ S={\\text { Head, Tail }} $ Rolling a die $S={1,2,3,4,5,6}$ Playing a football game $ S={\\text { Win, Lose, Tie}} $ Example: Infinite discrete sample space Consider the number of accidents that occur at a given intersection within a month. The sample space is the set of all nonnegative integers, $ S={0,1,2, \\ldots} $ Example: Continuous sample space When recording the lifetime of a light bulb, the outcome is the time until the bulb burns out. Therefore the sample space is the set of all nonnegative real number, $ S=\\mathbb{R}_{+}={t: t \\in \\mathbb{R}, t \\geqslant 0} $. Definition (Event): An event $ A $ is a subset of basic outcomes from the sample space $ S $. The event $ A $ is said to occur if the random experiment gives rise to one of the constituent basic outcomes in $ A $. That is, an event occurs if any of its basic outcomes has occurred. Example A die is rolled. The sample space $ S={1,2,3,4,5,6} . $ Event $ A $ is defined as “number resulting is even”, then $ A={2,4,6} . $ Event $ B $ is defined as “number resulting is 4”, then $ B={4} $. Remarks: The words “set” and “event” are interchangeable. Basic outcome $ \\in $ sample space; event $ \\subseteq $ sample space. ","date":"0001-01-01","objectID":"/2.-foundation-of-probability-theory/:2:0","tags":null,"title":"","uri":"/2.-foundation-of-probability-theory/"},{"categories":null,"content":"2.3 Review of Set Theory Definition (Containment): The event $ A $ is contained in the event $ B $, or $ B $ contains $ A, $ if every sample point of $ A $ is also a sample point of $ B $. If so, we write $ A \\subseteq B, $ or equivalently, $ B \\supseteq A $. Definition (Equality): Two events $ A $ and $ B $ are said to be equal, $ A=B, $ if $ A \\subseteq B $ and $ B \\subseteq A $. Definition (Empty Set): The set containing no elements is called the empty set and is denoted by $ \\varnothing . $ The event corresponding to $ \\varnothing $ is called a null (or impossible) event. Definition (Complement): The complement of $ A, $ denoted by $ A^{c}, $ is the set of basic outcomes of a random experiment belonging to $ S $ but not to $ A $. Definition (Union): The union of $ A $ and $ B $, denoted by $ A \\cup B, $ is the set of all basic outcomes in $ S $ that belong to either $ A $ or $ B $. The union of $ A $ and $ B $ occurs if and only if either $ A $ or $ B $ (or both) occurs. Definition (Intersection): The intersection of $ A $ and $ B, $ denoted by $ A \\cap B, $ is the set of basic outcomes in $ S $ that belong to both $ A $ and $ B $. The intersection occurs if and only if both events $ A $ and $ B $ occur. Definition (Difference): The difference of $ A $ and $ B $, denoted by $ A \\backslash B $ or $ A-B, $ is the set of basic outcomes in $ S $ that belong to $ A $ but not to $ B $, i.e. $ A \\backslash B=A \\cap B^{c} $. Definition (Exclusiveness): If $ A $ and $ B $ have no common basic outcomes, they are called mutually exclusive (or disjoint). Their intersection is empty set, i.e. $ A \\cap B=\\varnothing $. Definition (Collectively Exhaustiveness): Suppose $ A_{1}, A_{2}, \\ldots, A_{n} $ are $ n $ events in the sample space $ S, $ where $ n $ is any positive integer. If $ \\bigcup_{i=1}^{n} A_{i}=S, $ then these $ n $ events are said to be collectively exhaustive. Definition (Partition): A class of events $ \\mathcal{H}=\\left{A_{1}, A_{2}, \\ldots, A_{n}\\right} $ forms a partition of the sample space $ S $ if these events satisfy $ A_{i} \\cap A_{j}=\\varnothing $ for all $ i \\neq j $ (mutually exclusive), and $ A_{1} \\cup A_{2} \\cup \\cdots \\cup A_{n}=S $ (collectively exhaustive). Complementation $$ \\left(A^{c}\\right)^{c}=A, \\quad \\varnothing^{c}=S, \\quad S^{c}=\\varnothing $$ Commutativity $$ A \\cup B=B \\cup A, \\quad A \\cap B=B \\cap A $$ Associativity $$ A \\cup(B \\cup C)=(A \\cup B) \\cup C, \\quad A \\cap(B \\cap C)=(A \\cap B) \\cap C $$ Distributivity $$ A \\cap(B \\cup C)=(A \\cap B) \\cup(A \\cap C), \\quad A \\cup(B \\cap C)=(A \\cup B) \\cap(A \\cup C) $$ More generally, for $ n \\geqslant 1 $, $$ B \\cap\\left(\\bigcup_{i=1}^{n} A_{i}\\right)=\\bigcup_{i=1}^{n}\\left(B \\cap A_{i}\\right), \\quad B \\cup\\left(\\bigcap_{i=1}^{n} A_{i}\\right)=\\bigcap_{i=1}^{n}\\left(B \\cup A_{i}\\right) $$ De Morgan’s Laws $$ (A \\cup B)^{c}=A^{c} \\cap B^{c}, \\quad(A \\cap B)^{c}=A^{c} \\cup B^{c} $$ More generally, for $ n \\geqslant 1 $, $$ \\left(\\bigcup_{i=1}^{n} A_{i}\\right)^{c}=\\bigcap_{i=1}^{n}\\left(A_{i}\\right)^{c}, \\quad\\left(\\bigcap_{i=1}^{n} A_{i}\\right)^{c}=\\bigcup_{i=1}^{n}\\left(A_{i}\\right)^{c} $$ Definition (Countable Set): A set $ S $ is called countable if the set has one-to-one correspondence with a subset of the natural numbers $ \\mathbb{N} $. Remarks: A countable set is either a finite set or a countably infinite set. We can use a finite or infinite sequence to index all elements in a countable set. The set of natural numbers $ \\mathbb{N} $, the set of integers $ \\mathbb{Z} $, and the set of rational numbers $ \\mathbb{Q} $ are countable sets. The set of real numbers $ \\mathbb{R} $ is uncountable. The set of all real numbers in interval $ (a, b) $ is uncountable. Unions and intersections of infinitely countable collection of events: $$ \\begin{array}{l} \\bigcup_{i=1}^{\\infty} A_{i}=\\left{x \\in S: x \\in A_{i} \\text { for some } i\\right} \\ \\bigcap_{i=1}^{\\infty} A_{i}=\\left{x \\in S: x \\in A_{i} \\text { for all } i\\right} \\end{array} $$ Unions and intersections","date":"0001-01-01","objectID":"/2.-foundation-of-probability-theory/:3:0","tags":null,"title":"","uri":"/2.-foundation-of-probability-theory/"},{"categories":null,"content":"2.4 Fundamental Probability Laws: $ \\sigma $-Algebra Definition $\\sigma$-algebra: A $ \\sigma $ -algebra (or $ \\sigma $ -field), denoted by $ \\mathcal{B}, $ is a collection of subsets of $ S $ that satisfies $ \\varnothing \\in \\mathcal{B} $ (the empty set is contained in $ \\mathcal{B} $ ); If $ A \\in \\mathcal{B}, $ then $ A^{c} \\in \\mathcal{B}(\\mathcal{B} $ is closed under complement); and If $ A_{1}, A_{2}, \\ldots \\in \\mathcal{B}, $ then $ \\bigcup_{i=1}^{\\infty} A_{i} \\in \\mathcal{B}(\\mathcal{B} $ is closed under countable unions). Remarks: A $ \\sigma $ -algebra is a set of sets. (1) and (2) imply that $ S \\in \\mathcal{B} $. (2) and (3) imply that $ \\bigcap_{i=1}^{\\infty} A_{i} \\in \\mathcal{B} $ For a given $ S, $ we can construct many different $ \\sigma $ -algebras. $ {\\varnothing, S} $ is a $ \\sigma $ -algebra, usually called the trivial $ \\sigma $ -algebra. The collection of all possible subsets of $ S $ is a $ \\sigma $ -algebra. For any event $ A,\\left{\\varnothing, A, A^{c}, S\\right} $ is a $ \\sigma $ -algebra. Intersection of $ \\sigma $ -algebras is also a $ \\sigma $ -algebras. Theorem: For any non-empty collection $ \\mathcal{K} $ of subsets of sample space $ S, $ there exists a unique smallest $ \\sigma $ -algebra $ \\mathcal{B} $ containing $ \\mathcal{K} $. It is called the $ \\sigma $ -algebra generated by $ \\mathcal{K} $. Smallest: If $ \\mathcal{B}^{\\prime} $ is a $ \\sigma $ -algebra containing $ \\mathcal{K}, $ then $ \\mathcal{B} \\subseteq \\mathcal{B}^{\\prime} $ Unique: If $ \\mathcal{B}^{\\prime} $ is another smallest $ \\sigma $ -algebra containing $ \\mathcal{K}, $ then $ \\mathcal{B}^{\\prime}=\\mathcal{B} $ Example: Suppose $ S={1,2,3} . $ Let $ \\mathcal{K}{1}={{1}} $ and $ \\mathcal{K}{2}={{1},{1,2}} $. $ {\\varnothing,{1},{2,3}, S} $ is a $ \\sigma $ -algebra containing $ \\mathcal{K}_{1} $. (Smallest?) $ {\\varnothing,{1},{2},{3},{1,2},{2,3},{1,3}, S} $ is the smallest $ \\sigma $-algebra containing $ \\mathcal{K}_{2} $ Example (Borel algebra): Let $ S=\\mathbb{R}=(-\\infty,+\\infty), $ the real line. Let $ \\mathcal{B} $ be the collection of all sets that can be formed by taking complements, countable unions, and countable intersections of $ [a, b],[a, b),(a, b],(a, b) $ for any real numbers a and $ b $. Then $ \\mathcal{B} $ is the smallest $ \\sigma $ -algebra containing all the intervals. This $ \\sigma $ -algebra is called Borel algebra, and any set in $ \\mathcal{B} $ is called Borel set. Definition (Probability Function): Suppose a random experiment has a sample space $ S $ and an associated $ \\sigma $ -algebra $ \\mathcal{B} $. The probability function $ P: \\mathcal{B} \\rightarrow[0,1] $ is a mapping that satisfies $ 0 \\leqslant P(A) \\leqslant 1 $ for any event $ A \\in \\mathcal{B} $ $ P(S)=1 $ If countable number of events $ A_{1}, A_{2}, \\ldots \\in \\mathcal{B} $ are mutually exclusive (pairwise disjoint), then $ P\\left(\\bigcup_{i=1}^{\\infty} A_{i}\\right)=\\sum_{i=1}^{\\infty} P\\left(A_{i}\\right) $ (countable additivity) Remark: For a given measurable space $ (S, \\mathcal{B}), $ many different probability functions can be defined. Definition (Probability Space): A probability space is a triple $ (S, \\mathcal{B}, P), $ where $ S $ is the sample space corresponding to outcomes of the underlying random experiment; $ \\mathcal{B} $ is an associated $ \\sigma $ -algebra of $ S $, and elements of $ \\mathcal{B} $ are called events; $ P $ is a probability measure (or probability function). Properties of probability function: $ P(\\varnothing)=0 $ $ P\\left(A^{c}\\right)=1-P(A) $ If $ A \\subseteq B, $ then $ P(A) \\leqslant P(B) $ If $ A_{1}, A_{2}, \\ldots \\in \\mathcal{B} $ form a partition of $ S $ (i.e. mutually exclusive and collectively exhaustive), and $ A $ is an event in $ S, $ then $$ P(A)=\\sum_{i=1}^{\\infty} P\\left(A \\cap A_{i}\\right) $$ Sub-additivity: For any sequence of events $ A_{1}, A_{2}, \\ldots \\in \\mathcal{B} $ $$ P\\left(\\bigcup_{i=1}^{\\infty} A_{i}\\right) \\leqslant \\sum_{i=1}^{\\infty} P\\left(A_{i}\\right) $$ Theorem: For any $ ","date":"0001-01-01","objectID":"/2.-foundation-of-probability-theory/:4:0","tags":null,"title":"","uri":"/2.-foundation-of-probability-theory/"},{"categories":null,"content":"2.5 Methods of Counting Theorem (Fundamental Theorem of Counting): If a random experiment consists of $ k $ separated tasks, the $ i $-th of which can be done in $ n_{i} $ ways, $ i=1,2, \\ldots, k, $ then the entire job can be done in $ n_{1} \\times n_{2} \\times \\cdots \\times n_{k}=\\prod_{i=1}^{k} n_{i} $ ways. We consider two important counting methods: permutation and combination. ","date":"0001-01-01","objectID":"/2.-foundation-of-probability-theory/:5:0","tags":null,"title":"","uri":"/2.-foundation-of-probability-theory/"},{"categories":null,"content":"2.5.1 Permutation Example: Suppose we will choose choose two letters out of $ {\\mathrm{A}, \\mathrm{B}, \\mathrm{C}, \\mathrm{D}} $ in different orders, with each letter being used at most once each time. How many possible orders could we obtain? 12 ways: $ \\mathrm{AB}, \\mathrm{AC}, \\mathrm{AD}, \\mathrm{BA}, \\mathrm{BC}, \\mathrm{BD}, \\mathrm{CA}, \\mathrm{CB}, \\mathrm{CD}, \\mathrm{DA}, \\mathrm{DB}, \\mathrm{DC} $ Suppose there are $ k $ boxes in a row and there are $ n $ objects, where $ n \\geqslant k . $ If we choose $ k $ from the $ n $ objects to fill in the $ k $ boxes, how many possible different ordered sequences could we obtain? One object is selected to fill in box $ 1, $ so there are $ n $ ways; A second object is selected from the remaining $ n-1 $ objects to fill in box $ 2, $ so there are $ n-1 $ ways; … k. To fill the last box (box $ k $ ), there are $ n-(k-1) $ ways since $ n-(k-1) $ objects remain. The total number of different ways to fill boxes $ 1, \\ldots, k $ is $$ n \\times(n-1) \\times \\cdots \\times[n-(k-1)] $$ The experiment is equivalent to selecting $ k $ objects out of the $ n $ objects first, then arrange the selected $ k $ objects in a sequence. Each different arrangement of the sequence is called a permutation. The number of permutations of choosing $ k $ out of $ n, $ denoted by $ P_{n}^{k}, $ is $$ P_{n}^{k}=\\frac{n !}{(n-k) !}=n \\times(n-1) \\times \\cdots \\times[n-(k-1)] $$ where $ n !=n \\times(n-1) \\times \\cdots \\times 2 \\times 1 $. Convention: $ 0 !=1 $ Example (Birthday Problem): What is the probability that at least two persons in a group of $ k $ people $ (2 \\leqslant k \\leqslant 365) $ have the same birthday (i.e. the same day of the same month)? Assume no one is born on Feb 29 and each of the 365 days is equally likely to be the birthday of anyone. Define event $ A $ as “at least two persons have the same birthday”. How many possible ways in which the $ k $ people could be born? $ 365^{k}(\\text { total numbers of outcomes in sample space } S) $ Event $ A^{c} $ is \" $ k $ people have different birthdays\". How many ways that all can have different birthdays? $ P_{365}^{k} $ So $$ P(A)=1-P\\left(A^{c}\\right)=1-\\frac{P_{365}^{k}}{365^{k}} $$ ","date":"0001-01-01","objectID":"/2.-foundation-of-probability-theory/:5:1","tags":null,"title":"","uri":"/2.-foundation-of-probability-theory/"},{"categories":null,"content":"2.5.2 Combination Example: Suppose we will choose two letters out of $ {\\mathrm{A}, \\mathrm{B}, \\mathrm{C}, \\mathrm{D}} $ without ordering. If each letter is used at most once each time, how many possible pairs could we obtain? 6 pairs: $ (\\mathrm{A}, \\mathrm{B}),(\\mathrm{A}, \\mathrm{C}),(\\mathrm{A}, \\mathrm{D}),(\\mathrm{B}, \\mathrm{C}),(\\mathrm{B}, \\mathrm{D}),(\\mathrm{C}, \\mathrm{D}) $ Choose a subset of $ k $ elements without replacement from a set of $ n $ distinct elements. The order of the elements is irrelevant. For example, the subsets $ {a, b} $ and $ {b, a} $ are identical. Each subset is called a combination. The number of combinations of choosing $ k $ out of $ n $ is denoted by $ C_{n}^{k} . $ We have $$ \\begin{aligned} C_{n}^{k} \u0026=\\frac{\\text { number of choosing } k \\text { out of } n \\text { with ordering }}{\\text { number of ordering } k \\text { elements }} \\ \u0026=\\frac{P_{n}^{k}}{k !}=\\frac{n !}{k !(n-k) !} \\end{aligned} $$ $ C_{n}^{k} $ is also denoted by $ \\left(\\begin{array}{l}n \\ k\\end{array}\\right) . $ This is also called a binomial coefficient because of its appearance in the binomial theorem $$ (x+y)^{n}=\\sum_{k=0}^{n}\\left(\\begin{array}{l} n \\ k \\end{array}\\right) x^{k} y^{n-k} $$ Properties of the binomial coefficients $$ \\begin{array}{c} \\sum_{k=0}^{n} C_{n}^{k}=2^{n}, \\quad \\sum_{k=0}^{n}(-1)^{k} C_{n}^{k}=0 \\ \\sum_{k=0}^{m} C_{n}^{k} C_{n}^{m-k}=C_{2 n}^{m} \\ C_{n}^{k}+C_{n}^{k-1}=C_{n+1}^{k} \\end{array} $$ Example: Select 10 students randomly from a class containing 15 boys and 20 girls. What is the probability that exactly 3 boys are selected? The number of combinations of 10 students out of 35 students is $ C_{35}^{10} $ The number of combinations of 3 boys out of 15 boys is $ C_{15}^{3} $ The number of combinations of 7 girls out of 20 girls is $ C_{20}^{7} $ Thus the probability is $ \\left(C_{15}^{3} \\times C_{20}^{7}\\right) / C_{35}^{10} $. Matching Problem: There are $ n $ letters and $ n $ envelopes with the corresponding addresses. If we place the $ n $ letters in the $ n $ envelopes in a random manner, what is the probability that at least one letter will be placed in the correct envelope? Let $ A $ be the event “at least one letter is in the correct envelope”. Let $ A_{i}, i=1, \\ldots, n, $ be the event “letter $ i $ is placed in the correct envelope”. We shall determine $ P\\left(\\bigcup_{i=1}^{n} A_{i}\\right) $ Use formula $$ \\begin{aligned} P\\left(\\bigcup_{i=1}^{n} A_{i}\\right)=\u0026\\sum_{i} P\\left(A_{i}\\right)-\\sum_{i_{1}\u003ci_{2}} P\\left(A_{i_{1}} \\cap A_{i_{2}}\\right)\\ \u0026+\\sum_{i_{1}\u003ci_{2}\u003ci_{3}} P\\left(A_{i_{1}} \\cap A_{i_{2}} \\cap A_{i_{3}}\\right) \\ \u0026-\\cdots+(-1)^{n+1} P\\left(A_{1} \\cap A_{2} \\cap \\cdots \\cap A_{n}\\right) \\end{aligned} $$ For a given $ 1 \\leqslant k \\leqslant n, $ for any $ 1 \\leqslant i_{1}\u003c\\cdots\u003ci_{k} \\leqslant n $ $$ P\\left(A_{i_{1}} \\cap \\cdots \\cap A_{i_{k}}\\right)=(n-k) ! / n ! $$ $ \\sum_{i_{1}\u003c\\cdots\u003ci_{k}} P\\left(A_{i_{1}} \\cap \\cdots \\cap A_{i_{k}}\\right)=C_{n}^{k} \\times(n-k) ! / n !=1 / k ! $. Thus, $$ P\\left(\\bigcup_{i=1}^{n} A_{i}\\right)=1-\\frac{1}{2 !}+\\frac{1}{3 !}-\\cdots+(-1)^{n+1} \\frac{1}{n !}=\\sum_{k=1}^{n}(-1)^{k+1} \\frac{1}{k !} $$ Example: Three types of fruit: apple, banana, and orange. How many ways to load a pack of 5 pieces? Combination with replacement Number of ways to select subsets of size $ k $ from a set of $ n $ distinguishable elements: Ordered Unordered Without replacement $ P_{n}^{k} $ $ C_{n}^{k} $ With replacement $ n^{k} $ $ C_{n+k-1}^{k} $ ","date":"0001-01-01","objectID":"/2.-foundation-of-probability-theory/:5:2","tags":null,"title":"","uri":"/2.-foundation-of-probability-theory/"},{"categories":null,"content":"2.6 Conditional Probability Different economic events are generally related to each other. Because of the connection, the occurrence of event $ B $ may affect or contain the information about the probability that event $ A $ will occur. Example (Financial Contagion): A large drop of the price in one market can cause a large drop of the price in another market, given the speculations and reactions of market participants. When an event has occurred, some uncertainty is eliminated as new information arrives. We want to update probability calculations based on new information. Definition (Conditional Probability): Let $ A $ and $ B $ be two events in probability space $ (S, \\mathcal{B}, P) . $ The conditional probability of event $ A $ given event $ B $, denoted as $ P(A \\mid B), $ is defined as $$ P(A \\mid B)=\\frac{P(A \\cap B)}{P(B)} $$ provided that $ P(B)\u003e0 $. Properties of conditional probability: $ P(A)=P(A \\mid S) $ $ P(A \\mid B)=1-P\\left(A^{c} \\mid B\\right) $ Multiplication rule $$ P(A \\cap B)=P(A) P(B \\mid A)=P(B) P(A \\mid B) $$ Example: Suppose two balls are randomly selected, without replacement, from a box containing $ a $ red balls and $ b $ blue balls. What is the probability that the first is red and the second is blue? Let $ A={\\text { the first is red }} $ and $ B={\\text { the second is blue }} . $ Then $$ P(A \\cap B)=P(A) P(B \\mid A)=\\frac{a}{a+b} \\times \\frac{b}{a+b-1} $$ Theorem (Chain Rule of Joint Probability): For any events $ A_{1}, A_{2}, \\ldots, A_{n} $ in $ \\mathcal{B}, $ the joint probability of $ n $ events $$ \\begin{aligned} P\\left(A_{1} \\cap A_{2} \\cap \\cdots \\cap A_{n}\\right)=\u0026P\\left(A_{1}\\right) \\times P\\left(A_{2} \\mid A_{1}\\right) \\times P\\left(A_{3} \\mid A_{1} \\cap A_{2}\\right) \\ \u0026\\times \\cdots \\times P\\left(A_{n} \\mid A_{1} \\cap \\cdots \\cap A_{n-1}\\right) \\end{aligned} $$ is provided that $ P\\left(A_{1} \\cap \\cdots \\cap A_{n-1}\\right)\u003e0 $ Remark: $ P\\left(A_{1} \\cap \\cdots \\cap A_{n-1}\\right)\u003e0 $ implies $ P\\left(A_{1}\\right)\u003e0, P\\left(A_{1} \\cap A_{2}\\right)\u003e0 $, $ \\ldots, P\\left(A_{1} \\cap \\cdots \\cap A_{n-2}\\right)\u003e0 $ Theorem (Rule of Total Probability): Let $ \\left{A_{i}, i=1,2, \\ldots\\right} $ be a **partition** (i.e. mutually exclusive and collectively exhaustive) of sample space $ S, $ with $ P\\left(A_{i}\\right)\u003e0 $ for all $ i \\geqslant 1 . $ Then for any event $ B $ in $ \\mathcal{B} $ $$ P(B)=\\sum_{i=1}^{\\infty} P\\left(B \\mid A_{i}\\right) P\\left(A_{i}\\right) $$ Example: Suppose events $ B_{1}, B_{2} $ and $ B_{3} $ are mutually exclusive. If $ P\\left(B_{i}\\right)=\\frac{1}{3} $ and $ P\\left(A \\mid B_{i}\\right)=\\frac{i}{6} $ for $ i=1,2,3 . $ What is $ P(A) ? $ (Hint: $ B_{1}, B_{2} $ and $ B_{3} $ are also collectively exhaustive.) ","date":"0001-01-01","objectID":"/2.-foundation-of-probability-theory/:6:0","tags":null,"title":"","uri":"/2.-foundation-of-probability-theory/"},{"categories":null,"content":"2.7 Bayes' Formula Theorem (Bayes' Theorem): Suppose $ P(A)\u003e0 $ and $ P(B)\u003e0 . $ Then $$ P(A \\mid B)=\\frac{P(B \\mid A) P(A)}{P(B \\mid A) P(A)+P\\left(B \\mid A^{c}\\right) P\\left(A^{c}\\right)} $$ Remarks: We consider $ P(A) $ as the prior probability about event $ A $. $ P(A \\mid B) $ is the posterior probability given that $ B $ has occurred. ayes' Formula Theorem (Alternative Statement of Bayes' Theorem): Suppose $ A_{1}, \\ldots, A_{n} $ are $ n $ mutually exclusive and collectively exhaustive events in the sample space $ S, $ and $ B $ is an event with $ P(B)\u003e0 . $ Then the conditional probability of $ A_{i} $ given $ B $ is $$ P\\left(A_{i} \\mid B\\right)=\\frac{P\\left(B \\mid A_{i}\\right) P\\left(A_{i}\\right)}{P(B)}=\\frac{P\\left(B \\mid A_{i}\\right) P\\left(A_{i}\\right)}{\\sum_{j=1}^{n} P\\left(B \\mid A_{j}\\right) P\\left(A_{j}\\right)} $$ Example (Medical Test): Suppose there is a certain disease randomly found in 0.5% of the general population. Some blood test will yield a positive result in 99% of the case where the disease is present; but it also yields false-positive results in 5% of the case where a person is not infected. What is the probability that a person gets the disease if his test result is positive? Let $ A $ and $ B $ denote the events “disease is present” and “positive test result”, respectively. Then $$ \\begin{aligned} P(A \\mid B) \u0026=\\frac{P(B \\mid A) P(A)}{P(B \\mid A) P(A)+P\\left(B \\mid A^{c}\\right) P\\left(A^{c}\\right)} \\ \u0026=\\frac{0.99 \\times 0.005}{0.99 \\times 0.005+0.05 \\times(1-0.005)} \\approx 0.0905 \\end{aligned} $$ Example (Monty Hall Problem): In a TV game there are three doors $ A, B, $ and $ C, $ of which two hide nothing while behind the third there is a prize. The prize is won if it is guessed correctly that which door hides it. You choose one of the door first, say A. Before door $ A $ is opened to reveal what is behind it, the game host open one of the other two doors, say $ \\mathrm{B}, $ and shows that there is nothing behind it. He then offers you the option to change your decision (from door $ A $ to door $ C $ ). Should you stick to your original choice or change to $ \\mathrm{C} ? $ Let $ A, B, $ and $ C $ be the events “prize is behind door A” (respectively, B and C). Assume $ P(A)=P(B)=P(C)=\\frac{1}{3} . $ Let $ B^{*} $ be the event “host shows that nothing is behind door $ \\mathrm{B} $”. Then, $$ P\\left(A \\mid B^{}\\right)=\\frac{P\\left(B^{} \\mid A\\right) P(A)}{P\\left(B^{} \\mid A\\right) P(A)+P\\left(B^{} \\mid B\\right) P(B)+P\\left(B^{*} \\mid C\\right) P(C)} $$ If the prize is behind door A, the hosts randomly open door B or door $ \\mathrm{C}, $ so $ P\\left(B^{*} \\mid A\\right)=\\frac{1}{2} $ If the prize is behind door B, then $ P\\left(B^{*} \\mid B\\right)=0 $. If the prize is behind door $ C $, then $ P\\left(B^{*} \\mid C\\right)=1 $ Therefore, $ P\\left(A \\mid B^{}\\right)=\\frac{1}{3}, $ and similarly, $ P\\left(C \\mid B^{}\\right)=\\frac{2}{3} $. ","date":"0001-01-01","objectID":"/2.-foundation-of-probability-theory/:7:0","tags":null,"title":"","uri":"/2.-foundation-of-probability-theory/"},{"categories":null,"content":"2.8 Independence Definition (Independence): Events $ A $ and $ B $ are said to be statistically independent if $ P(A \\cap B)=P(A) P(B) $. Remarks: By definition, $$ P(A \\mid B)=\\frac{P(A \\cap B)}{P(B)}=\\frac{P(A) P(B)}{P(B)}=P(A) $$ Similarly, $ P(B \\mid A)=P(B) $. Therefore, the occurrence of either one does not affect the probability of the occurrence of the other; that is, the knowledge of $ B $ does not help in predicting $ A $. Mutual exclusiveness does not necessarily imply independence. If $ P(A)=0, $ then any event $ B $ is independent of $ A $. Theorem: Let $ A $ and $ B $ be two independent events. Then (1) $ A $ and $ B^{c},(2) $ $ A^{c} $ and $ B, $ (3) $ A^{c} $ and $ B^{c} $ are all independent. Definition (Independence among Several Events): Events $ A_{1}, A_{2}, \\ldots, A_{n} $ are (jointly) independent if, for every possible subset $ \\left{A_{i_{1}}, \\ldots, A_{i_{k}}\\right} $ where $ i_k=2, \\ldots, n $ $$ P\\left(A_{i_{1}} \\cap \\cdots \\cap A_{i_{k}}\\right)=P\\left(A_{i_{1}}\\right) \\times \\cdots \\times P\\left(A_{i_{k}}\\right) $$ Remark: There are $ \\left(2^{n}-n-1\\right) $ conditions to be verified. For example, three events $ A, B $ and $ C $ are independent if $$ \\begin{aligned} P(A \\cap B) \u0026=P(A) P(B) \\ P(A \\cap C) \u0026=P(A) P(C) \\ P(B \\cap C) \u0026=P(B) P(C) \\ P(A \\cap B \\cap C) \u0026=P(A) P(B) P(C) \\end{aligned} $$ It is possible to find that three events are pairwise independent but not jointly independent. Example: Suppose $ S={a, b, c, d} $ and each basic outcome is equally likely to occur. Let $ A_{1}={a, b}, A_{2}={b, c} $ and $ A_{3}={a, c} . $ Then, $$ \\begin{array}{c} P\\left(A_{1}\\right)=P\\left(A_{2}\\right)=P\\left(A_{3}\\right)=\\frac{1}{2} \\ P\\left(A_{1} \\cap A_{2}\\right)=P\\left(A_{1} \\cap A_{3}\\right)=P\\left(A_{2} \\cap A_{3}\\right)=\\frac{1}{4} \\end{array} $$ but $$ P\\left(A_{1} \\cap A_{2} \\cap A_{3}\\right)=0 \\neq \\frac{1}{8} $$ It is also possible to find three events $ A, B $, $ C $ that satisfy $ P(A \\cap B \\cap C)=P(A) P(B) P(C) $ but not independent. Example: Suppose $ S={a, b, c, d, e, f, g, h} $ and each basic outcome is equally likely to occur. Let $ A_{1}=A_{2}={a, b, c, d} $ and $ A_{3}={a, e, f, g} . $ Then, $$ P\\left(A_{1}\\right)=P\\left(A_{2}\\right)=P\\left(A_{3}\\right)=\\frac{1}{2}, \\quad P\\left(A_{1} \\cap A_{2} \\cap A_{3}\\right)=\\frac{1}{8} $$ but $$ P\\left(A_{1} \\cap A_{2}\\right)=\\frac{1}{2} \\neq P\\left(A_{1}\\right) P\\left(A_{2}\\right) $$ Theorem: If events $ A_{1}, A_{2}, \\ldots, A_{n} $ are independent, then $$ P\\left(A_{1} \\cup \\cdots \\cup A_{n}\\right)=1-\\left{\\left[1-P\\left(A_{1}\\right)\\right] \\times \\cdots \\times\\left[1-P\\left(A_{n}\\right)\\right]\\right} $$ ","date":"0001-01-01","objectID":"/2.-foundation-of-probability-theory/:8:0","tags":null,"title":"","uri":"/2.-foundation-of-probability-theory/"},{"categories":null,"content":"3. Random Variable and Univariate Distributions [Toc] ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:0:0","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.1 Random Variables Definition (Random Variable): A random variable $ X(\\cdot) $ is a $ \\mathcal{B} $ -measurable mapping from the sample space $ S $ to $ \\mathbb{R} $ such that to each $ s \\in S, $ there exists a corresponding unique real number $ X(s) $ (i.e. point function). Remarks: For each outcome $ s \\in S, X(s) $ is a real number. The collection of all possible values that the random variable $ X $ can take, also called the range of $ X(\\cdot), $ constitutes a new sample space, denoted as $ \\Omega $. Example: Toss a coin three times. $$ S={\\mathrm{HHH}, \\mathrm{HHT}, \\mathrm{HTH}, \\mathrm{THH}, \\mathrm{HTT}, \\mathrm{THT}, \\mathrm{TTH}, \\mathrm{TTT}} $$ The associated $ \\sigma $-algebra is $ \\mathcal{B}={\\text { all subsets of } S} . $ Define $ X(s) $ as “number of heads in $ s \" $, then $$ \\begin{array}{l} X(\\mathrm{T} \\mathrm{T} \\mathrm{T})=0, \\quad X(\\mathrm{T} \\mathrm{TH})=X(\\mathrm{THT})=X(\\mathrm{HTT})=1 \\ X(\\mathrm{HHT})=X(\\mathrm{HTH})=X(\\mathrm{THH})=2, \\quad X(\\mathrm{HHH})=3 \\end{array} $$ For any set $ A \\in \\mathcal{B}{\\Omega}, $ where $ \\mathcal{B}{\\Omega} $ is a $ \\sigma $ -field associated with $ \\Omega, $ we can define a probability function $ P_{X}: \\mathcal{B}_{\\Omega} \\rightarrow \\mathbb{R} $ in terms of the original probability function $ P(\\cdot) $ as $$ P_{X}(A)=P({s \\in S: X(s) \\in A}) $$ $ P_{X}(\\cdot), $ so-called the induced probability function, is indeed a probability function. $ P_{X}(\\Omega)=P(S)=1 $ Require additional condition to ensure that $$ {s \\in S: X(s) \\in A} \\in \\mathcal{B} $$ Definition (Measurable Function): A function $ X: S \\rightarrow \\mathbb{R} $ is $ \\mathcal{B} $ -measurable if for every real number $ x $, the set $ {s \\in S: X(s) \\leqslant x} \\in \\mathcal{B} $ This is a regulation condition to guarantee that $ P({s \\in S: X(s) \\leqslant x}) $ exists. Equivalently, $ X $ is $ \\mathcal{B} $ -measurable if $ {s \\in S: X(s) \\in B} \\in \\mathcal{B} $ for every Borel set $ B $. Let $ X(\\cdot) $ and $ Y(\\cdot) $ be $ \\mathcal{B} $ -measurable functions and $ c \\in \\mathbb{R} $. Then $ c X(\\cdot), X(\\cdot)+Y(\\cdot), X(\\cdot) Y(\\cdot), $ and $ |X(\\cdot)| $ are $ \\mathcal{B} $ -measurable; $ X(\\cdot) / Y(\\cdot) $ is $ \\mathcal{B} $ -measurable if $ Y(s) \\neq 0 $ for all $ s $ Let $ X_{1}(\\cdot), X_{2}(\\cdot), \\ldots $ be a sequence of $ \\mathcal{B} $ -measurable functions. Then $ \\min \\left{X_{1}(\\cdot), \\ldots, X_{n}(\\cdot)\\right} $ and $ \\max \\left{X_{1}(\\cdot), \\ldots, X_{n}(\\cdot)\\right} $ are $ \\mathcal{B} $-measurable; if $ X(\\cdot)=\\lim {n \\rightarrow \\infty} X{n}(\\cdot) $ exists then $ X(\\cdot) $ is $ \\mathcal{B} $ -measurable. Let $ X(\\cdot) $ be a $ \\mathcal{B} $ -measurable function and $ h(\\cdot) $ be a Borel-measurable real valued function, then $ h(X(\\cdot)) $ is also $ \\mathcal{B} $ -measurable. ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:1:0","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.2 Cumulative Distribution Function Definition (Cumulative Distribution Function): The cumulative distribution function $ (\\mathrm{CDF}) $ of a random variable $ X $ is defined as $$ F_{X}(x)=P(X \\leqslant x) \\text { for all } x \\in \\mathbb{R} $$ Example: Toss a coin three times. Define $ X(s) $ as “number of heads in $ s^{\\prime \\prime} $ Then, $$ P(X=0)=\\frac{1}{8}, P(X=1)=P(X=2)=\\frac{3}{8}, P(X=3)=\\frac{1}{8} $$ and therefore, $$ F_{X}(x)=\\left{\\begin{array}{ll} 0 \u0026 \\text { if } x\u003c0 \\ 1 / 8 \u0026 \\text { if } 0 \\leqslant x\u003c1 \\ 1 / 2 \u0026 \\text { if } 1 \\leqslant x\u003c2 \\ 7 / 8 \u0026 \\text { if } 2 \\leqslant x\u003c3 \\ 1 \u0026 \\text { if } x \\geqslant 3 \\end{array}\\right. $$ Example: The experiment consists of shooting once at a circular target $ T $ of radius $ r . $ Assume that it is certain that the target will be hit, and the probability of hitting a particular section $ A $ is the ratio of the area of $ A $ to the area of $ T $. Define random variable $ X(s) $ as the distance between the hitting point $ s $ and the center. Clearly, $ 0 \\leqslant X \\leqslant r . $ So $ F_{X}(x)=0 $ for $ x\u003c0 $ and $ F_{X}(x)=1 $ for $ x\u003er $ For $ 0 \\leqslant x \\leqslant r $ $$ F_{X}(x)=P(X \\leqslant x)=\\left(\\pi x^{2}\\right) /\\left(\\pi r^{2}\\right)=x^{2} / r^{2} $$ Properties of CDF: $ F x(x) $ is non-decreasing, i.e. $ F_X \\left(x_{1}\\right) \\leqslant F_X \\left(x_{2}\\right) $ for any $ x_{1}\u003cx_{2} $ $ \\lim {x \\rightarrow-\\infty} F{X}(x)=0, \\lim {x \\rightarrow+\\infty} F{X}(x)=1 $ $ F_{X}(x) $ is right-continuous, i.e. for all $ x $ $$ \\lim {\\delta \\rightarrow 0^{+}} F{X}(x+\\delta)=F_{X}(x) $$ Remark: If a function $ F(\\cdot) $ satisfies properties (1),(2) and (3), then there is a random variable $ X^{} $ such that $ P\\left(X^{} \\leqslant x\\right)=F(x) $ In some textbooks, the CDF is defined as $ F_{X}(x)=P(X\u003cx) $. Then $ F_{X}(x) $ is left-continuous under this definition. Properties of CDF: $ P(a\u003cX \\leqslant b)=F_{X}(b)-F_{X}(a) $ for $ a\u003cb $ $ P(X\u003eb)=1-F_{X}(b) $ Suppose $ F_{1}(x) $ and $ F_{2}(x) $ are two CDF’s, then for $ 0\u003cp\u003c1 $ $ F(x)=p F_{1}(x)+(1-p) F_{2}(x) $ is also a $ C D F $ Definition (Identical Distribution): Two random variables $ X $ and $ Y $ are identically distributed if for every Borel set $ B $, $ P(X \\in B)=P(Y \\in B) $ Theorem: Two random variables $ X $ and $ Y $ are identically distributed if and only if $ F_{X}(x)=F_{Y}(x) $ for all $ x \\in \\mathbb{R} $ ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:2:0","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.3 Discrete Random Variable Definition (Discrete Random Variable): If a random variable $ X $ can only take a countable number of values, then $ X $ is called a discrete random variable (DRV). Definition (Probability Mass Function): The probability mass function $ (\\mathrm{PMF}) $ of a DRV $ X $ is defined as $$ f_{X}(x)=P(X=x) \\text { for all } x \\in \\mathbb{R} $$ Definition (Support of DRV): The collection of the points at which a DRV $ X $ has a positive probability is called the support of $ X $, denoted as $$ \\text { Support }(X)=\\left{x \\in \\mathbb{R}: f_{X}(x)\u003e0\\right} $$ Properties of PMF: $ 0 \\leqslant f_{X}(x) \\leqslant 1 $ for all $ x \\in \\mathbb{R} $ $ F_{X}(x)=\\sum_{y \\in \\text { Support }(X), y \\leqslant x} f_{X}(y) $ $ \\sum_{y \\in \\text { Support }(X)} f_{X}(y)=1 $ $ f_{X}(x)=F_{X}(x)-\\lim _{\\delta \\rightarrow 0^{+}} F_{X}(x-\\delta) $ Example (Discrete Uniform Distribution): A DRV $ X $ follows uniform distribution $ U(n) $ if its PMF $$ f_{X}(x)=\\left{\\begin{array}{ll} 1 / n \u0026 \\text { if } x=1,2, \\ldots, n \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$ The CDF of uniform distributed DRV $ X $ is $$ F_{X}(x)=\\left{\\begin{array}{ll} 0 \u0026 \\text { if } x\u003c1 \\ i / n \u0026 \\text { if } i \\leqslant x\u003ci+1, i=1, \\ldots, n-1 \\ 1 \u0026 \\text { if } x \\geqslant n \\end{array}\\right. $$ Example (Bernoulli Distribution): A DRV $ X $ follows a Bernoulli(p) $ (0\u003cp\u003c1) $ distribution if its PMF $$ f_{X}(x)=\\left{\\begin{array}{ll} p \u0026 \\text { if } x=1 \\ 1-p \u0026 \\text { if } x=0 \\end{array}\\right. $$ Example (Binomial Distribution): A DRV $ X $ follows a binomial $ B(n, p) $ distribution if its PMF $$ f_{X}(x)=C_{n}^{x} p^{x}(1-p)^{n-x}, \\quad x=0,1, \\ldots, n $$ Remark: Toss a coin $ n $ times independently. Each time the head has probability $ p . $ The total number of heads follows $ B(n, p) $ distribution. Example (Poisson Distribution): A DRV $ X $ follows a Poisson$ (\\lambda)(\\lambda\u003e0) $ distribution if its PMF $$ f_{X}(x)=\\frac{e^{-\\lambda} \\lambda^{x}}{x !}, \\quad x=0,1,2, \\ldots $$ Remarks: Support of Poisson distribution is an infinite countable set. $ \\sum_{x=0}^{\\infty} f_{X}(x)=e^{-\\lambda} \\sum_{x=0}^{\\infty} \\lambda^{x} / x !=e^{-\\lambda} e^{\\lambda}=1 $ In a Poisson process with intensity $ \\lambda, $ the total number of occurrences over $ (0, t] $ follows a Poisson $ (\\lambda t) $ distribution. Poisson distribution can be used to describe the number of jumps in financial markets in a certain period. Theorem (Poisson Limit Theorem): Denote $ p_{n} $ as the probability that the event $ A $ occurs in a random experiment and it is related to the number of total experiments $ n $. $ X $ is the number that $ A $ occurs in $ n $ experiments. If $ n p_{n} \\rightarrow \\lambda $ as $ n \\rightarrow \\infty, $ then we have $$ P(X=k) \\rightarrow \\frac{\\lambda^{k}}{k !} e^{-\\lambda} $$ Example (Geometric Distribution): The geometric distribution is the probability distribution of the number of Bernoulli trials required to obtain the first success. The PMF of a geometric distributed DRV $ X $ is $$ f_{X}(x)=p(1-p)^{x-1} $$ Remarks: The geometric distribution is the simplest of the waiting time distributions. The geometric distribution has the so-called “memoryless” property in the sense that for integers $ s\u003et, $ we have $$ P(X\u003es \\mid X\u003et)=P(X\u003es-t) $$ ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:3:0","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.4 Continuous Random Variables Definition (Continuous Random Variable): A random variable is called continuous (CRV) if its cumulative distribution function $ F_{X}(x) $ is continuous for all $ x \\in \\mathbb{R} $ Definition (Absolute Continuity \u0026 Probability Density Function): The cumulative distribution function $ F_{X}(x) $ of a random variable $ X $ is called **absolutely continuous** with respect to Lebesgue measure if there exists a Borel-measurable function $ f_{X}(\\cdot) $ such that $$ F_{X}(x)=\\int_{-\\infty}^{x} f_{X}(u) \\mathrm{d} u \\quad \\text { for all } x \\in \\mathbb{R} $$ The function $ f_{X}: \\mathbb{R} \\rightarrow \\mathbb{R} $ is called a probability density function (PDF) of $ X $. An absolutely continuous CDF is continuous; For some continuous CDF, absolute continuity may not hold. For those $ x $ ’s where $ F_X(x) $ is differentiable, $ f_{X}(x)=F_{X}^{\\prime}(x) $. Theorem (Properties of PDF): A function $ f_{X}(x) $ is a PDF of a CRV $ X $ if and only if $ f_{X}(x) \\geqslant 0 $ for almost everywhere $ x, $ and $ \\int_{-\\infty}^{\\infty} f_{X}(x) d x=1 $ Remark: For any nonnegative function $ g(x) $ with finite integral, i.e. $ 0\u003c\\int_{-\\infty}^{\\infty} g(x) \\mathrm{d} x\u003c\\infty, f(x)=g(x) / \\int_{-\\infty}^{\\infty} g(u) \\mathrm{d} u $ is a $ \\mathrm{PDF} $, where $ \\int_{-\\infty}^{\\infty} g(u) \\mathrm{d} u $ is called the normalizing constant. For a given continuous random variable, CDF is unique but $ \\mathrm{PDF} $ is not. The probability density functions of a continuous random variable can be different on a set of measure $ 0 . $ The value of the PDF can be changed arbitrarily on a sequence of countable points without altering the distribution of $ X . $ For example, $ f_{X}(x)=\\left{\\begin{array}{ll}e^{-x} \u0026 \\text { if } x\u003e0, \\ 0 \u0026 \\text { otherwise },\\end{array} \\text { and } f_{X}(x)=\\left{\\begin{array}{ll}e^{-x} \u0026 \\text { if } x \\geqslant 0 \\ 0 \u0026 \\text { otherwise }\\end{array}\\right.\\right. $ represent the same distribution. Definition (Support of CRV): The support of a CRV $ X $ with PDF $ f_{X}(x) $ is defined as $$ \\text { Support }(X)=\\left{x \\in \\mathbb{R}: f_{X}(x)\u003e0\\right} $$ Example (Continuous Uniform Distribution): A CRV $ X $ follows a uniform distribution on $ [a, b] $ if its PDF $$ f_{X}(x)=\\left{\\begin{array}{ll} \\frac{1}{b-a} \u0026 \\text { if } a \\leqslant x \\leqslant b \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$ Example (Normal Distribution): A normally distributed random variable $ X \\sim N\\left(\\mu, \\sigma^{2}\\right) $ has the PDF $$ f_{X}(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left[-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right] $$ where $ -\\infty\u003c\\mu\u003c\\infty $ and $ \\sigma\u003e0 $. PDF of Normal Distributions CDF of Normal Distributions \r \r Example (Log-normal Distribution): $ X $ follows a log-normal $ \\left(\\mu, \\sigma^{2}\\right) $ distribution if its PDF $$ f_{X}(x)=\\left{\\begin{array}{ll} \\frac{1}{\\sqrt{2 \\pi} \\sigma x} \\exp \\left[-\\frac{(\\ln x-\\mu)^{2}}{2 \\sigma^{2}}\\right] \u0026 \\text { if } x\u003e0 \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$ If $ X \\sim $ log-normal $ \\left(\\mu, \\sigma^{2}\\right), $ then $ \\ln X \\sim N\\left(\\mu, \\sigma^{2}\\right) $. Example (Exponential Distribution): CRV $ X $ follows Exponential $ (\\beta) $ distribution if its PDF $$ f_{X}(x)=\\left{\\begin{array}{ll} \\frac{1}{\\beta} e^{-x / \\beta} \u0026 \\text { if } x\u003e0 \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$ where $ \\beta\u003e0 $. Remark: The PDF can be written in expression with $ \\lambda=1 / \\beta $ Exponential distribution is popular in modeling duration between financial events or economic events because of its “memoryless” property: $$ P(X\u003et+s \\mid X\u003et)=P(X\u003es), \\quad s, t\u003e0 $$ Example (Gamma Distribution): A CRV $ X $ follows a Gamma $ (\\alpha, \\beta) $ distribution if its PDF $$ f_{X}(x)=\\left{\\begin{array}{ll} \\frac{1}{\\Gamma(\\alpha) \\beta^{\\alpha}} x^{\\alpha-1} e^{-x / \\beta} \u0026 \\text { if } x\u003e0 \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$ where $ \\alpha, \\beta\u003e0 $ and $ \\Gamma(\\alpha)=\\int_{0}^{\\infty} t^{\\alp","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:4:0","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.5 Functions of Random Variable Suppose $ g: \\mathbb{R} \\rightarrow \\mathbb{R} $ is a Borel-measurable function (i.e. the preimage of any Borel set under $ g $ is a Borel set), then $ Y=g(X) $ is also a random variable. What is the distribution function of the new random variable $ Y $? ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:5:0","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.5.1 Discrete Case If $ X $ is a discrete random variable with PMF $ f_{X}(x), $ then the PMF of $ Y $ can by obtained by using $$ f_{Y}(y)=\\sum_{x \\in \\Omega_{X}(y)} f_{X}(x) $$ where $ \\Omega_{X}(y)=\\left{x \\in \\Omega_{X}: g(x)=y\\right} $ is the set of all possible values of $ x $ in the sample space $ \\Omega x $ of $ X $ such that $ g(x)=y $. Moreover, if function $ g: \\mathbb{R} \\rightarrow \\mathbb{R} $ be strictly monotonic $$ f_{Y}(y)=\\sum_{x \\in \\Omega_{X}(y)} f_{X}(x)= \\sum_{x \\in g^{-1}(y)} f_{X}(x) $$ for y in support. Example: Suppose random variable $ X $ has the distribution $x$ -2 -1 0 1 2 3 4 $ f_X(x) $ 0.1 0.2 0.1 0.1 0.1 0.2 0.2 For function $ Y=X^2 $, the distribution is $ y $ 0 1 4 9 16 $ f_Y(y) $ 0.1 0.1+0.2 0.1+0.1 0.2 0.2 Example (Binomial transformation): a CRV $ X $ has a binomial distribution and its PMF is $$ f_{X}(x)=P(X=x)=\\left(\\begin{array}{l} n \\ x \\end{array}\\right) p^{x}(1-p)^{n-x} $$ Consider the random variable $Y=g(x)=n-X $, $$ \\begin{aligned} f_{Y}(y) \u0026=\\sum_{x \\in g^{-1}(y)} f_{X}(x) \\ \u0026=f_{X}(n-y) \\ \u0026=\\left(\\begin{array}{c} n \\ n-y \\end{array}\\right) p^{n-y}(1-p)^{n-(n-y)} \\ \u0026=\\left(\\begin{array}{l} n \\ y \\end{array}\\right)(1-p)^{y} p^{n-y} \\end{aligned} $$ ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:5:1","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.5.2 Continuous Case Method 1: The CDF approach The basic idea is first to find the CDF $ F_{Y}(y) $ of $ Y $ and then its $ \\mathrm{PDF} $ by differentiation, $ f_{Y}(y)=F_{Y}^{\\prime}(y) $ Identify the possible values of $ Y $ (i.e. the support of $ Y $ ). For this purpose, it is useful to plot the function $ g(x) $ Find $ F_{Y}(y): F_{Y}(y)=P(Y \\leqslant y)=P(g(X) \\leqslant y) $ Differentiate the CDF $ F_{Y}(y) $ with respect to $ y $ : $$ f_{Y}(y)=F_{Y}^{\\prime}(y) $$ If function $ g: \\mathbb{R} \\rightarrow \\mathbb{R} $ be strictly monotonic, let $$ \\mathcal{X}=\\left{x: f_{X}(x)\u003e0\\right} \\text{ and } \\mathcal{Y}={y: y=g(x) \\text { for some } x \\in \\mathcal{X}} $$ Monotone: one-to-one mapping Increasing $$ {x \\in \\mathcal{X}: g(x) \\leq y}=\\left{x \\in \\mathcal{X}: g^{-1}(g(x)) \\leq g^{-1}(y)\\right}=\\left{x \\in \\mathcal{X}: x \\leq g^{-1}(y)\\right} $$ Decreasing $$ {x \\in \\mathcal{X}: g(x) \\leq y}=\\left{x \\in \\mathcal{X}: g^{-1}(g(x)) \\leq g^{-1}(y)\\right}=\\left{x \\in \\mathcal{X}: x \\geq g^{-1}(y)\\right} $$ Transformation (Increasing) $$ F_{Y}(y)=\\int_{\\left{x \\in \\mathcal{X}: x \\leq g^{-1}(y)\\right}} f_{X}(x) d x=\\int_{-\\infty}^{g^{-1}(y)} f_{X}(x) d x=F_{X}\\left(g^{-1}(y)\\right) $$ Transformation (decreasing) $$ F_{Y}(y)=\\int_{g^{-1}(y)}^{\\infty} f_{X}(x) d x=1-F_{X}\\left(g^{-1}(x)\\right) $$ Theorem: Let $ X $ have cdf $ F_{X}(x), $ let $ Y=g(X), $ and let $ \\mathcal{X} $ and $ \\mathcal{Y} $ be defined as above. If $ g $ is an increasing function on $ \\mathcal{X}, F_{Y}(y)=F_{X}\\left(g^{-1}(y)\\right) $ for $ y \\in \\mathcal{Y} $. If $ g $ is a decreasing function on $ \\mathcal{X} $ and $ X $ is a continuous random variable, $ F_{Y}(y)=1-F_{X}\\left(g^{-1}(y)\\right) $ for $ y \\in \\mathcal{Y} $ Example: Suppose a CRV $ X $ has a PDF $$ f_{X}(x)=\\left{\\begin{array}{ll} 1 \u0026 \\text { if }-\\frac{1}{2}\u003cx\u003c\\frac{1}{2} \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$ Find the PDF of the following $ Y $: $ Y=a+b X, b \\neq 0 $ $ Y=X^{2} $ $ Y=|X| $ $ X \\sim U\\left(-\\frac{1}{2}, \\frac{1}{2}\\right), Y=a+b X $ with $ b \\neq 0 $ If $ b\u003e0, $ since Support $ (X)=\\left(-\\frac{1}{2}, \\frac{1}{2}\\right), $ the support of $ Y $ is given by $ a-\\frac{b}{2}\u003cy\u003ca+\\frac{b}{2} . $ So for $ y \\in\\left(a-\\frac{b}{2}, a+\\frac{b}{2}\\right) $ $$ \\begin{aligned} F_{Y}(y)\u0026=P(Y\\leqslant y)=P(a+b X \\leqslant y)=P\\left(X \\leqslant \\frac{y-a}{b}\\right) \\ \u0026=F_X \\left(\\frac{y-a}{b}\\right)=\\int_{-\\frac{1}{2}}^{\\frac{y-a}{b}} f_{X}(x) \\mathrm{d} x=\\frac{y-a}{b}+\\frac{1}{2} \\end{aligned} $$ then $ f_{Y}(y)=F_{Y}^{\\prime}(y)=\\frac{1}{b} $. Therefore, $$ f_{Y}(y)=\\left{\\begin{array}{ll} \\frac{1}{b} \u0026 \\text { if } a-\\frac{b}{2}\u003cy\u003ca+\\frac{b}{2} \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$ Similarly, if $ b\u003c0, $ the support of $ Y $ is $ \\left(a+\\frac{b}{2}, a-\\frac{b}{2}\\right) . $ So for $ y \\in\\left(a+\\frac{b}{2}, a-\\frac{b}{2}\\right) $, $$ \\begin{array}{c} F_{Y}(y)\u0026=P(a+b X \\leqslant y)=P\\left(X \\geqslant \\frac{y-a}{b}\\right) \\ \u0026=1-F x\\left(\\frac{y-a}{b}\\right)=1-\\left(\\frac{y-a}{b}+\\frac{1}{2}\\right) \\end{array} $$ and then $ f_{Y}(y)=F_{Y}^{\\prime}(y)=-\\frac{1}{b} $. It follows that $$ f_{Y}(y)=\\left{\\begin{array}{ll} -\\frac{1}{b} \u0026 \\text { if } a+\\frac{b}{2}\u003cy\u003ca-\\frac{b}{2} \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$ $ x \\sim U\\left(-\\frac{1}{2}, \\frac{1}{2}\\right), Y=X^{2} $ Observe that $ 0 \\leqslant x^{2}\u003c1 / 4 $ for $ x $ in Support $ (X), $ then $ F_{Y}(y)=0 $ if $ y\u003c0 $ and $ F_{Y}(y)=1 $ if $ y\u003e1 / 4 $. For $ y \\in\\left[0, \\frac{1}{4}\\right] $ $$ \\begin{array}{c} F_{Y}(y)=P(Y \\leqslant y)=P\\left(X^{2} \\leqslant y\\right)=P(-\\sqrt{y} \\leqslant X \\leqslant \\sqrt{y}) \\ =F_{X}(\\sqrt{y})-F_{X}(-\\sqrt{y})=\\int_{-\\sqrt{y}}^{\\sqrt{y}} f_{X}(x) \\mathrm{d} x=2 \\sqrt{y} \\end{array} $$ By differentiation, $ f_{Y}(y)=\\frac{1}{\\sqrt{y}} . $ Therefore, we have $$ f_{Y}(y)=\\left{\\begin{array}{ll} \\frac{1}{\\sqrt{y}} \u0026 \\text { if } 0\u003cy\u003c\\frac{1}{4} \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$ $ X \\sim U\\left(-\\frac{1}{2}, \\frac{1}{2}\\right), Y=|X| $ Since the Support $ (X","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:5:2","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.6 Mathematical Expectations Definition (Expectation): Suppose $ X $ is a random variable with PMF or PDF $ f_{X}(x) $. Then the expectation of a measurable function $ g(X) $ is defined as $$ \\begin{aligned} E[g(X)] \u0026=\\int_{-\\infty}^{\\infty} g(X) \\mathrm{d} F_{X}(x) \\ \u0026=\\left{\\begin{array}{ll} \\sum_{x \\in \\Omega_{X}} g(x) f_{X}(x) \u0026 \\text { if } X \\text { is a } \\mathrm{DRV} \\ \\int_{-\\infty}^{\\infty} g(x) f_{X}(x) \\mathrm{d} x \u0026 \\text { if } X \\text { is a } \\mathrm{CRV} \\end{array}\\right. \\end{aligned} $$ where $ \\Omega_{X} $ is the support of $ X $. Remarks: $ E[g(X)] $ can be considered as the weighted average of $ g(X) $ If $ E|g(X)|=\\infty, $ we say $ E[g(X)] $ does not exist. If $ a $ is a constant, then $ E(a)=a $ The expectation is a linear operator, namely, $$ E\\left[a \\cdot g_{1}(X)+b \\cdot g_{2}(X)\\right]=a \\cdot E\\left[g_{1}(X)\\right]+b \\cdot E\\left[g_{2}(X)\\right] $$ $ Y=g(X) $ is also a random variable. Let the PMF or PDF of Y be $ f_{Y}(y), $ then we can also compute $ E[g(X)] $ by $$ \\begin{aligned} E[g(X)] \u0026=E(Y) \\ \u0026=\\left{\\begin{array}{ll} \\sum_{y \\in \\Omega_{Y}} y f_{Y}(y) \u0026 \\text { if } Y \\text { is } D R V \\ \\int_{-\\infty}^{\\infty} y f_{Y}(y) d y \u0026 \\text { if } Y \\text { is } C R V \\end{array}\\right. \\end{aligned} $$ ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:6:0","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.7 Moments Definition $ k $-th moment: The $ k $-th moment of a random variable $ X $ is defined as $$ E\\left(X^{k}\\right)=\\left{\\begin{array}{ll} \\sum_{x \\in \\Omega_{X}} x^{k} f_{X}(x) \u0026 \\text { if } X \\text { is a } D R V \\ \\int_{-\\infty}^{\\infty} x^{k} f_{X}(x) d x \u0026 \\text { if } X \\text { is a } C R V \\end{array}\\right. $$ Similarly, the $ k $-th central moment of a random variable $ X $ is defined as $$ E(X-\\mu_X)^{k}=\\left{\\begin{array}{ll} \\sum_{x \\in \\Omega_{X}}\\left(x-\\mu_{X}\\right)^{k} f_{X}(x) \u0026 \\text { if } X \\text { is a } D R V \\ \\int_{-\\infty}^{\\infty}\\left(x-\\mu_{X}\\right)^{k} f_{X}(x) d x \u0026 \\text { if } X \\text { is a } C R V \\end{array}\\right. $$ Relationship between uncentered moments and centered moments: $$ E\\left(X-\\mu_{X}\\right)^{k}=\\sum_{i=0}^{k}\\left(\\begin{array}{c} k \\ i \\end{array}\\right)\\left(-\\mu_{X}\\right)^{k-i} E\\left(X^{i}\\right) $$ and $$ E\\left(X^{k}\\right)=E\\left[\\left(X-\\mu_{X}\\right)+\\mu_{X}\\right]^{k}=\\sum_{i=0}^{k}\\left(\\begin{array}{c} k \\ i \\end{array}\\right) \\mu_{X}^{k-i} E\\left(X-\\mu_{X}\\right)^{i} $$ ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:7:0","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.7.1 Mean Definition (Mean): The mean of a random variable $ X $ is defined as $$ \\mu_{X}=E(X)=\\left{\\begin{array}{ll} \\sum_{x \\in \\Omega_{X}} x f_{X}(x) \u0026 \\text { if } X \\text { is a } D R V \\ \\int_{-\\infty}^{\\infty} x f_{X}(x) d x \u0026 \\text { if } X \\text { is a } C R V \\end{array}\\right. $$ where $ \\Omega_{X} $ is the support of $ X $. Remarks: The mean $ \\mu_X $ is also the expectation of $ X $ (i.e. $ g(X)=X $ ), or the first moment of $ X $. $ \\mu_X $ is a measure of central tendency for the distribution of $ X $. The mean $ \\mu x $ exists if and only if $ \\sum_{x}|x| f_{X}(x)\u003c\\infty $ for $ D R V $ $ X $ or $ \\int_{-\\infty}^{\\infty}|x| f_{X}(x) \\mathrm{d} x\u003c\\infty $ for $ \\mathrm{CRV} X $ Example (Binomial Mean): lf $ X $ has a binomial distribution, its pmf is given by $$ P(X=x)=\\left(\\begin{array}{l} n \\ x \\end{array}\\right) p^{x}(1-p)^{n-x}, x=0,1, \\cdots, n $$ where $ n $ is a positive integer, $ 0 \\leq p \\leq 1 $, and for every fixed pair $ n $ and $ p $ the pmf sums to 1. Calculate the $ E X $. $$ \\begin{aligned} E X \u0026=\\sum_{x=0}^{n} x\\left(\\begin{array}{c} n \\ x \\end{array}\\right) p^{x}(1-p)^{n-x} \\ \u0026=\\sum_{x=0}^{n} x \\frac{n !}{x !(n-x) !} p^{x}(1-p)^{n-x} \\ \u0026=\\sum_{x=0}^{n} \\frac{n !}{(x-1) !(n-x) !} p^{x}(1-p)^{n-x} \\end{aligned} $$ Since the $ x=0 $ term vanishes. Let $ y=x-1 $ and $ m=n-1 . $ Subbing $ x=y+1 $ and $ n=m+1 $ into the last sum (and using the fact that the limits $ x=1 $ and $ x=n $ correspond to $ \\mathrm{y}=0 \\text { and } y=n-1=m, \\text { respectively }) $ $$ \\begin{aligned} E X \u0026=\\sum_{y=0}^{m} \\frac{(m+1) !}{y !(m-y) !} p^{y+1}(1-p)^{m-y} \\ \u0026=(m+1) p \\sum_{y=0}^{m} \\frac{m !}{y !(m-y) !} p^{y}(1-p)^{m-y} \\ \u0026=n p \\end{aligned} $$ Example (Cauchy distribution): A random variable $ X $ follows Cauchy $ \\left(x_{0}, \\gamma\\right) $ distribution if its PDF $$ f_{X}(x)=\\frac{1}{\\pi \\gamma}\\left[\\frac{\\gamma^{2}}{\\left(x-x_{0}\\right)^{2}+\\gamma^{2}}\\right], \\quad-\\infty\u003cx\u003c\\infty $$ where $ x_{0} $ is the location parameter and $ \\gamma $ is the scale parameter. Suppose $ X $ follows Cauchy (0,1) distribution, then $$ E|X|=\\int_{-\\infty}^{\\infty} \\frac{|x|}{\\pi\\left(1+x^{2}\\right)} \\mathrm{d} x=\\infty $$ so mean of Cauchy distribution does not exist. Theorem: Suppose $ E\\left(X^{2}\\right) $ exists. Then $$ \\mu x=\\arg \\min _{a} E(X-a)^{2} $$ ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:7:1","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.7.2 Variance Definition (Variance \u0026 Standard Deviation): The variance of random variable $ X $ is defined as $$ \\begin{aligned} \\operatorname{Var}(X)\u0026=\\sigma_{X}^{2}= E\\left(X-\\mu_{X}\\right)^{2} \\ \u0026=\\left{\\begin{array}{ll} \\sum_{x \\in \\Omega_{X}}\\left(x-\\mu_{X}\\right)^{2} f_{X}(x) \u0026 \\text { if } X \\text { is a DRV, } \\ \\int_{-\\infty}^{\\infty}\\left(x-\\mu_{X}\\right)^{2} f_{X}(x) d x \u0026 \\text { if } X \\text { is a CRV. } \\end{array}\\right. \\end{aligned} $$ where $ \\Omega X $ is the support of $ X $. The quantity $ \\sigma_X =\\sqrt{\\sigma_{X}^{2}} $ is called the standard deviation of $ X $. Remarks: $ \\sigma_{X}^{2} $ is a measure of the degree of spread of a distribution around its mean. In economics, it is interpreted as a measure of uncertainty or risk. It is often called a measure of volatility of $ X $. If $ \\sigma_{X}^{2}=0, $ then $ X=\\mu_{X} $ with probability 1 and there is no variation in $ X $. This is so-called **degenerate distribution**. Theorem $$ \\sigma_{X}^{2}=E\\left(X^{2}\\right)-\\mu_{X}^{2} $$ Remark: $ \\sigma_{X}^{2} $ is called the second central moment and $ E\\left(X^{2}\\right) $ is called the second moment. Theorem: If $ Y=a X+b, $ then $ \\mu_{Y}=a \\mu_{X}+b $ $ \\sigma_{Y}^{2}=a^{2} \\sigma_{X}^{2} $ Example (Portfolio selection): Assume that the investor likes higher return but lower risk. That is, his utility function $ u\\left(\\mu, \\sigma^{2}\\right) $ is such that $ \\partial u / \\partial \\mu\u003e0 $ (the more expected return, the better and $ \\partial u / \\partial \\sigma^{2}\u003c0 $ (the smaller risk, the better $ ) $. An example of $ u\\left(\\mu, \\sigma^{2}\\right) $ is $$ u\\left(\\mu, \\sigma^{2}\\right)=a \\mu-b \\sigma^{2}, \\quad a, b\u003e0 $$ Assume that the investor has totally I dollars to be split between stock $ (w) $ and saving $ (I-w) . $ What is the portfolio that maximizes the utility function? The rate of return on stocks is a random variable $ Y $ with mean $ \\mu_{Y} $ and variance $ \\sigma_{Y}^{2} $. The rate of return on saving is constant $ r, $ which can be considered as a random variable $ Z $ with mean $ r $ and variance $ 0 . $ Usually $ r\u003c\\mu_{Y} $. The return of a portfolio is $ X=w Y+(I-w) Z, $ then $$ u\\left(\\mu, \\sigma^{2}\\right)=a\\left[w \\mu_{Y}+(I-w) r\\right]-b w^{2} \\sigma_{Y}^{2} $$ It is maximized when $$ w=\\frac{a\\left(\\mu_{Y}-r\\right)}{2 b \\sigma_{Y}^{2}} $$ If $ b=0 $ (i.e. the investor does not care risk), $ u\\left(\\mu, \\sigma^{2}\\right) $ is maximized at $ w=\\infty $. If $ a=0, $ then $ u\\left(\\mu, \\sigma^{2}\\right) $ is maximized at $ w=0 $ ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:7:2","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.7.3 Skewness Definition (Skewness): The third central moment $ E\\left[\\left(X-\\mu_{X}\\right)^{3}\\right] $ is a measure of “skewness” (or asymmetry) of the distribution of $ X $. Skewness is defined as $$ S_{X}=\\frac{E\\left[\\left(X-\\mu_{X}\\right)^{3}\\right]}{\\sigma_{X}^{3}} $$ The skewness has been used to measure financial crashes. Negative (or positive) skewness indicates a higher (or lower) probability of experiencing large losses than large gains. \r ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:7:3","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.7.4 Kurtosis Definition (Kurtosis): The fourth central moment $ E\\left[\\left(X-\\mu_{X}\\right)^{4}\\right] $ is a measure of how heavy the tail of a distribution is. Kurtosis is defined as $$ K_{X}=\\frac{E\\left[\\left(X-\\mu_{X}\\right)^{4}\\right]}{\\sigma_{X}^{4}} $$ \r Remarks: Kurtosis of normal distribution is 3, regardless of the values of parameters $ \\mu $ and $ \\sigma^{2} $. The excess kurtosis of a random variable $ X $ is defined as $ K_{X}-3 $. A distribution with positive excess kurtosis has a more acute peak around the mean and fatter tails. A distribution with negative excess kurtosis has a lower, wider peak and thinner tails. ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:7:4","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.8 Quantile Definition ($ \\alpha $-quantile): Suppose random variable $ X $ has a $ \\operatorname{CDF} F_{X}(x) . $ Let $ \\alpha \\in(0,1), $ then the $ \\alpha $-quantile of the distribution $ F_X (x) $ is defined as $ Q(\\alpha), $ which satisfies $$ F_{X}(Q(\\alpha))=P(X \\leqslant Q(\\alpha))=\\alpha $$ Remarks: When $ F_{X}(x) $ is continuous and strictly increasing, $$ Q(\\alpha)=F_{X}^{-1}(\\alpha) $$ In case $ F_{X}(x) $ has flat regions or is discontinuous, we can define the $ \\alpha $-quantile as $$ Q(\\alpha)=\\inf \\left{x \\in \\mathbb{R}: F_{X}(x) \\geqslant \\alpha\\right} $$ For $ \\alpha $-quantile $ Q(\\alpha), $ we have $$ \\int_{-\\infty}^{Q(\\alpha)} f_{X}(x) \\mathrm{d} x=\\alpha $$ 0.5-quantile is called the median. $ 0.25 $-quantile and $ 0.75 $-quantile are called lower quartile and upper quartile. 0-quantile is the minimum value; 1-quantile is the maximum value. \r Difference between mean and median: Median is the cutoff point that divides the population in half. Mean can be misleading when used to measure the location of highly skewed data. In contrast, median is a more robust measure of the central tendency of a distribution in the sense that it is not much affected by a few outliers. Median is the optimal solution for minimizing the mean absolute error, that is, $$ Q(0.5)=\\arg \\min _{a} E|X-a| $$ while mean is the optimal solution for minimizing the mean squared error, that is, $$ E(X)=\\arg \\min _{a} E(X-a)^{2} $$ For symmetric distribution, e.g. normal distribution, mean and median are the same. For skewed distributions, mean and median are different. \r Example (Value at Risk): The value at risk (VaR) at level $ \\alpha, V_{t}(\\alpha), $ of a portfolio over a certain time horizon is defined as $$ P\\left(X_{t}\u003c-V_{t}(\\alpha) \\mid I_{t-1}\\right)=\\alpha $$ where $ X_{t} $ is the return on the portfolio over the holding period $ t $, and $ I_{t-1} $ is the information available at time $ t-1 $. $ V_{t}(\\alpha), $ which is the negative conditional quantile of portfolio return at level $ \\alpha, $ is the threshold that actual loss will exceed with probability $ \\alpha $. ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:8:0","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.9 Moment Generating Function Definition (Moment Generating Function): The moment generating function(MGF) of a random variable $ X $ is defined as $$ \\begin{aligned} M_{X}(t) \u0026=E\\left[e^{t X}\\right] \\ \u0026=\\left{\\begin{array}{ll} \\sum_{x \\in \\Omega_{X}} e^{t x} f_{X}(x) \u0026 \\text { if } X \\text { is a } D R V \\ \\int_{-\\infty}^{\\infty} e^{t x} f_{X}(x) d x \u0026 \\text { if } X \\text { is a } C R V \\end{array}\\right. \\end{aligned} $$ Remarks: $ M_{X}(t) $ may not exist for some $ t \\in \\mathbb{R} . $ If the expectation does not exist for any small neighborhood of 0, then we say that MGF does not exist for the distribution of $ X $. The existence of the MGF $ M_{X}(t) $ implies the existence of an infinite set of moments. Theorem: If $ M_{X}(t) $ exists for $ t $ in some neighborhood of $ 0, $ then $ M_{X}(0)=1 $ for $ k=1,2, \\ldots $, $ M_{X}^{(k)}(0)=E\\left(X^{k}\\right) $ the MGF of $ Y=a+b X $ is $$ M_{Y}(t)=e^{a t} M_{X}(b t) $$ for all $ t $ in a small neighborhood of 0. Proof: Assuming that we can differentiate under the integral sign, we have $$ \\begin{aligned} \\frac{d}{d t} M_{X}(t) \u0026=\\frac{d}{d t} \\int_{-\\infty}^{\\infty} e^{t x} f_{X}(x) d x \\ \u0026=\\int_{-\\infty}^{\\infty}\\left(\\frac{d}{d t} e^{t x}\\right) f_{X}(x) d x \\ \u0026=\\int_{-\\infty}^{\\infty}\\left(x e^{t x}\\right) f_{X}(x) d x \\ \u0026=E\\left(X e^{t X}\\right) \\end{aligned} $$ Thus, $ \\left.\\frac{d}{d t} M_{X}(t)\\right|_{t=0}=\\left.E\\left(X e^{t X}\\right)\\right|_{t=0}=E X $. Proceeding in an analogous manner, we can establish that $$ \\left.\\frac{d^{n}}{d t^{n}} M_{X}(t)\\right|_{t=0}=\\left.E\\left(X^{n} e^{t X}\\right)\\right|_{t=0}=E\\left(X^{n}\\right) $$ ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:9:0","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.9.1 Discrete Distributions Example (Binomial distribution): For binomial distribution with PMF $$ \\begin{array}{c} f_{X}(x)=\\left(\\begin{array}{c} n \\ x \\end{array}\\right) p^{x}(1-p)^{n-x}, \\quad x=0,1, \\ldots, n \\ E(X)=n p, E\\left(X^{2}\\right)=n(n-1) p^{2}+n p, M_{X}(t)=\\left(p e^{t}+1-p\\right)^{n} \\end{array} $$ Proof: $$ \\begin{aligned} M_{X}(t) \u0026=\\sum_{x=0}^{n} e^{t x}\\left(\\begin{array}{c} n \\ x \\end{array}\\right) p^{x}(1-p)^{n-x} \\ \u0026=\\sum_{x=0}^{n}\\left(\\begin{array}{c} n \\ x \\end{array}\\right)\\left(p e^{t}\\right)^{x}(1-p)^{n-x} \\end{aligned} $$ Using the binomial formula $$ \\sum_{x=0}^{n}\\left(\\begin{array}{l}n \\ x\\end{array}\\right) u^{x} v^{n-x}, $$ hence, letting $$ u=p e^{t} $$ and $$ v=1-p, $$ we have $$ M_{X}(t)=\\left[p e^{t}+(1-p)\\right]^{n} $$ Example (Poisson distribution): For Poisson distribution with PMF $$ \\begin{array}{c} f_{X}(x)=e^{-\\lambda} \\frac{\\lambda^{x}}{x !}, \\quad x=0,1,2, \\ldots, \\ E(X)=\\lambda, E\\left(X^{2}\\right)=\\lambda^{2}+\\lambda, M_{X}(t)=\\exp \\left(e^{t} \\lambda-\\lambda\\right) \\end{array} $$ Proof: $$ M_{Y}(t)=\\sum_{x=0}^{\\infty} e^{t x} \\frac{e^{-\\lambda \\lambda^{x}}}{x !}=e^{-\\lambda} \\sum_{x=1}^{\\infty} \\frac{\\left(e^{t} \\lambda\\right)^{x}}{x !}=e^{\\lambda} e^{\\lambda e^{t}}=e^{\\lambda\\left(e^{t}-1\\right)} $$ **Example (Uniform distribution)**: For continuous uniform distribution on $ [a, b], $ its PDF $$ f_{X}(x)=\\left{\\begin{array}{ll} \\frac{1}{b-a} \u0026 \\text { if } a \\leqslant x \\leqslant b \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$ Then $$ \\begin{array}{c} E\\left(X^{k}\\right)=\\frac{b^{k+1}-a^{k+1}}{(k+1)(b-a)} \\ E(X)=\\frac{a+b}{2}, \\quad \\sigma_{X}^{2}=\\frac{(b-a)^{2}}{12} \\ M_{X}(t)=\\frac{e^{t b}-e^{t a}}{(b-a) t} \\end{array} $$ ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:9:1","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.9.2 Continuous Distributions Example (Normal distribution): For normal distribution $ N\\left(\\mu, \\sigma^{2}\\right) $ with PDF $$ \\begin{array}{c} f_{X}(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left[-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right], \\quad-\\infty\u003c\\mu\u003c\\infty, \\sigma\u003e0 \\ M_{X}(t)=\\exp \\left(\\mu t+\\sigma^{2} t^{2} / 2\\right) \\end{array} $$ Proof: $$ M_{X}(t)=E\\left(e^{x t}\\right)=\\int e^{x t} \\frac{1}{\\sqrt{2 \\sigma^{2} \\pi}} e^{-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}} d x $$ Define $$ z=\\frac{x-\\mu}{\\sigma}, $$ which implies $$ x=\\mu+\\sigma z, $$ hence, $$ M_{X}(t)=e^{\\mu t} \\int e^{z \\sigma t} \\frac{1}{\\sqrt{2 \\pi}} e^{\\frac{-z^{2}}{2}}\\left|\\frac{d x}{d z}\\right| d z=e^{\\mu t} e^{\\frac{1}{2} \\sigma^{2} t^{2}} $$ Example (Gamma mgf) Gamma pdf : $$ f(x)=\\frac{1}{\\Gamma(\\alpha) \\beta^{\\alpha}} x^{\\alpha-1} e^{-x / \\beta}, 0\u003cx\u003c\\infty, \\alpha\u003e0, \\beta\u003e0 $$ Mgf: $$ \\begin{aligned} M_{X}(t) \u0026=\\frac{1}{\\Gamma(\\alpha) \\beta^{\\alpha}} \\int_{0}^{\\infty} e^{t x} x^{\\alpha-1} e^{-x / \\beta} d x \\ \u0026=\\frac{1}{\\Gamma(\\alpha) \\beta^{\\alpha}} \\int_{0}^{\\infty} x^{\\alpha-1} e^{-\\left(\\frac{1}{\\beta}-t\\right) x} d x \\ \u0026=\\frac{1}{\\Gamma(\\alpha) \\beta^{\\alpha}} \\int_{0}^{\\infty} x^{\\alpha-1} e^{-x /\\left(\\frac{\\beta}{1-\\beta t}\\right)} d x \\ \u0026=\\left(\\frac{1}{1-\\beta t}\\right)^{\\alpha} \\int_{0}^{\\infty} \\frac{1}{\\Gamma(\\alpha)\\left(\\frac{\\beta}{1-\\beta t}\\right)^{\\alpha}} x^{\\alpha-1} e^{-x /\\left(\\frac{\\beta}{1-\\beta t}\\right)} d x \\ \u0026=\\left(\\frac{1}{1-\\beta t}\\right)^{\\alpha} \\text { if } t\u003c\\frac{1}{\\beta} \\end{aligned} $$ Example (Log-normal distribution): Suppose $ X $ follows a log-normal $ \\left(\\mu, \\sigma^{2}\\right) $ distribution. Let $ Y=\\ln X $ then $ Y \\sim N\\left(\\mu, \\sigma^{2}\\right) $ and therefore $$ E\\left(X^{k}\\right)=E\\left(e^{k Y}\\right)=M_{Y}(k)=\\exp \\left(\\mu k+\\frac{\\sigma^{2} k^{2}}{2}\\right) $$ The MGF $ M_{X}(t) $ does not exists for $ t\u003e0 $. ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:9:2","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.9.3 Uniqueness of MGF Recall that random variables $ X $ and $ Y $ are identically distributed if two CDF’s $ F_{X}(\\cdot) $ and $ F_{Y}(\\cdot) $ are the same, i.e. $ F_{X}(x)=F_{Y}(x) $ If $ X $ and $ Y $ are identically distributed, then for any $ g(\\cdot) $, $$ E[g(X)]=E[g(Y)] $$ Identity of the distributions of $ X $ and $ Y $ does not imply $ X=Y $. Example Example: Suppose a fair coin is tossed $ n $ times, and let $ X $ be the number of heads obtained, $ Y $ be the number of tails obtained. Then $ F_{X}(x)=F_{Y}(x) $ but $ X+Y=n $ and possibly $ X \\neq Y $. Theorem (Uniqueness of MGF): Suppose two random variables $ X $ and $ Y $ with MGF’s $ M_{X}(t) $ and $ M_{Y}(t) $ existing in a neighborhood of $ 0, N_{\\epsilon}(0)={t:-\\epsilon\u003ct\u003c\\epsilon} $. Then $ X $ and $ Y $ have the same $ M_{X}(t) $ and $ M_{Y}(t) $ for all $ t $ in $ N_{\\epsilon}(0) $, if and only if $ F_{X}(u)=F_{Y}(u) $ for all $ u \\in \\mathbb{R} $. If $ X $ and $ Y $ have bounded support, then $ F_{X}(u)=F_{Y}(u) $ for all $ u $ if and only if $ E\\left(X^{r}\\right)=E\\left(Y^{r}\\right) $ for all integers $ r=0,1,2, \\cdots $ Remarks: If the MGF $ M_{X}(t) $ exists in a neighborhood of $ 0, $ it uniquely characterizes a distribution function. Given some MGF $ M_{X}(t), $ suppose we can find some $ \\mathrm{CDF} $ $ F_{X}(x) $ that corresponds to $ M_{X}(t) . $ Then $ F_{X}(x) $ must be the only distribution that generates $ M_{X}(t) $ Uniqueness of MGF can be used to prove CLT. Example: A DRV $ X $ has $ M_{X}(t)=\\frac{1}{2}+\\frac{e^{t}}{4}+\\frac{e^{-t}}{4} . $ Then its PMF $$ f_{X}(x)=\\left{\\begin{array}{ll} 1 / 4 \u0026 \\text { if } x=-1 \\ 1 / 2 \u0026 \\text { if } x=0 \\ 1 / 4 \u0026 \\text { if } x=1 \\end{array}\\right. $$ Example: Suppose $ M_{X}(t)=e^{2 t^{2}} . $ Recall the normal distribution $ N\\left(\\mu, \\sigma^{2}\\right) $ has $ \\mathrm{MGF} $ $$ M_{X}(t)=\\exp \\left(\\mu t+\\frac{\\sigma^{2}}{2} t^{2}\\right) $$ Let $ \\mu=0 $ and $ \\sigma^{2}=4, $ then $ M_{X}(t)=\\exp \\left(2 t^{2}\\right) . $ So $ X \\sim N(0,4) $ Example: If a DRV $ X $ has $ M_{X}(t)=\\frac{1-r}{1-r e^{t}} $ for $ \\left|e^{t}\\right|\u003c1 / r, $ where $ r\u003e0 . $ What is the probability distribution of $ X ? $ Note that $$ M_{X}(t)=(1-r) \\sum_{x=0}^{\\infty}\\left(r e^{t}\\right)^{x}=\\sum_{x=0}^{\\infty}(1-r) r^{x} \\cdot e^{t x}=\\sum_{x=0}^{\\infty} f_{X}(x) e^{t x} $$ where $ f_{X}(x)=(1-r) r^{x}, x=0,1,2, \\ldots . $ since $$ \\sum_{x=0}^{\\infty} f_{X}(x)=(1-r) \\cdot \\frac{1}{1-r}=1 $$ $ f_{X}(x) $ is the PMF of $ X $. Existence of the MGF The existence of the MGF $ M_{X}(t) $ implies the existence of $$ E(X), E\\left(X^{2}\\right), E\\left(X^{3}\\right), \\ldots $$ The existence of all moments is not equivalent to the existence of moment generating function in a neighborhood of $ 0 . $ Example (Log-normal distribution): Suppose $ X $ follows a log-normal $ \\left(\\mu, \\sigma^{2}\\right) $ distribution. Then $$ E\\left(X^{k}\\right)=\\exp \\left(\\mu k+\\frac{\\sigma^{2} k^{2}}{2}\\right) $$ but the MGF $$ M_{X}(t)=E\\left(e^{x t}\\right)=\\int_{0}^{\\infty} \\frac{1}{\\sqrt{2 \\pi} \\sigma x} \\exp \\left[t x-\\frac{(\\ln x-\\mu)^{2}}{2 \\sigma^{2}}\\right] \\mathrm{d} x=\\infty $$ for $ t\u003e0 $. The set of moments $ E\\left(X^{k}\\right), k=1,2, \\ldots $ does not uniquely characterize a distribution function. Example: Consider two distributions with support $ (0, \\infty) $ : $$ f_{X}(x)=\\frac{\\exp \\left[-\\frac{(\\ln x)^{2}}{2}\\right]}{\\sqrt{2 \\pi} x}, \\quad f_{Y}(y)=f_{X}(y)[1+\\sin (2 \\pi \\ln y)] $$ for $ x, y\u003e0 . E\\left(X^{k}\\right)=E\\left(Y^{k}\\right) $ for $ k=1,2, \\ldots $ Theorem: Let $ F_{X}(x) $ and $ F_{Y}(y) $ be two $ C D F^{\\prime} s $ both of which have bounded support. Then $ F_{X}(z)=F_{Y}(z) $ for all $ z \\in \\mathbb{R} $ if and only if $ E\\left(X^{k}\\right)=E\\left(Y^{k}\\right) $ for all integers $ k=1,2, \\ldots $ Theorem (Convergence of MGF): Suppose $ \\left{X_{n}, n=1,2, \\ldots\\right} $ is a sequence of random variables, each with $ \\mathrm{CDF} F_{n}(x) $ and $ \\mathrm{MGF} M_{n}(t) . $ Furthermore, suppose that $$ \\lim _{n \\ri","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:9:3","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"3.10 Characteristic Function Definition (Characteristic Function): The characteristic function of a random variable $ X $ with $ \\operatorname{CDF} F_{X}(x) $ is defined as $$ \\begin{aligned} \\varphi_{X}(t) \u0026=E\\left(e^{i t X}\\right) \\ \u0026=\\int_{-\\infty}^{\\infty} e^{i t x} d F_{X}(x) \\end{aligned} $$ where $ i=\\sqrt{-1} $ and $ e^{i t x}=\\cos (t x)+i \\sin (t x) $. For continuous random variable, the characteristic function is the Fourier transform of the PDF, so the PDF $ f_{X}(x) $ can be recovered from the characteristic function by $$ f_{X}(x)=\\frac{1}{2 \\pi} \\int_{-\\infty}^{\\infty} e^{-i t x} \\varphi_{X}(t) d t $$ Properties of characteristic function: For any probability distribution, the characteristic function always exists and is bounded, i.e. for any $ t \\in \\mathbb{R} $, $$ |\\varphi x(t)| \\leqslant \\int_{-\\infty}^{\\infty}\\left|e^{i t x}\\right| d F_{X}(x)=\\int_{-\\infty}^{\\infty} d F_{X}(x)=1 $$ $ \\varphi_{X}(0)=1 $ $ \\varphi x(t) $ is continuous over $ \\mathbb{R} $ If the MGF $ M_{X}(t) $ exists for $ t $ in some neighborhood of $ 0, $ then $ \\varphi_{X}(t)=M_{X}(\\text { it }) $ for all $ t \\in \\mathbb{R} $ Theorem: Suppose the $ k $-th moment of $ X $ exists. Then $ \\varphi_{X}(t) $ is differentiable up to order $ k $ and $$ \\varphi_{X}^{(k)}(0)=\\mathrm{i}^{k} E\\left(X^{k}\\right) $$ Theorem (Uniqueness of Characteristic Function): Suppose two random variables $ X $ and $ Y $ have characteristic functions $ \\varphi_{X}(t) $ and $ \\varphi_{Y}(t) $ respectively. Then $ X $ and $ Y $ are identically distributed if and only if $ \\varphi x(t)=\\varphi_{Y}(t) $ for all $ t \\in \\mathbb{R} $ Remark: It is important to check all $ t $ on the entire real line. But for MGF’s, it is only necessary to check $ t $ in a neighborhood of $ 0 . $ Theorem (Convergence of Characteristic Function): Let $ \\left{X_{n}\\right} $ be a sequence of random variables with CDF’s $ F_{n}(x) $ and characteristic functions $ \\varphi_{n}(t) $. Let $ X $ be a random variable with CDF $ F_X(x) $ and characteristic function $ \\varphi_X (t) $. Let $ n \\rightarrow \\infty $. If $ F_{n}(x) \\rightarrow F_{X}(x) $ for all continuous points $ x $ of $ F_{X}(\\cdot), $ then $ \\varphi_{n}(t) \\rightarrow \\varphi_{X}(t) $ for every $ t \\in \\mathbb{R} $ Further, if $ \\varphi_{n}(t) \\rightarrow \\varphi x(t) $ for all $ t \\in \\mathbb{R}, $ then $ F_{n}(x) \\rightarrow F_{X}(x) $ for all continuous points $ x $ of $ F x(\\cdot) $. Leibnitz’s Rule: if $ f(x, \\theta), a(\\theta), $ and $ b(\\theta) $ are differentiable respect to $ \\theta, $ then $$ \\frac{d}{d \\theta} \\int_{a(\\theta)}^{b(\\theta)} f(x, \\theta) d x=f(b(\\theta), \\theta) \\frac{d}{d \\theta} b(\\theta)-f(a(\\theta), \\theta) \\frac{d}{d \\theta} a(\\theta)+\\int_{a(\\theta)}^{b(\\theta)} \\frac{\\partial}{\\theta} f(x, \\theta) d x $$ ","date":"0001-01-01","objectID":"/3.-random-variable-and-univariate-distributions/:10:0","tags":null,"title":"","uri":"/3.-random-variable-and-univariate-distributions/"},{"categories":null,"content":"4. Important Probability Distributions [toc] A class of probability measures, say PMF/ PDF $ f(x, \\theta) $, indexed by parameter $ \\theta, $ where the functional form $ f(\\cdot, \\cdot) $ is known The family of probability distributions is called a class of parametric probability distribution models. ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:0:0","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.1 Discrete Probability Distributions ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:1:0","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.1.1 Discrete Uniform Distribution A random variable $ X $ has a discrete uniform $ (1, \\mathrm{N}) $ distribution if $$ P(X=x \\mid N)=\\frac{1}{N}, x=1,2, \\cdots, N $$ where $ N $ is a specified integer. The distribution puts equal mass on each of the outcomes $ 1,2, \\cdots, N $. Since $$ \\sum_{i=1}^{k} i=\\frac{k(k+1)}{2} \\text { and } \\sum_{i=1}^{n} i^{2}=\\frac{k(k+1)(2 k+1)}{6} $$ Hence, we have $$ E(X)=\\sum_{x=1}^{N} x P(X=x \\mid N)=\\sum_{x=1}^{N} x \\frac{1}{N}=\\frac{N+1}{2} $$ and $$ E X^{2}=\\sum_{x=1}^{N} x^{2} \\frac{1}{N}=\\frac{(N+1)(2 N+1)}{6} $$ and so $$ \\operatorname{Var}(X)=E X^{2}-(E X)^{2}=\\frac{(N+1)(2 N+1)}{6}-\\left(\\frac{N+1}{2}\\right)^{2}=\\frac{(N+1)(N-1)}{12} $$ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:1:1","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.1.2 Bernoulli Distribution Bernoulli distribution has wide applications in economics and finance. Model binary data where the outcome only has two possibilities. $ X \\sim $ Bernoulli $ (p) $, if $ \\mathrm{PMF} $ $$ \\begin{aligned} f_{X}(x) \u0026=\\left{\\begin{array}{ll} p \u0026 \\text { if } x=1 \\ 1-p \u0026 \\text { if } x=0 \\end{array}\\right.\\ \u0026=p^{x}(1-p)^{1-x}, \\quad x \\in{0,1} \\end{aligned} $$ where the parameter $ p \\in(0,1) $ $ \\boldsymbol{E}\\left(X^{k}\\right)=p $ for all $ k=1,2, \\ldots $ $ \\operatorname{Var}(X)=p(1-p) $ $ M_{X}(t)=p e^{t}+1-p, \\quad t \\in \\mathbb{R} $ The parameter $ p $ (the probability that the binary random variable takes value 1 ) will vary across individuals as a function of observable variables, say $ Z $. For example, logit model assume $$ P(X=1 \\mid Z)=\\frac{1}{1+\\exp \\left(-\\beta^{\\prime} Z\\right)} $$ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:1:2","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.1.3 Binomial Distribution If $ X_{1}, X_{2}, \\ldots, X_{n} $ are i.i.d. Bernoulli $ (p) $ random variables, then $$ X=\\sum_{i=1}^{n} X_{i} $$ follows a $ B(n, p) $ distribution. $ B(n, p) $ is the distribution of the number of successes in a sequence of $ n $ independent Bernoulli trials, each of which yields success with probability $ p $. Bernoulli $ (p) $ is in fact $ B(1, p) $. $ X \\sim B(n, p) $, if PMF $$ f_{X}(x)=\\left(\\begin{array}{l} n \\ x \\end{array}\\right) p^{x}(1-p)^{n-x}, \\quad x=0,1,2, \\ldots, n $$ where parameter $ n \\in \\mathbb{N} $ and parameter $ p \\in(0,1) $ Theorem: For any real numbers $ x $ and $ y $ and integer $ n \\geq 0 $, $$ (x+y)^{n}=\\sum_{i=0}^{n}\\left(\\begin{array}{l} n \\ i \\end{array}\\right) x^{i} y^{n-i} $$ If we take $$ x=p $$ and $$ y=1-p, $$ we get $$ 1=(p+1-p)^{n}=\\sum_{i=0}^{n}\\left(\\begin{array}{l} n \\ i \\end{array}\\right) p^{i}(1-p)^{n-i} $$ $ E(X)=n p $ $ \\operatorname{Var}(X)=n p(1-p) $ $ M_{X}(t)=\\left(p e^{t}+1-p\\right)^{n} $ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:1:3","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.1.4 Geometric Distribution Geometric distribution is the distribution of the number of independent Bernoulli trials needed to get one success, supposing each trial has a probability $ p $ of success. $ X \\sim \\text { Geom }(p) $, if PMF $$ f_{X}(x)=(1-p)^{x-1} p, \\quad x=1,2, \\ldots $$ where parameter $ p \\in(0,1) $. $ E(X)=1 / p $ $ \\operatorname{Var}(X)=(1-p) / p^{2} $ $ M_{X}(t)=\\frac{p e^{t}}{1-(1-p) e^{t}}, \\quad t\u003c-\\ln (1-p) $ Memoryless property: For integers $ s, t\u003e0 $, $$ P(X\u003et+s \\mid X\u003et)=P(X\u003es) $$ To show this, first note that $$ \\begin{aligned} P(X\u003et) \u0026=1-P(X \\leqslant t) \\ \u0026=1-\\sum_{x=1}^{t} p(1-p)^{x-1} \\ \u0026=1-\\frac{p-p(1-p)^{t}}{1-(1-p)} \\ \u0026=1-\\left[1-(1-p)^{t}\\right]=(1-p)^{t} \\end{aligned} $$ and $ P(X\u003es)=(1-p)^{s}, P(X\u003et+s)=(1-p)^{t+s} $. Then $$ \\begin{aligned} P(X\u003et+s \\mid X\u003et) \u0026=\\frac{P(X\u003et+s, X\u003et)}{P(X\u003et)} \\ \u0026=\\frac{P(X\u003et+s)}{P(X\u003et)} \\ \u0026=\\frac{(1-p)^{t+s}}{(1-p)^{t}} \\ \u0026=(1-p)^{s}=P(X\u003es) \\end{aligned} $$ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:1:4","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.1.5 Negative Binomial Distribution $ N B(r, p) $ is the distribution of the number of independent Bernoulli trials such that the $ r $-th success occurs at the $ X $-th trial, supposing each trial has a probability $ p $ of success, e.g. A family wish to have some fixed number of boys. If $ X_{1}, X_{2}, \\ldots, X_{r} $ are i.i.d. Geom $ (p) $ random variables, then $$ X=\\sum_{i=1}^{r} X_{i} $$ follows a $ N B(r, p) $ distribution. $ X \\sim N B(r, p) $, if PMF $$ \\begin{aligned} f_{X}(x)=\u0026\\left[\\left(\\begin{array}{l} x-1 \\ r-1 \\end{array}\\right) p^{r-1}(1-p)^{(x-1)-(r-1)}\\right] \\cdot p \\ =\u0026\\left(\\begin{array}{l} x-1 \\ r-1 \\end{array}\\right) p^{r}(1-p)^{x-r}, \\quad x=r, r+1, \\ldots \\end{aligned} $$ where parameter $ p \\in(0,1) $. $ E(X)=r / p $ $ \\operatorname{Var}(X)=r(1-p) / p^{2} $ $ M_{X}(t)=\\left[\\frac{p e^{t}}{1-(1-p) e^{t}}\\right]^{r}, \\quad t\u003c-\\ln (1-p) $ Example: Suppose that in a population of fruit flies, we are interested in the proportion having vestigial wings and decide to sample until we have found 100 such flies. The probability that we will have to examine at least $ N $ flies is $$ \\begin{aligned} P(X \\geq N) \u0026=\\sum_{x=N}^{\\infty}\\left(\\begin{array}{c} x-1 \\ 99 \\end{array}\\right) p^{100} (1-p)^{x-100} \\ \u0026=1-\\sum_{x=100}^{N-1}\\left(\\begin{array}{c} x-1 \\ 99 \\end{array}\\right) p^{100}(1-p)^{x-100} \\end{aligned} $$ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:1:5","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.1.6 Hyper-geometric Distribution Large urn with $ N $ balls that are identical in every way except that $ M $ are red and $ N-M $ are green. We reach in, blindfolded, and select $ K $ balls at random. What is the probability that exactly $ x $ of the balls are red? The numbers of size $ K $ that can be drawn from $ N $ balls is $ C^K_N $. choosing $ x $ red balls is $ C_M^x $ choosing $ K-x $ green balls is $ C^{K-x}_{N-M} $ Then $ X $ has a hyper-geometric distribution given by $$ P(X=x \\mid N, M, K)=\\frac{ C_M^x C^{K-x}_{N-M} }{ C^K_N } $$ Support: $ M \\geq x $ and $ N-M \\geq K-x \\Rightarrow M-(N-K) \\leq x \\leq M$ Expectation $$ E X=\\sum_{x=0}^{K} x \\frac{\\left(\\begin{array}{c} M \\ x \\end{array}\\right)\\left(\\begin{array}{c} N-M \\ K-x \\end{array}\\right)}{\\left(\\begin{array}{c} N \\ K \\end{array}\\right)} $$ by using the fact $ \\sum_{x=1}^{K} \\frac{\\left(\\begin{array}{c}M-1 \\ x-1\\end{array}\\right)\\left(\\begin{array}{c}N-M \\ K-x\\end{array}\\right)}{\\left(\\begin{array}{c}N-1 \\ K-1\\end{array}\\right)}=1, $ we have $$ E X=\\frac{K M}{N} \\sum_{x=1}^{K} \\frac{\\left(\\begin{array}{c} M-1 \\ x-1 \\end{array}\\right)\\left(\\begin{array}{c} N-M \\ K-x \\end{array}\\right)}{\\left(\\begin{array}{c} N-1 \\ K-1 \\end{array}\\right)}=\\frac{K M}{N} $$ Variance $$ \\operatorname{Var}(X)=\\frac{K M}{N}\\left(\\frac{(N-M)(N-K)}{N(N-1)}\\right) $$ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:1:6","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.1.7 Poisson Distribution $ X \\sim $ Poisson $ (\\lambda) $, if $ \\mathrm{PMF} $ $$ f_{X}(x)=e^{-\\lambda} \\frac{\\lambda^{x}}{x !}, \\quad x=0,1,2, \\ldots $$ where the intensity parameter $ \\lambda\u003e0 $. Recall the Taylor series expansion of $ e^{y}, e^{y}=\\sum_{i=0}^{\\infty} \\frac{y^{i}}{i !} $. $$ \\sum_{x=0}^{\\infty} P(X=x \\mid \\lambda)=e^{-\\lambda} \\sum_{x=0}^{\\infty} \\frac{\\lambda^{x}}{x !}=e^{-\\lambda} e^{\\lambda}=1 $$ Mean of $ X $ $$ \\begin{aligned} E X \u0026=\\sum_{x=0}^{\\infty} x \\frac{e^{-\\lambda} \\lambda^{x}}{x !}=\\sum_{x=1}^{\\infty} x \\frac{e^{-\\lambda} \\lambda^{x}}{x !} \\ \u0026=\\lambda e^{-\\lambda} \\sum_{x=1}^{\\infty} \\frac{\\lambda^{x-1}}{(x-1) !}=\\lambda \\end{aligned} $$ $ \\operatorname{Var}(X)=\\lambda $ $ \\boldsymbol{M}_{X}(t)=e^{\\lambda\\left(e^{t}-1\\right)} $ Poisson process Counting the number of random events starting from $ t=0 $. $ N(t)= $ the number of events that have occurred during the time period $ [0, t] $. Assumptions: Stationary: For all $ m \\geqslant 0 $ and for any time intervals $ \\Delta_{1}=\\Delta_{2}, P\\left(m \\text { events in } \\Delta_{1}\\right)=P\\left(m \\text { events in } \\Delta_{2}\\right) $ Independent increments: For all $ m \\geqslant 0 $ and for any time interval $ (t, t+s], P(m \\text { events in }(t, t+s]) $ is independent of how many events have occurred earlier or how they have occurred; Sequencing: The occurring of two or more events in a very small time interval is practically impossible, i.e. $ \\lim _{\\Delta \\rightarrow 0} P(N(\\Delta)\u003e1) / \\Delta=0 $ A stochastic process $ {N(t)} $ satisfying these three assumptions is called a stationary Poisson process. Suppose $ N(0)=0, $ then there exists $ \\lambda\u003e0 $ such that $$ P(N(t)=m)=\\frac{(\\lambda t)^{m} e^{-\\lambda t}}{m !}, \\quad m=0,1,2, \\ldots $$ that is, for $ t\u003e0, N(t) \\sim $ Poisson $ (\\lambda t) $ $ \\lambda $ is the average number of events during a unit time period. Poisson approximation We can use a Poisson $ (\\lambda) $ distribution to approximate a $ B(n, p) $ distribution when $ p \\rightarrow 0 $ and $ n p \\rightarrow \\lambda $ as $ n \\rightarrow \\infty $. The MGF of the binomial distribution $ B(n, p) $ is $$ M_{B}(t)=\\left(p e^{t}+1-p\\right)^{n}=\\left[1+\\frac{n p \\cdot\\left(e^{t}-1\\right)}{n}\\right]^{n} $$ For every $ t, $ when $ n \\rightarrow \\infty $ and $ n p \\rightarrow \\lambda, $ we have $$ M_{B}(t) \\rightarrow e^{\\lambda\\left(e^{t}-1\\right)}=M_{P}(t) $$ which is the MGF of Poisson $ (\\lambda) $. We can also show that the PMF of the binomial distribution $ B(n, p) $ converges to the PMF of Poisson distribution. Example: A typesetter on average makes one error in every 500 words typeset. What is the probability that there will be no more than two errors in 1500 words? Use binomial distribution: Assume that making error in typing a word follows a Bernoulli distribution with $ p=1 / 500 $. Then the number of errors in 1500 words, $ X, $ follows a binomial distribution B(1500, p). So, $$ P(X \\leqslant 2)=\\sum_{x=0}^{2} \\left(\\begin{array}{c} 1500 \\ x \\end{array}\\right) \\left(\\frac{1}{500}\\right) \\times\\left(\\frac{499}{500}\\right)^{1500-x} \\approx 0.4230 $$ Use Poisson approximation: Let $ \\lambda=n p=3 $, then $$ P(X \\leqslant 2) \\approx \\sum_{x=0}^{2} \\frac{\\lambda^{x} e^{-\\lambda}}{x !}=e^{-3}\\left(1+\\frac{3}{1 !}+\\frac{3^{2}}{2 !}\\right) \\approx 0.4232 $$ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:1:7","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.2 Continuous Probability Distributions ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:2:0","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.2.1 Continuous Uniform Distribution $ X \\sim U[a, b] $, if $ \\mathrm{PDF} $ $$ f_{X}(x)=\\left{\\begin{array}{ll} \\frac{1}{b-a} \u0026 \\text { if } a \\leqslant x \\leqslant b \\ 0 \u0026 \\text { otherwise } \\end{array}\\right. $$ $ \\boldsymbol{E}(X)=\\frac{a+b}{2}, \\quad E\\left(X^{k}\\right)=\\frac{1}{b-a} \\cdot \\frac{b^{k+1}-a^{k+1}}{k+1} $ $ \\operatorname{Var}(X)=\\frac{1}{12}(b-a)^{2} $ $ M_{X}(t)=\\frac{1}{t(b-a)}\\left(e^{t b}-e^{t a}\\right), \\quad t \\in \\mathbb{R} $ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:2:1","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.2.2 Beta Distribution $ X \\sim \\operatorname{Beta}(\\alpha, \\beta) $, if $ \\mathrm{PDF} $ $$ f_{X}(x)=\\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1}(1-x)^{\\beta-1}, \\quad 0 \\leqslant x \\leqslant 1 $$ where $ \\alpha\u003e0, \\beta\u003e0, $ and $ B(\\alpha, \\beta) $ is the Beta function $$ B(\\alpha, \\beta)=\\int_{0}^{1} t^{\\alpha-1}(1-t)^{\\beta-1} \\mathrm{d} t $$ Moments $$ \\begin{aligned} E X^{n} \u0026=\\frac{1}{B(\\alpha, \\beta)} \\int_{0}^{1} x^{n} x^{\\alpha-1}(1-x)^{\\beta-1} d x \\ \u0026=\\frac{1}{B(\\alpha, \\beta)} \\int_{0}^{1} x^{\\alpha+n-1}(1-x)^{\\beta-1} d x \\end{aligned} $$ We now recognize the integrand as the kernel of a beta $ (\\alpha+n, \\beta) $ PDF, which means $$ E\\left(X^{n}\\right)=\\frac{B(\\alpha+n, \\beta)}{B(\\alpha, \\beta)}=\\frac{\\Gamma(\\alpha+n) \\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha+\\beta+n) \\Gamma(\\alpha)} $$ Set $ \\mathrm{n}=1, \\mathrm{n}=2, $ We then have $$ E X=\\frac{\\alpha}{\\alpha+\\beta} \\text { and } \\operatorname{Var}(X)=\\frac{\\alpha \\beta}{(\\alpha+\\beta)^{2}(\\alpha+\\beta+1)} $$ And $ \\boldsymbol{M}{X}(t)=1+\\sum{j=1}^{\\infty}\\left(\\prod_{i=0}^{j-1} \\frac{\\alpha+i}{\\alpha+\\beta+i}\\right) \\frac{t^{j}}{j !}, \\quad t \\in \\mathbb{R} $. ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:2:2","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.2.3 Gamma Distribution $ X \\sim \\text { Gamma }(\\alpha, \\beta) $, if $ \\mathrm{PDF} $ $$ f_{X}(x)=\\left{\\begin{array}{ll} \\frac{1}{\\beta^{\\alpha} \\Gamma(\\alpha)} x^{\\alpha-1} e^{-x / \\beta} \u0026 \\text { if } x\u003e0 \\ 0 \u0026 \\text { if } x \\leqslant 0 \\end{array}\\right. $$ where $ \\alpha\u003e0 $ is the shape parameter, $ \\beta\u003e0 $ is the scale parameter, and $ \\Gamma(\\alpha)=\\int_{0}^{\\infty} t^{\\alpha-1} e^{-t} \\mathrm{d} t $ is the **Gamma function**. Remark: Gamma function is an extension of the factorial function. $ \\Gamma(k)=(k-1) ! $ if $ k \\in \\mathbb{N} $ $ \\Gamma(\\alpha+1)=\\alpha \\Gamma(\\alpha) $ $ \\Gamma(1 / 2)=\\sqrt{\\pi} $ Mean $$ E X=\\frac{1}{\\Gamma(\\alpha) \\beta^{\\alpha}} \\int_{0}^{\\infty} x x^{\\alpha-1} e^{-x / \\beta} d x $$ Since $ \\int_{0}^{\\infty} x^{\\alpha-1} e^{-x / \\beta} d x=\\Gamma(\\alpha) \\beta^{\\alpha} $ (property of PDF), $$ \\begin{aligned} E X \u0026=\\frac{1}{\\Gamma(\\alpha) \\beta^{\\alpha}} \\int_{0}^{\\infty} x^{\\alpha} e^{-x / \\beta} d x \\ \u0026=\\frac{1}{\\Gamma(\\alpha) \\beta^{\\alpha}} \\Gamma(\\alpha+1) \\beta^{\\alpha+1} \\ \u0026=\\frac{\\alpha \\Gamma(\\alpha) \\beta}{\\Gamma(\\alpha)} \\ \u0026=\\alpha \\beta \\end{aligned} $$ Moreover, we have $ E\\left(X^{k}\\right)=\\beta^{k} \\prod_{i=0}^{k-1}(\\alpha+i) $ $ \\operatorname{Var}(X)=\\alpha \\beta^{2} $ $ \\boldsymbol{M}_{X}(t)=(1-\\beta t)^{-\\alpha}, \\quad t\u003c1 / \\beta $ If $ X_{1}, X_{2}, \\ldots, X_{n} $ are independent and $ X_{i} \\sim \\operatorname{Gamma}\\left(\\alpha_{i}, \\beta\\right) $ for $ i=1,2, \\ldots, n, $ then $$ \\sum_{i=1}^{n} X_{i} \\sim \\text { Gamma }\\left(\\sum_{i=1}^{n} \\alpha_{i}, \\beta\\right) $$ If $ X \\sim \\operatorname{Gamma}(\\alpha, \\beta) $ and $ c\u003e0, $ then $$ c X \\sim \\text { Gamma }(\\alpha, c \\beta) $$ Gamma-Poisson Relation, If $ X $ is a Gamma $ (\\alpha, \\beta) $ random variable, where $ \\alpha $ is an integer, then for any $ X $ $$ P(X \\leq x)=P(Y \\geq \\alpha) $$ If we set $ \\alpha=p / 2, $ where $ p $ is an integer, and $ \\beta=2, $ then the gamma PDF becomes $$ f(x \\mid p)=\\frac{1}{\\Gamma(p / 2) 2^{p / 2}} x^{p / 2-1} e^{-x / 2} $$ which is the Chi-squared PDF with p degrees of freedom. When we set $ \\alpha=1, $ we then have $$ f(x \\mid \\beta)=\\frac{1}{\\beta} e^{-x / \\beta}, 0\u003cx\u003c\\infty $$ which is an exponential PDF. It is also memoryless (lifetimes). ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:2:3","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.2.4 Normal Distribution Importance of the normal distribution: Central Limit Theorem Under certain conditions, the sample average of sufficiently large number of i.i.d. random variables, will be approximately normally distributed, regardless of the underlying distribution. $ X \\sim N\\left(\\mu, \\sigma^{2}\\right) $, if $ \\mathrm{PDF} $ $$ f_{X}(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}}, \\quad x \\in \\mathbb{R} $$ where $ \\mu \\in \\mathbb{R} $ and $ \\sigma^{2}\u003e0 $, and $$ E(X)=\\mu, \\quad \\operatorname{Var}(X)=\\sigma^{2} $$ Standardize normal distributions: $$ \\begin{array}{l} X \\sim N\\left(\\mu, \\sigma^{2}\\right) \\Rightarrow \\frac{X-\\mu}{\\sigma} \\sim N(0,1) \\ X \\sim N(0,1) \\Rightarrow \\mu+\\sigma X \\sim N\\left(\\mu, \\sigma^{2}\\right) \\end{array} $$ The MGF of $ Y \\sim N(0,1) $ is $$ \\begin{aligned} M_{Y}(t)\u0026=E\\left(e^{t Y}\\right)=\\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} e^{t y} e^{-y^{2} / 2} d y \\ \u0026=\\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(y^{2}-2 t y+t^{2}\\right)+\\frac{1}{2} t^{2}} d y \\ \u0026=e^{t^{2} / 2} \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2}(y-t)^{2}} \\mathrm{d} y=e^{t^{2} / 2} \\end{aligned} $$ Then the MGF of $ X=\\mu+\\sigma Y \\sim N\\left(\\mu, \\sigma^{2}\\right) $ is $$ \\begin{aligned} M_{X}(t)\u0026=E\\left(e^{t X}\\right)=E \\left[e^{t(\\mu+\\sigma Y)}\\right]=e^{\\mu t} E\\left(e^{\\sigma t Y}\\right) \\ \u0026=e^{\\mu t} M_{Y}(\\sigma t)=e^{\\mu t} e^{\\sigma^{2} t^{2} / 2}=e^{\\mu t+\\sigma^{2} t^{2} / 2} \\end{aligned} $$ All odd central moments, $ E(X-\\mu)^{2 k-1} $ for $ k \\in \\mathbb{N}, $ are $ 0 . $ Even central moments are given by $$ E(X-\\mu)^{2 k}=\\sigma^{2 k}(2 k-1) ! !, \\quad k \\in \\mathbb{N} $$ Calculate the even central moments Brute-force integration $$ \\int_{-\\infty}^{\\infty}(x-\\mu)^{2 k} \\cdot \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}} \\mathrm{d} x $$ Differentiate the MGF of $ Y=X-\\mu $ and evaluate $ M_{Y}^{(2 k)}(0) $ Using Stein’s Lemma Lemma (Stein’s Lemma): Suppose $ X \\sim N\\left(\\mu, \\sigma^{2}\\right), $ and $ g(\\cdot) $ is a differentiable function satisfying $ E\\left|g^{\\prime}(X)\\right|\u003c\\infty . $ Then $$ E[g(X)(X-\\mu)]=\\sigma^{2} E\\left[g^{\\prime}(X)\\right] $$ Apply Stein’s Lemma to calculate $ E(X-\\mu)^{4} $ : Let $ g(X)=(X-\\mu)^{3}, $ then $$ E(X-\\mu)^{4}=E\\left[(X-\\mu)^{3}(X-\\mu)\\right]=E[g(X)(X-\\mu)] $$ By Stein’s Lemma, $$ E(X-\\mu)^{4}=\\sigma^{2} E\\left[3(X-\\mu)^{2}\\right]=3 \\sigma^{4} $$ Thus, the kurtosis of normal distribution is $ 3 . $ Normal Approximation: Let $ X \\sim $Binomial $ (25,0.6) $. We can approximate $ X $ with a normal random variable, $ Y, $ with mean $ \\mu=25(0.6)=15 $ and standard deviation $ \\sigma=(25(0.6)(0.4))^{1 / 2}=2.45 . $ Thus $$ P(X \\leq 13) \\approx P(Y \\leq 13)=P\\left(Z \\leq \\frac{13-15}{2.45}\\right)=P(Z \\leq-0.82)=0.206 $$ The exact binomial calculation gives $$ P(X \\leq 13)=\\sum_{x=0}^{13}\\left(_{x}^{25}\\right)(0.6)^{x}(0.4)^{25-x}=0.267 $$ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:2:4","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.2.5 Chi-Square Distribution $ \\chi_{\\nu}^{2} $ is the distribution of the sum of $ \\nu $ squared independent standard normal random variables. $ X \\sim \\chi_{\\nu}^{2} $, if $ \\mathrm{PDF} $ $$ f_{X}(x)=\\left{\\begin{array}{ll} \\frac{1}{2^{\\nu / 2} \\Gamma(\\nu / 2)} \\times^{\\frac{\\nu}{2}-1} e^{-\\frac{x}{2}} \u0026 \\text { if } x\u003e0 \\ 0 \u0026 \\text { if } x \\leqslant 0 \\end{array}\\right. $$ where $ \\nu\u003e0 $ is the degree of freedom. $ \\chi_{\\nu}^{2} $ is Gamma $ (\\nu / 2,2) $ $ E(X)=\\nu $ $ \\operatorname{Var}(X)=2 \\nu $ $ M_{X}(t)=(1-2 t)^{-\\nu / 2}, \\quad t\u003c\\frac{1}{2} $ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:2:5","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.2.6 The Student’s t Distribution Let $ X \\sim N(0,1) $ and $ Y_{n} \\sim \\chi_{n}^{2}, $ where $ X $ and $ Y_{n} $ are independent. Then the distribution of the random variable $$ T_{n}=\\frac{X}{\\sqrt{Y_{n} / n}} $$ is called the (Student’s) t distribution with n degrees of freedom and is denoted by $ t_{n} $. The conditional density $ h_{n}(x \\mid y) $ of $ T_{n} $ given $ Y_{n}=y $ is the density of the $ N(1, n / y) ; $ hence, the unconditional density of $ T_{n} $ is $$ \\begin{aligned} h_{n}(x) \u0026=\\frac{\\exp \\left(-\\left(x^{2} / n\\right) y / 2\\right)}{\\sqrt{n / y} \\sqrt{2 \\pi}} \\times \\frac{y^{n / 2-1} \\exp (-y / 2)}{\\Gamma(n / 2) 2^{n / 2}} d y \\ \u0026=\\frac{\\Gamma((n+1) / 2)}{\\sqrt{n \\pi} \\Gamma(n / 2)\\left(1+x^{2} / n\\right)^{(n+1) / 2}} \\end{aligned} $$ The expectation of $ T_{n} $ does not exist if $ n=1, $ and is zero for $ n \\geq 2 $ by symmetry. The variance of $ T_{n} $ is infinite for $ n=2, $ whereas for $ n \\geq 3 $ var $ \\left(T_{n}\\right)=E\\left(T_{n}^{2}\\right)=\\frac{n}{n-2} $ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:2:6","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.2.7 The F distribution Let $ X \\sim \\chi_{m}^{2} $ and $ Y_{n} \\sim \\chi_{n}^{2}, $ where $ X_{m} $ and $ Y_{n} $ are independent. Then the distribution of the random variable $$ F=\\frac{X_{m} / m}{Y_{n} / n} $$ is said to be $ \\mathrm{F} $ with $ \\mathrm{m} $ and $ \\mathrm{n} $ degrees of freedom and is denoted by $ F_{m, n} $. Its' density is $$ h_{m, n}(x)=\\frac{m^{m / 2} \\Gamma(m / 2+n / 2) x^{m / 2-1}}{n^{m / 2} \\Gamma(m / 2) \\Gamma(n / 2)[1+m x / n]^{m / 2+n / 2}} $$ Expectation $$ E(F)=\\left{\\begin{array}{ll} n / n-2, \u0026 \\text { if } n \\geq 3 \\ =\\infty, \u0026 \\text { if } n=1,2 \\end{array}\\right. $$ Variance $$ \\operatorname{var}(F)=\\left{\\begin{array}{ll} \\frac{2 n^{2}(m+n-4)}{m(n-2)^{2}(n-4)}, \u0026 \\text { if } n \\geq 5 \\ =\\infty, \u0026 \\text { if } n=3,4 \\ =\\text { not defined }, \u0026 \\text { if } n=1,2 \\end{array}\\right. $$ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:2:7","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.2.8 Log-normal Distribution $ X \\sim $ Log-normal $ \\left(\\mu, \\sigma^{2}\\right) $, if $ \\mathrm{PDF} $ $$ f_{X}(x)=\\left{\\begin{array}{ll} \\frac{1}{\\sqrt{2 \\pi} \\sigma x} e^{-\\frac{(\\ln x-\\mu)^{2}}{2 \\sigma^{2}}} \u0026 \\text { if } x\u003e0 \\ 0 \u0026 \\text { if } x \\leqslant 0 \\end{array}\\right. $$ If $ Y \\sim N\\left(\\mu, \\sigma^{2}\\right), $ then $ X=e^{Y} \\sim $ Log-normal $ \\left(\\mu, \\sigma^{2}\\right) $ Calculate the moments of log-normal distribution using the MGF of normal distribution $$ E\\left(X^{k}\\right)=E\\left(e^{k Y}\\right)=M_{Y}(k)=e^{k \\mu+\\sigma^{2} k^{2} / 2} $$ So $ E(X)=e^{\\mu+\\sigma^{2} / 2} $, and $ \\operatorname{Var}(X)=e^{2 \\mu+\\sigma^{2}}\\left(e^{\\sigma^{2}}-1\\right) $. But MGF does not exist. Example: Assume $$ X_{t}=X_{t-1}\\left(1+Y_{t}\\right) $$ and $ \\left{Y_{t}\\right} $ is a sequence of i.i.d. random variables such that $ Y_{t} $ is independent of $ X_{t-1} $. Then $$ X_{T}=X_{0} \\prod_{t=1}^{T}\\left(1+Y_{t}\\right) \\Rightarrow \\ln X_{T}=\\ln X_{0}+\\sum_{t=1}^{T} \\ln \\left(1+Y_{t}\\right) $$ By the CLT, for sufficiently large $ T, \\sum_{t=1}^{T} \\ln \\left(1+Y_{t}\\right) $ will be approximately normally distributed, after suitable standardization. Thus, $ \\prod_{t=1}^{n}\\left(1+Y_{t}\\right) $ and $ X_{T} $ are approximately log-normally distributed. ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:2:8","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.2.9 Cauchy Distribution $ X \\sim \\operatorname{Cauchy}(\\mu, \\sigma) $, if PDF $$ f_{X}(x)=\\frac{1}{\\pi \\sigma}\\left[1+\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right]^{-1}, \\quad x \\in \\mathbb{R} $$ where $ \\mu $ is the location parameter and $ \\sigma\u003e0 $ is the scale parameter. Cauchy distribution is symmetric about $ \\mu $ but has heavier tails than normal distribution. $ E\\left(X^{k}\\right) $ does not exist for any $ k \\geqslant 1 $ MGF does not exist. ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:2:9","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.2.10 Exponential Distribution $ X \\sim \\operatorname{Exp}(\\beta) $, if $ \\mathrm{PDF} $ $$ f_{X}(x)=\\left{\\begin{array}{ll} \\frac{1}{\\beta} e^{-x / \\beta} \u0026 \\text { if } x\u003e0 \\ 0 \u0026 \\text { if } x \\leqslant 0 \\end{array}\\right. $$ where $ \\beta\u003e0 $ is the scale parameter. $ \\operatorname{Exp}(\\beta) $ is Gamma $ (1, \\beta) $ $ E(X)=\\beta $ $ \\operatorname{Var}(X)=\\beta^{2} $ $ \\boldsymbol{M}_{X}(t)=\\frac{1}{1-\\beta t}, \\quad t\u003c1 / \\beta $ Exponential distribution is the only continuous distribution with memoryless property. In a Poisson process with intensity/rate $ \\lambda, $ the waiting time between consecutive events follows $ \\operatorname{Exp}(1 / \\lambda) $ distribution. Exponential distribution is the only continuous distribution that has a constant hazard rate. Remark: Hazard rate or hazard function is defined as $$ \\begin{aligned} \\lambda(x) \u0026=\\lim {\\Delta x \\rightarrow 0^{+}} \\frac{P(X \\leqslant x+\\Delta x \\mid X\u003ex)}{\\Delta x} \\ \u0026=\\lim {\\Delta x \\rightarrow 0^{+}} \\frac{P(x\u003cX \\leqslant x+\\Delta x)}{P(X\u003ex) \\cdot \\Delta x} \\ \u0026=\\frac{1}{P(X\u003ex)} \\lim {\\Delta x \\rightarrow 0^{+}} \\frac{\\int{x}^{x+\\Delta x} f{X}(u) \\mathrm{d} u}{\\Delta x} \\ \u0026=\\frac{f{X}(x)}{P(X\u003ex)}=\\frac{f_{X}(x)}{1-F_{X}(x)} \\end{aligned} $$ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:2:10","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"4.2.11 Double Exponential Distribution The double exponential distribution is formed by reflecting the exponential distribution around its mean, The PDF is given by $$ f(x \\mid \\mu, \\sigma)=\\frac{1}{2 \\sigma} e^{-|x-\\mu| / \\sigma},-\\infty\u003cx\u003c\\infty,-\\infty\u003c\\mu\u003c\\infty, \\sigma\u003e0 $$ Mean and Variance $$ E X=\\mu \\text { and } \\operatorname{Var}(X)=2 \\sigma^{2} $$ ","date":"0001-01-01","objectID":"/4.-important-probability-distributions/:2:11","tags":null,"title":"","uri":"/4.-important-probability-distributions/"},{"categories":null,"content":"7 Convergences and Limit Theorems ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:0:0","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.1 Limits and Orders of Magnitude 【limit】Let $\\left{b_{n}, n=1,2, \\ldots\\right}$ be a sequence of (non-stochastic) real numbers. If there exists a real number $b$ such that for every real number $\\varepsilon\u003e0,$ there exists a finite integer $N(\\varepsilon)$ such that $$ \\left|b_{n}-b\\right|\u003c\\varepsilon \\quad \\text { for all } n \\geqslant N(\\varepsilon) $$ then $b$ is called the limit of the sequence $\\left{b_{n}\\right} .$ We write $b_{n} \\rightarrow b$ as $n \\rightarrow \\infty,$ or $\\lim _{n \\rightarrow \\infty} b_{n}=b$ Remarks: If $a_{n} \\rightarrow a$ and $b_{n} \\rightarrow b$ as $n \\rightarrow \\infty,$ then when $n \\rightarrow \\infty$ $a_{n}+b_{n} \\rightarrow a+b$ $a_{n} b_{n} \\rightarrow a b$ $ a_{n} / b_{n} \\rightarrow a / b$ if $b \\neq 0$ 【Continuity】The function $g: \\mathbb{R} \\rightarrow \\mathbb{R}$ is continuous at point b if for any sequence $\\left{b_{n}\\right}$ such that $b_{n} \\rightarrow b$ as $n \\rightarrow \\infty,$ we have $$ g\\left(b_{n}\\right) \\rightarrow g(b) \\text { as } n \\rightarrow \\infty $$ Remarks 1: An alternative but equivalent definition: for each given $\\epsilon\u003e0$ there exists a $\\delta=\\delta(\\epsilon)$ such that whenever $\\left|b_{n}-b\\right|\u003c\\delta,$ we have $\\left|g\\left(b_{n}\\right)-g(b)\\right|\u003c\\epsilon$. When $g(\\cdot)$ is continuous at $b,$ we can write $$ \\lim {b{n} \\rightarrow b} g\\left(b_{n}\\right)=g\\left(\\lim _{n \\rightarrow \\infty} b_{n}\\right)=g(b) $$ In other words, the limit of a sequence of values for a continuous function is equal to the value of the function at the limit. Remarks 2: for any $\\varepsilon\u003e0,$ there exists $\\delta(\\varepsilon)\u003e0$ such that $|g(x)-g(b)|\u003c\\varepsilon$ for all $b\u003cx\u003cb+\\delta(\\varepsilon),$ then we say $g$ is right continuous at $b,$ denoted by $\\lim _{x \\rightarrow b^{+}} g(x)=g(b)$ for any $\\varepsilon\u003e0,$ there exists $\\delta(\\varepsilon)\u003e0$ such that $|g(x)-g(b)|\u003c\\varepsilon$ for all $b-\\delta(\\varepsilon)\u003cx\u003cb,$ then we say $g$ is left continuous at $b,$ denoted by $\\lim _{x \\rightarrow b^{-}} g(x)=g(b)$ Function $g$ is continuous at $b$ if and only if $g$ is left continuous and right continuous at $b$ 【Order of Magnitude】Suppose $\\left{f_{n}\u003e0, n=1,2, \\ldots\\right}$ is a sequence of real numbers. A sequence $\\left{b_{n}, n=1,2, \\ldots\\right}$ is at most of order $f_{n},$ denoted by $$ b_{n}=\\mathcal{O}\\left(f_{n}\\right) \\quad \\text { or } \\quad b_{n} / f_{n}=\\mathcal{O}(1) $$ if for some (fixed and sufficiently large) number $0\u003cM\u003c\\infty$ , there exists a finite integer $N(M)$ such that for all $n \u003e N(M)$, $$ \\left|b_{n} / f_{n}\\right|\u003c M $$ A sequence $\\left{b_{n}, n=1,2, \\ldots\\right}$ is of order smaller than $f_{n}$ denoted by $$ b_{n}=o\\left(f_{n}\\right) \\quad \\text { or } \\quad b_{n} / f_{n}=o(1) $$ if for every real number $\\epsilon \u003e 0 $there exists a finite integer $N(\\epsilon)$ such that for all $n \u003e N(\\epsilon)$, we have $$ \\left|b_{n} / f_{n}\\right|\u003c \\epsilon $$ namely, $b_{n} / f_{n} \\rightarrow 0$ as $n \\rightarrow \\infty$ Remark: $b_{n}=o\\left(f_{n}\\right)$ implies $b_{n}=\\mathcal{O}\\left(f_{n}\\right)$ In the definition of $b_{n}=O\\left(n^{\\lambda}\\right),$ the constant $M$ is usually set to be a very big number. Note that it suffices to find one constant $M$ only. For $\\lambda\u003e0, b_{n}=O\\left(n^{\\lambda}\\right)$ implies that $b_{n}$ grows to infinity at a rate slower than or at most equal to $n^{\\lambda}$. In particular, if $\\lim _{n \\rightarrow \\infty} \\frac{b_{n}}{n \\lambda}=C\u003c\\infty$, then $b_{n}=O\\left(n^{\\lambda}\\right)$ or $n^{-\\lambda} b_{n}=O(1)$ 【Lemma 1】Let $a_{n}$ and $b_{n}$ be scalars (1) If $a_{n}=O\\left(n^{\\lambda}\\right)$ and $b_{n}=O\\left(n^{\\mu}\\right),$ then $$ \\begin{aligned} a_{n} b_{n} \u0026=O\\left(n^{\\lambda+\\mu}\\right) \\ a_{n}+b_{n} \u0026=O\\left(n^{\\kappa}\\right) \\end{aligned} $$ where $\\kappa=\\max (\\lambda, \\mu)$ (2) If $a_{n}=o\\left(n^{\\lambda}\\right)$ and $b_{n}=o\\left(n^{\\mu}\\right),$ then $$ \\begin{aligned} a_{n} b_{n} \u0026=o\\left(n^{\\lambda+\\mu}\\right) \\ a_{n}+b_{n} \u0026=o\\left(n^{\\kappa}\\right) \\end{aligned} $$ where ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:1:0","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.2 Motivation for Convergence Concepts Why do we need convergence concepts in econometrics? Recall that a random sample $\\mathrm{X}^{n}=\\left(X_{1}, \\cdots, X_{n}\\right)$ of size $n$ is a sequence of random variables $X_{1}, \\cdots, X_{n} .$ It can be viewed as an n-dimensional real-valued random vector, where the dimension $n$ may go to infinity. Its realization is an n-dimensional vector $\\mathbf{x}^{n}=\\left(x_{1}, \\ldots, x_{n}\\right) .$ A realization $\\mathbf{x}^{\\mathbf{n}}$ of $\\mathrm{X}^{n}$ is usually called a sample point or a data set generated from the random sample $\\mathrm{X}^{n}$. Since $\\mathrm{X}^{n}$ is a sequence of $n$ random variables, we can use the joint probability distribution of $\\mathrm{X}^{n}$ to characterize the random sample. Put $\\mathrm{x}^{i-1}=\\left(x_{i-1}, \\cdots, x_{1}\\right)$ for $i=1,2, \\cdots .$ Then repeatedly applying the multiplication rule, the joint PMF/PDF of X $^{n}$ $$ f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}\\right)=\\prod_{i=1}^{n} f_{X_{i}\\left|\\mathbf{X}^{i-1}\\left(x_{i} | \\mathbf{X}^{i-1}\\right)\\right.} $$ where $f_{X_{i}\\left|\\mathbf{X}^{i-1}\\left(X_{i} | \\mathbf{X}^{i-1}\\right)\\right.}$ is the conditional PMF/PDF of $X_{i}$ given $\\mathbf{X}^{i-1}=\\mathbf{x}^{i-1} .$ By convention, $f_{X_{1} | \\mathrm{X}^{0}}\\left(x_{1} | x^{0}\\right)=f_{X_{1}}\\left(x_{1}\\right)$ is the unconditional PMF/PDF of $X_{1}$. When $\\mathrm{X}^{n}$ is an IID random sample from a population $\\mathrm{PMF} / \\mathrm{PDF} f_{X}(\\cdot)$, the joint $\\mathrm{PMF} / \\mathrm{PDF}$ of the random sample $\\mathrm{X}^{n}$ is given by $$ f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}\\right)=\\prod_{i=1}^{n} f_{X}\\left(x_{i}\\right) $$ This joint probability distribution is called the sampling distribution of the random sample $\\mathrm{X}^{n}$. It completely describes the probability law of the random sample $\\mathrm{X}^{n}$. For an IID random sample $X^{n}$, each $X_{i}$ has the same PMF/PDF $f_{X}(x)$, the so-called population, $f_{X}(x)$ is assumed to be a parametric model in the sense that $f_{X}(x)=f(x, \\theta)$ for some value of a finite-dimensional parameter $\\theta,$ where the functional form of $f(\\cdot, \\cdot)$ is known but $\\theta$ is unknown. For example, if $f_{X}(x)$ is assumed to follow a normal $N\\left(\\mu, \\sigma^{2}\\right)$ distribution, we have $$ \\begin{aligned} f_{X}(x) \u0026=f(x, \\theta) \\ \u0026=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-\\frac{1}{2 \\vec{z}^{2}}(x-\\mu)^{2}} \\end{aligned} $$ where $\\theta=\\left(\\mu, \\sigma^{2}\\right)$ One important objective of statistical analysis is to estimate the unknown parameter $\\theta$ when we are given a data set $\\mathrm{x}^{n}$, which is a realization of the random sample $\\mathrm{X}^{n}$. An estimator for $\\theta$ is a function of $\\mathrm{X}^{n}$, and so it is a statistic. Recall that a statistic $Z_{n}=T\\left(\\mathrm{X}^{n}\\right)$ is a function of $\\mathrm{X}^{n}$ only and does not involve any unknown parameters. It is a random variable or vector. To motivate the importance of various convergence concepts, we consider two simple statistic the sample mean and sample variance. Let $\\mathrm{X}^{n}$ be an IID random sample of size $n,$ from a population distribution with mean $\\mu$ and variance $\\sigma^{2}$. Suppose the mean $\\mu$ is unknown, so we need to use the sample information $\\mathbf{X}^{n}$ to estimate it. It is expected that the sample mean estimator $$ \\bar{X}{n}=T\\left(\\mathbf{X}^{n}\\right)=n^{-1} \\sum{i=1}^{n} X_{i} $$ can be used to estimate $\\mu$. Similarly, suppose $\\sigma^{2}$ is unknown. We can use the sample variance estimator $$ S_{n}^{2}=(n-1)^{-1} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2} $$ to estimate $\\sigma^{2}$. If the sample size $n$ is sufficiently large, then it is expected that $\\bar{X}{n}$ will be “close” to $\\mu$ and $S{n}^{2}$ will be “close” to $\\sigma^{2}$. The larger the sample size $n$ is, the closer $\\bar{X}{n}$ is to $\\mu$ and the closer $S{n}^{2}$ is to $\\sigma^{2}$. Question: How can one measure the closeness of $\\bar{X}{n}","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:2:0","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.3 Convergence in Quadratic Mean and $L_{p}$-Convergence 【Convergence in Quadratic Mean】Let $\\left{Z_{n}, n=1,2, \\cdots\\right}$ be a sequence of random variables and $Z$ be a random variable. Then the sequence ${Z_{n}}$ converges in quadratic mean (or converges in mean square) to $Z$ if $$ E\\left(Z_{n}-Z\\right)^{2} \\rightarrow 0 \\text { as } n \\rightarrow \\infty $$ or equivalently $$ \\lim {n \\rightarrow \\infty} E\\left(Z{n}-Z\\right)^{2}=0 $$ denoted by $z_{n} \\stackrel{q m}{\\longrightarrow} z$ or $z_{n}-z=o_{q m}(1)$. Remarks: The convergence in quadratic mean means that the weighted average of the squared deviations between $Z_{n}(s)$ and $Z(s)$ vanishes to 0 as $n \\rightarrow \\infty,$ where the average is taken over all possible basic outcomes ${s}$ weighted by their probabilities of of curing. When $Z_{n}$ converge to $Z$ in quadratic mean, it’s possible that there exist some sample paths for which $Z_{n}(s)$ does not converge to $Z(s)$. However, the quadratic deviations of these sample paths all together weighted by the probabilities of their occurings become negligible as $n$ becomes large. 【Example】Suppose $X^{n}=\\left(X_{1}, \\cdots, \\cdots, X_{n}\\right)$ is an IID random sample from a population with mean $\\mu$ and variance $\\sigma^{2}$. Define $Z_{n}=\\bar{X}_{n}$. Show $$ \\bar{x}_{n}\\stackrel{q m}{\\longrightarrow} \\mu $$ Solution: It suffices to show $\\lim {n-\\infty} E\\left(\\bar{X}{n}-\\mu\\right)^{2}=0 .$ Note $E\\left(\\bar{X}_{n}\\right)=\\mu,$ we have $$ \\begin{aligned} E\\left(\\bar{X}{n}-\\mu\\right)^{2} \u0026=\\operatorname{var}\\left(\\bar{X}{n}\\right) \\ \u0026=\\operatorname{var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right) \\ \u0026=\\frac{1}{n^{2}} \\operatorname{var}\\left(\\sum_{i=1}^{n} X_{i}\\right) \\ \u0026=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\operatorname{var}\\left(X_{i}\\right) \\ \u0026=\\frac{\\sigma^{2}}{n} \\ \u0026 \\rightarrow 0 \\text { as } n \\rightarrow \\infty \\end{aligned} $$ 【$L_{p}-\\text{convergence}$】Let $0\u003cp\u003c\\infty,$ and let $\\left{Z_{n}, n=1,2, \\cdots\\right}$ be a sequence of random variables with $E\\left|Z_{n}\\right|^{p}\u003c\\infty,$ and let $Z$ be a random variable with $E|Z|^{p}\u003c\\infty .$ Then $Z_{n}$ converges in $L_{p}$ to $Z$ if $$ \\lim {n \\rightarrow \\infty} E\\left|Z{n}-Z\\right|^{p}=0 $$ Remarks: In connection with $L_{p}$ -convergence, the following inequalities are useful: Holder’s inequality $$ E|X Y| \\leq\\left(E|X|^{p}\\right)^{1 / p}\\left(E|Y|^{q}\\right)^{1 / q} $$ where $p\u003e1$ and $1 / p+1 / q=1$ Minkowski’s inequality $$ E|X+Y|^{p} \\leq\\left[\\left(E|X|^{p}\\right)^{1 / p}+\\left(E|Y|^{p}\\right)^{1 / p}\\right]^{p} $$ for $p \\geq 1$ Question: What happens if $Z_{n}$ and $Z$ are $d \\times 1$ random vectors, where $d$ is fixed (i.e., $d$ does not change as $n \\rightarrow \\infty) ?$ A sequence of random vectors $\\left{Z_{n}\\right}$ converges to $Z$ in $L_{p},$ if each component of the vector $Z_{n}, Z_{i n} \\stackrel{L_{p}}{\\rightarrow} Z_{i}$ for $i=1, \\cdots, d$. In other words, component-wise convergences ensure joint convergence of the entire vector $Z_{n},$ and vice versa ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:3:0","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.4 Convergence in Probability ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:4:0","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.4.1 Definition 【Convergence in Probability / weak Convergence】A sequence of random variables $\\left{Z_{n}, n=\\right.$ $1,2, \\cdots}$ converges in probability to a random variable $Z$ if for every small constant $\\epsilon\u003e0$ $$ P\\left[\\left|Z_{n}-Z\\right|\u003e\\epsilon\\right] \\rightarrow 0 \\text { as } n \\rightarrow \\infty $$ When $Z_{n}$ converges in probability to $Z$, we write $$ \\lim _{n \\rightarrow \\infty} P\\left(\\left|Z_{n}-Z\\right|\u003e\\epsilon\\right)=0 $$ for every $\\epsilon\u003e0,$ or $p \\lim _{n \\rightarrow \\infty} Z_{n}=Z,$ or $Z_{n} \\stackrel{p}{\\rightarrow} Z,$ or $Z_{n}-Z=o_{P}(1),$ or $Z_{n}-Z \\stackrel{p}{\\rightarrow} 0$ Remarks: An alternative definition of convergence in probability: Given any $\\epsilon\u003e0$ and any $\\delta\u003e0,$ there exists a finite integer $N=N(\\epsilon, \\delta)$ such that for all $n\u003eN,$ we have $$ P\\left(\\left|Z_{n}-Z\\right|\u003e\\epsilon\\right)\u003c\\delta $$ Define $A_{n}(\\epsilon)=\\left{s \\in S:\\left|Z_{n}(s)-Z(s)\\right| \\leq \\epsilon\\right}$ as a subset of $S$ that consists of all basic outcomes $s \\in S$ such that the difference $\\left|Z_{n}(s)-Z(s)\\right|$ is small. $Z_{n} \\stackrel{p}{\\rightarrow} Z$ means that the probability of “small deviations” $$ P\\left[\\left|Z_{n}-Z\\right|\\right.\\leq \\epsilon]=P\\left[A_{n}(\\epsilon)\\right] \\rightarrow 1 \\text { as } n \\rightarrow \\infty $$ For this reason, convergence in probability is also called convergence with probability approaching 1. When $Z_{n} \\stackrel{p}{\\rightarrow} b,$ where $b$ is a constant, we say that $Z_{n}$ is **consistent** for $b$. 【Order of Convergence in Probability】Suppose $\\left{f_{n}\u003e0, n=1,2, \\ldots\\right}$ is a sequence of real numbers A sequence of random variables $\\left{Z_{n}, n=1,2, \\ldots\\right}$ is of order smaller than $f_{n}$ in probability, denoted by $Z_{n}=o_{p}\\left(f_{n}\\right)$, if $$ Z_{n} / f_{n} \\stackrel{p}{\\rightarrow} 0 \\quad \\text{as } n \\rightarrow \\infty $$ A sequence of random variables $\\left{Z_{n}, n=1,2, \\ldots\\right}$ is at most of order $f_{n}$ in probability, denoted by $Z_{n}=\\mathcal{O}_{p}\\left(f_{n}\\right)$, if for every $\\varepsilon\u003e0,$ there exists a constant $M(\\varepsilon)\u003c\\infty$ and an integer $N(\\varepsilon)$ such that $$ P\\left(\\left|Z_{n} / f_{n}\\right|\u003eM(\\varepsilon)\\right)\u003c\\varepsilon $$ for all $n \\geqslant N(\\varepsilon)$ Remark: A sequence of random variables $\\left{Z_{n}, n=1,2, \\ldots\\right}$ is **bounded in probability** if $Z_{n}=\\mathcal{O}_{p}(1),$ that is, for any $\\varepsilon\u003e0,$ there exists a constant $M(\\varepsilon)\u003c\\infty$ and an integer $N(\\varepsilon)$ such that $P\\left(\\left|Z_{n}\\right|\u003eM(\\varepsilon)\\right)\u003c\\varepsilon$ for all $n \\geqslant N(\\varepsilon)$ Let $\\left{X_{n}\\right}$ and $\\left{Y_{n}\\right}$ be two sequences of random variables. If $X_{n}=\\mathcal{O}_{p}\\left(n^{\\lambda}\\right)$ and $Y_{n}=\\mathcal{O}_{p}\\left(n^{\\tau}\\right),$ then $$ X_{n} Y_{n}=\\mathcal{O}_{p}\\left(n^{\\lambda+\\tau}\\right), \\quad X_{n}+Y_{n}=\\mathcal{O}_{p}\\left(n^{\\max (\\lambda, \\tau)}\\right) $$ If $X_{n}=o_{p}\\left(n^{\\lambda}\\right)$ and $Y_{n}=o_{p}\\left(n^{\\tau}\\right),$ then $$ X_{n} Y_{n}=o_{p}\\left(n^{\\lambda+\\tau}\\right), \\quad X_{n}+Y_{n}=o_{p}\\left(n^{\\max (\\lambda, \\tau)}\\right) $$ If $X_{n}=\\mathcal{O}_{p}\\left(n^{\\lambda}\\right)$ and $Y_{n}=o_{p}\\left(n^{\\tau}\\right),$ then $$ X_{n} Y_{n}=o_{p}\\left(n^{\\lambda+\\tau}\\right), \\quad X_{n}+Y_{n}=\\left{\\begin{array}{ll} \\mathcal{O}_{p}\\left(n^{\\lambda}\\right) \u0026 \\text { if } \\lambda \\geqslant \\tau \\ o_{p}\\left(n^{\\tau}\\right) \u0026 \\text { if } \\lambda\u003c\\tau \\end{array}\\right. $$ ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:4:1","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.4.2 Markov’s Inequality 【Markov’s Inequality】Suppose $X$ is a random variable and $g(X)$ is a nonnegative function. Then for any $\\epsilon\u003e0,$ and any $k\u003e0,$ we have $$ P[g(X) \\geq \\epsilon] \\leq \\frac{E\\left[g(X)^{k}\\right]}{\\epsilon^{k}} $$ Proof: Let $\\mathbf{1}(\\cdot)$ be the indicator function that takes values 1 and $0,$ depending on whether the statement is true. Then $$ \\begin{aligned} P[g(X)\u003e\\epsilon] \u0026=\\int_{{x : g(x)\u003e\\varepsilon}} d F_{X}(x) \\ \u0026=\\int_{-\\infty}^{\\infty} 1[g(x)\u003e\\epsilon] d F_{X}(x) \\ \u0026 \\leq \\int_{-\\infty}^{\\infty} 1[g(x)\u003e\\epsilon] \\frac{g(x)^{k}}{\\epsilon^{k}} d F_{X}(x) \\ \u0026 \\leq \\int_{-\\infty}^{\\infty} \\frac{g(x)^{k}}{\\epsilon^{k}} d F_{X}(x) \\ \u0026=\\frac{1}{\\epsilon^{k}} E\\left[g(X)^{k}\\right] \\end{aligned} $$ Remarks: Markov’s inequality bounds the tail probability by a moment condition. The thickness of the tail probability depends on the magnitude of moments of the distribution. When $k = 2$ and $g(X) = |X-\\mu|$; Markov’s inequality is called Chebyshev’s inequality, i.e. $$ P[|X-\\mu|\\geq \\epsilon] \\leq \\frac{E\\left[(X-\\mu)^{2}\\right]}{\\epsilon^{k}} $$ ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:4:2","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.4.3 WLLN 【Weak Law of Large Numbers(WLLN)】Let $\\mathrm{X}^{n}=\\left(X_{1}, \\cdots, X_{n}\\right)$ be an IID random sample with $E\\left(X_{i}\\right)=\\mu$ and var $\\left(X_{i}\\right)=\\sigma^{2}\u003c\\infty .$ Define $\\bar{X}_{n}=n^{-1} \\sum_{i=1}^{n} X_{i} .$ Then $$ \\bar{X}_{n} \\stackrel{p}{\\rightarrow} \\mu $$ **Proof:** First, noting $E\\left(\\bar{X}{n}\\right)=\\mu$ and $\\operatorname{var}\\left(\\bar{X}{n}\\right)=\\sigma^{2} / n,$ we have by Chebychev’s inequality $$ \\begin{aligned} P\\left[\\left|\\bar{X}{n}-\\mu\\right|\\right.\u0026\u003e\\epsilon] \\ \u0026 \\leq \\frac{E\\left(\\bar{X}{n}-\\mu\\right)^{2}}{\\epsilon^{2}} \\ \u0026=\\frac{\\operatorname{var}\\left(\\bar{X}_{n}\\right)}{\\epsilon^{2}} \\ \u0026=\\frac{\\sigma^{2}}{n \\epsilon^{2}} \\end{aligned} $$ It follows that $$ \\begin{aligned} P\\left[\\left|\\bar{X}{n}-\\mu\\right|\\right.\u0026\\leq \\epsilon] \\ \u0026=1-P\\left[\\left|\\bar{X}{n}-\\mu\\right|\u003e\\epsilon\\right] \\ \u0026 \\geq 1-\\frac{\\sigma^{2}}{n \\epsilon^{2}} \\ \u0026 \\rightarrow 1 \\end{aligned} $$ as $n \\rightarrow \\infty .$ Therefore, $\\bar{X}_{n} \\stackrel{p}{\\rightarrow} \\mu$ Remarks: In the WLLN theorem, we have assumed a finite variance. Although such an assumption is true and desirable in most applications, it is, in fact, a stronger assumption than is needed. The only moment condition needed is that $E\\left|X_{i}\\right|\u003c\\infty$ (see Resnik 1999, Chapter 7 , or Billingsley $1995,$ Section 22 ) To appreciate why a moment condition is needed for the WLLN, one can consider the example of an IID random sample from the Cauchy distribution whose moments do not exist. ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:4:3","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.4.4 Relationships Between Convergence 【Lemma 2】Suppose $Z_{n} \\rightarrow Z$ in $L_{p}$ as $n \\rightarrow \\infty .$ Then $Z_{n} \\stackrel{p}{\\rightarrow} Z$ as $n \\rightarrow \\infty$. Proof: By Markov’s inequality, we have for all $\\epsilon\u003e0$ $$ P\\left[\\left|Z_{n}-Z\\right|\\right.\u003e\\epsilon] \\leq \\frac{E\\left|Z_{n}-Z\\right|^{p}}{\\epsilon^{p}} \\rightarrow 0 $$ if $\\lim {n-\\infty} E\\left|Z{n}-Z\\right|^{p}=0$. 【Example】Suppose $\\mathbf{X}^{n}=\\left(X_{1}, \\cdots, X_{n}\\right)$ is and IID $N \\left(\\mu, \\sigma^{2}\\right)$ random sample. Show $$ S_{n}^{2} \\stackrel{p}{\\longrightarrow} \\sigma^{2} $$ **Solution:** We have shown that under a normal random sample $\\mathrm{X}^{n}$ $$ \\frac{(n-1) S_{n}^{2}}{\\sigma^{2}} \\sim \\chi_{n-1}^{2} $$ for all $n\u003e1$ It follows that $E\\left(S_{n}^{2}\\right)=\\sigma^{2}$ and $\\operatorname{var}\\left(S_{n}^{2}\\right)=2 \\sigma^{4} /(n-1) .$ Hence, we have $$ \\begin{aligned} E\\left(S_{n}^{2}-\\sigma^{2}\\right)^{2} \u0026=\\operatorname{var}\\left(S_{n}^{2}\\right) \\ \u0026=\\frac{2 \\sigma^{4}}{n-1} \\ \u0026 \\rightarrow 0 \\text { as } n \\rightarrow \\infty \\end{aligned} $$ Thus $S^2_n \\rightarrow \\sigma^2$ in $L_{p}$. It follows that $S_{n}^{2} \\stackrel{p}{\\rightarrow} \\sigma^{2}$ as $n \\rightarrow \\infty$ But $Z_{n} \\stackrel{p}{\\rightarrow} Z$ does not imply $Z_{n} \\rightarrow Z$ in $L_{p}$. 【Example】Suppose a sequence of binary random variables $\\left{Z_{n}\\right}$ is defined as $$ \\begin{array}{c|cc} Z_{n} \u0026 \\frac{1}{n} \u0026 n \\ \\hline f_{Z_{n}}\\left(z_{n}\\right) \u0026 1-\\frac{1}{n} \u0026 \\frac{1}{n} \\end{array} $$ (1) Does $Z_{n}$ converge in quadratic mean to 0? Give your reasoning clearly (2) Does $Z_{n}$ converge in probability to o? Give your reasoning clearly. Solution: (1) $Z_{n}$ does not converge in quadratic mean to 0 because $$ \\begin{aligned} E\\left(Z_{n}-0\\right)^{2} \u0026=\\sum\\left(z_{n}-0\\right)^{2} f_{Z_{n}}\\left(z_{n}\\right) \\ \u0026=n^{-2}\\left(1-n^{-1}\\right)+n^{2}\\left(n^{-1}\\right)\\ \u0026\u003en \\to \\infty \\end{aligned} $$ (2) Given any $\\epsilon\u003e0,$ for all $n\u003eN(\\epsilon)=\\left[\\epsilon^{-1}\\right]+1\\left(\\text { so } n^{-1}\u003c\\epsilon\\right)$ $$ \\begin{aligned} P\\left(\\left|Z_{n}-0\\right|\\right.\\leq \\epsilon) \u0026=P\\left(Z_{n}=n^{-1}\\right) \\ \u0026=1-n^{-1} \\ \u0026 \\rightarrow 1 \\text { as } n \\rightarrow \\infty \\end{aligned} $$ Therefore, $Z_{n}$ converges to 0 in probability Remarks: The square of $Z_{n}$ grows to infinity at a rate of $n^{2}$, which is faster than the rate $n^{-1}$ of the vanishing probability $P\\left(Z_{n}=n\\right) .$ As a result, the second moment does not exist. More generally, convergence in probability implies that there may exist a set of outcomes in sample space with “large deviations” and the probability of this set shrinks to zero as $n \\rightarrow \\infty$ but it is nonzero for a finite $n$. Some large deviations may be even explosive at a rate faster than their shrinking probabilities. As a result, the $L_{p}$ -convergence may not exist. ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:4:4","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.4.5 Continuous Mapping Theorem 【Continuous Mapping Theorem】Suppose $g$ (.) is a continuous function, and $Z_{n}\\stackrel{p}{\\rightarrow} Z$. Then $g\\left(Z_{n}\\right)\\stackrel{p}{\\rightarrow}g(Z)$. Or equivalently $p \\lim g\\left(Z_{n}\\right)=g\\left(p \\lim Z_{n}\\right)$. Proof: By continuity of function $g(\\cdot):$ given any $\\epsilon\u003e0,$ there exists a constant $\\delta=\\delta(\\epsilon),$ such that whenever $\\left|Z_{n}-Z\\right| \\leq \\delta$, we have $$ \\left|g\\left(Z_{n}\\right)-g(Z)\\right|\u003c\\epsilon $$ Now, define two events $$ \\begin{array}{l} A_{n}(\\delta) \\equiv\\left{s \\in S:\\left|Z_{n}(s)-Z(s)\\right| \\leq \\delta\\right} \\ B_{n}(\\epsilon) \\equiv\\left{s \\in S:\\left|g\\left[Z_{n}(s)\\right]-g[Z(s)]\\right| \\leq \\epsilon\\right} \\end{array} $$ Then continuity of $g(\\cdot)$ implies $A_{n}(\\delta) \\subseteq B_{n}(\\epsilon),$ that is, $A_{n}(\\delta)$ is a subset of $B_{n}(\\epsilon)$ It follows that $P\\left[A_{n}(\\delta)\\right] \\leq P\\left[B_{n}(\\epsilon)\\right],$ and so $$ P\\left[B_{n}(\\epsilon)^{c}\\right] \\leq P\\left[A_{n}(\\delta)^{c}\\right] \\rightarrow 0 \\text { as } n \\rightarrow \\infty $$ where $B_{n}(\\epsilon)^{c}$ and $A_{n}(\\delta)^{c}$ are the complements of $B_{n}(\\epsilon)$ and $A_{n}(\\delta)$ respectively. Because $\\epsilon$ is arbitrary, so is $\\delta .$ It follows that $g\\left(Z_{n}\\right) \\stackrel{p}{\\rightarrow} g(Z)$ as $n \\rightarrow \\infty$. Remarks: The $p \\text{ lim}$ operator passes through nonlinear functions, provided they are continuous. This is analogous to the well-known result in calculus that the limit of a continuous function is equal to the function of the limit. The expectation operator $E(\\cdot),$ which is used in the $L_{p}$-convergence, does not have this feature, and this makes finite sample analysis difficult for many statistics. ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:4:5","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.4.6 Continuity 【Lemma 7.3】Continuity: Suppose $ a_{n} \\stackrel{p}{\\rightarrow} $ a and $ b_{n} \\stackrel{p}{\\rightarrow} b, $ and $ g(\\cdot) $ and $ h(\\cdot) $ are continuous functions. Then $$ \\begin{array}{l}g\\left(a_{n}\\right)+h\\left(b_{n}\\right) \\stackrel{p}{\\rightarrow} g(a)+h(b), \\text { and } \\ g\\left(a_{n}\\right) h\\left(b_{n}\\right) \\stackrel{p}{\\rightarrow} g(a) h(b)\\end{array} $$ ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:4:6","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.5 Almost Sure Convergence ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:5:0","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.5.1 Definition 【Almost Sure Convergence / strong convergence】A sequence of random variables $\\left{Z_{n}\\right}$ converges almost surely to a random variable $Z$ if for every given constant $\\epsilon\u003e0$ $$ P\\left[\\lim _{n \\rightarrow \\infty}\\left|Z_{n}-Z\\right|\u003e\\epsilon\\right]=0 $$ or $$ P\\left[s \\in S: \\lim {n \\rightarrow \\infty}\\left|Z{n}(s)-Z(s)\\right| \\leq \\epsilon\\right]=1 $$ or equivalently $$ P\\left[s \\in S: \\lim {n \\rightarrow \\infty}Z{n}(s)=Z(s)\\right]=1 $$ where $S$ is the sample space. When $Z_{n}$ converges almost surely to $Z$, we write $Z_{n} \\stackrel{\\text { a.s. }}{\\rightarrow} Z$ or $Z_{n}-Z=o_{a . s .}(1),$ or $Z_{n}-Z \\stackrel{\\text { a.s. }}{\\rightarrow} 0$ \r Remarks: Almost sure convergence can also be expressed as $$ P\\left(\\lim {n \\rightarrow \\infty}\\left|Z{n}-Z\\right|=0\\right)=1 $$ For this reason, almost sure convergence is also called convergence with probability 1 When $Z=b,$ a constant, we say that $Z_{n}$ is **strongly consistent** for $b$ if $Z_{n}$ converges to $b$ with probability one. Define a set $$ \\begin{aligned} A(\\epsilon) \u0026=\\left{s \\in S: \\lim {n \\rightarrow \\infty}\\left|Z{n}(s)-Z(s)\\right| \\leq \\epsilon\\right} \\ \u0026=\\left{s \\in S:\\left|Z_{n}(s)-Z(s)\\right| \\leq \\epsilon \\text { for all } n\u003eN(\\epsilon, s)\\right} \\end{aligned} $$ i.e. $A(\\epsilon)$ is a subset of $S$ that consists of all basic outcomes $s \\in S$ such that $\\lim {n \\rightarrow \\infty}\\left|Z{n}(s)-Z(s)\\right|\u003c\\epsilon .$ Intuitively $A(\\epsilon)$ is a convergence set in $S$ in the sense that for each $s \\in A(\\epsilon),$ the sample path $\\left{Z_{n}(s)\\right}$ converges to $Z(s)$ as $n \\rightarrow \\infty$. Then $$ \\begin{aligned} P\\left(\\lim _{n \\rightarrow \\infty}\\left|Z_{n}-Z\\right| \\leq \\epsilon\\right) \u0026=P[A(\\epsilon)] \\ \u0026=1 \\end{aligned} $$ 【Example】Suppose the probability space is $(S, \\mathcal{B}, P),$ where the sample space $S=[0,1], \\mathcal{B}$ contains all the Borel sets $B$ such that $B \\subset S, P$ is the Lebesgue measure on $S$, i.e. $P({s \\in[a, b]})=b-a$ for any $0 \\leqslant a \\leqslant b \\leqslant 1 .$ Then $Z_{n}(s)=s+s^{n}, n=1,2, \\ldots$ and $Z(s)=s$ are random variables. For $0 \\leqslant s\u003c1, Z_{n}(s) \\rightarrow Z(s)$ as $n \\rightarrow \\infty$ For $s=1, \\lim {n \\rightarrow \\infty} Z{n}(s)=2 \\neq Z(s)$ $P\\left(\\left{s \\in S: Z_{n}(s) \\rightarrow Z(s)\\right}\\right)=P([0,1))=1,$ so $Z_{n} \\stackrel{a . s,}{\\longrightarrow} Z$ ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:5:1","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.5.2 Order of Almost sure convergence 【Almost sure convergence with order $n^{\\alpha}$】, where $\\alpha$ can be positive or negative: The sequence of random variables $\\left{Z_{n}\\right}$ is said to be of order smaller than $n^{\\alpha}$ with probability one if $$ Z_{n} / n^{\\alpha} \\stackrel{a. s.}{\\longrightarrow} 0 \\text { as } n \\rightarrow \\infty $$ This is denoted as $Z_{n}=o_{a, s}\\left(n^{\\alpha}\\right)$ The sequence of random variables $\\left{Z_{n}\\right}$ is said to be at most of order $n^{\\alpha}$ with probability one if there exists some constant $M\u003c\\infty$ such that $$ P\\left(\\lim _{n \\rightarrow \\infty}\\left|Z_{n} / n^{\\alpha}\\right|\u003eM\\right)=0 $$ This is denoted as $Z_{n}=O_{a, s}\\left(n^{\\alpha}\\right)$ Remarks: $Z_{n}=O_{a, s}(1)$ implies that with probability one, $Z_{n}$ is bounded by some large constant for all $n$ sufficiently large. ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:5:2","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.5.3 Relationships Between Convergence 【Lemma 7.4】Suppose $ Z_n \\stackrel{a. s.}{\\longrightarrow} Z $, then $Z_n \\stackrel{p.}{\\rightarrow} Z$. $Z_{n} \\stackrel{\\text {a.s.}}{\\longrightarrow} Z$ means for any $\\varepsilon\u003e0$ $$ \\begin{array}{l}P\\left(\\left{s \\in S: \\lim _{n \\rightarrow \\infty} Z_{n}(s)=Z(s)\\right}\\right) \\\\quad=P\\left(\\left{s \\in S:\\left|Z_{n}(s)-Z(s)\\right| \\leqslant \\varepsilon \\text { for all } n\u003eN(\\varepsilon, s)\\right}\\right)=1\\end{array} $$ $Z_{n} \\stackrel{p}{\\rightarrow} Z$ means for any $\\varepsilon\u003e0$ $$ P\\left(\\left{s \\in S:\\left|Z_{n}(s)-Z(s)\\right| \\leqslant \\varepsilon\\right}\\right) \\rightarrow 1 $$ We first compare the difference in notations between almost sure convergence $$ P\\left[\\lim {n \\rightarrow \\infty}\\left|Z{n}-Z\\right|\u003e\\epsilon\\right]=0 $$ and convergence in probability $$ \\lim {n \\rightarrow \\infty} P\\left[\\left|Z{n}-Z\\right|\u003e\\epsilon\\right]=0 $$ The fact that the set $A_{n}(\\epsilon)$ of “large” differences for $\\left|Z_{n}-Z\\right|$ may have a nonzero probability for any finite $n$ implies that convergence in probability is weaker than almost sure convergence. If for some $s \\in S, Z_{n}(s) \\rightarrow Z(s)$ as $n \\rightarrow \\infty,$ then the difference $\\left|Z_{n}(s)-Z(s)\\right|$ will eventually become “small” (i.e., smaller than $\\epsilon$ ) for all $n$ sufficiently large. Hence, almost sure convergence implies convergence in probability. While almost sure convergence implies convergence in probability, the converse may not be true. $$ Z_{n} \\stackrel{p}{\\rightarrow} Z \\nRightarrow Z_{n} \\stackrel{a . s .}{\\longrightarrow} Z $$ 【Example】Suppose the probability space is $(S, \\mathcal{B}, P),$ where the sample space $S=[0,1], \\mathcal{B}$ contains all the Borel sets $B$ such that $B \\subset S, P$ is the Lebesgue measure on $S$. Let $Z(s)=0$ and $Z_{1}(s)=1(0\u003cs \\leqslant 1)$ where $1(\\cdot)$ is the indicator function $Z_{2}(s)=1\\left(0\u003cs \\leqslant \\frac{1}{2}\\right), Z_{3}(s)=1\\left(\\frac{1}{2}\u003cs \\leqslant 1\\right)$ $Z_{n}(s)=1\\left(\\frac{n-4}{4}\u003cs \\leqslant \\frac{n-3}{4}\\right), n=4, \\ldots, 7$ $Z_{n}(s)=1\\left(\\frac{n-8}{8}\u003cs \\leqslant \\frac{n-7}{8}\\right), n=8, \\ldots, 15, \\cdots$ For every $\\varepsilon\u003e0, P\\left(\\left|Z_{n}-Z\\right|\u003e\\varepsilon\\right) \\rightarrow 0$ as $n \\rightarrow \\infty,$ so $Z_{n} \\stackrel{p}{\\rightarrow} Z$ $E\\left|Z_{n}-Z\\right|^{p}=1 / 2^{k} \\rightarrow 0$ as $n \\rightarrow \\infty,$ where $k=\\left[\\log _{2} n\\right],$ so $Z_{n} \\rightarrow Z$ in $L_{p}$ For any $s\u003e0, \\lim {n \\rightarrow \\infty} Z{n}$ does not exist, so $Z_{n}$ does not converge to $Z$ almost surely. Almost sure convergence does not imply $L_p$-convergence and $L_p$-convergence does not imply almost sure convergence. $$ in $$ in $L_{p}$ $$ Z_{n} \\rightarrow Z \\text{ in }L_{p} \\nRightarrow Z_{n} \\stackrel{\\text { a.s. }}{\\longrightarrow} Z, \\quad Z_{n} \\stackrel{\\text { a.s. }}{\\longrightarrow} Z \\neq Z_{n} \\rightarrow Z \\text{ in }L_{p} $$ 【Example】Suppose the probability space is $(S, \\mathcal{B}, P),$ where the sample space $S=[0,1], \\mathcal{B}$ contains all the Borel sets $B$ such that $B \\subset S, P$ is the Lebesgue measure on $S$. Let $Z(s)=0$ and $$ Z_{n}(s)=\\left{\\begin{array}{ll} 0 \u0026 \\text { if } \\frac{1}{n}\u003cs \\leqslant 1 \\ e^{n} \u0026 \\text { if } 0 \\leqslant s \\leqslant \\frac{1}{n} \\end{array}\\right. $$ For every $\\varepsilon\u003e0, P\\left(\\left|Z_{n}(s)-Z(s)\\right|\u003e\\varepsilon\\right) \\leqslant 1 / n$ as $n \\rightarrow \\infty,$ so $Z_{n} \\stackrel{p}{\\rightarrow} Z$ For any $0\u003cs \\leq 1, \\lim {n \\rightarrow \\infty} Z{n}(s)=0,$ so $P\\left(\\left{s \\in S: Z_{n}(s) \\rightarrow Z(s)\\right}\\right)=P((0,1])=1$ and $Z_{n} \\stackrel{\\text { a.s. }}{\\longrightarrow} Z$ $E\\left|Z_{n}-Z\\right|^{p}=\\frac{1}{n} e^{n p} \\rightarrow \\infty,$ so $Z_{n}$ does not converge to $Z$ in $L_{p}$ ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:5:3","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.5.4 Continuous Mapping Theorem 【Continuous Mapping Theorem】Suppose $g(\\cdot)$ is a continuous function, and $Z_{n}\\stackrel{a.s.}{\\rightarrow} Z$. Then $g\\left(Z_{n}\\right)\\stackrel{a.s.}{\\rightarrow}g(Z)$ as $n \\to \\infty$ Proof: The proof is similar to the proof of convergence in probability of a continuous function. Let $s \\in S$ be a basic outcome. since $Z_{n}(s) \\rightarrow Z(s)$ as $n \\rightarrow \\infty$ implies $g\\left[Z_{n}(s)\\right] \\rightarrow$ $g[Z(s)]$ as $n \\rightarrow \\infty$ by the continuity of $g(\\cdot),$ we have $$ \\left{s \\in S: Z_{n}(s) \\rightarrow Z(s)\\right} \\subseteq\\left{s \\in S: g\\left[Z_{n}(s)\\right] \\rightarrow g[Z(s)]\\right} $$ Hence $$ P\\left[s \\in S: g\\left[Z_{n}(s)\\right] \\rightarrow g[Z(s)]\\right] \\geq P\\left[s \\in S: Z_{n}(s) \\rightarrow Z(s)\\right] \\rightarrow 1 $$ It follows that $g\\left(Z_{n}\\right) \\stackrel{\\text {a.s.}}{\\rightarrow} g(Z)$ ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:5:4","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.5.5 Continuity 【Lemma 7.3】Continuity: Suppose $ a_{n} \\stackrel{a.s.}{\\rightarrow} $ a and $ b_{n} \\stackrel{a.s.}{\\rightarrow} b, $ and $ g(\\cdot) $ and $ h(\\cdot) $ are continuous functions. Then $$ \\begin{array}{l}g\\left(a_{n}\\right)+h\\left(b_{n}\\right) \\stackrel{a.s.}{\\rightarrow} g(a)+h(b), \\text { and } \\ g\\left(a_{n}\\right) h\\left(b_{n}\\right) \\stackrel{a.s.}{\\rightarrow} g(a) h(b)\\end{array} $$ ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:5:5","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.5.6 SLLN \u0026 USLLN 【Kolmogorov’s Strong Law of Large Numbers(SLLN)】Suppose $\\mathrm{X}^{n}=$ $\\left(X_{1}, \\cdots, X_{n}\\right)$ be an IID random sample with $E\\left(X_{i}\\right)=\\mu$ and $E\\left|X_{i}\\right|\u003c\\infty .$ Define $\\bar{X}_{n}=n^{-1} \\sum_{i=1}^{n} X_{i}$, then $$ \\bar{X}_{n} \\stackrel{\\text {a.s.}}{\\longrightarrow} \\mu \\text { as } n \\rightarrow \\infty $$ Proof: See (e.g.) Gallant (1997, pp.132-135). 【Uniform Strong Law of Large Numbers (USLLN)】Suppose $ \\mathrm{X}^{n}=\\left(X_{1}, \\cdots, X_{n}\\right)$ is an IID random sample function $g(x, \\theta)$ is continuous over $\\Omega \\times \\Theta$ where $\\Omega$ is the support of $X_{i}$ and $\\Theta$ is a compact set in $\\mathbb{R}^{d}$ with $d$ finite and fixed; $ E\\left[\\text { sup }{\\theta \\in \\Theta} \\left| g\\left(X{i}, \\theta\\right)\\right|\\right]\u003c\\infty$, where the expectation $E(\\cdot)$ is taken over the population distribution of $X_{i}$. Then as $n \\to \\infty$, $$ \\sup_{\\theta \\in \\Theta}\\left| n^{-1} \\sum_{i=1}^{n} g\\left(X_{i}, \\theta\\right)-E\\left[g\\left(X_{i}, \\theta\\right)\\right] \\right|\\rightarrow 0 \\text { almost surely.} $$ Moreover, $E\\left[g\\left(X_{i}, \\theta\\right)\\right]$ is a continuous function of $\\theta$ over $\\Theta$. Remarks: Different from the SLLN, $g\\left(X_{i}, \\theta\\right)$ depends on both random variable $X_{i}$ and parameter $\\theta$. USLLN says that, for each $n$, the worst deviation of the sample average $n^{-1} \\sum_{i=1}^{n} g\\left(X_{i}, \\theta\\right)$ from the population mean $E\\left[g\\left(X_{i}, \\theta\\right)\\right]$ that one could find over all possible values in $\\Theta$ converges to zero almost surely. The uniform convergence is with respect to the parameter space $\\Theta$. USLLN is rather useful when investigating the asymptotic behavior of nonlinear metric estimators ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:5:6","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.6 Convergence in Distribution ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:6:0","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.6.1 Definition 【Convergence in Distribution】Let $\\left{Z_{n}\\right}$ be a sequence of random variables with a sequence of corresponding CDF$\\left{F_{n}(z)\\right},$ and let $Z$ be a random variable with $\\operatorname{CDF} F(z) .$ Then $Z_{n}$ converges in distribution to $Z$ as $n \\rightarrow \\infty$ if the CDF $ F_{n}(z)$ converges to $F(z)$ at every continuity point $t \\in(-\\infty, \\infty),$ namely $$ \\lim _{n \\rightarrow \\infty} F_{n}(z)=F(z) $$ at every point $z$ where $F_{n}(z)$ is continuous. Here, $F(z)$ is called a **limiting or asymptotic distribution** of the sequence of random variables $\\left{Z_{n}\\right}$. Convergence in distribution is denoted as $Z_{n} \\stackrel{d}{\\longrightarrow} Z$ as $n \\rightarrow \\infty$. Remarks: Although we refer to a sequence of random variables, $\\left{Z_{n}\\right}$, converging in distribution to a random variable $Z$, it is actually the sequence of CDF’s $\\left{F_{n}(\\cdot), n=1,2, \\ldots\\right}$ that converges to the CDF $F(\\cdot)$. In other words, convergence in distribution means that their CDF’s converge, not the random variables $\\left{Z_{n}\\right}$ themselves. This is different from the concepts of convergence in $L_{p},$ convergence in probability and almost sure convergence. The latter all characterize the convergence or closeness of the random variables $\\left{Z_{n}\\right}$ to random variable $Z$. The distribution $F(z)$ is called the limiting distribution or asymptotic distribution of $Z_{n} .$ Suppose $F(\\cdot)$ has mean $\\mu$ and variance $\\sigma^{2} .$ Then they will be called, respectively, the asymptotic mean and asymptotic variance of the distribution $F_{n}(\\cdot) .$ since $F(\\cdot)$ is not the limit of $F_{n}(\\cdot),$ the asymptotic mean and variance may not be the limits of the mean and variances of the distribution $F_{n}(\\cdot)$ respectively, even if the latter exist. It should be emphasized that the limiting distribution $F(\\cdot)$ might not be obtained by taking the limit of $F_{n}(\\cdot) .$ For example, suppose $Z_{n} \\sim N\\left(0, \\frac{1}{n}\\right) .$ Then it has the distribution function $$ \\begin{aligned} F_{n}(z) \u0026=\\int_{-\\infty}^{z} \\frac{1}{\\sqrt{1 / n} \\sqrt{2 \\pi}} e^{-n u^{2} / 2} d u \\ \u0026=\\int_{-\\infty}^{\\sqrt{n} z} \\frac{1}{\\sqrt{2 \\pi}} e^{-v^{2} / 2} d v \\ \u0026=\\Phi(\\sqrt{n} z) \\end{aligned} $$ where $\\Phi(\\cdot)$ is the $N(0,1)$ CDF. Obviously, we have $$ \\lim {n \\rightarrow \\infty} F{n}(z)=\\left{\\begin{array}{ll} 0 \u0026 \\text { if } z\u003c0 \\ \\frac{1}{2} \u0026 \\text { if } z=0 \\ 1 \u0026 \\text { if } z\u003e0 \\end{array}\\right. $$ Now define the function $$ F(z)=\\left{\\begin{array}{ll} 0 \u0026 \\text { if } z\u003c0 \\ 1 \u0026 \\text { if } z \\geq 0 \\end{array}\\right. $$ Then $F(z)$ is a CDF and $\\lim {n \\rightarrow \\infty} F{n}(z)=F(z)$ at every continuity point of $F(z)$. (The function $F(z)$ is not continuous at point $z=0 .$ ) Therefore, $F(\\cdot)$ is a limiting distribution of $Z_{n} .$ However $F(\\cdot)$ cannot be obtained by taking the limit of $F_{n}(\\cdot),$ because $\\lim _{n \\rightarrow \\infty} F_{n}(0) \\neq F(0)$ at zero. Note that in this example $\\lim _{n \\rightarrow \\infty} F_{n}(z)$ is not a CDF because it is not right-continuous. 【Example】Suppose $\\mathrm{X}^{n}=\\left(X_{1}, \\cdots, X_{n}\\right)$ is an IID $U[0, \\theta]$ random sample, where $\\theta$ is an unknown parameter. Let $Z_{n}=\\max _{1 \\leq i \\leq n}\\left(X_{i}\\right)$ be an estimator of $\\theta$. Derive the limiting distribution of $n\\left(\\theta-Z_{n}\\right)$ solution: For any given $u \\geq 0,$ we have $$ \\begin{aligned} P\\left[n\\left(\\theta-Z_{n}\\right)\u003eu\\right] \u0026=P\\left(Z_{n}\u003c\\theta-\\frac{u}{n}\\right) \\ \u0026=P\\left(X_{1}\u003c\\theta-\\frac{u}{n}, \\cdots, X_{n}\u003c\\theta-\\frac{u}{n}\\right) \\ \u0026=\\prod_{i=1}^{n} P\\left(X_{i}\u003c\\theta-\\frac{u}{n}\\right) \\ \u0026=\\left(1-\\frac{u}{n \\theta}\\right)^{n} \\ \u0026 \\rightarrow e^{-u / \\theta} \\end{aligned} $$ as $n \\rightarrow \\infty,$ where we have used the formula $\\left(1-\\frac{a}{n}\\right)^{n} \\rightarrow e^{-a}$ as $n \\rightarrow \\infty .$ It follows that, for $u \\geq 0$ $$ \\begin{ali","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:6:1","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.6.2 Relationships Between Convergence 【Lemma 7.5】Let $Z_{n}$ be a random variable with CDF $ F_{n}(\\cdot),$ and let $Z$ be a random variable with a continuous $\\operatorname{CDF} F(\\cdot) .$ If $Z_{n} \\stackrel{d}{\\rightarrow} Z$ as $n \\rightarrow \\infty,$ then $Z_{n}=O_{P}(1)$ Proof: For any given constant $\\epsilon\u003e0,$ let $M=M(\\epsilon)$ be a (large) constant such that $P(|Z|\u003e$ $M)\u003c\\epsilon .$ Let $F_{n}(z)$ be the CDF of $Z_{n} .$ Given $Z_{n} \\stackrel{d}{\\rightarrow} Z,$ and $F(z)$ is continuous everywhere, we have $\\left|F_{n}(z)-F(z)\\right| \\leq \\epsilon$ for any point $z \\in(-\\infty, \\infty)$ and for all $n$ sufficiently large. This implies that for all $n$ sufficiently large, we have $$ \\begin{array}{l} P\\left(Z_{n}\u003eM\\right)-P(Z\u003eM) \\leq \\epsilon \\ P\\left(Z_{n} \\leq-M\\right)-P(Z \\leq-M) \\leq \\epsilon \\end{array} $$ It follows that $$ \\begin{aligned} P\\left(Z_{n}\u003eM\\right)+P\\left(Z_{n}\u003c-M\\right) \u0026 \\leq P\\left(Z_{n}\u003eM\\right)+P\\left(Z_{n} \\leq-M\\right) \\ \u0026\u003cP(Z\u003eM)+P(Z \\leq-M)+2 \\epsilon \\ \u0026=P(Z\u003eM)+P(Z\u003c-M)+2 \\epsilon \\end{aligned} $$ where $P(Z=-M)=0$ given that $Z$ follows a continuous distribution. Therefore, $$ P\\left(\\left|Z_{n}\\right|\u003eM\\right)\u003cP(|Z|\u003eM)+2 \\epsilon\u003c3 \\epsilon \\equiv \\delta $$ Because $\\epsilon$ is arbitrary, so is $\\delta,$ and therefore $Z_{n}=O_{P}(1)$ Remarks: If the probability distribution of $Z_{n}$ converges to a well-defined continuous probability distribution as $n \\rightarrow \\infty,$ then $Z_{n}$ is bounded in probability. 【Example】Recall from that for $Z_{n}=\\max _{1 \\leq i \\leq n}\\left(X_{i}\\right),$ where $\\mathrm{X}^{n}=\\left(X_{1}, \\cdots\\right.,X_{n}$ ) is an IID random sample from a $U[0, \\theta]$ distribution, we have shown that $n\\left(\\theta-Z_{n}\\right) \\stackrel{d}{\\to} EXP(\\theta)$ as $n \\rightarrow \\infty$. Therefore, $n\\left(\\theta-Z_{n}\\right)=O_{P}(1)$ and $Z_{n}-\\theta=O_{P}\\left(n^{-1}\\right) .$ This implies that the convergence rate of $Z_{n}$ to $\\theta$ in probability is at most $n^{-1}$, which is rather rapid. Remarks: The observations of $n$ random variables $\\left{X_{i}\\right}_{i=1}^{n}$ will be more or less equally spread over the interval $[0, \\theta]$. Therefore, the maximal observation of $\\left{X_{i}\\right}_{i=1}^{n}$ will approach the upper bound of $\\theta$ at a rate equal to $n$ 【Lemma 7.6】If $Z_{n} \\stackrel{p}{\\longrightarrow} Z$ as $n \\rightarrow \\infty,$ then $Z_{n} \\stackrel{d}{\\rightarrow} Z$ as $n \\rightarrow \\infty$ Proof: We want to show for every continuous point $z$ of $F_{Z}$ and every $\\varepsilon\u003e0,$ there is an $N$ such that $\\left|F_{n}(z)-F_{Z}(z)\\right|\u003c\\varepsilon$ for all $n \\geqslant N$. Because $F_{Z}(z)$ is continuous at $z,$ we can find $\\delta\u003e0$ such that $\\left|F_{Z}(z+\\delta)-F_{Z}(z)\\right|\u003c\\varepsilon / 2$ and $\\left|F_{Z}(z-\\delta)-F_{Z}(z)\\right|\u003c\\varepsilon / 2$ $$ \\begin{aligned} F_{Z}(z-\\delta) \u0026=P(Z \\leqslant z-\\delta) \\ \u0026=P\\left(Z \\leqslant z-\\delta, Z_{n} \\leqslant z\\right)+P\\left(Z \\leqslant z-\\delta, Z_{n}\u003ez\\right) \\ \u0026 \\leqslant P\\left(Z_{n} \\leqslant z\\right)+P\\left(Z_{n}-Z\u003e\\delta\\right) \\ \u0026 \\leqslant F_{n}(z)+P\\left(\\left|Z_{n}-Z\\right|\u003e\\delta\\right) \\end{aligned} $$ So we have $F_{Z}(z-\\delta)-P\\left(\\left|Z_{n}-Z\\right|\u003e\\delta\\right) \\leqslant F_{n}(z)$ Similarly, $F_{n}(z) \\leqslant F_{Z}(z+\\delta)+P\\left(\\left|Z_{n}-Z\\right|\u003e\\delta\\right) .$ Therefore, $$ F_{Z}(z-\\delta)-P\\left(\\left|Z_{n}-Z\\right|\u003e\\delta\\right) \\leqslant F_{n}(z) \\leqslant F_{Z}(z+\\delta)+P\\left(\\left|Z_{n}-Z\\right|\u003e\\delta\\right) $$ and then $$ \\left|F_{n}(z)-F_{Z}(z)\\right|\u003c\\frac{\\varepsilon}{2}+P\\left(\\left|Z_{n}-Z\\right|\u003e\\delta\\right) $$ since $Z_{n} \\stackrel{P}{\\rightarrow} Z,$ we can find $N$ large enough such that $P\\left(\\left|Z_{n}-Z\\right|\u003e\\delta\\right)\u003c\\varepsilon / 2$ for all $n \\geqslant N$ Then $\\left|F_{n}(z)-F_{Z}(z)\\right|\u003c\\varepsilon$ for all $n \\geqslant N$. Hence $Z_{n} \\stackrel{d}{\\rightarrow} Z$ Remarks: When $Z_{n}$ converges to $Z$ in probability as $n \\rightarrow \\infty,$ the random variable $Z_{n}$ will be arbitrarily c","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:6:2","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.6.3 Continuous Mapping Theorem 【Continuous Mapping Theorem】 Suppose a sequence of $k \\times 1$ random vectors $Z_{n} \\stackrel{d}{\\rightarrow} Z$ as $n \\rightarrow \\infty$ and $g: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{l}$ is a continuous vector-valued function. Then $g\\left(Z_{n}\\right) \\stackrel{d}{\\rightarrow} g(Z)$. Remarks: Once we know the limiting distribution of $Z_{n}$, we can find the limiting distribution of many interesting functions of $Z_{n}$. This is particularly useful for deriving the limiting distributions of statistic $T\\left(Z_{n}\\right)$ once the limiting distribution of $Z_{n}$ is known. ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:6:3","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.7 Central Limit Theorems ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:7:0","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.7.1 Lindeberg-Levy’s CLT 【Lindeberg-Levy’s Central Limit Theorem (CLT)】$\\operatorname{Let} \\mathrm{X}^{n}=\\left(X_{1}, \\cdots\\right.$ $\\left., X_{n}\\right)^{\\prime}$ be an IID random sample from a population with mean $\\mu$ and variance $0\u003c\\sigma^{2}\u003c\\infty$ Define $\\bar{X}_{n}=n^{-1} \\sum_{i=1}^{n} X_{i} .$ Then the standardized sample mean $$ \\begin{aligned} Z_{n} \u0026=\\frac{\\bar{X}_{n}-E\\left(\\bar{X}_{n}\\right)}{\\sqrt{\\operatorname{var}\\left(\\bar{X}_{n}\\right)}} \\ \u0026=\\frac{\\bar{X}_{n}-\\mu}{\\sigma / \\sqrt{n}} \\ \u0026=\\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma} \\ \u0026 \\stackrel{d}{\\rightarrow} N(0,1) \\text { as } n \\rightarrow \\infty \\end{aligned} $$ Proof: Define a standardized random variable $$ Y_{i}=\\frac{X_{i}-\\mu}{\\sigma}, \\quad i=1, \\cdots, n $$ with characteristic function $\\varphi_{Y}(t)=E\\left(e^{\\mathrm{it} Y_{i}}\\right),$ where $\\mathrm{i}=\\sqrt{-1}$. Then $Y_{i}$ has zero mean and unit variance. It follows that $$ \\begin{array}{l} \\varphi_{Y}^{\\prime}(0)=\\mathbf{i} \\cdot 0=0 \\ \\varphi_{Y}^{\\prime \\prime}(0)=\\mathbf{i}^{2} \\cdot \\sigma_{Y}^{2}=-1 \\end{array} $$ We now write the standardized sample mean $$ \\begin{aligned} Z_{n} \u0026=\\frac{\\bar{X}_{n}-\\mu}{\\sigma / \\sqrt{n}} \\ \u0026=\\sqrt{n} \\frac{\\bar{X}_{n}-\\mu}{\\sigma} \\ \u0026=\\sqrt{n}\\left(n^{-1} \\sum_{i=1}^{n} \\frac{X_{i}-\\mu}{\\sigma}\\right) \\ \u0026=\\sqrt{n} \\bar{Y}_{n} \\ \u0026=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} Y_{i} \\end{aligned} $$ Since $X_{i}$ may not have a well-defined MGF, we take a characteristic function approach, that is, we shall show that $\\varphi_{n}(t) \\rightarrow e^{-\\frac{1}{2} t^{2}}$ as $n \\rightarrow \\infty,$ where $\\varphi_{n}(t)=E\\left(e^{\\mathrm{i} t Z_{n}}\\right)$ is the characteristic function of $Z_{n}$ and $e^{-\\frac{1}{2} t^{2}}$ is the characteristic function of $N(0,1)$. The characteristic function of $Z_{n}$ $$ \\begin{aligned} \\varphi_{n}(t) \u0026=E\\left(e^{\\mathrm{i} t \\sqrt{n} \\bar{Y}_{n}}\\right) \\ \u0026=\\left[E\\left(e^{\\mathrm{i} t Y_{1} / \\sqrt{n}}\\right)\\right]^{n} \\ \u0026=\\left[\\varphi_{Y}(t / \\sqrt{n})\\right]^{n} \\ \u0026=\\left[\\varphi_{Y}(0)+\\varphi_{Y}^{\\prime}(0) \\frac{t}{\\sqrt{n}}+\\frac{1}{2} \\varphi_{Y}^{\\prime \\prime}(0)\\left(\\frac{t}{\\sqrt{n}}\\right)^{2}+r\\left(\\frac{t}{\\sqrt{n}}\\right)\\right]^{n} \\ \u0026=\\left(1-\\frac{t^{2}}{2 n}+o\\left(n^{-1}\\right)\\right)^{n} \\ \u0026 \\rightarrow e^{-t^{2} / 2} \\end{aligned} $$ where $r(t / \\sqrt{n})$ denotes a reminder term, and we have used the formula $\\left(1+\\frac{a}{n}\\right)^{n} \\rightarrow e^{a}$ as $n \\rightarrow \\infty$. Therefore, $Z_{n} \\stackrel{d}{\\rightarrow} N(0,1)$ as $n \\rightarrow \\infty$ Remarks: Let $Z \\sim N(0,1) .$ The $N(0,1)$ CDF is denoted as $$ \\Phi(z)=\\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2 \\pi}} e^{-x^{2} / 2} d x $$ CLT says that when $n \\rightarrow \\infty, Z_{n} \\stackrel{d}{\\rightarrow} Z,$ i.e. $$ F_{n}(z) \\equiv P\\left(Z_{n} \\leq z\\right) \\rightarrow \\Phi(z) $$ for all $z \\in(-\\infty, \\infty)$ Therefore, for each sufficiently large (but finite) integer $n,$ the distribution of $\\bar{X}{n}$ will be approximately a $N\\left(\\mu, \\sigma^{2} / n\\right)$ or equivalently, the distribution of the sum $\\sum{i=1}^{n} X_{i}$ will be approximately a $N\\left(n \\mu, n \\sigma^{2}\\right)$ distribution. Sometimes CLT is interpreted incorrectly as implying that the distribution of $\\bar{X}{n}$ approaches a normal distribution as $n \\rightarrow \\infty .$ This is incorrect because $\\operatorname{var}\\left(\\bar{X}{n}\\right) \\rightarrow 0$ and $\\bar{X}_{n}$ converges to a degenerate distribution $F(\\cdot)$ such that $F(x)=0$ if $x\u003c\\mu$ and $F(x)=1$ if $x \\geq \\mu$. Historically, CLT was first established for a random sample from a Bernoulli distribution by A. de Moivre in the early eighteenth century. The proof for a random sample from an arbitrary distribution was given independently by J.W. Lindeberg and P. Levy in the early 1920s. CLT occupies a central position in statistical inference. Although CLT provides a simple and useful general approximation, there is no automatic way of knowing how good the approximation i","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:7:1","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.7.2 Relaxing Assumptions The assumption of a finite variance is essentially necessary for CLIT. It implies that we always obtain an approximate normality from the sum of “small” (finite variance) independent disturbances. Although the finite variance assumption can be relaxed somewhat, it cannot be eliminated. 【Example】Suppose $X_n$ is an IID random sample from the Cauchy(0, 1) distribution. Then it can be shown by characteristic function that $\\bar{X}_n \\sim Cauchy(0, 1)$ for all $n \\geq 1$. On the other hand, the identical distribution assumption can be relaxed. In other words, CLT continues to hold when there exist certain degrees of heterogeneity in observations 【Liapounov (1901) CLT for Independent Random Variables】 Suppose the random variables $X_{1}, \\cdots, X_{n}$ are jointly independent and $E\\left|X_{i}-\\mu_{i}\\right|^{3}\u003c\\infty$ for $i=1, \\cdots, n$ where $E\\left(X_{i}\\right)=\\mu_{i} .$ Also, suppose $$ \\lim _{n \\rightarrow \\infty} \\frac{\\sum_{i=1}^{n} E\\left|X_{i}-\\mu_{i}\\right|^{3}}{\\left(\\sum_{i=1}^{n} \\sigma_{i}^{2}\\right)^{3 / 2}}=0 $$ Then as $n \\rightarrow \\infty,$ we have the standardized random variable $$ \\begin{aligned} Z_{n}=\u0026 \\frac{\\sum_{i=1}^{n} X_{i}-\\sum_{i=1}^{n} \\mu_{i}}{\\left(\\sum_{i=1}^{n} \\sigma_{i}^{2}\\right)^{1 / 2}} \\ \u0026 \\stackrel{d}{\\rightarrow} N(0,1) \\end{aligned} $$ Remarks: CLT also holds when there exists certain degree of dependence among the random variables $X_{1}, \\cdots, X_{n}$ Although the dependence allowed cannot be too strong (see what happens if $X_{1}=X_{2}=\\cdots=X_{n}$ in an extreme case), one can relax the independence assumption to some extent. See, for example, Billingsley (1995, section 27). Also see White (1999) for CLT for dependent random samples. This allows application of CLT to some time series data. ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:7:2","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.7.3 Slutsky’s Theorem 【Slutsky’s Theorem】Suppose $X_{n} \\stackrel{d}{\\rightarrow} X$ and $c_{n} \\stackrel{p}{\\longrightarrow} c,$ a constant. Then (1) $X_{n}+c_{n} \\stackrel{d}{\\longrightarrow} X+c$ (2) $X_{n}-c_{n} \\stackrel{d}{\\rightarrow} X-c$ (3) $X_{n} c_{n} \\stackrel{d}{\\longrightarrow} c X$ (4) $\\frac{X_{n}}{c_{n}} \\stackrel{d}{\\rightarrow} \\frac{X}{c},$ for $c \\neq 0$ 【Example】Suppose the standardized sample mean $$ \\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma} \\stackrel{d}{\\rightarrow} N(0,1) $$ and $S_{n}^{2} \\stackrel{p}{\\rightarrow} \\sigma^{2}$ as $n \\rightarrow \\infty$ (so $S_{n} \\stackrel{p}{\\rightarrow} \\sigma$ by the continuity of the square root function). Then by the Slutsky theorem $$ \\begin{aligned} \\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{S_{n}}=\u0026 \\frac{\\sigma}{S_{n}} \\frac{\\sqrt{n}\\left(\\bar{X}_{n}-\\mu\\right)}{\\sigma} \\ \u0026 \\stackrel{d}{\\rightarrow} N(0,1) \\end{aligned} $$ 【Example】Suppose $X_{n} \\stackrel{d}{\\rightarrow} X$ and $Y_{n} \\stackrel{d}{\\rightarrow} Y$ as $n \\rightarrow \\infty .$ Do we have the following results? Give your reasoning: (1) $X_{n} \\pm Y_{n} \\stackrel{\\text { d }}{\\longrightarrow} X \\pm Y$ as $n \\rightarrow \\infty$ (2) $X_{n} Y_{n} \\stackrel{d}{\\longrightarrow} X Y$ as $n \\rightarrow \\infty$ Solution: The answer is generally no, because the dependence between $X_{n}$ and $Y_{n}$ is not taken into account. In other words, convergence in marginal distribution does not imply convergence in joint distribution. This is different from other convergence concepts, such as convergence in quadratic mean, convergence in probability and almost sure convergence. For these convergences, element-by-element convergences are equivalent to joint convergence. ","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:7:3","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"7.7.4 Delta Method 【Delta Method】Suppose $\\sqrt{n}\\left(\\bar{X}{n}-\\mu\\right) / \\sigma \\stackrel{d}{\\rightarrow} N(0,1)$ as $n \\rightarrow \\infty,$ and function $g(\\cdot)$ is continuously differentiable with $g^{\\prime}(\\mu) \\neq 0 .$ Then as $n \\rightarrow \\infty$ $$ \\sqrt{n}\\left[g\\left(\\bar{X}{n}\\right)-g(\\mu)\\right] \\stackrel{d}{\\rightarrow} N\\left(0, \\sigma^{2}\\left[g^{\\prime}(\\mu)\\right]^{2}\\right) $$ and $$ \\frac{\\sqrt{n}\\left[g\\left(\\bar{X}{n}\\right)-g(\\mu)\\right]}{\\sigma g^{\\prime}(\\mu)} \\stackrel{d}{\\rightarrow} N(0,1) $$ Proof: First, by Lemma 4,$\\sqrt{n}\\left(\\bar{X}{n}-\\mu\\right) / \\sigma \\stackrel{d}{\\rightarrow} N(0,1)$ implies $\\sqrt{n}\\left(\\bar{X}{n}-\\mu\\right) / \\sigma=O{P}(1)$. Therefore, we have $\\bar{X}{n}-\\mu=O{P}\\left(n^{-1 / 2}\\right)=o_{P}(1)$. Next, by the mean value theorem, we have $$ Y_{n}=g\\left(\\bar{X}_{n}\\right)=g(\\mu)+g^{\\prime}\\left(\\bar{\\mu}_{n}\\right)\\left(\\bar{X}_{n}-\\mu\\right) $$ where $\\bar{\\mu}{n}=\\lambda \\mu+(1-\\lambda) \\bar{X}{n}$ for some $\\lambda \\in[0,1] .$ Note that $\\left|\\bar{\\mu}{n}-\\mu\\right|=\\left|(1-\\lambda)\\left(\\bar{X}{n}-\\mu\\right)\\right| \\leq \\left|\\bar{X}{n}-\\mu\\right|=o{P}(1)$ It follows by the Slutsky theorem that $$ \\begin{aligned} \\sqrt{n}\\left[\\frac{g\\left(\\bar{X}{n}\\right)-g(\\mu)}{\\sigma}\\right] \u0026=g^{\\prime}\\left(\\bar{\\mu}{n}\\right) \\sqrt{n} \\frac{\\bar{X}_{n}-\\mu}{\\sigma} \\ \u0026 \\stackrel{d}{\\rightarrow} N\\left[0, g^{\\prime}(\\mu)^{2}\\right] \\end{aligned} $$ where $g^{\\prime}\\left(\\bar{\\mu}{n}\\right) \\stackrel{p}{\\rightarrow} g^{\\prime}(\\mu)$ given $\\bar{\\mu}{n} \\stackrel{p}{\\rightarrow} \\mu$ and the continuity of the first derivative $g^{\\prime}(\\cdot)$. By the Slutsky theorem again, we have $$ \\frac{\\sqrt{n}\\left[g\\left(\\bar{X}{n}\\right)-g(\\mu)\\right]}{\\sigma g^{\\prime}\\left(\\bar{X}{n}\\right)} \\stackrel{d}{\\rightarrow} N(0,1) $$ Remarks: The Delta method can be viewed as a Taylor series approximation in a statistical context. It linearizes a smooth (i.e., continuously differentiable) nonlinear statistic so that CLT can be applied to the linearized statistic. Therefore, it can be viewed as a generalization of CLT. Question: To apply the delta method, it is required that $g^{\\prime}(\\mu) \\neq 0 .$ What happens to the delta method if $g^{\\prime}(\\mu)=0 ?$ 【Second Order Delta Method】Suppose random variables $\\sqrt{n}(\\bar{X}{n}-\\mu) / \\sigma \\stackrel{d}{\\longrightarrow} N(0,1),$ and function $g(\\cdot)$ is twice continuously differentiable such that $g^{\\prime}(\\mu)=0$ and $g^{\\prime \\prime}(\\mu) \\neq 0 .$ Then as $n \\rightarrow \\infty$ $$ \\frac{n\\left[g\\left(\\bar{X}{n}\\right)-g(\\mu)\\right]}{\\sigma^{2}} \\stackrel{d}{\\to} \\frac{g^{\\prime \\prime}(\\mu)}{2} \\chi_{1}^{2} $$ Question: We have considered the sequence of scalar random variables $\\left{\\bar{X}{n}, n=1,2, \\cdots\\right}$ How can one derive the asymptotic distribution for a random vector $Z{n} ?$ The following Cramer-Wold device will allow us to derive the asymptotic distribution of a sequence of random vectors. 【Cramer-Wold Device】A sequence of random vector $Z_{n}=\\left(Z_{1 n}, \\ldots, Z_{d n}\\right)$ converges in distribution to a random vector $Z$ if an only if $$ a^{\\prime} Z_{n} \\stackrel{d}{\\rightarrow} a^{\\prime} Z $$ for every constant vector $a \\neq 0$ in $\\mathbb{R}^{d}$. 【Example 7.33】Suppose $ Z_{n} \\stackrel{d}{\\rightarrow} Z \\sim N(0, \\Sigma), $ where $ Z $ is an $ m \\times 1 $ random vector and $ \\Sigma $ is an $ m \\times m $ nonsingular matrix, where the dimension $ m $ is fixed. If $ \\Sigma_{n} \\stackrel{p}{\\rightarrow} \\Sigma $ as $ n \\rightarrow \\infty, $ then the quadratic form $$ Z_{n}^{\\prime} \\Sigma_{n}^{-1} Z_{n} \\stackrel{d}{\\rightarrow} Z^{\\prime} \\Sigma^{-1} Z \\sim \\chi_{m}^{2} $$ Proof: First, by the Cramer-Wold device and the Slutsky‘s theorem, we can show that $$ \\Sigma^{-\\frac{1}{2}} Z_{n} \\stackrel{d}{\\rightarrow} \\Sigma^{-1 / 2} Z \\sim N\\left(0, I_{m}\\right) \\text { as } n \\rightarrow \\infty $$ where $ I_{m} $ is an $ m \\times m $ identity matrix. It follows from the continuous ma","date":"0001-01-01","objectID":"/7.-convergences-and-limit-theorems/:7:4","tags":null,"title":"","uri":"/7.-convergences-and-limit-theorems/"},{"categories":null,"content":"8. Parameter Estimation and Evaluation ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:0:0","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.1 Population and Distribution Model A random sample $\\mathbf{X}^{n}=\\left(X_{1}, \\cdots, X_{n}\\right)$ is from a population distribution $f_{X}(x)$. A realization $\\mathrm{x}^{n}$ of the random sample $\\mathrm{X}^{n}$ constitutes a data set with sample size $n$. 【Sampling Inference】The primary purpose of statistical inference is to make inference of the population distribution $f_{X}(x)$ using an observed data $\\mathbf{x}^{n}$. 【Parametric Approach】Consider a class of parametric candidate distributions $$ \\mathbb{F}={f(\\cdot, \\theta): \\theta \\in \\Theta} $$ where $f: \\Omega \\times \\Theta \\rightarrow \\mathbb{R}^{+}$ is a known PMF/PDF function, $\\Omega$ is the support of $X_{i}, \\Theta$ is a parameter space that contains all plausible values for a $p \\times 1$ parameter vector $\\theta,$ with $p$ a fixed integer. Each value of $\\theta \\in \\Theta$ gives a distribution model for $f_{X}(x)$. 【Correct Model Specification】Assume that the class of functions $\\mathbb{F}$ contains the population distribution $f_{X}(x)$ that generates the observed data. That is, there exists some unknown parameter value $\\theta_{0} \\in \\Theta$ such that $$ f_{X}(x)=f\\left(x, \\theta_{0}\\right) \\text { for all } x \\in \\Omega $$ If $\\mathbb{F}$ contains $f_{X}(\\cdot),$ we call that the class of models $\\mathbb{F}$ is correctly specified for the population distribution $f_{X}(\\cdot),$ and $\\theta_{0}$ is called the true value of $\\theta$. In contrast, $\\mathbb{F}$ is said to be misspecified for $f_{X}(\\cdot)$ if there exists no value for $\\theta \\in \\Theta$ such that $f_{X}(x)=f(x, \\theta)$ for all $x \\in \\Omega .$ This can occur when, e.g., we specify a class of normal distribution models but the true population is a Gamma distribution. \r 【Example 8.1】Discrete Choice Probit and Logit Models: The Probit and Logit models are popularly used when a dependent variable has binary outcomes, i.e., there are two possible outcomes 0 and 1. Examples include whether or not an individual is employed, whether or not a consumer makes a purchase, and whether or not a financial crisis (e.g., default risk) occurs. A probit model assumes $$ P\\left(Y_{i}=1 | X_{i}\\right)=\\Phi\\left(\\theta_{1}+\\theta_{2} X_{i}\\right), \\quad i=1, \\cdots, n $$ where $\\Phi(\\cdot)$ is the $N(0,1)$ CDF, and $X_{i}$ is an explanatory variable. A logit model assumes $$ P\\left(Y_{i}=1 | X_{i}\\right)=\\frac{1}{1+e^{-\\left(\\theta_{1}+\\theta_{2} X_{i}\\right)}} $$ 【Example 8.2】Survival/Duration Analysis: Suppose we are interested in the time length it takes for an unemployed person to find a job, the time length that elapses between two trades or two price changes, the length of a strike, the length before a cancer patient dies, and the length before a financial crisis (e.g., credit default risk) comes out. Such analysis is called duration analysis or survival analysis. In practice, the main interest often lies in the question of how long a duration will continue, given that it has not finished yet. The hazard rate measures the chance that the duration will end now, given that it has not ended before. This hazard rate can be interpreted as the chance to find a job, to trade, to end a strike, etc. Suppose $T_{i}$ is the duration from a population with $\\operatorname{PDF} f(t)$ and $\\operatorname{CDF} F(t)$. Then the survival function is $$ S(t)=P\\left(T_{i}\u003et\\right)=1-F(t) $$ and the hazard rate $$ \\begin{aligned} \\lambda(t) \u0026=\\lim {\\delta \\rightarrow 0^{+}} \\frac{P\\left(t\u003cT{i} \\leq t+\\delta | T_{i}\u003et\\right)}{\\delta} \\ \u0026=\\frac{f(t)}{S(t)} \\end{aligned} $$ Intuitively, the hazard rate $\\lambda(t)$ is the instantaneous probability that an event of interest will end at time t given that it has lasted for period t. Note that the specification of $\\lambda(t)$ is equivalent to a specification of PDF $f(t),$ but $\\lambda(t)$ is more interpretable from an economic point of view. The hazard rate may not be the same for all individuals. To control heterogeneity across individuals, we assume that the individual-specifi","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:1:0","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.2 Maximum Likelihood Estimation ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:2:0","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.2.1 Maximum Likelihood Estimator Question: How to estimate $\\theta_{0}$ based on a data set $\\mathbf{x}^{n}$ under Correct Model Specification? R. Fisher proposed a general method of estimation called MLE. He demonstrated the advantage of this method by showing that it yields a sufficient estimator for parameter $\\theta$ whenever it exists and the MLE is asymptotically most efficient in terms of some sensible criterion. 【Definition (8.1)】Likelihood Function: Given $\\mathrm{X}^{n}=\\mathrm{x}^{n}$, the joint $P M F / P D F$ of the random sample $\\mathrm{X}^{n}$ as a function of $\\theta$ $$ \\hat{L}\\left(\\theta | \\mathbf{x}^{n}\\right)=f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right) $$ is called the likelihood function of the random sample $\\mathrm{X}^{n}$ when $\\mathrm{x}^{n}$ is observed. Also, $\\ln \\hat{L}\\left(\\theta | \\mathrm{x}^{n}\\right)$ is called the log-likelihood function of the random sample $\\mathrm{X}^{n}$ when $\\mathrm{x}^{n}$ is observed. Likelihood Function \u0026 PDF/PMF The likelihood function $L\\left(\\theta | \\mathbf{x}^{n}\\right)$ is algebraically equal to the joint PDF or PMF of the random sample $\\mathbf{X}^{n}$ when $\\mathbf{X}^{n}=\\mathbf{x}^{n}$ The conceptual difference between them is that the likelihood $L\\left(\\theta | \\mathbf{x}^{n}\\right)$ is a function of $\\theta,$ with $\\mathbf{x}^{n}$ held fixed. Given $\\theta,$ the likelihood $L\\left(\\theta | \\mathbf{x}^{n}\\right)$ is a measure of the probability or probability density with which the observed sample $\\mathbf{x}^{n}$ will occur. The joint PDF/PMF of the random sample $\\mathbf{X}^{n}, f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right),$ is different from the population distribution $f(x, \\theta) .$ The latter is the PDF/PMF of each random variable $X_{i}$ 【Definition 8.2】Maximum Likelihood Estimator (MLE): Suppose the statistic $\\hat{\\theta}=\\hat{\\theta}_{n}\\left(\\mathbf{X}^{n}\\right)$ maximizes $\\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right)$ over $\\theta \\in \\Theta,$ conditional on $\\mathbf{X}^{n},$ where $\\Theta$ is a finite-dimensional parameter space. That is $$ \\hat{\\theta} \\equiv \\hat{\\theta}_{n}\\left(\\mathbf{X}^{n}\\right)=\\arg \\max _{\\theta \\in \\Theta} \\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right) $$ Then, when exists, $\\hat{\\theta} \\equiv \\hat{\\theta}_{n}\\left(\\mathbf{X}^{n}\\right)$ is called the MLE (estimator) for parameter $\\theta$. Given a sample point (or a a data set) $ \\mathbf{x}^{n}$ for the random sample $\\mathbf{X}^{n}, \\theta\\left(\\mathbf{x}^{n}\\right)$ is called a maximum likelihood estimate for $\\theta$. Estimator——估计量 Estimate——估计量的实现值，即估计值 MLE is the parameter estimate which makes the observed data $x^{n}$ most likely to occur. In other words, by choosing a parameter estimate $\\hat{\\theta}_{n}\\left(\\mathbf{x}^{n}\\right),$ MLE maximizes the probability that $\\mathrm{X}^{n}=\\mathrm{x}^{n}$, that is, the probability that $\\mathrm{X}^{n}$ takes the value of the observed data $\\mathbf{x}^{n}$. ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:2:1","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.2.2 MLE procedure 【Theorem 8.1】Existence of MLE: Suppose, with probability one, $\\hat{L}\\left(\\theta | X^{n}\\right)$ is a continuous function of $\\theta \\in \\Theta$, and $\\Theta$ is a compact set (close and bounded, e.g., $a \\leq x \\leq b$). Then there exists a global maximizer $\\hat{\\theta}$ that solves the problem $$ \\hat{\\theta} \\equiv \\hat{\\theta}_{n}\\left(\\mathbf{X}^{n}\\right)=\\arg \\max _{\\theta \\in \\Theta} \\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right) $$ Remarks: It is often convenient to solve $\\max _{\\theta \\in \\Theta} \\ln \\hat{L}\\left(\\theta | \\mathrm{X}^{n}\\right)$, where $\\ln \\hat{L}\\left(\\theta | \\mathrm{X}^{n}\\right)$ is called the log-likelihood function, which is a strictly monotonic increasing function of $\\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right)$. MLE may not be unique. Given an observed data set $\\mathbf{x}^{n}$, MLE may be obtained at more than one point in $\\Theta$. Thus, multiple solutions for MLE are possible. The maximum is obtained over parameter space $\\Theta$, where $\\Theta$ may be subject to restriction. For example, when estimating a Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) model (Bollerslev 1986), $\\theta$ may be subject to strictions to ensure that the conditional variance is always nonnegative. When $\\ln \\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right)$ is twice continuously differentiable with respect to $\\theta \\in \\Theta,$ MLE solution will be easy to find. A necessary condition for MLE is that $\\theta$ must satisfy the first order conditions (FOC) $$ \\left.\\frac{\\partial \\ln \\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right)}{\\partial \\theta}\\right|_{\\theta=\\hat{\\theta}}=0 $$ which consists of $p$ equations if $\\theta$ is a $p \\times 1$ parameter vector. From this set of first order conditions, we can solve for $\\hat{\\theta}$. Graphically, MLE $\\hat{\\theta}$ responds to a zero slope for the likelihood function. To find a global maximum, we need to check the second order condition (SOC): If the $p \\times p$ sample Hessian matrix $$ \\hat{H}(\\theta)=\\frac{\\partial^{2} \\ln \\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right)}{\\partial \\theta \\partial \\theta^{\\prime}} $$ is negative definite for all $\\theta \\in \\Theta,$ then $\\hat{\\theta}$ is the global maximum. In many cases, it may be difficult to verify that $\\hat{H}(\\theta)$ is negative definite for all $\\theta \\in \\Theta$ It is relatively straightforward to verify that $\\hat{H}(\\hat{\\theta})$ is negative definite, which will imply that $\\hat{\\theta}$ is a local maximizer. It may be emphasized that the zeros of the first derivatives only locate extreme points in the interior of the domain of a function. If the extrema occur on the boundary, the first derivative may not be zero. Thus, the boundary must be checked separately for extrema. This can be done by using the Kuhn-Tucker theorem. The MLE procedure: Find the log-likelihood function, $\\ln \\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right) .$ For an IID random sample with population PDF/PMF $f(x, \\theta),$ we have $\\ln \\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right)=\\sum_{i=1}^{n} \\ln f\\left(X_{i}, \\theta\\right)$. Solve for the FOC and find $\\hat{\\theta}$. Check the second order conditions (SOC) to ensure $\\hat{\\theta}$ is a global maximizer or at least a local maximizer. 【Example (8.3)】Let $\\mathbf{X}^{n}$ be an IID $N(\\mu, 1)$ random sample. Find the MLE for $\\mu$. Solution: Put $ \\theta=\\mu .$ Because $\\mathbf{X}^{n}$ is an IID $N(\\mu, 1)$ random sample, the likelihood function of the sample $\\mathbf{X}^{n}$ $$ \\begin{aligned} \\hat{L}\\left(\\mu | \\mathbf{X}^{n}\\right) \u0026=\\prod_{i=1}^{n} f\\left(X_{i}, \\theta\\right) \\ \u0026=\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(X_{i}-\\mu\\right)^{2}} \\ \u0026=(2 \\pi)^{-n / 2} e^{-\\frac{1}{2} \\Sigma_{i=1}^{n}\\left(X_{i}-\\mu\\right)^{2}} \\end{aligned} $$ It follows that the log-likelihood function $$ \\ln \\hat{L}\\left(\\mu | \\mathbf{X}^{n}\\right)=-\\frac{n}{2} \\ln (2 \\pi)-\\frac{1}{2} \\sum_{i=1}^{n}\\left(X_{i}-\\mu\\right)^{2} $$ The FOC is given by $$ \\begin","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:2:2","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.2.3 Properties of MLE Question: Suppose we treat $\\sigma$ instead of $\\sigma^{2}$ as a parameter. Can we obtain the same MLE solution in the above example? The answer is yes. We will obtain $$ \\hat{\\sigma}=\\sqrt{\\hat{\\sigma}^{2}} $$ This follows from the invariance property of MLE. 【Theorem 8.2】Invariance Property of MLE: Suppose $\\hat{\\theta}$ is the MLE of $\\theta \\in \\Theta,$ and $g(\\cdot)$ is a one-to-one function over parameter space $\\Theta .$ Then $g(\\hat{\\theta})$ is a MLE of $g(\\theta)$. Proof: Because $g(\\theta)$ is a one-to-one function over $\\Theta$, there exists a unique inverse function $h(\\cdot)$ such that $h[g(\\theta)]=\\theta$ for all $\\theta \\in \\Theta$. Define a new parameter $\\tau=g(\\theta)$ and we are interested in MLE for $\\tau .$ Then $\\theta=h(\\tau)$. It follows that the likelihood function of the random sample $\\mathbf{X}^{n}$ $$ \\begin{aligned} \\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right) \u0026=\\hat{L}\\left[h(\\tau) | \\mathbf{X}^{n}\\right] \\ \u0026=\\tilde{L}\\left(\\tau | \\mathbf{X}^{n}\\right) \\end{aligned} $$ where $\\tilde{L}\\left(\\tau | \\mathbf{X}^{n}\\right)$ is the likelihood function of $\\mathbf{X}^{n}$ with respect to the transformed parameter $\\tau$. Now suppose $\\hat{\\theta}$ is a global $\\mathrm{MLE}$ of $\\theta \\in \\Theta$. Then we have $$ \\hat{L}\\left(\\hat{\\theta} | \\mathbf{X}^{n}\\right) \\geq \\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right) \\text { for all } \\theta \\in \\Theta $$ Put $\\hat{\\tau}=g(\\hat{\\theta}) .$ Then $\\hat{\\theta}=h(\\hat{\\tau}),$ and for any $\\theta \\in \\Theta$ $$ \\begin{aligned} \\tilde{L}\\left(\\hat{\\tau} | \\mathbf{X}^{n}\\right) \u0026=\\hat{L}\\left[h(\\hat{\\tau}) | \\mathbf{X}^{n}\\right] \\ \u0026= \\hat{L}\\left(\\hat{\\theta} | \\mathbf{X}^{n}\\right) \\ \u0026 \\geq \\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right)=\\hat{L}\\left[h(\\tau) | \\mathbf{X}^{n}\\right] \\end{aligned} $$ where $\\tau=g(\\theta) .$ It follows that $$ \\tilde{L}\\left(\\hat{\\tau} | \\mathbf{X}^{n}\\right) \\geq \\tilde{L}\\left(\\tau | \\mathbf{X}^{n}\\right) \\text { for all } \\tau \\in \\Gamma $$ where $\\Gamma={\\tau: \\tau=g(\\theta) \\text { for all } \\theta \\in \\Theta}$ is the parameter space for the transformed parameter $\\tau .$ Therefore, $\\hat{\\tau}$ is an MLE for $\\tau$. 【Theorem 8.3】Sufficiency of MLE: Suppose $\\mathbf{X}^{n}$ is a random sample with the likelihood function $f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right),$ and $T\\left(\\mathbf{X}^{n}\\right)$ is a sufficient statistic for parameter $\\theta \\in \\Theta .$ Then the MLE $\\hat{\\theta}$ that maximizes the likelihood function $f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right)$ of the random sample $\\mathbf{X}^{n}$ is also the MLE that maximizes the likelihood function $f_{T\\left(\\mathbf{X}^{n}\\right)}\\left[T\\left(\\mathbf{x}^{n}\\right), \\theta\\right]$ of the sufficient statistic $T\\left(\\mathbf{X}^{n}\\right)$. Proof: By definition, we have $\\hat{\\theta}=\\arg \\max {\\theta \\in \\Theta} \\ln f{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right)$ Because $T\\left(\\mathbf{X}^{n}\\right)$ is a sufficient statistic for $\\theta,$ we have $$ \\begin{aligned} f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right) \u0026=f_{T\\left(\\mathbf{X}^{n}\\right)}\\left[T\\left(\\mathbf{x}^{n}\\right), \\theta\\right] f_{\\mathbf{X}^{n} | T\\left(\\mathbf{X}^{n}\\right)}\\left[\\mathbf{x}^{n} | T\\left(\\mathbf{x}^{n}\\right)\\right] \\ \u0026=f_{T\\left(\\mathbf{X}^{n}\\right)}\\left[T\\left(\\mathbf{x}^{n}\\right), \\theta\\right] h\\left(\\mathbf{x}^{n}\\right) \\end{aligned} $$ where the conditional distribution $f_{\\mathbf{X}^{n} | T\\left(\\mathbf{X}^{n}\\right)}\\left[\\mathbf{x}^{n} | T\\left(\\mathbf{x}^{n}\\right)\\right]$ of $\\mathbf{X}^{n}=\\mathbf{x}^{n}$ given $T\\left(\\mathbf{X}^{n}\\right)=T\\left(\\mathbf{x}^{n}\\right)$ does not depend on $\\theta$ and is denoted as function $h\\left(\\mathbf{x}^{n}\\right)$. It follows that $$ \\ln f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right)=\\ln f_{T\\left(\\mathbf{X}^{n}\\right)}\\left[T\\left(\\mathbf{x}^{n}\\right), \\theta\\right]+\\ln h\\left(\\mathbf{x}^{n}\\right) $$ and maximizing $\\ln f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:2:3","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.3 Asymptotic Properties of MLE Because MLE $\\hat{\\theta}{n}$ is generally a highly nonlinear function of the random sample $\\mathbf{X}^{n},$ it is a prohibitive task to compute the mean, variance and sampling distribution of MLE $\\hat{\\theta}{n}$ for any given sample size $n,$ when the random sample $X^{n}$ is not generated from a normal population. Below, we use the asymptotic theory developed in Chapter 7 to investigate the asymptotic properties (i.e., when $n \\rightarrow \\infty$ ) of MLE $\\hat{\\theta}{n}$. In particular, we will show that MLE $\\hat{\\theta}{n}$ is consistent for the true parameter value $\\theta_{0} \\in \\Theta$ and will converge to a normal distribution, after proper standardization. ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:3:0","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.3.1 Regularity Conditions / Assumption 【Assumption M.1】IID:$\\mathrm{X}^{n}=\\left(X_{1}, \\cdots, X_{n}\\right)$ is an IID random sample from some unknown population distribution $f_{X}(x)$. 【Assumption M.2】Correct Model Specification For each $\\theta \\in \\Theta, f(x, \\theta)$ is a probability PMF/PDF model with $f(x, \\theta)\u003e0$ for all $x,$ where $\\Theta$ is a finite-dimensional parameter space there exists a parameter value $\\theta_{0} \\in \\Theta$ such that $f\\left(x, \\theta_{0}\\right)$ coincides with the population distribution $f_{X}(x)$ the function $\\ln f(x, \\theta)$ is continuous in $(x, \\theta)$ and its absolute value is bounded by a nonnegative function $b(x)$ with $E\\left[b\\left(X_{i}\\right)\\right]\u003c\\infty,$ where the expectation $E(\\cdot)$ is taken under the population distribution $f_{X}(x)$ 【Assumption M.3】Compact Parameter Space: $\\Theta$ is closed and bounded, or equivalently $\\Theta$ is compact. Assumption M.3 helps ensure the existence of MLE. 【Assumption M.4】Unique Identification: The parameter value $\\theta_{0}$ is the unique maximizer of $E\\left[\\ln f\\left(X_{i}, \\theta\\right)\\right]$. Assumption M.4 ensures that the true parameter value $\\theta_{0}$ is the well-defined probability limit of MLE $\\hat{\\theta}$. It is important to note that $$ \\begin{aligned}E\\left[\\ln f\\left(X_{i}, \\theta\\right)\\right] \u0026=\\int_{-\\infty}^{\\infty} \\ln f(x, \\theta) f_{X}(x) d x \\\u0026 \\neq \\int_{-\\infty}^{\\infty} \\ln f(x, \\theta) f(x, \\theta) d x\\end{aligned} $$ unless $\\theta=\\theta_{0}$. Proof: Jensen’s inequality: If $g(\\cdot)$ is concave, then $g[E(X)] \\geqslant E[g(X)]$ Because $\\ln x$ is a concave function, we have for any $\\theta \\in \\Theta$ $$ \\begin{aligned} E\\left[\\ln f\\left(X_{i}, \\theta\\right)\\right]-E\\left[\\ln f\\left(X_{i}, \\theta_{0}\\right)\\right] \u0026=\\int_{\\Omega_{X}}\\left[\\ln \\frac{f(x, \\theta)}{f\\left(x, \\theta_{0}\\right)}\\right] f\\left(x, \\theta_{0}\\right) d x \\ \u0026 \\leqslant \\ln \\left[\\int_{\\Omega_{X}} \\frac{f(x, \\theta)}{f\\left(x, \\theta_{0}\\right)} f\\left(x, \\theta_{0}\\right) d x\\right] \\ \u0026=\\ln \\int_{\\Omega_{X}} f(x, \\theta) d x \\ \u0026=\\ln 1=0 \\end{aligned} $$ Hence, $E\\left[\\ln f\\left(X_{i}, \\theta_{0}\\right)\\right] \\geqslant E\\left[\\ln f\\left(X_{i}, \\theta\\right)\\right]$ for all $\\theta \\in \\Theta$. 【Assumption M.5】Interior Solution: $\\theta_{0}$ is in the interior of parameter space $\\Theta$. 【Assumption M.6】Smoothness and Moment Conditions: For each interior point $\\theta \\in \\Theta, f(x, \\theta)$ is twice continuously differentiable with respect to $\\theta$ such that the functions $\\frac{\\partial}{\\partial \\theta} \\ln f(x, \\theta), \\frac{\\partial^{2}}{\\partial \\theta^{2}} \\ln f(x, \\theta)$ are continuous in $(x, \\theta),$ and their absolute values are bounded by a nonnegative function $b(x)$ with $E\\left[b\\left(X_{i}\\right)\\right]\u003c\\infty$ and $E\\left[b^{2}\\left(X_{i}\\right)\\right]\u003c$ $\\infty$ the absolute value of the function $H(\\theta)=E\\left[\\frac{\\partial^{2}}{\\partial \\theta^{2}} \\ln f\\left(X_{i}, \\theta\\right)\\right]$ is bounded by some constant and is nonzero. Assumptions M.5 and M.6 facilitate the application of the Taylor series expansion in order to derive the asymptotic distribution of MLE $\\hat{\\theta}$. The function $\\frac{\\partial}{\\partial \\theta} \\ln f(x, \\theta)$ is called the score function; The function $H(\\theta)$ is called the Hessian matrix (it is a square matrix when $\\theta$ is a parameter vector). Remarks: The scalar parameter assumption is made for simplicity. The results obtained below can be extended to the case of a parameter vector in a straightforward manner, but it does not offer much additional insight into the asymptotic properties of MLE. ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:3:1","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.3.2 Consistency of MLE 【Lemma 8.4】Extreme Estimator Lemma; White (1994, Theorem 3.4): Suppose $Q(\\theta)$ is a nonstochastic function continuous in $\\theta \\in \\Theta,$ and $\\theta_{0} \\in \\Theta$ is the unique maximizer of $Q(\\theta)$ over $\\Theta,$ where $\\Theta$ is a compact set, with probability one, $\\hat{Q}_{n}(\\theta)$ is a sequence of random functions continuous in $\\theta \\in \\Theta$ $ \\lim _{n \\rightarrow \\infty} \\sup {\\theta \\in \\Theta}\\left|\\hat{Q}{n}(\\theta)-Q(\\theta)\\right|=0$ almost surely. Then $\\hat{\\theta}=\\arg \\max {\\theta \\in \\Theta} \\hat{Q}{n}(\\theta)$ exists and $\\hat{\\theta} \\rightarrow \\theta_{0}$ a.s. $n \\rightarrow \\infty$. Proof: See White (1994, Proof of Theorem 3.4). Remarks: It implies that the largest difference between $\\hat{Q}_{n}(\\theta)$ and $Q(\\theta)$ over the parameter space $\\Theta$ vanishes to zero as $n \\rightarrow \\infty$ almost surely. 【Theorem 8.5】Consistency of MLE: Suppose Assumptions M.1-M.4 hold, and $\\hat{\\theta}=$ $\\arg \\max {\\theta \\in \\Theta} \\sum{i=1}^n \\ln f\\left(X_{i}, \\theta\\right) .$ Then as $n \\rightarrow \\infty$ $$ \\hat{\\theta} \\rightarrow \\theta_{0} \\text { a.s.} $$ Proof: We apply the extreme estimator lemma. Given Assumption M.2, we have $Q(\\theta)=E\\left[\\ln f\\left(X_{i}, \\theta\\right)\\right]$ is a continuous function of $\\theta \\in \\Theta,$ and $\\theta_{0}$ is the unique maximizer of $Q(\\theta)$ over the compact set $\\Theta$ by Assumptions M.3 and M.4.​ Now, put $$ \\begin{aligned} \\hat{Q}{n}(\\theta)\u0026= \\frac{1}{n} \\ln \\hat{L}(\\theta | \\mathbf{X}^n) \\ \u0026=n^{-1} \\ln \\prod^n{i=1} f(X_i, \\theta) \\ \u0026=n^{-1} \\sum_{i=1}^{n} \\ln f\\left(X_{i}, \\theta\\right) \\end{aligned} $$ Then, given Assumptions M.1-M.3 and USLLN in Lemma 7.10, Chapter 7, we have $\\sup_{\\theta \\in \\Theta}\\left|\\hat{Q}_{n}(\\theta)-Q(\\theta)\\right| \\rightarrow 0$ as $n \\rightarrow \\infty$ almost surely. It follows from the extreme estimator lemma that MLE $\\hat{\\theta} \\rightarrow \\theta_{0}$ almost surely $n \\rightarrow \\infty$. Remarks: We do not require $\\theta_{0}$ to be an interior point of parameter space $\\Theta$ in establishing the consistency of MLE $\\hat{\\theta}$. The consistency theorem allows that $\\theta_{0}$ is a corner solution (i.e., $\\theta_{0}$ can be on the boundary of $\\Theta$). There is no need to assume differentiability of the log-likelihood function $\\ln f(x, \\theta)$ with respect to $\\theta$. In fact, the FOC may fail when there exists a corner solution even if $\\ln f(x, \\theta)$ is differentiable with respect to $\\theta$. ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:3:2","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.3.3 Asymptotic Normality 【Lemma 8.6】Zero Expectation for Score Function: Suppose $f(x, \\theta)$ is a PDF model and $f(x, \\theta)$ is continuously differentiable with respect to $\\theta \\in \\Theta,$ where $\\theta$ is an interior point in parameter space$\\Theta .$ Then for all $\\theta$ in the interior of $\\Theta$ $$ E_{\\theta}[S(x, \\theta)]=\\int_{-\\infty}^{\\infty} \\frac{\\partial \\ln f(x, \\theta)}{\\partial \\theta} f(x, \\theta) d x=0 $$ where Score function $$ S(X_i,\\theta) = \\frac{\\partial \\ln f(X_i, \\theta)}{\\partial \\theta} $$ A similar result holds for a PMF model. Proof: Given $f(x, \\theta)$ is a PDF model, $f(x, \\theta)$ is a PDF for any $\\theta \\in \\Theta .$ It follows that for any $\\theta$ in the interior of $\\Theta$ $$ \\int_{-\\infty}^{\\infty} f(x, \\theta) d x=1 $$ Differentiating this equation and exchanging the order of integration and differentiation, we have $$ \\begin{aligned} \u0026 \\frac{d}{d \\theta} \\int_{-\\infty}^{\\infty} f(x, \\theta) d x =\\frac{d}{d \\theta}(1)=0 \\ \\implies \u0026 \\int_{-\\infty}^{\\infty} \\frac{\\partial f(x, \\theta)}{\\partial \\theta} d x =0 \\ \\implies \u0026 \\int_{-\\infty}^{\\infty}\\left[\\frac{\\partial \\ln f(x, \\theta)}{\\partial \\theta}\\right] f(x, \\theta) d x =0 \\end{aligned} $$ Remarks: It should be noted that $$ \\begin{aligned} \u0026 E_{\\theta}\\left[\\frac{\\partial \\ln f\\left(X_{i}, \\theta\\right)}{\\partial \\theta}\\right] \\ =\u0026 \\int_{-\\infty}^{\\infty}\\left[\\frac{\\partial \\ln f(x, \\theta)}{\\partial \\theta}\\right] f(x, \\theta) d x \\ \\neq \u0026 E\\left[\\frac{\\partial \\ln f\\left(X_{i}, \\theta\\right)}{\\partial \\theta}\\right] \\end{aligned} $$ unless $\\theta=\\theta_{0},$ where $E_{\\theta}(\\cdot)$ is the expectation with respect to the model distribution $f(x, \\theta),$ while $E(\\cdot)$ is the expectation with respect to the population distribution $f_{X}(x)$. 【Lemma 8.7】Information Matrix Equality: Suppose a PDF model $f(x, \\theta)$ is twice continuously differentiable with respect to $\\theta \\in \\Theta,$ where $\\theta$ is an interior point in parameter space $\\Theta .$ Define $$ \\begin{aligned} I(\\theta) \u0026 \\equiv E_{\\theta}\\left[\\frac{\\partial \\ln f\\left(X_{i}, \\theta\\right)}{\\partial \\theta}\\right]^2 =\\int_{-\\infty}^{\\infty}\\left[\\frac{\\partial \\ln f(x, \\theta)}{\\partial \\theta}\\right]^{2} f(x, \\theta) d x \\ H(\\theta) \u0026 \\equiv E_{\\theta}\\left[\\frac{\\partial^{2} \\ln f\\left(X_{i}, \\theta\\right)}{\\partial \\theta^{2}}\\right] =\\int_{-\\infty}^{\\infty}\\left[\\frac{\\partial^{2} \\ln f(x, \\theta)}{\\partial \\theta^{2}}\\right] f(x, \\theta) d x \\end{aligned} $$ Then for all $\\theta$ in the interior of $\\Theta$ $$ I(\\theta)+H(\\theta)=0 $$ A similar result holds for a PMF model. Proof: By differentiating the identity $\\int_{-\\infty}^{\\infty} f(x, \\theta) d x=1$ with respect to $\\theta,$ we then obtain $$ \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta} f(x, \\theta) d x=0 $$ This can be rewritten as $$ \\int_{-\\infty}^{\\infty} \\frac{\\partial \\ln f(x, \\theta)}{\\partial \\theta} f(x, \\theta) d x=0 $$ If we further differentiate this equation with respect to $\\theta,$ we obtain $$ \\int_{-\\infty}^{\\infty}\\left{\\left[\\frac{\\partial^{2} \\ln f(x, \\theta)}{\\partial \\theta^{2}}\\right] f(x, \\theta)+\\left[\\frac{\\partial \\ln f(x, \\theta)}{\\partial \\theta}\\right] \\frac{\\partial f(x, \\theta)}{\\partial \\theta}\\right} d x=0 $$ or equivalently $$ \\int_{-\\infty}^{\\infty}\\left[\\frac{\\partial^{2} \\ln f(x, \\theta)}{\\partial \\theta^{2}}\\right] f(x, \\theta) d x+\\int_{-\\infty}^{\\infty}\\left[\\frac{\\partial \\ln f(x, \\theta)}{\\partial \\theta}\\right]^{2} f(x, \\theta) d x=0 $$ Remarks: $ I(\\theta) $ is called Fisher’s Information Matrix, indicates average of squared slopes of likelihood function. By intuition, we can find MLE more easily when $ I(\\theta) $ is larger(i.e., the likelihood function is steeper). $ H(\\theta) $ is called the Hessian matrix of the PMF/PDF model $f(x, \\theta)$. This function is negative definite and its absolute value magnitude measures the average degree of the curvature of the likelihood function at $\\theta$. At MLE $ \\hat{\\theta} $, $H({","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:3:3","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.4 Method of Moments and Generalized Method of Moments ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:4:0","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.4.1 Method of Moments Estimation Suppose $f(x, \\theta),$ where $\\theta \\in \\Theta,$ is a parametric PMF/ PDF model for the unknown population distribution $f_{X}(x)$ and $f_{X}(x)=f\\left(x, \\theta_{0}\\right)$ for some parameter value $\\theta_{0} \\in \\Theta$. This implies that the parametric probability model $f(x, \\theta)$ is correctly specified for the population distribution $f_{X}(x)$. Suppose $\\mathrm{X}^{n}$ is an IID random sample from the population distribution $f_{X}(x)$. 【sample moment】We first define a $q \\times 1$ statistic vector $$ \\hat{m}=\\frac{1}{n} \\sum_{i=1}^{n} m\\left(\\mathbf{X}^{n}\\right) $$ which will converge in probability to $E\\left[m\\left(\\mathbf{X}^{n}\\right)\\right]$ where $E(\\cdot)$ is under the unknown true joint distribution of $\\mathrm{X}^{n}$. 【Population moment】Next, we compute its mathematical expectation under the model distribution: $$ \\begin{aligned} M(\\theta) \u0026=E_{\\theta}\\left[m\\left(\\mathbf{X}^{n}\\right)\\right] \\ \u0026=\\int_{\\mathbb{R}^{n}} m\\left(\\mathbf{x}^{n}\\right) f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right) d \\mathbf{x}^{n} \\end{aligned} $$ where the mathematical expectation $E_{\\theta}(\\cdot)$ is taken under the joint distribution $f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right)$ of $\\mathbf{X}^{n},$ where $f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right)=\\Pi_{i=1}^{n} f\\left(x_{i}, \\theta\\right)$ when $\\mathbf{X}^{n}$ is an IID random sample 【Moment matching】Then we solve the system of $q$ equations $$ \\hat{m}=M(\\hat{\\theta}) $$ That is, we choose a parameter value $\\hat{\\theta}=\\hat{\\theta}_{n}\\left(\\mathrm{X}^{n}\\right)$ to match the sample moment $\\hat{m}$ with the population moment $M(\\theta)$. The solution $\\hat{\\theta}$ is called the method of moments estimator (MME) for the true parameter value $\\theta_{0}$. Alternatively, we can define a vector-valued sample moment function $$ \\hat{m}(\\theta)=m\\left(\\mathbf{X}^{n}\\right)-M(\\theta), \\quad \\theta \\in \\Theta $$ The method of moments estimator $\\hat{\\theta}$ is the solution of the equations $$ \\hat{m}(\\theta)=0 $$ 【MME Procedure】Often, when $\\mathbf{X}^{n}$ is an IID random sample from population $f_{X}(x)=$ $f\\left(x, \\theta_{0}\\right)$ for some parameter value $\\theta_{0},$ we have the following procedures: Compute population moments $E_{\\theta}\\left(X_{i}^{k}\\right), k=1,2, \\cdots,$ under the PMF/PDF model $f(x, \\theta)$ $$ \\begin{aligned} M_{k}(\\theta) \u0026=E_{\\theta}\\left(X_{i}^{k}\\right) \\ \u0026=\\left{\\begin{array}{ll} \\sum_{x \\in \\Omega_{X}} x^{k} f(x, \\theta) \u0026 \\text { if } X \\text { is a DRV } \\ \\int_{-\\infty}^{\\infty} x^{k} f(x, \\theta) d x \u0026 \\text { if } X \\text { is a CRV } \\end{array}\\right. \\end{aligned} $$ Note that the population moment $M_{k}(\\theta)$ depends on parameter $\\theta$. Compute the sample moments from random sample $\\mathbf{X}^{n}=\\left(X_{1}, \\cdots, X_{n}\\right)$ $$ \\hat{m}_{k}=n^{-1} \\sum_{i=1}^{n} X_{i}^{k}, \\quad k=1,2, \\cdots $$ For each $n,$ match the sample moments and the population moments by choosing some parameter value $\\hat{\\theta}$. In general, if $\\theta$ is a $p \\times 1$ parameter vector, we need $p$ equations: $$ \\left{\\begin{array}{l} \\hat{m}{1}=M{1}(\\hat{\\theta}) \\ \\hat{m}{2}=M{2}(\\hat{\\theta}) \\ \\qquad \\dots \\ \\hat{m}{p}=M{p}(\\hat{\\theta}) \\end{array}\\right. $$ Solving for these $p$ equations will yield MME $\\hat{\\theta}=\\hat{\\theta}_{n}\\left(\\mathbf{X}^{n}\\right)$ 【Consistency of MME】MME $\\hat{\\theta}$ consistently estimate the true parameter value $\\theta_{0} $. Proof: By WLLN, the sample moment $$ \\begin{aligned} \\hat{m}{k} = \\frac{1}{n} \\sum^n{i=1} X_i^k\\stackrel{p}{\\rightarrow} E\\left(X_{i}^{k}\\right) \u0026=\\int_{-\\infty}^{\\infty} x^{k} f_{X}(x) d x \\ \u0026=\\int_{-\\infty}^{\\infty} x^{k} f\\left(x, \\theta_{0}\\right) d x \\ \u0026=M_{k}\\left(\\theta_{0}\\right) \\end{aligned} $$ where the second equality follows by correct model specification. If we set $$ \\hat{m}{k}=M{k}(\\hat{\\theta}) \\text { for any } n $$ we will have $$ \\hat{m}{k} =M{k}(\\hat{\\theta}) \\stackrel{p}{\\rightarrow} M","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:4:1","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.4.2 Generalized Method of Moments Estimation Often in econometrics, the population moment function $ M ( \\theta ) = E _ { \\theta } \\left[ m \\left( X ^ { n } \\right) \\right] $ is not attainable, due to the fact that the population distribution of an economic process is usually not specified. However, economic and financial theory often implies that certain moment conditions must hold when evaluated at the true parameter value $ \\theta _ { 0 } . $ In other words, economic theories or hypotheses are often characterized by a set of moment conditions. Specifically, suppose $ \\theta $ is a $ p \\times 1 $ parameter vector, and there exists a $ p \\times 1 $ moment function $ m ( X , \\theta ) $ such that $$ E \\left[ m \\left( X , \\theta _ { 0 } \\right) \\right] = 0 \\text { for some } \\theta _ { 0 } \\in \\Theta $$ where the expectation is taken over the unknown true distribution of $X$: This may follow from economic theory (e.g., the equilibrium condition in a rational expectations model). 【Example 8.7】An investor who maximizes an intertemporal utility function $$ \\max {\\left{C{t}\\right}} U\\left(C_{t}, C_{t+1}\\right)=u\\left(C_{t}\\right)+\\beta E\\left[u\\left(C_{t+1}\\right) | I_{t}\\right] $$ subject to an inter-temporal budget constraint will choose a sequence of consumptions $ \\left{C_{t}\\right} $ that satisfies the first order condition $$ P_{t}=\\beta E\\left[\\frac{u^{\\prime}\\left(C_{t+1}\\right)}{u^{\\prime}\\left(C_{t}\\right)} Y_{t+1} | I_{t}\\right] $$ where $ \\beta $ is a time discount factor parameter, $ Y_{t+1} $ is the random payoff of an asset at time $ t+1 $ and $ P_{t} $ is the price of the asset at time $ t $, and $ E_{t}(\\cdot)=E\\left(\\cdot | I_{t}\\right) $ is the conditional expectation given the information set $ I_{t} $ available at time $t$. The FOC is also called the Euler equation. It states that in the equilibrium, the current asset price should be equal to the expected future payoff of the asset after risk compensation. Here, the factor $ \\beta \\frac{u^{\\prime}\\left(C_{t+1}\\right)}{u^{\\prime}\\left(C_{t}\\right)} $ is called the stochastic discount factor; it charcterizes the risk attitude of the representative economic agent. Define the stochastic pricing error $$ \\varepsilon_{t+1}(\\theta)=\\beta \\frac{u^{\\prime}\\left(C_{t+1}\\right)}{u^{\\prime}\\left(C_{t}\\right)} Y_{t+1}-P_{t} $$ where parameter $ \\theta $ contains time discount factor $ \\beta $ and any other structral parameters. The Euler equation can be equivalently characterized by the following conditional moment condition: $$ E\\left[\\varepsilon_{t+1}\\left(\\theta_{0}\\right) | I_{t}\\right]=0 $$ This implies that a rational economic agent does not make any systematic pricing error in each time period. Now define a moment function $$ m\\left(X_{t+1}, \\theta\\right)=\\left[\\beta \\frac{u^{\\prime}\\left(C_{t+1}\\right)}{u^{\\prime}\\left(C_{t}\\right)} Y_{t+1}-P_{t}\\right] Z_{t} $$ where $ X_{t+1}=\\left(C_{t}, C_{t+1}, P_{t}, Y_{t+1}, Z_{t}^{\\prime}\\right)^{\\prime} $ and $ Z_{t} \\in I_{t} $ is the vector of so-called instrumental variables available to the economic agent at time $t$. Then by the law of iterated expectations, we have $$ \\begin{aligned} E\\left[m\\left(X_{t+1}, \\theta_{0}\\right)\\right] \u0026=E\\left{E\\left[m\\left(X_{t+1}, \\theta_{0}\\right) | I_{t}\\right]\\right} \\ \u0026=0 \\end{aligned} $$ where $ E(\\cdot) $ is the unconditional expectation under the unknown population distribution. Question: In this example, where does parameter $ \\theta $ come from? Besides the time discount parameter $ \\beta, $ some parameter(s) may arise from the utility function to characterize risk aversion of the economic agent. For example, when the economic agent has a constant relative risk aversion utility function $$ u\\left(C_{t}\\right)=\\frac{C_{t}^{\\gamma}-1}{\\gamma} $$ the parameter $$ \\gamma=-C_{t} \\frac{u^{\\prime \\prime}\\left(C_{t}\\right)}{u^{\\prime}\\left(C_{t}\\right)} $$ measures the degree of risk aversion of the economic agent. In this case, $ \\theta=(\\beta, \\gamma)^{\\prime} $. 【Example 8.8】Capita","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:4:2","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.5 Asymptotic Properties of GMM ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:5:0","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.5.1 Regularity Conditions/ Assumption 【Assumption G.1】IID: $ \\mathrm{X}^{n}=\\left(X_{1}, \\cdots, X_{n}\\right) $ is an IID random sample from some unknown population distribution $ f_{X}(x) $. 【Assumption G.2】Moment Function: The $ q \\times 1 $ moment function $ m(x, \\theta) $ is continuous in $ (x, \\theta) $ and the absolute values of its components are bounded by a nonnegative function $ b(x) $ with $ E\\left[b\\left(X_{i}\\right)\\right]\u003c\\infty, $ where the expectation $ E(\\cdot) $ is taken under the unknown population distribution $ f_{X}(x) $ $$ \\sup_{\\theta \\in \\Theta}|m_k(X_i, \\theta)| \\leq b(X_i) $$ which will be used as condition for USLLN. 【Assumption G.3】Unique Identification: There exists one and only one $ p \\times 1 $ parameter value $ \\theta_{0} $ in $ \\Theta $ such that $ E\\left[m\\left(X_{i}, \\theta_{0}\\right)\\right]=0 $. Assumption G. 3 is an identification condition for $ \\theta_{0}, $ which is called the true parameter value of the model characterized by the population moment condition $ E\\left[m\\left(X_{i}, \\theta_{0}\\right)\\right]= 0$. The identification condition ensures that $ \\theta_{0} $ is the unique probability limit of the GMM estimator $ \\hat{\\theta} $. 【Assumption G.4】Compact Parameter Space: The $ p $-dimensional parameter space $ \\Theta $ is closed and bounded. 【Assumption G.5】Weighting: The $ q \\times q $ stochastic weighting matrix $ \\hat{W} \\rightarrow W $ as $ n \\rightarrow \\infty $ almost surely, where $ W $ is symmetric, bounded and nonsingular. 【Assumption G.6】Interior Solution: The parameter value $ \\theta_{0} $ is an interior point of the parameter space $ \\Theta $. The interior solution assumption for $ \\theta_{0} $ is not needed for the consistency of the GMM estimator $ \\hat{\\theta}, $ but is needed when we derive the asymptotic normality of the GMM estimator $ \\hat{\\theta}, $ where we will use a Taylor series expansion of the FOC of the GMM estimation. 【Assumption G.7】Smoothness and Moment Conditions: The functions/ matrix $ \\frac{\\partial}{\\partial \\theta} m(x, \\theta) $ and $ \\frac{\\partial^{2}}{\\partial \\theta \\partial \\theta^{\\prime}} m(x, \\theta) $ are continuous in $ (x, \\theta) $ and the absolute values of their component functions are bounded by a nonnegative function $ b(x) $ with $ E\\left[b\\left(X_{i}\\right)\\right]\u003c\\infty $. The $ q \\times q $ symmetric matrix $ V=E\\left[m\\left(X_{i}, \\theta_{0}\\right) m\\left(X_{i}, \\theta_{0}\\right)^{\\prime}\\right] $ is bounded and nonsingular; The $ q \\times p $ gradient matrix $ G\\left(\\theta_{0}\\right)=E\\left[\\frac{\\partial}{\\partial \\theta} m\\left(X_{i}, \\theta_{0}\\right)\\right] $ is of full rank (equal to $ p $ given $ p \\leq q) $ With assumption G.3 $ E\\left[m\\left(X_{i}, \\theta_{0}\\right)\\right]= 0$,we have $$ \\begin{aligned} V\u0026=E\\left[m\\left(X_{i}, \\theta_{0}\\right) m\\left(X_{i}, \\theta_{0}\\right)^{\\prime}\\right]\\ \u0026=Var [m(X_i, \\theta_0)] \\ \\end{aligned} $$ i.e., $V$ is variance-covariance matrix of $m(X_i, \\theta)$. ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:5:1","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.5.2 Consistency of GMM 【Theorem 8.10】Consistency of GMM: Suppose Assumptions G.1-G.5 hold. Then as $ n \\rightarrow \\infty $ $$ \\hat{\\theta} \\rightarrow \\theta_{0} \\text { a.s.} $$ **Proof:** The proof is similar to the consistency proof of the MLE. We apply the Lemma 8.4 (Extreme Estimator Lemma). Suppose $$ \\begin{aligned} Q(\\theta) \u0026= -[Em(X_i,\\theta)]‘W^{-1}[Em(X_i,\\theta)] \\ \\hat{Q}(\\theta) \u0026= - \\hat{m}(\\theta)'\\hat{W}^{-1} \\hat{m}(\\theta) \\end{aligned} $$ where $\\hat{m}(\\theta) = n^{-1} \\sum^n_{i=1} m(X_i,\\theta)$. With Assumption G.3 (unique identification), $\\theta_0$ is the unique maximizer of $Q(\\theta)$. While in GMM $$ \\begin{aligned} \\hat{\\theta}\u0026=\\arg \\min {\\theta \\in \\Theta} \\hat{m}(\\theta)^{\\prime} \\hat{W}^{-1} \\hat{m}(\\theta)\\ \u0026=\\arg \\max {\\theta \\in \\Theta}- \\hat{m}(\\theta)^{\\prime} \\hat{W}^{-1} \\hat{m}(\\theta) \\end{aligned} $$ Under assumption G.1, G.2 and G.4, by USLLN, we have $$ \\sup{\\theta \\in \\Theta} |\\hat{m}(\\theta)-Em(X_i,\\theta)| \\stackrel{a.s.}{\\rightarrow} 0 $$ By continuity, $$ \\sup{\\theta \\in \\Theta} |\\hat{Q}(\\theta)-Q(\\theta)| \\stackrel{a.s.}{\\rightarrow} 0 $$ With, lemma 8.4 $$ \\hat{\\theta}\\stackrel{a.s.}{\\rightarrow} \\theta $$ Note that $ \\theta_{0} $ could be a corner solution so that the FOC may not hold. \r ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:5:2","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.5.3 Asymptotic Normality 【Theorem 8.11】Asymptotic Normality: Suppose Assumptions G.1-G.7 hold. Then Asymptotic Normality: as $ n \\rightarrow \\infty $, $$ \\sqrt{n}\\left(\\hat{\\theta}-\\theta_{0}\\right) \\stackrel{d}{\\rightarrow} N(0, \\Omega) $$ where $$ \\begin{array}{c} \\Omega=\\Psi V \\Psi^{\\prime} \\end{array} $$ $V=E\\left[m\\left(X_{1}, \\theta_{0}\\right) m\\left(X_{1}, \\theta_{0}\\right)^{\\prime}\\right]$, and $ \\Psi=\\left[G\\left(\\theta_{0}\\right) W^{-1} G\\left(\\theta_{0}\\right)^{\\prime}\\right]^{-1} G\\left(\\theta_{0}\\right) W^{-1}$, and $ G\\left(\\theta_{0}\\right)=E\\left[\\frac{\\partial}{\\partial \\theta} m\\left(X_{i}, \\theta_{0}\\right)\\right] $ Optimal Weighting Matrix: Moreover, if $ W=V $, then as $ n \\rightarrow \\infty $, $$ \\sqrt{n}\\left(\\hat{\\theta}-\\theta_{0}\\right) \\stackrel{d}{\\rightarrow} N\\left(0,\\left[G\\left(\\theta_{0}\\right) V^{-1} G\\left(\\theta_{0}\\right)^{\\prime}\\right]^{-1}\\right) $$ **Proof:** Define the objective function $$ \\hat{Q}{n}(\\theta)=\\hat{m}(\\theta)^{\\prime} \\hat{W}^{-1} \\hat{m}(\\theta) $$ Noting that the prespecified weighting matrix $ \\hat{W} $ is not a function of $ \\theta $. With assumption G.6 (Interior Solution) and $ \\hat{\\theta}\\stackrel{a.s.}{\\rightarrow} \\theta $, we can obtain the FOC as follows: $$ \\frac{d \\hat{Q}{n}(\\hat{\\theta})}{d \\theta}=2 \\hat{G}(\\hat{\\theta}) \\hat{W}^{-1} \\hat{m}(\\hat{\\theta})=0 $$ where the $ p \\times q $ sample matrix $$ \\begin{aligned} \\hat{G}(\\theta) \u0026=\\frac{d \\hat{m}(\\theta)}{d \\theta^{\\prime}} \\ \u0026=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial m\\left(X_{i}, \\theta\\right)}{\\partial \\theta^{\\prime}} \\end{aligned} $$ By the mean value theorem/ 中值定理, we have $$ \\hat{m}(\\hat{\\theta})=\\hat{m}\\left(\\theta_{0}\\right)+\\hat{G}(\\bar{\\theta})^{\\prime}\\left(\\hat{\\theta}-\\theta_{0}\\right) $$ where $ \\bar{\\theta} $ lies on the segment between $ \\hat{\\theta} $ and $ \\theta_{0}, $ i.e., $ \\bar{\\theta}=\\lambda \\hat{\\theta}+(1-\\lambda) \\theta_{0} $ for some $ \\lambda \\in[0,1] $. Substituting this expression into the FOC, we obtain $$ \\hat{G}(\\hat{\\theta}) \\hat{W}^{-1} \\hat{m}\\left(\\theta_{0}\\right)+\\hat{G}(\\hat{\\theta}) \\hat{W}^{-1} \\hat{G}(\\bar{\\theta})^{\\prime}\\left(\\hat{\\theta}-\\theta_{0}\\right)=0 $$ Following the reasoning similar to that for the sample Hessian matrix $ \\hat{H}(\\bar{\\theta}) $ in the proof for the asymptotic normality of the MLE, we can show that as $ n \\rightarrow \\infty $ $$ \\begin{array}{l} \\hat{G}(\\hat{\\theta}) \\rightarrow G\\left(\\theta_{0}\\right) \\text { a.s. } \\ \\hat{G}(\\bar{\\theta}) \\rightarrow G\\left(\\theta_{0}\\right) \\text { a.s. } \\end{array} $$ First, We write $$ \\hat{G}(\\hat{\\theta})-G\\left(\\theta_{0}\\right)=[\\hat{G}(\\hat{\\theta})-G(\\hat{\\theta})]+\\left[G(\\hat{\\theta})-G\\left(\\theta_{0}\\right)\\right] $$ For the second term, we have $$ G(\\hat{\\theta})-G\\left(\\theta_{0}\\right) \\rightarrow 0 \\text { a.s. } $$ by Lemma 7.8, the continuity of the function $ G\\left(\\theta\\right)=E\\left[\\frac{\\partial}{\\partial \\theta} m\\left(X_{i}, \\theta\\right)\\right] $ given Assumption G.7, and $\\bar{\\theta}-\\theta_{0} \\rightarrow 0$ as $n \\rightarrow \\infty$ almost surely. For the first term, we have $$ \\begin{array}{l} \\quad|\\hat{G}(\\hat{\\theta})-G(\\hat{\\theta})| \\ =\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial m\\left(X_{i}, \\hat{\\theta}\\right)}{\\partial \\theta}-\\left{E\\left[\\frac{\\partial m\\left(X_{i}, \\theta\\right)}{\\partial \\theta}\\right]\\right}_{\\theta=\\hat{\\theta}}\\right| \\ \\leq \\sup _{\\theta \\in \\Theta}\\left|\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial m \\left(X_{i}, \\theta\\right)}{\\partial \\theta}-E\\left[\\frac{\\partial m \\left(X_{i}, \\theta\\right)}{\\partial \\theta}\\right]\\right| \\ \\rightarrow 0 \\text { a.s. } \\end{array} $$ as $n \\rightarrow \\infty$ by USLLN in Lemma 7.10 . It follows that as $n \\rightarrow \\infty,$ we have $ \\hat{G}(\\hat{\\theta}) \\rightarrow G\\left(\\theta_{0}\\right) $ as $n \\rightarrow \\infty$ almost surely, and so $$ \\hat{G}(\\bar{\\theta})-G\\left(\\theta_{0}\\right)=[\\hat{G}(\\bar{\\theta})-\\hat{G}(\\hat{\\theta})]+\\left[\\hat{G}(\\hat{\\theta})-G\\left(\\theta_{0}","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:5:3","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.5.4 Asymptotic Efficiency of GMM 【Theorem 8.12】Asymptotic Efficiency of GMM: Put $ \\Omega_{0}=\\left[G\\left(\\theta_{0}\\right) V^{-1} G\\left(\\theta_{0}\\right)^{\\prime}\\right]^{-1} $ i.e., let $ W=V $. Then $ \\Omega-\\Omega_{0} $ is positive semi-definite (PSD) for all finite and nonsingular matrix $ W $, where $ \\Omega $ is given in Theorem 8.11. Proof: Observe that $ \\Omega-\\Omega_{0} $ is PSD if and only if $ \\Omega_{0}^{-1}-\\Omega^{-1} $ is PSD. For notational simplicity, put $ G_{0}=G\\left(\\theta_{0}\\right) $ and decompose $ V=V^{1 / 2} V^{1 / 2} , V^{-1}= V^{-1/2}V^{-1/2}$, where $ V $ is a $ q \\times q $ symmetric and nonsingular matrix. Noting $ (A B C)^{-1}=C^{-1} B^{-1} A^{-1} $ for nonsingular matrices $ A, B, $ and $ C $, we therefore consider $$ \\begin{aligned} \\Omega_{0}^{-1}-\\Omega^{-1} \u0026=G_{0}^{\\prime} V^{-1} G_{0}-G_{0}^{\\prime} W^{-1} G_{0}\\left[G_{0}^{\\prime} W^{-1} V W^{-1} G_{0}\\right]^{-1} G_{0}^{\\prime} W^{-1} G_{0} \\ \u0026=G_{0}^{\\prime} V^{-\\frac{1}{2}}\\left[I-V^{\\frac{1}{2}} W^{-1} G_{0}\\left[G_{0}^{\\prime} W^{-1} V W^{-1} G_{0}\\right]^{-1} G_{0}^{\\prime} W^{-1} V^{\\frac{1}{2}}\\right] V^{-\\frac{1}{2}} G_{0} \\ \u0026=G_{0}^{\\prime} V^{-\\frac{1}{2}} \\Pi V^{-\\frac{1}{2}} G_{0} \\end{aligned} $$ where $G_0 = G(\\theta_0)$ and the $ q \\times q $ matrix $$ \\Pi \\equiv I-V^{\\frac{1}{2}} W^{-1} G_{0}\\left[G_{0}^{\\prime} W^{-1} V W^{-1} G_{0}\\right]^{-1} G_{0}^{\\prime} W^{-1} V^{\\frac{1}{2}} $$ is a symmetric and idempotent matrix (i.e., $ \\Pi=\\Pi^{\\prime}, \\Pi^{2}=\\Pi $). It follows that we have $$ \\begin{aligned} \\Omega_{0}^{-1}-\\Omega^{-1} \u0026=\\left(G_{0}^{\\prime} V^{-\\frac{1}{2}} \\Pi\\right)\\left(\\Pi V^{-\\frac{1}{2}} G_{0}\\right) \\ \u0026=\\left(\\Pi V^{-\\frac{1}{2}} G_{0}\\right)^{\\prime}\\left(\\Pi V^{-\\frac{1}{2}} G_{0}\\right) \\end{aligned} $$ which is always PSD (why?). For any $\\lambda$, we always have $$ \\begin{aligned} \\lambda’(D’D)\\lambda\u0026=(\\lambda D)'(\\lambda D)' \\ \u0026 \\geq 0 \\end{aligned} $$ Thus matrix like $D’D$ is always PSD. This completes the proof. In practice, we make $\\hat{W} \\stackrel{a.s.}{\\rightarrow} V $, then the GMM is asymptotic efficient. ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:5:4","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.6 Criterion for Estimators Question: Which is the best estimator for $ \\theta $ ? For example, we have obtained two different estimators for population variance $ \\sigma^{2} $ one is the sample variance $$ S_{n}^{2}=(n-1)^{-1} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2} $$ and the other is the MLE estimator $$ \\hat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2} $$ Which estimator is better? ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:6:0","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.6.1 Mean Squared Error Criterion 【Definition 8.3】Mean Squared Error (MSE): Let $\\theta$ be a population parameter. The MSE of an estimator $ \\hat{\\theta}=\\hat{\\theta}{n}\\left(\\mathrm{X}^{n}\\right) $ of parameter $ \\theta $ is defined as $$ M S E{\\theta}(\\hat{\\theta})=E_{\\theta}(\\hat{\\theta}-\\theta)^{2} $$ where $ E_{\\theta}(\\cdot) $ denotes the expectation which is taken under the joint distribution $ f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right) $ of the random sample $X_n$; or equivalently under the sampling distribution of $X_n$. If $MSE_{\\theta}(\\hat{\\theta}) \\rightarrow 0$, then $\\hat{\\theta} \\stackrel{q.m.}{\\rightarrow} \\theta $ as $n \\to \\infty $. ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:6:1","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"后期再精细化整理 Remarks: $ \\operatorname{MSE}{\\theta}(\\hat{\\theta}) $ measures the average of variations or deviations of the estimator $ \\hat{\\theta} $ from the parameter $ \\theta $ The difference $ \\hat{\\theta}-\\theta $ is usually called the estimation error, so $ \\mathrm{MSE}{\\theta}(\\hat{\\theta}) $ is a measure of the magnitude of the estimation error. MSE is not the only criterion, but it is intuitive and analytically simple, and therefore is most commonly used in practice. 【Definition 8.4】Bias: The bias of a point estimator $ \\hat{\\theta} $ of parameter $ \\theta $ is defined as $$ \\operatorname{Bias}{\\theta}(\\hat{\\theta})=E{\\theta}(\\hat{\\theta})-\\theta $$ An estimator $ \\hat{\\theta} $ for $ \\theta $ is called an unbiased estimator for $ \\theta $ if $ \\operatorname{Bias}_{\\theta}(\\hat{\\theta})=0 $ Remarks: The bias $ \\operatorname{Bias}_{\\theta}(\\hat{\\theta}) $ measures the accuracy of estimator $ \\hat{\\theta} $. More specifically, the bias measures the closeness of all possible estimates on average to the true parameter value. It characterizes the degree of a systematic estimation error. An unbiased estimator gives the right answer on average in a large number of repeated estimates. That is, there is no systematic upward or downward estimation for the parameter $ \\theta $ 【Example 8.9】Suppose $ \\mathbf{X}^{n} $ is an IID random sample from some population with mean $ \\mu $ and variance $ \\sigma^{2} . $ Find an unbiased estimator for $ \\operatorname{var}{\\theta}\\left(\\bar{X}{n}\\right) $ Solution: $ \\operatorname{Put} \\theta=\\left(\\mu, \\sigma^{2}\\right) $ and $ \\tau=\\frac{\\sigma^{2}}{n} $ $ - $ Because $ \\operatorname{var}{\\theta}\\left(\\bar{X}{n}\\right)=\\frac{\\sigma^{2}}{n}, $ an unbiased estimator of $ \\tau $ can be given as follows: $$ \\hat{\\tau}=\\frac{S_{n}^{2}}{n} $$ because $ E_{\\theta}(\\hat{\\tau})=n^{-1} E_{\\theta}\\left(S_{n}^{2}\\right)=\\operatorname{var}_{\\theta}\\left(\\bar{X}_{n}\\right)=\\tau $ $ - $ It follows that $ \\operatorname{Bias}_{\\theta}(\\hat{\\tau})=E_{\\theta}(\\hat{\\tau})-\\tau=0 $ 【Example 8.10】Suppose $ \\mathbf{X}^{n} $ is an IID random sample from some population with mean $ \\mu $ and variance $ \\sigma^{2} $. Find an unbiased estimator for $ \\mu^{2} $ Solution: $ \\operatorname{Put} \\theta=\\left(\\mu, \\sigma^{2}\\right) $ $ - $ For parameter $ \\tau=\\mu^{2}, $ an unbiased estimator is $$ \\hat{\\tau}=\\bar{X}{n}^{2}-\\frac{S{n}^{2}}{n} $$ This follows because $$ \\begin{aligned} E(\\hat{\\tau}) \u0026=E_{\\theta}\\left(\\bar{X}_{n}^{2}\\right)-\\frac{E_{\\theta}\\left(S_{n}^{2}\\right)}{n} \\ \u0026=\\operatorname{var}_{\\theta}\\left(\\bar{X}_{n}\\right)+\\left[E_{\\theta}\\left(\\bar{X}_{n}\\right)\\right]^{2}-\\frac{E_{\\theta}\\left(S_{n}^{2}\\right)}{n} \\ \u0026=\\frac{\\sigma^{2}}{n}+\\mu^{2}-\\frac{\\sigma^{2}}{n} \\ \u0026=\\mu^{2}=\\tau \\end{aligned} $$ Intuitively, since the sample mean $ \\bar{X}_{n} $ is a good estimator for $ \\mu, $ so we expect that $ \\bar{X}_{n}^{2} $ is a good estimator for $ \\mu^{2} $. However, $ \\bar{X}_{n}^{2} $ is a nonlinear function of $ \\bar{X}_{n} $ which introduces a bias $ \\frac{\\sigma^{2}}{n} . $ This bias can be corrected by subtracting the unbiased estimator $ \\frac{S_{n}^{2}}{n} $ for $ \\frac{\\sigma^{2}}{n} $. 【Theorem 8.13】MSE Decomposition: $$ E_{\\theta}(\\hat{\\theta}-\\theta)^{2}=\\operatorname{var}_{\\theta}(\\hat{\\theta})+\\left[\\operatorname{Bias}_{\\theta}(\\hat{\\theta})\\right]^{2} $$ Proof: $$ \\begin{aligned} E_{\\theta}(\\hat{\\theta}-\\theta)^{2} \u0026=E_{\\theta}\\left[\\hat{\\theta}-E_{\\theta}(\\hat{\\theta})+E_{\\theta}(\\hat{\\theta})-\\theta\\right]^{2} \\ \u0026=E_{\\theta}\\left[\\hat{\\theta}-E_{\\theta}(\\hat{\\theta})\\right]^{2}+\\left[E_{\\theta}(\\hat{\\theta})-\\theta\\right]^{2}+2 E_{\\theta}\\left{\\left[\\hat{\\theta}-E_{\\theta}(\\hat{\\theta})\\right]\\left[E_{\\theta}(\\hat{\\theta})-\\theta\\right]\\right} \\ \u0026=E_{\\theta}\\left[\\hat{\\theta}-E_{\\theta}(\\hat{\\theta})\\right]^{2}+\\left[E_{\\theta}(\\hat{\\theta})-\\theta\\right]^{2} \\end{aligned} $$ where the cross-product term $$ \\begin{aligned} E_{\\theta}\\left{\\left[\\hat{\\theta}-E_{\\theta}(\\hat{\\theta})\\right]\\left[E_{\\","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:6:2","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.6.2 Best Unbiased Estimators Question: What is the best estimator if a class of estimators of parameter $ \\theta $ is available? We could define the best estimator as the one that has the smallest MSE. Unfortunately, such a best estimator is very difficult to obtain, because the class of estimators we have to compare is very huge. For simplicity, we focus on a class of unbiased estimators and find the best estimator within this class. 【Definition 8.6】Generalized Unbiased Estimator: $ \\hat{\\tau}=\\hat{\\tau}{n}\\left(\\mathrm{X}^{n}\\right) $ is an unbiased estimator for the parameter $ \\tau(\\theta) $ if $$ E{\\theta}(\\hat{\\tau})=\\tau(\\theta) \\text { for all } \\theta \\in \\Theta $$ When $ \\tau(\\theta)=\\theta, $ we return to Definition 8.4 of the unbiased estimator for parameter $ \\theta $. Example $ 13(8.13) . $ Suppose $ \\mathbf{X}^{n} $ is an IID $ \\left(\\mu, \\sigma^{2}\\right) $ random sample. Put $ \\theta=\\left(\\mu, \\sigma^{2}\\right) $ and $ \\tau(\\theta)= $ $ (\\mu-2)^{2} . $ Find an unbiased estimator for $ \\tau(\\theta) $ solution: $ - $ We first try an estimator $ \\tilde{\\tau}=\\left(\\bar{X}{n}-2\\right)^{2} $. Then $$ \\begin{aligned} E{\\theta}(\\tilde{\\tau}) \u0026=E_{\\theta}\\left(\\bar{X}_{n}-2\\right)^{2} \\ \u0026=E_{\\theta}\\left[\\left(\\bar{X}_{n}-\\mu+\\mu-2\\right)^{2}\\right] \\ \u0026=E_{\\theta}\\left(\\bar{X}_{n}-\\mu\\right)^{2}+(\\mu-2)^{2} \\ \u0026=\\frac{\\sigma^{2}}{n}+\\tau(\\theta) \\end{aligned} $$ It follows that $$ E_{\\theta}(\\tilde{\\tau})-\\tau(\\theta)=\\frac{\\sigma^{2}}{n} \\neq 0 $$ We then make a necessary bias correction: $$ \\hat{\\tau}=\\left(\\bar{X}_{n}-2\\right)^{2}-\\frac{1}{n} S_{n}^{2} $$ This bias-corrected estimator $ \\hat{\\tau} $ is unbiased for the transformed parameter $ \\tau(\\theta)= $ $$ (\\mu-2)^{2} $$ 【Definition 8.7】Uniform Best Unbiased Estimator: Let $ \\Gamma $ be a class of unbiased estimators of parameter $ \\tau(\\theta), $ where $ \\theta \\in \\Theta $ and $ \\Theta $ is a known parameter space. An estimator $ \\hat{\\tau}^{*} $ $ \\in \\Gamma $ is a uniformly best unbiased estimator for $ \\tau(\\theta) $ over parameter space $ \\Theta $ within the class $ \\Gamma $ of estimators if $ E_{\\theta}\\left(\\hat{\\tau}^{*}\\right)=\\tau(\\theta) $ for all $ \\theta \\in \\Theta $ $ \\operatorname{var}{\\theta}\\left(\\hat{\\tau}^{*}\\right) \\leq \\operatorname{var}{\\theta}(\\hat{\\tau}) $ for any estimator $ \\hat{\\tau} $ of $ \\tau(\\theta) $ in $ \\Gamma $ and for all $ \\theta \\in \\Theta $ Remarks: The estimator $ \\hat{\\tau}^{*} $ is a uniform minimum variance unbiased estimator (UMVUE) of $ \\tau(\\theta) $ over parameter space $ \\Theta $ within the class $ \\Gamma $ of estimators for $ \\tau(\\theta) $ Here, uniformity means that $ \\hat{\\tau}^{*} $ is always the best unbiased estimator for $ \\tau(\\theta) $ no matter what value the parameter $ \\theta $ will take in $ \\Theta $ (not just for a particular value of $ \\theta) $ 【Example 8.14】Let $ \\mathrm{X}^{n} $ be an $ \\mathrm{IID}\\left(\\mu, \\sigma^{2}\\right) $ random sample. Define a class of linear unbiased estimators of $ \\mu $ as follows: $$ \\Gamma=\\left{\\hat{\\mu}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R} | \\hat{\\mu}=\\sum_{i=1}^{n} c_{i} X_{i} \\text { for }\\left(c_{1}, \\cdots, c_{n}\\right)^{\\prime} \\in \\mathbb{R}^{n}\\right} $$ Show that $ \\hat{\\mu} $ is an unbiased estimator of $ \\mu $ for all $ n \\geq 1 $ if and only if $ \\sum_{i=1}^{n} c_{i}=1 $ Find the uniformly most efficient unbiased estimator of $ \\mu $ within the class $ \\Gamma $ of estimators for $ \\mu $ Solution: Note that we have $ \\hat{\\tau}=\\hat{\\mu}, $ and $ \\tau(\\theta)=\\mu $ in this application. Given $ \\hat{\\mu}=\\sum_{i=1}^{n} c_{i} X_{i} $ and $ \\theta=\\left(\\mu, \\sigma^{2}\\right), $ we have $ E_{\\theta}(\\hat{\\mu})=\\mu \\sum_{i=1}^{n} c_{i} . $ Therefore, if $ \\hat{\\mu} $ is an unbiased estimator for $ \\mu, $ i.e. if $$ E_{\\theta}(\\hat{\\mu})=\\mu \\text { for all } \\mu \\in \\mathbb{R} $$ we must have $$ \\sum_{i=1}^{n} c_{i}=1 $$ On the other hand, if $ \\sum_{i=1}^{n} c_{i}=1, $ then from the fact that $ E_{\\theta}(\\hat{\\mu})=\\mu \\sum_{i=1}^{n} c_{i}, $ we hav","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:6:3","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.7 Cramer-Rao Lower Bound Remarks: There is an alternative method of evaluating parameter estimators when the population distribution model $ f(x, \\theta) $ is available. For simplicity, we assume that parameter $ \\theta $ is a scalar here 【Theorem 8.14】Cramer-Rao Lower Bound: Let $ \\mathrm{X}^{n} $ be a random sample with joint PMF/ PDF $ f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right), $ and let $ \\hat{\\tau}=\\hat{\\tau}_{n}\\left(\\mathbf{X}^{n}\\right) $ be any estimator of parameter $ \\tau(\\theta) $ where $ E_{\\theta}(\\hat{\\tau}) $ is a differentiable function of $ \\theta . $ Suppose the joint PMF/ PDF $ f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right) $ of the random sample $ \\mathbf{X}^{n} $ satisfies the condition that $$ \\frac{d}{d \\theta} \\int_{\\mathbb{R}^{n}} h\\left(\\mathbf{x}^{n}\\right) f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right) d \\mathbf{x}^{n}=\\int_{\\mathbb{R}^{n}} h\\left(\\mathbf{x}^{n}\\right) \\frac{\\partial f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right)}{\\partial \\theta} d \\mathbf{x}^{n} $$ for any measurable function $ h: \\mathbb{R}^{n} \\rightarrow \\mathbb{R} $ with $ E_{\\theta}\\left|h\\left(\\mathbf{X}^{n}\\right)\\right|\u003c\\infty $, where $ E_{\\theta}(\\cdot) $ is taken over the joint PMF/ PDF $ f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right) $ of $ \\mathbf{X}^{n} . $ Then for all $ n\u003e0 $ and all $ \\theta \\in \\Theta $ $$ \\operatorname{var}_{\\theta}(\\hat{\\tau}) \\geq B_{n}(\\theta) \\equiv \\frac{\\left[\\frac{d E_{\\theta}(\\hat{\\tau})}{d \\theta}\\right]^{2}}{E_{\\theta}\\left[\\frac{\\partial \\ln f_{\\mathbf{X}} n\\left(\\mathbf{X}^{n}, \\theta\\right)}{\\partial \\theta}\\right]^{2}} $$ where $ B_{n}(\\theta) $ is called the Cramer-Rao lower bound. In particular, when $ \\hat{\\tau} $ is unbiased for parameter $ \\tau(\\theta), $ we have $$ B_{n}(\\theta)=\\frac{\\left[\\tau^{\\prime}(\\theta)\\right]^{2}}{E_{\\theta}\\left[\\frac{\\partial \\ln f_{\\mathbf{X}^{n}}\\left(\\mathbf{X}^{n}, \\theta\\right)}{\\partial \\theta}\\right]^{2}} $$ Unbiased estimator with $\\operatorname{var}_{\\theta}(\\hat{\\tau}) = B_{n}(\\theta)$ must be a best unbiased estimator. Proof: We consider the case of continuous distributions; the proof for the case of discrete distributions is similar. Suppose we have: (1) $$ \\qquad E_{\\theta}\\left[\\frac{\\partial \\ln f_{\\mathbf{X}^{n}}\\left(\\mathbf{X}^{n}, \\theta\\right)}{\\partial \\theta}\\right]^{2}=\\operatorname{var}_{\\theta}\\left[\\frac{\\partial \\ln f_{\\mathbf{X}^{n}}\\left(\\mathbf{X}^{n}, \\theta\\right)}{\\partial \\theta}\\right] $$ (2) $$ \\frac{d E_{\\theta}(\\hat{\\tau})}{d \\theta}=\\operatorname{cov}_{\\theta}\\left[\\hat{\\tau}_{n}\\left(\\mathbf{X}^{n}\\right), \\frac{\\partial \\ln f_{\\mathbf{X}^{n}}\\left(\\mathbf{X}^{n}, \\theta\\right)}{\\partial \\theta}\\right] $$ Then by the Cauchy-Schwarz inequality, we have \r $$ \\left{\\operatorname{cov}{\\theta}\\left[\\hat{\\tau}{n}\\left(\\mathbf{X}^{n}\\right), \\frac{\\partial \\ln f_{\\mathbf{X}^{n}}\\left(\\mathbf{X}^{n}, \\theta\\right)}{\\partial \\theta}\\right]\\right}^{2} \\leq \\operatorname{var}_{\\theta}(\\hat{\\tau}) \\cdot \\operatorname{var}_{\\theta}\\left[\\frac{\\partial \\ln f_{\\mathbf{X}^{n}}\\left(\\mathbf{X}^{n}, \\theta\\right)}{\\partial \\theta}\\right] $$ This implies that $$ \\begin{aligned} \\operatorname{var}{\\theta}(\\hat{\\tau}) \u0026 \\geq \\frac{\\left{\\operatorname{cov}{\\theta}\\left[\\hat{\\tau}, \\frac{\\partial \\ln f_{\\mathbf{X}^{n}}\\left(\\mathbf{X}^{n}, \\theta\\right)}{\\partial \\theta}\\right]\\right}^{2}}{\\operatorname{var}_{\\theta}\\left[\\frac{\\partial \\ln f_{\\mathbf{X}^{n}}\\left(\\mathbf{X}^{n}, \\theta\\right)}{\\partial \\theta}\\right]} \\ \u0026=\\frac{\\left[\\frac{d E_{\\theta}(\\hat{\\tau})}{d \\theta}\\right]^{2}}{E_{\\theta}\\left[\\frac{\\partial \\ln f_{\\mathbf{X}^{n}\\left(\\mathbf{X}^{n}, \\theta\\right)}}{\\partial \\theta}\\right]^{2}} \\ \u0026=B_{n}(\\theta) \\end{aligned} $$ given results (1) and (2) above. Therefore, it suffices to show results (1) and (2). We first prove result (1). We shall show that the mean of the score function $ \\frac{\\partial \\ln f_{\\mathbf{x}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right)}{\\partial \\theta}","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:7:0","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"8.8 Conclusion In this chapter, we first introduce two important estimation methods: MLE and MME, the latter having been extended by econometricians to GMM. MLE is based on a correctly specified parametric PMF/PDF model for the population distribution, while MME and GMM are based on a set of moment conditions on the population. Because it utilizes the information on the joint distribution of the random sample, MLE is expected to be more efficient than MME and GMM, unless for the latter, the sample moments used are sufficient statistics for the parameters of interest. GMM does not require the knowledge of the population distribution of the DGP. To gain insight into the properties of the MLE and MME / GMM, we develop the asymptotic theory for the proposed two estimators. In order to evaluate different estimators for the same population parameter, we introduce MSE to measure the closeness between an estimator and the population parameter. We then develop two important methods to evaluate unbiased parameter estimators. One is the Lagrange multiplier approach, which does not require the knowledge of the population $ \\mathrm{PMF} / \\mathrm{PDF} $ the other is the Cramer-Rao lower bound approach which requires the knowledgement of the joint likelihood function of the random sample. We have focused on the methods to find the best unbiased estimator(s). It should be emphasized that the best unbiased estimator may not the most efficient estimator. A biased estimator may outperform an unbiased estimator by substantially reducing the variance with a bit increase in the bias. This is the case with many machine learning algorithms. ","date":"0001-01-01","objectID":"/8.-parameter-estimation-and-evaluation/:8:0","tags":null,"title":"","uri":"/8.-parameter-estimation-and-evaluation/"},{"categories":null,"content":"9. Hypothesis Testing ","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:0:0","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.1 Introduction Given a realization $x_{1}, \\ldots, x_{n}$ of a random sample $X_{1}, \\ldots, X_{n}$ from some population distribution $f(x ; \\theta),$ we want to know whether the true parameter $\\theta$ belongs to some specific subset $\\Theta_{0}$ of the parameter space $\\Theta$. 【Example 9.1】Return to Education: Let $\\theta$ measure the change in hourly wage given another year of education, holding all other factors fixed. Labor economists are interested in testing whether the return to education, controlling other factors, is zero. That is, whether or not $ \\theta $ equal to zero. ","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:1:0","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.1.1 Hypothesis 【Definition 9.1】Hypothesis: a hypothesis is a statement about the population or population parameter. The two complementary hypotheses in a hypothesis testing problem are called the null hypothesis and the alternative hypothesis, denoted by $H_{0}$ and $H_{1}\\left(\\text { or } H_{A}\\right),$ respectively. **$H_{0}$ and $ H_1 $** A null hypothesis is a statement about the population or some attributes of the population. The alternative hypothesis is that the statement in the null hypothesis is false. The goal of hypothesis testing is to decide, based on an observed data set $\\mathbf{x}^{n}$ generated from a population, which of two complementary hypotheses is true. Suppose a random sample $\\mathrm{X}^{n}$ is generated from a population distribution $f(x, \\theta)$ with some unknown value of parameter $\\theta \\in \\Theta$, where $\\Theta$ is a known finite-dimensional parameter space. In hypothesis testing, the parameter space $\\Theta$ is divided into two mutually exclusive and collectively exhaustive subsets $\\Theta_{0}$ and $\\Theta_{A},$ namely $\\Theta_{0} \\cap \\Theta_{A}=\\varnothing$ and $\\Theta_{0} \\cup \\Theta_{A}=\\Theta$. The problem is to determine to which of these two subsets the true value of $\\theta$ belongs. That is, based upon an observed data set $\\mathrm{x}^{n}$, one is trying to choose between the two hypotheses $$ \\mathbb{H}{0}: \\theta \\in \\Theta{0} $$ ​ versus $$ \\mathbb{H}{A}: \\theta \\in \\Theta{A} $$ The first hypothesis $\\mathbb{H}{0}$ is called the null hypothesis, and the second, $\\mathbb{H}{A}$, is called the alternative hypothesis. $\\mathbb{H}{0}$ is called the “null” hypothesis because it is often stated as “no effects” or “no relationship”. One example, is $\\mathbb{H}{0}: \\theta=0$ versus $\\mathbb{H}_{A}: \\theta \\neq 0,$ as is the case of Example 9.1 【Example 9.2】If $\\theta$ denotes the proportion of defective items for some manufactured product. We might be interested in testing $\\mathbb{H}{0}: \\theta \\leq \\theta{0}$ versus $\\mathbb{H}{A}: \\theta\u003e\\theta{0},$ where $\\theta_{0}$ is the maximum acceptable proportion of defective items. The null hypothesis states that the proportion of defective items is below an unacceptable threshold. This hypothesis testing is the basic idea for **statistical quality control**. 【Example 9.3】Constant Return to Scale Hypothesis: a production function $$ Y=F(L, K) $$ tells how much output $Y$ to produce using inputs of labor $L$ and capital $K$. A production technology is said to display a constant return to scale if the output increases by the proportion as the inputs increase; that is, for all $\\lambda\u003e0$, $$ \\lambda F(L, K)=F(\\lambda L, \\lambda K) $$ Suppose a production function is given by $$ Y=A K^{\\alpha} L^{\\beta} $$ where $Y$ is the output, $K$ and $L$ are the capital and labor inputs, $A$ is a constant, and $\\theta=(\\alpha, \\beta)$ is a parameter vector. Then the constant return to scale hypothesis can be stated as $$ \\mathbb{H}_{0}: \\alpha+\\beta=1 $$ The alternative hypothesis $\\mathbb{H}_{1}: \\alpha+\\beta \\neq 1$ consists of two cases: $\\alpha+\\beta\u003e1$ and $\\alpha+\\beta\u003c1$, which imply an increasing return to scale and a decreasing return to scale respectively. The hypotheses can be divided into two basic categories simple hypotheses and composite hypotheses. 【Definition 9.2】Simple Hypothesis Versus Composite Hypothesis: A hypothesis is simple if and only if it contains exactly one population, i.e. parameter $\\theta$ only has one value. If the hypothesis contains more than one population, it is called a composite hypothesis. In Example $9.1, \\mathbb{H}{0}$ contains only one parameter value, so $\\mathbb{H}{0}$ is a simple hypothesis. In contrast, the null hypotheses in Examples 9.2 and 9.3 contain more than one parameter values, so they are composite hypotheses. ","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:1:1","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.1.2 Hypothesis Testing 【Definition 9.3】Hypothesis Testing: A hypothesis testing procedure or a hypothesis test is a decision rule that specifies for what sample values $\\mathbf{x}^{n}$ the decision is made to accept $\\mathbb{H}_{0}$ as true, and for what sample values $\\mathbf{x}^{n}, \\mathbb{H}{0}$ is rejected and $\\mathbb{H}{A}$ is accepted as true. The key to the construction of the decision rule for a hypothesis testing procedure is to determine the rules of rejection or acceptance of the null hypothesis. 【Definition 9.4】Critical Region or Rejection Region: The set $\\mathbb{C}$ of the sample points of the random sample $\\mathrm{X}^{n}$ for which $\\mathbb{H}_{0}$ will be rejected is called the rejection region or critical region. The complement of the rejection region is called the acceptance region. Remarks: A standard approach to hypothesis testing is to choose a statistic $T\\left(\\mathbf{X}^{n}\\right)$ and use it to divide the sample space of $\\mathrm{X}^{n}$ into two mutually exclusive and exhaustive regions $$ \\mathbb{A}_{n}(c)=\\left{\\mathbf{x}^{n}: T\\left(\\mathbf{x}^{n}\\right) \\leq c\\right} $$ and $$ \\mathbb{C}_{n}(c)=\\left{\\mathbf{x}^{n}: T\\left(\\mathbf{x}^{n}\\right)\u003ec\\right} $$ for some pre-specified constant $c$. The first region, $\\mathbb{A}{n}(c),$ is the acceptance region, and the second, $\\mathbb{C}{n}(c),$ is the rejection region. The cutoff point $c$ is called the critical value and $T\\left(\\mathbf{X}^{n}\\right)$ is called a test statistic. An important issue in hypothesis testing is to determine a suitable value of $c$ given a data set $\\mathbf{x}^{n} .$ In general, we need to know the sampling distribution of $T\\left(\\mathbf{X}^{n}\\right)$ under $\\mathbb{H}_{0}$ in order to determine the threshold value $c$. 【Example 9.4】Suppose $\\mathrm{X}^{n}$ is an IID $ N\\left(\\mu, \\sigma^{2}\\right)$ random sample, where $\\mu$ is unknown but $\\sigma^{2}$ is known. We are interested in testing for $\\mathbb{H}{0}: \\mu=\\mu{0}$ versus $\\mathbb{H}{A}: \\mu \\neq \\mu{0},$ where $\\mu_{0}$ is a known number. Here, $\\Theta_{0}=\\left{\\mu_{0}\\right}$ contains one parameter value $\\mu_{0}$ and $\\Theta_{A}$ contains all parameter values on the real line $\\mathbb{R}$ except $\\mu_{0}$. Remarks: To test the null hypothesis $\\mathbb{H}{0}: \\mu=\\mu{0},$ we consider the following test statistic $$ T\\left(\\mathbf{X}^{n}\\right)=\\frac{\\bar{X}{n}-\\mu{0}}{\\sigma / \\sqrt{n}} $$ where $\\sigma$ is known. Under the null hypothesis $\\mathbb{H}{0}: \\mu=\\mu{0},$ we have $$ \\begin{aligned} T\\left(\\mathbf{X}^{n}\\right) \u0026=\\frac{\\bar{X}{n}-\\mu{0}}{\\sigma / \\sqrt{n}} \\ \u0026 \\sim N(0,1) \\end{aligned} $$ Under $\\mathbb{H}{A}: \\mu \\neq \\mu{0}$ $$ \\begin{aligned} T\\left(\\mathbf{X}^{n}\\right) = \u0026\\frac{\\bar{X}{n}-\\mu}{\\sigma / \\sqrt{n}}+\\frac{\\mu-\\mu{0}}{\\sigma / \\sqrt{n}} \\ \\sim \u0026N\\left(\\frac{\\sqrt{n}\\left(\\mu-\\mu_{0}\\right)}{\\sigma}, 1\\right) \\end{aligned} $$ which diverges to infinity with probability approaching one as $n \\rightarrow \\infty$. One can accept $\\mathbb{H}{A}$ if $\\left|T\\left(\\mathbf{X}^{n}\\right)\\right|$ is large. How large $T\\left(\\mathbf{X}^{n}\\right)$ should be in order to be considered as “large” is determined by the sampling distribution $(N(0,1))$ of $T\\left(\\mathbf{X}^{n}\\right)$ under $\\mathbb{H}{0}$. Specifically, by setting $c=z_{\\alpha / 2},$ where $z_{\\alpha / 2}$ is the upper-tailed critical value of $N(0,1)$ at level $\\frac{\\alpha}{2} \\in(0,1),$ i.e., $P\\left(Z\u003ez_{\\alpha / 2}\\right)=\\frac{\\alpha}{2}$ where $Z \\sim N(0,1),$ we can define the acceptance and rejection regions as follows: $$ \\begin{array}{l} \\mathrm{A}{n}(c)=\\left{\\mathrm{x}^{n}:\\left|\\frac{\\bar{x}{n}-\\mu_{0}}{\\sigma / \\sqrt{n}}\\right| \\leq z_{\\frac{\\alpha}{2}}\\right} \\ \\mathbb{C}_{n}(c)=\\left{\\mathrm{x}^{n}:\\left|\\frac{\\bar{x}_{n}-\\mu_{0}}{\\sigma / \\sqrt{n}}\\right|\u003ez_{\\frac{\\alpha}{2}}\\right} \\end{array} $$ The $N(0,1)$ test decision rule: Accept $\\mathbb{H}{0}: \\mu=\\mu{0}$ at the significance level $\\alpha$ if $$ \\mathbf{x}^{n} \\in \\mathbb{A}_{n}(c) $$ Reject $\\mathb","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:1:2","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.1.3 Evaluation of Hypothesis Testing 【Definition 9.5】Power of Test If $\\mathbb{C}$ is the rejection region of a test of the null hypothesis $\\mathbb{H}{0}: \\theta \\in \\Theta{0},$ then the function $\\pi(\\theta)=P_{\\theta}\\left(\\mathrm{X}^{n} \\in \\mathbb{C}\\right)$ is called the power of the test with the rejection region $\\mathbb{C},$ where $P_{\\theta}(\\cdot)$ is the probability measure when the random sample follows the distribution $f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta\\right)$ The power function $\\pi(\\theta)$ is the probability of rejecting $\\mathbb{H}_{0}$. In Example 9.4, the power of the test statistic $T\\left(\\mathbf{X}^{n}\\right)=\\sqrt{n}\\left(\\bar{X}{n}-\\mu{0}\\right) / \\sigma$ is given by $$ \\pi(\\mu)=P\\left(\\left|\\frac{\\bar{X}{n}-\\mu{0}}{\\sigma / \\sqrt{n}}\\right|\u003ez_{\\alpha / 2}\\right) $$ 【Definition 9.6】Type I and Type II Errors: If $\\mathbb{H}{0}: \\theta \\in \\Theta{0}$ holds and the observed data $\\mathbf{x}^{n}$ falls into the critical region $\\mathbb{C},$ then a Type I error is made. The probability of making Type I error is $$ \\alpha(\\theta) \\equiv P_{\\theta}\\left(\\mathbf{X}^{n} \\in \\mathbb{C} | \\mathbb{H}_{0}\\right) $$ If $\\mathbb{H}{A}: \\theta \\in \\Theta{0}^{c}$ holds and the observed data $\\mathbf{x}^{n}$ is in the acceptance region, then a Type II error is made. The probability of making Type II error is $$ \\begin{aligned} \\beta(\\theta) \u0026 \\equiv P_{\\theta}\\left(\\mathrm{X}^{n} \\in \\mathbb{A} | \\mathbb{H}_{A}\\right) \\ \u0026=1-P_{\\theta}\\left(\\mathrm{X}^{n} \\in \\mathbb{C} | \\mathbb{H}_{A}\\right) \\end{aligned} $$ Remarks: Under $\\mathbb{H}{0}: \\theta \\in \\Theta{0}$, the power function $\\pi(\\theta)$ is the probability of making Type I error, namely incorrectly rejecting a correct null hypothesis. Type I errors are unavoidable because under $\\mathbb{H}_{0}$, because a test statistic $T\\left(\\mathbf{X}^{n}\\right)$ may still take large values with nontrivial (though small) probabilities. Under $\\mathbb{H}{A}: \\theta \\in \\Theta{0}^{c}$, the power function $\\pi(\\theta)$ is the probability of rejecting a false null hypothesis, and it is equal to $1-\\beta(\\theta)$, where $\\beta(\\theta)$ is the probability of making Type II error, namely accepting an false null hypothesis. A test is called unbiased if $P\\left[T\\left(\\mathrm{X}^{n}\\right)\u003ec | \\mathbb{H}{A}\\right]\u003eP\\left[T\\left(\\mathrm{X}^{n}\\right)\u003ec | \\mathbb{H}{0}\\right]$. That is, the probability to reject $\\mathbb{H}{0}$ when $\\mathbb{H}{0}$ is false is strictly larger than when it is true. There exists a trade off between Type I errors and Type II errors given any sample size $n$. For any given $n$, if the critical region $\\mathbb{C}$ shrinks, the probability of making Type error deceases, but the probability of making Type II error increases. similarly, if the critical region $\\mathbb{C}$ increases, the Type II error $\\beta(\\theta)$ decreases, but Type I error $\\alpha(\\theta)$ increases. Usually, hypothesis tests are evaluated and compared through their probabilities of making mistakes. The classical approach to hypothesis testing is to bound the probability of Type I error by some value $\\alpha \\in(0,1)$ over all values of $\\theta$ in $\\Theta_{0}$ and to try to find a test that minimizes the probability of a Type II error over all values of $\\theta$ in $\\Theta_{A}$. Specifically, one is trying to find a test statistic $T\\left(\\mathbf{X}^{n}\\right)$ that satisfies $$ P\\left[T\\left(\\mathbf{X}^{n}\\right)\u003ec | \\mathbb{H}{0}\\right] \\leq \\alpha $$ for all $\\theta \\in \\Theta{0}$ and has the property that $$ P\\left[T\\left(\\mathbf{X}^{n}\\right)\u003ec | \\mathbb{H}{A}\\right] \\geq P\\left[G\\left(\\mathbf{X}^{n}\\right)\u003ec | \\mathbb{H}{A}\\right] $$ for any other test statistic $G\\left(\\mathbf{X}^{n}\\right)$ for all $\\theta \\in \\Theta_{A}$. In other words, one is trying to find a test statistic $T\\left(\\mathrm{X}^{n}\\right)$ that has the best power while its Type I error is under control. A test $T\\left(\\mathbf{X}^{n}\\right)$ that has these properties is called uniformly most powerfu","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:1:3","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.1.4 Gap Between Economic Hypothesis and Statistical Hypothesis To test a economic hypothesis, we have to transform it into a statistical hypothesis and then test the statistical hypothesis using an observed economic data. In transforming an economic hypothesis into a statistical hypothesis, some auxiliary assumptions are often imposed. This induces a gap between the original economic hypothesis and the resulting statistical hypothesis. This gap may cause some problem in economic interpretation of the empirical results testing the statistical hypothesis. 【Example 9.5】Efficient Market Hypothesis Suppose $R_{t}$ is the return on some asset portfolio in time period $t,$ and $I_{t-1}=\\left(R_{t-1}, R_{t-2}, \\cdots\\right)$ denotes the historical asset return information available at time $t-1 .$ The asset market is called informationally weakly efficient if $$ E\\left(R_{t} | I_{t-1}\\right)=E\\left(R_{t}\\right) $$ That is, the historical asset return information has no predictive power for future asset return To test this economic hypothesis, one can consider a linear autoregressive model $$ R_{t}=\\alpha_{0}+\\sum_{j=1}^{k} \\alpha_{j} R_{t-j}+\\varepsilon_{t} $$ where $\\varepsilon_{t}$ is a stochastic disturbance. Under the efficient market hypothesis (EMH), we have $$ \\mathbb{H}{0}: \\alpha{1}=\\alpha_{2}=\\cdots=\\alpha_{k}=0 $$ If one has evidence that at least one $\\alpha_{j}, j \\in{1, \\cdots, k},$ is not zero, it will imply that the efficient market hypothesis doesn’t hold. However, when one does not reject the statistical hypothesis $\\mathbb{H}_{0},$ this does not necessarily imply that the original economic hypothesis-the efficient market hypothesis holds. The reason is that the linear autoregressive model is just one of many (possibly infinite) ways to test predictability of the historical asset returns for future asset return. In other words, the predictability may arise in a nonlinear manner. Therefore, there exists a gap between the efficient market hypothesis and the statistical hypothesis $\\mathbb{H}{0}$. Because of this gap, when one does not reject $\\mathbb{H}{0}$ one can only say that no evidence against the efficient market hypothesis is found rather than conclude that the efficient market hypothesis holds. ","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:1:4","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.2 Neyman-Pearson Lemma 【Theorem 9.1】Neyman-Pearson Lemma: Consider testing a simple null hypothesis $\\mathbb{H}{0}: \\theta=\\theta{0}$ versus a simple alternative hypothesis $\\mathbb{H}{A}: \\theta=\\theta{1},$ where the PMF/PDF of the random sample $\\mathrm{X}^{n}$ corresponding to $\\theta_{i}, i \\in{0,1},$ is $f_{\\mathrm{X}^{n}}\\left(\\mathrm{x}^{n}, \\theta_{i}\\right) .$ Suppose a test with rejection and acceptance regions $\\mathbb{C}_{n}(c)$ and $\\mathbb{A}_{n}(c)$ respectively is defined as follows: (a)​ $$ \\mathbb{C}{n}(c)=\\left{\\mathbf{x}^{n}: \\frac{f{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta_{1}\\right)}{f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta_{0}\\right)}\u003ec\\right} $$ and $$ \\mathbb{A}{n}(c)=\\left{\\mathbf{x}^{n}: \\frac{f{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta_{1}\\right)}{f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta_{0}\\right)} \\leq c\\right} $$ for some constant $c \\geq 0,$ and (b)​ $$ P\\left[\\mathrm{X}^{n} \\in \\mathbb{C}{n}(c) | \\mathbb{H}{0}\\right]=\\alpha $$ Then [Sufficiency] Any test that satisfies conditions (a) and (b) is a uniformly most powerful level $\\alpha$ test. [Necessity] If there exists a test satisfying conditions (a) and (b) with $c\u003e0$, then every uniformly most powerful level $\\alpha$ test is a size $\\alpha$ test ( i.e., satisfying condition (b) ), and every uniformly most powerful level $\\alpha$ test satisfies condition (a) except perhaps on a set $A$ in the sample space of $\\mathbf{X}^{n}$ satisfying $P\\left(\\mathbf{X}^{n} \\in A | \\mathbb{H}{0}\\right)=P\\left(\\mathbf{X}^{n} \\in A | \\mathbb{H}{A}\\right)=0$. Proof: Note that $$ \\begin{aligned} P\\left[\\mathbf{X}^{n}\\in \\mathbb{C}{n}(c) | \\mathbb{H}{0}\\right] \u0026=E\\left{1\\left[\\mathbf{X}^{n} \\in \\mathbb{C}{n}(c)\\right] | \\mathbb{H}{0}\\right} \\ \u0026=\\int 1\\left[\\mathbf{x}^{n} \\in \\mathbb{C}{n}(c)\\right] f\\left(\\mathbf{x}^{n}, \\theta{0}\\right) d \\mathbf{x}^{n} \\end{aligned} $$ where $1(\\cdot)$ is the indicator function. [Sufficiency] We first show that a test (denoted as $T\\left(\\mathbf{X}^{n}\\right)$ ) that satisfies conditions (a) and (b) is uniformly most powerful. Suppose there is another test (denoted $\\left.T_{1}\\left(\\mathrm{X}^{n}\\right)\\right)$ with $E\\left{1\\left[\\mathrm{X}^{n} \\in \\mathbb{C}_{1 n}\\right] | \\mathbb{H}_{0}\\right} \\leq \\alpha$. (The test $T_{1}\\left(\\mathrm{X}^{n}\\right)$ need not be a likelihood ratio test.) We shall show that $T_{1}\\left(\\mathrm{X}^{n}\\right)$ is not more powerful than $T\\left(\\mathbf{X}^{n}\\right)$. Observe that if $1\\left[\\mathrm{x}^{n} \\in \\mathbb{C}{n}(c)\\right]\u003e1\\left[\\mathrm{x}^{n} \\in \\mathbb{C}{1 n}\\right]$, i.e. $\\mathrm{x}^{n} \\in \\mathbb{C}{n}(c)$ and $\\mathrm{x}^{n} \\notin \\mathbb{C}{1 n}$, then the sample point $\\mathrm{x}^{n}$ is in the critical region $\\mathbb{C}{n}(c)$ of the test $T\\left(\\mathbf{X}^{n}\\right),$ and so $f{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta_{1}\\right)\u003ec f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta_{0}\\right) ;$ on the other hand, if $1\\left[\\mathrm{x}^{n} \\in \\mathbb{C}{n}(c)\\right]\u003c1\\left[\\mathrm{x}^{n} \\in \\mathbb{C}{1 n}\\right]$, i.e. $\\mathrm{x}^{n} \\notin \\mathbb{C}{n}(c)$ and $ \\mathrm{x}^{n} \\in \\mathbb{C}{1 n} $, then the sample point $\\mathrm{x}^{n}$ is in the acceptance region $\\mathbb{A}{n}(c)$ of the test $T\\left(\\mathbf{X}^{n}\\right),$ and so $f{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta_{1}\\right) \\leq c f_{\\mathbf{X}^{n}}\\left(\\mathbf{x}^{n}, \\theta_{0}\\right) .$ In either case, we have: $$ \\left{1\\left[\\mathrm{x}^{n} \\in \\mathbb{C}_{n}(c)\\right]-1\\left[\\mathrm{x}^{n} \\in \\mathbb{C}_{1 n}\\right]\\right}\\left[f_{\\mathrm{X}^{n}}\\left(\\mathrm{x}^{n}, \\theta_{1}\\right)-c f_{\\mathrm{X}^{n}}\\left(\\mathrm{x}^{n}, \\theta_{0}\\right)\\right] \\geq 0 $$ Thus, $$ \\int_{\\mathbb{R}^{n}}\\left{1\\left[\\mathrm{x}^{n} \\in \\mathbb{C}_{n}(c)\\right]-1\\left[\\mathrm{x}^{n} \\in \\mathbb{C}_{1 n}\\right]\\right}\\left[f_{\\mathrm{X}^{n}}\\left(\\mathrm{x}^{n}, \\theta_{1}\\right)-c f_{\\mathrm{X}^{n}}\\left(\\mathrm{x}^{n}, \\theta_{0}\\right)\\right] d \\mathrm{x}^{n} \\geq 0 $$ This implies $$","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:2:0","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.3 Wald Test Consider the hypotheses of interest $$ \\mathbb{H}{0}: g(\\theta)=0 $$ versus $$ \\mathbb{H}{A}: g(\\theta) \\neq 0 $$ where $ g: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{q} $ is a continuously differentiable $ q $-dimensional vector-valued function of a $ p $-dimensional parameter vector $ \\theta $ The integer $ q $ is the number of restrictions on parameter vector $ \\theta $. We assume $ q \\leq p $. One important example is a linear vector-valued function $$ g(\\theta)=R \\theta-r $$ where $ R $ is a $ q \\times p $ known constant matrix, $ r $ is a $ q \\times 1 $ known constant vector. The null hypothesis $$ \\mathbb{H}_{0}: R \\theta=r $$ imposes $ q $ linear restrictions on the $ p $-dimensional parameter vector $ \\theta $. ","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:3:0","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.3.1 Regularity Conditions 【Assumption W.1】Asymptotic Normality: $ \\sqrt{n}\\left(\\hat{\\theta}-\\theta_{0}\\right) \\stackrel{d}{\\rightarrow} N(0, V)$, where $ V $ is a $ p \\times p $ symmetric bounded and nonsingular matrix, $ \\theta_{0} $ is the true parameter value which is an interior point in $ \\Theta, $ and $ \\Theta $ is a compact parameter space. Assumption W.1 allows the estimator $ \\hat{\\theta} $ to be any root-$ n $ consistent asymptotically normal estimator. Both MLE and GMM estimator satisfy Assumption W.1. 【Assumption W.2】Consistent Variance Estimator: $ \\hat{V} \\stackrel{p}{\\rightarrow} V $ as $ n \\rightarrow \\infty $. Assumption W.2 assumes that there exists a consistent estimator $ \\hat{ V} $ for the asymptotic variance $ V $ of $ \\sqrt{n}\\left(\\hat{\\theta}-\\theta_{0}\\right) . $ 【Assumption W.3】Smooth Condition on Restriction Function: $ g: \\mathbb{R}^{p} \\rightarrow \\mathbb{R}^{q} $ is a continuously differentiable function of $ \\theta \\in \\Theta, $ and the $ q \\times p $ matrix $ G\\left(\\theta_{0}\\right)=\\frac{\\partial}{\\partial \\theta} g\\left(\\theta_{0}\\right) $ has rank $ q $ (i.e., full rank), where $ q \\leq p $. Let $ g(\\theta)=[g_1(\\theta), g_2(\\theta), \\ldots, g_q(\\theta)]' $, then the gradient of $g(\\theta) $ is $$ G(\\theta)=\\left[\\begin{array}{cc} \\frac{\\partial g_{1}(\\theta)}{\\partial \\theta_{1}} \u0026 \\frac{\\partial g_{1}(\\theta)}{\\partial \\theta_{2}} \u0026\\cdots \u0026\\frac{\\partial g_{1}(\\theta)}{\\partial \\theta_{p}} \\ \\frac{\\partial g_{2}(\\theta)}{\\partial \\theta_{1}} \u0026 \\frac{\\partial g_{2}(\\theta)}{\\partial \\theta_{2}} \u0026\\cdots \u0026\\frac{\\partial g_{2}(\\theta)}{\\partial \\theta_{p}} \\ \\cdots \u0026 \\cdots \u0026 \\cdots \u0026 \\cdots \\ \\frac{\\partial g_{q}(\\theta)}{\\partial \\theta_{1}} \u0026 \\frac{\\partial g_{q}(\\theta)}{\\partial \\theta_{2}} \u0026\\cdots \u0026\\frac{\\partial g_{q}(\\theta)}{\\partial \\theta_{p}} \\end{array}\\right] $$ Assumption W.3 is a regularity condition on the restriction function $ g(\\cdot) . $ The full rank condition for the $ q \\times p $ matrix $ G\\left(\\theta_{0}\\right) $ and $ q \\leq p $ ensure that the $ q \\times q $ symmetric matrix $ G\\left(\\theta_{0}\\right) V G\\left(\\theta_{0}\\right)^{\\prime} $ is nonsingular. ","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:3:1","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.3.2 Wald Test 【Theorem 9.3】Wald Test: Suppose Assumptions W.1-W.3 and $ \\mathbb{H}{0} $ hold. Then as $ n \\rightarrow \\infty $ $$ W=n \\cdot g(\\hat{\\theta})^{\\prime}\\left[G(\\hat{\\theta}) \\hat{V} G(\\hat{\\theta})^{\\prime}\\right]^{-1} g(\\hat{\\theta}) \\stackrel{d}{\\rightarrow} \\chi{q}^{2} $$ Proof: To test $ \\mathbb{H}{0}: g\\left(\\theta{0}\\right)=0, $ a natural approach is to base a test on statistic $ g(\\hat{\\theta}), $ where $ \\hat{\\theta} $ is a consistent estimator of $ \\theta_{0} . $ Because $ g(\\cdot) $ is continuous, we always have $ g(\\hat{\\theta}) \\stackrel{p}{\\rightarrow} g\\left(\\theta_{0}\\right) $ whenever $ \\hat{\\theta} \\stackrel{p}{\\rightarrow} \\theta_{0} $ as $ n \\rightarrow \\infty . $ It follows that $ g(\\hat{\\theta}) $ will be close to zero under $ \\mathbb{H}_{0} $ and will converge to a nonzero limit under $ \\mathbb{H}_{A} $. Thus, we can test $ \\mathbb{H}_{0} $ by checking whether $ g(\\hat{\\theta}) $ is close to zero. How large the value of $ g(\\hat{\\theta}) $ should be in order to be considered as significantly different from zero will be determined by the sampling distribution of $ g(\\hat{\\theta}) $ under $ \\mathbb{H}_{0} $. By the mean value theorem/中值定理, we have $$ g(\\hat{\\theta})=g\\left(\\theta_{0}\\right)+G(\\bar{\\theta})\\left(\\hat{\\theta}-\\theta_{0}\\right) $$ where $ \\bar{\\theta}=\\lambda \\hat{\\theta}+(1-\\lambda) \\theta_{0} $ for some $ \\lambda \\in[0,1], $ and the gradient function $$ G(\\theta)=\\frac{d g(\\theta)}{d \\theta} $$ is a $ q \\times p $ matrix. Given that $ \\left|\\bar{\\theta}-\\theta_{0}\\right|=\\left|\\lambda\\left(\\hat{\\theta}-\\theta_{0}\\right)\\right| \\leq\\left|\\hat{\\theta}-\\theta_{0}\\right| \\stackrel{p}{\\rightarrow} 0 $ as $ n \\rightarrow \\infty $ and the continuity of $ G(\\cdot), $ we have $ G(\\bar{\\theta}) \\stackrel{p}{\\rightarrow} G\\left(\\theta_{0}\\right) $ as $ n \\rightarrow \\infty . $ Then by the asymptotic normality that $ \\sqrt{n}\\left(\\hat{\\theta}-\\theta_{0}\\right) \\stackrel{d}{\\rightarrow} N(0, V), $ and the Slutsky’s theorem, we have $$ \\begin{aligned} \\sqrt{n}\\left[g(\\hat{\\theta})-g\\left(\\theta_{0}\\right)\\right]\u0026=G(\\bar{\\theta}) \\sqrt{n} \\left(\\hat{\\theta}-\\theta_{0}\\right)\\ \u0026\\stackrel{d}{\\rightarrow} N\\left(0, G\\left(\\theta_{0}\\right) V G\\left(\\theta_{0}\\right)^{\\prime}\\right) \\end{aligned} $$ Under $ \\mathbb{H}{0}: g\\left(\\theta{0}\\right)=0, $ we have $$ \\begin{aligned} \\sqrt{n} g(\\hat{\\theta}) \u0026\\stackrel{d}{\\rightarrow} N\\left(0, G\\left(\\theta_{0}\\right) V G\\left(\\theta_{0}\\right)^{\\prime}\\right)\\ \u0026=O_p(1) \\end{aligned} $$ since the $ q \\times q $ matrix $ G\\left(\\theta_{0}\\right) V G\\left(\\theta_{0}\\right)^{\\prime} $ is nonsingular given the full rank condition on $ G\\left(\\theta_{0}\\right) $ and the nonsingularity condition on $ V, $ the quadratic form $$ \\sqrt{n} g(\\hat{\\theta})^{\\prime}\\left[G\\left(\\theta_{0}\\right) V G\\left(\\theta_{0}\\right)^{\\prime}\\right]^{-1} \\sqrt{n} g(\\hat{\\theta}) \\stackrel{d}{\\rightarrow} \\chi_{q}^{2} $$ Because $ G(\\hat{\\theta}) \\stackrel{p}{\\rightarrow} G\\left(\\theta_{0}\\right) $ as $ n \\rightarrow \\infty $ by continuity of $ G(\\cdot) $ and $ \\hat{\\theta} \\stackrel{p}{\\rightarrow} \\theta_{0}, $ and $ \\hat{V} \\stackrel{p}{\\rightarrow} V $ by Assumption W.2, we have $$ G(\\hat{\\theta}) \\hat{V} G(\\hat{\\theta})^{\\prime} \\stackrel{p}{\\rightarrow} G\\left(\\theta_{0}\\right) V G\\left(\\theta_{0}\\right)^{\\prime} $$ Namely, $G(\\hat{\\theta}) \\hat{V} G(\\hat{\\theta})^{\\prime} -G\\left(\\theta_{0}\\right) V G\\left(\\theta_{0}\\right)^{\\prime}=o_p(1)$, with $\\sqrt{n} g(\\hat{\\theta})=O_p(1)$, also the stochastic matrix $ G(\\hat{\\theta}) \\hat{V} G(\\hat{\\theta})^{\\prime} $ is nonsingular for $ n $ sufficiently large. We have $$ \\begin{aligned} \u0026\\sqrt{n} g(\\hat{\\theta})^{\\prime}\\left[G(\\hat{\\theta}) \\hat{V} G(\\hat{\\theta})^{\\prime}\\right]^{-1} \\sqrt{n} g(\\hat{\\theta}) -\\sqrt{n} g(\\hat{\\theta})^{\\prime}\\left[G\\left(\\theta_{0}\\right) V G\\left(\\theta_{0}\\right)^{\\prime}\\right]^{-1} \\sqrt{n} g(\\hat{\\theta}) \\ \u0026\\sqrt{n} g(\\hat{\\theta})^{\\prime}\\left{ \\left[G(\\hat{\\theta}) \\hat{V} G(\\hat{\\","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:3:2","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.4 Lagrangian Multiplier (LM) Test Next, we introduce the Lagrange Multiplier (LM) test. It is also called Rao’s (1959) efficient score test in statistics. Suppose we have an IID random sample $ \\mathrm{X}^{n} $ from the population $ f\\left(x, \\theta_{0}\\right), $ where $ \\theta_{0} $ is an unknown parameter value in $ \\Theta $ (i.e. correct model specification). Consider the normalized log-likelihood $$ \\hat{l}(\\theta)=\\frac{1}{n} \\sum_{i=1}^{n} \\ln f\\left(X_{i}, \\theta\\right) $$ and the constrained maximum likelihood estimator that solves the constrained maximization problem $$ \\begin{aligned} \\tilde{\\theta}=\\arg \u0026\\max _{\\theta \\in \\Theta} \\hat{l}(\\theta) \\ \u0026s.t. g(\\theta)=0 \\end{aligned} $$ 【Theorem 9.4】**LM Test**: Suppose Assumptions M.1-M.6, Assumption W.3, and $ \\mathbb{H}_{0} $ hold. Define $$ L M=n \\tilde{\\lambda}^{\\prime} G(\\tilde{\\theta})[-\\hat{H}(\\tilde{\\theta})]^{-1} G(\\tilde{\\theta})^{\\prime} \\tilde{\\lambda} $$ Then under $ \\mathbb{H}_{0} $ $$ L M \\stackrel{d}{\\rightarrow} \\chi_{q}^{2} \\text { as } n \\rightarrow \\infty $$ Define the Lagrangian function $$ L(\\theta, \\lambda)=\\hat{l}(\\theta)+\\lambda^{\\prime} g(\\theta) $$ where $ \\lambda $ is a $q \\times 1$ Lagrangian multiplier. Let $ \\tilde{\\lambda} $ be the corresponding maximizing value of $ \\lambda $. Then the first order conditions (FOC) are $$ \\begin{aligned} \\frac{\\partial L(\\tilde{\\theta}, \\tilde{\\lambda})}{\\partial \\theta} \u0026=\\frac{\\partial \\hat{l}(\\tilde{\\theta})}{\\partial \\theta}+G(\\tilde{\\theta})^{\\prime} \\tilde{\\lambda}=0 \\quad \u0026(9.1)\\ \\frac{\\partial L(\\tilde{\\theta}, \\tilde{\\lambda})}{\\partial \\lambda} \u0026=g(\\tilde{\\theta})=0 \\quad \u0026(9.2) \\end{aligned} $$ Equation (9.1) is $p \\times 1$ and equation (9.2) is $q \\times 1$. Therefore, we have $p+q$ equations, and also $ p+q $ parameters to estimate ($ \\theta $ and $ \\lambda $). By equation (1) and the mean value theorem/ 中值定理, we have $$ \\begin{aligned} G(\\tilde{\\theta})^{\\prime} \\tilde{\\lambda} \u0026=-\\frac{d \\hat{l}(\\tilde{\\theta})}{d \\theta} \\ \u0026=-\\frac{d \\hat{l}\\left(\\theta_{0}\\right)}{d \\theta}-\\frac{d^{2} \\hat{l}\\left(\\bar{\\theta}_{a}\\right)}{d \\theta d \\theta^{\\prime}}\\left(\\tilde{\\theta}-\\theta_{0}\\right) \\end{aligned} $$ where $ \\bar{\\theta}{a}=a \\tilde{\\theta}+(1-a) \\theta{0} $ for some $ a \\in[0,1], $ which lies on the segment between $ \\tilde{\\theta} $ and $ \\theta_{0} $ Note $$ \\frac{d^{2} \\hat{l}(\\theta)}{d \\theta d \\theta^{\\prime}}=\\hat{H}(\\theta) $$ is the sample Hessian matrix. Given the regularity conditions in Section 8.3, we have shown there that $ \\hat{H}(\\hat{\\theta}) \\rightarrow H\\left(\\theta_{0}\\right) $ as $ n \\rightarrow \\infty $ almost surely for any consistent estimator $ \\hat{\\theta} $ of $ \\theta_{0} $. Because $ H\\left(\\theta_{0}\\right) $ is nonsingular, $ \\hat{H}^{-1}(\\hat{\\theta}) \\rightarrow H^{-1}\\left(\\theta_{0}\\right) $ as $ n \\rightarrow \\infty $ almost surely, and $ \\hat{H}^{-1}(\\hat{\\theta}) $ exists for $ n $ sufficiently large. It follows that $$ \\hat{H}\\left(\\bar{\\theta}_{a}\\right)^{-1} G(\\tilde{\\theta})^{\\prime} \\tilde{\\lambda}=-\\hat{H}\\left(\\bar{\\theta}_{a}\\right)^{-1} \\frac{d \\hat{l}\\left(\\theta_{0}\\right)}{d \\theta}-\\left(\\tilde{\\theta}-\\theta_{0}\\right) \\quad (9.3) $$ Next, by the mean value theorem again, we have $$ 0=g(\\tilde{\\theta})=g\\left(\\theta_{0}\\right)+G\\left(\\bar{\\theta}_{b}\\right)\\left(\\tilde{\\theta}-\\theta_{0}\\right) $$ where $ \\bar{\\theta}_{b}=b \\tilde{\\theta}+(1-b) \\theta_{0} $ for some $ b \\in[0,1], $ which lies on the segment between $ \\tilde{\\theta} $ and $ \\theta_{0} $. It follows that under $ \\mathbb{H}{0}: g\\left(\\theta{0}\\right)=0, $ we have $$ G\\left(\\bar{\\theta}{b}\\right)\\left(\\tilde{\\theta}-\\theta{0}\\right)=0 \\quad (9.4) $$ Hence, multiplying Eq.(9.3) by $ G\\left(\\bar{\\theta}{b}\\right) $ and using Eq.(9.4), we obtain $$ \\begin{aligned} \u0026 G\\left(\\bar{\\theta}{b}\\right) \\hat{H}\\left(\\bar{\\theta}{a}\\right)^{-1} G(\\tilde{\\theta})^{\\prime} \\tilde{\\lambda} \\ =\u0026-G\\left(\\bar{\\theta}{b}\\right) \\hat{H}\\left(\\bar{\\theta}{a}\\right)^{-1} \\frac{d \\hat{l}\\","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:4:0","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.5 Likelihood Ratio Test(之后待整理) Since $ \\mathrm{X}^{n} $ is an IID random sample from the population $ f_{X}(x)=f\\left(x, \\theta_{0}\\right), $ where $ \\theta_{0} $ is unknown, we have the likelihood function of $ \\mathrm{X}^{n} $ $$ f_{\\mathbf{X}^{n}}\\left(\\mathbf{X}^{n}, \\theta\\right)=\\prod_{i=1}^{n} f\\left(X_{i}, \\theta\\right) $$ We define the likelihood ratio statistic $$ \\begin{aligned} \\hat{\\Lambda} \u0026=\\frac{\\max {\\theta \\in \\Theta} f{\\mathbf{X}^{n}}\\left(\\mathbf{X}^{n}, \\theta\\right)}{\\max {\\theta \\in \\Theta{0}} f_{\\mathbf{X}^{n}}\\left(\\mathbf{X}^{n}, \\theta\\right)} \\ \u0026=\\frac{\\prod_{i=1}^{n} f\\left(X_{i}, \\hat{\\theta}\\right)}{\\prod_{i=1}^{n} f\\left(X_{i}, \\tilde{\\theta}\\right)} \\end{aligned} $$ where $ \\hat{\\theta} $ and $ \\tilde{\\theta} $ are the unconstrained and constrained MLEs respectively, namely, $$ \\begin{aligned} \\hat{\\theta} \u0026=\\arg \\max _{\\theta \\in \\Theta} \\hat{l}(\\theta) \\ \\tilde{\\theta} \u0026=\\arg \\max _{\\theta \\in \\Theta_{0}} \\hat{l}(\\theta) \\end{aligned} $$ with $$ \\hat{l}(\\theta)=\\frac{1}{n} \\sum_{i=1}^{n} \\ln f\\left(X_{i}, \\theta\\right) = \\frac{1}{n} \\ln \\hat{L}(\\theta | X^n) $$ is the sample average of log-likelihood functions, and $ \\Theta_{0} $ is the parameter space $ \\Theta $ subject to the constraint $ g(\\theta)=0, $ i.e., $ \\Theta_{0}={\\theta \\in \\Theta: g(\\theta)=0} $. Mathematically, $\\hat{\\Lambda} \\geq 1$. Suppose the null hypothesis $ \\mathbb{H}{0}: g\\left(\\theta{0}\\right)=0 $ holds. Then both unconstrained and constrained MLEs $ \\hat{\\theta} $ and $ \\tilde{\\theta} $ are consistent for $ \\theta_{0} $, and imposing the restriction should not lead to a large reduction in the log-likelihood function. Therefore, we expect that the likelihood ratio $ \\hat{\\Lambda} $ will be close to 1. On the other hand, if $ \\mathbb{H}{0} $ is false, then the unconstrained MLE $ \\hat{\\theta} $ is consistent for $ \\theta{0} $ but the constrained MLE $ \\theta $ is not. As a consequence, we expect that the likelihood ratio $ \\hat{\\Lambda} $ is larger than 1. Hence, we can test $ \\mathbb{H}_{0} $ by comparing whether $ \\hat{\\Lambda} $ is significantly larger than 1 or whether $ \\ln \\hat{\\Lambda} $ is greater than 0. How large $ \\hat{\\Lambda} $ or $ \\ln \\hat{\\Lambda} $ must be in order to be considered as significantly large will be determined by the sampling distribution of $ \\hat{\\Lambda} $. Formally, we define the likelihood ratio test statistic as follows. $$ L R=2 \\ln \\hat{\\Lambda}=2 n[\\hat{l}(\\hat{\\theta})-\\hat{l}(\\tilde{\\theta})] $$ 【Theorem 9.5】LR Test: Suppose Assumptions M.1-M.6, Assumption W.3, and $ \\mathbb{H}{0} $ hold. Then under $ \\mathbb{H}{0} $ $$ L R \\stackrel{d}{\\rightarrow} \\chi_{q}^{2} \\text { as } n \\rightarrow \\infty $$ **Proof:** By a second order Taylor’s series expansion of $ \\hat{l}(\\tilde{\\theta}) $ around the unconstrained MLE $ \\hat{\\theta}, $ we have $$ \\begin{aligned} L R \u0026=2 n\\left{\\hat{l}(\\hat{\\theta})-\\left[\\hat{l}(\\hat{\\theta})+\\frac{d \\hat{l}(\\hat{\\theta})}{d \\theta}(\\tilde{\\theta}-\\hat{\\theta})+\\frac{1}{2}(\\tilde{\\theta}-\\hat{\\theta})^{\\prime} \\frac{d^{2} \\hat{l}\\left(\\bar{\\theta}{a}\\right)}{d \\theta d \\theta^{\\prime}}(\\tilde{\\theta}-\\hat{\\theta})\\right]\\right} \\ \u0026=\\sqrt{n}(\\tilde{\\theta}-\\hat{\\theta})^{\\prime}\\left[-\\hat{H}\\left(\\bar{\\theta}{a}\\right)\\right] \\sqrt{n}(\\tilde{\\theta}-\\hat{\\theta}) \\end{aligned} $$ where $ \\bar{\\theta}_{a}=a \\tilde{\\theta}+(1-a) \\hat{\\theta} $ for some $ a \\in[0,1], $ lies on the segment between $ \\tilde{\\theta} $ and $ \\hat{\\theta}, $ and $ \\frac{d}{d \\theta} \\hat{l}(\\hat{\\theta})=0 $ which is the FOC of the unconstrained MLE $ \\hat{\\theta}, $ and again $$ \\hat{H}(\\theta)=\\frac{d^{2} \\hat{l}(\\theta)}{d \\theta d \\theta^{\\prime}} $$ is the sample Hessian matrix. Next, applying the mean value theorem for $ \\frac{d}{d \\theta} \\hat{l}(\\hat{\\theta}) $ around the constrained $ \\mathrm{MLE} \\tilde{\\theta}, $ we obtain $$ \\begin{aligned} 0 \u0026=\\frac{d \\hat{l}(\\hat{\\theta})}{d \\theta} \\ \u0026=\\frac{d \\hat{l}(\\tilde{\\theta})}{d \\theta}+\\hat{H}\\left(\\bar{\\theta}{b}\\right","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:5:0","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.6 Illustrative Examples ","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:6:0","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.6.1 Hypothesis Testing under the Bernoulli Distribution Suppose $ \\mathbf{X}^{n} $ is an IID random sample from a Bernoulli $ (\\theta) $ distribution, where a Bernoulli random variable takes two possible values: $$ X_{i}=\\left{\\begin{array}{l} 1, \\text { with probability } \\theta \\ 0, \\text { with probability } 1-\\theta \\end{array}\\right. $$ The parameter $ \\theta \\in(0,1) $ is unknown. Suppose we are interested in testing $$ \\mathbb{H}{0}: \\theta=\\theta{0} $$ versus $$ \\mathbb{H}{1}: \\theta \\neq \\theta{0} $$ Hence, we have $ g(\\theta)=\\theta-\\theta_{0}, $ and the gradient $$ G(\\theta)=\\frac{d g(\\theta)}{d \\theta}=1 $$ since the population PMF $ f(x, \\theta)=\\theta^{x}(1-\\theta)^{1-x} $ for $ x=0,1, $ the log-likelihood function of the IID random sample $ \\mathbf{X}^{n} $ is given by $$ \\begin{aligned} \\ln \\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right) \u0026=\\sum_{i=1}^{n} \\ln f\\left(X_{i}, \\theta\\right) \\ \u0026=n \\bar{X}_{n} \\ln \\theta+n\\left(1-\\bar{X}_{n}\\right) \\ln (1-\\theta) \\end{aligned} $$ where the sample mean $ \\bar{X}_{n} $ is a sufficient statistic for $ \\theta $. The FOC of the MLE is $$ \\frac{\\partial \\ln \\hat{L}\\left(\\theta | \\mathbf{X}^{n}\\right)}{\\partial \\theta}=\\frac{n \\bar{X}_{n}}{\\hat{\\theta}}-\\frac{n-n \\bar{X}_{n}}{1-\\hat{\\theta}}=0 $$ Thus we have the MLE $ \\hat{\\theta}=\\bar{X}_{n} $. The Wald Test Recall the Hessian matrix $$ H(\\theta)=E_{\\theta}\\left[\\frac{\\partial^{2} \\ln f\\left(X_{i}, \\theta\\right)}{\\partial \\theta^{2}}\\right] $$ Because $$ \\frac{\\partial^{2} \\ln f\\left(X_{i}, \\theta\\right)}{\\partial \\theta^{2}}=-\\frac{X_{i}}{\\theta^{2}}-\\frac{1-X_{i}}{(1-\\theta)^{2}} $$ the sample Hessian matrix $$ \\begin{aligned} \\hat{H}(\\theta) \u0026=n^{-1} \\sum_{i=1}^{n} \\frac{\\partial^{2} \\ln f\\left(X_{i}, \\theta\\right)}{\\partial \\theta} \\ \u0026=-\\frac{\\sum_{i=1}^{n} X_{i}}{n \\theta^{2}}-\\frac{\\sum_{i=1}^{n}\\left(1-X_{i}\\right)}{n(1-\\theta)^{2}} \\ \u0026=-\\frac{\\bar{X}_{n}}{\\theta^{2}}-\\frac{1-\\bar{X}_{n}}{(1-\\theta)^{2}} \\end{aligned} $$ Hence, we have $$ \\hat{H}(\\hat{\\theta})=-\\frac{1}{\\bar{X}_{n}\\left(1-\\bar{X}_{n}\\right)} $$ The Wald test statistic $$ \\begin{aligned} \\hat{W} \u0026=n g(\\hat{\\theta})^{\\prime}\\left[-G(\\hat{\\theta}) \\hat{H}^{-1}(\\hat{\\theta}) G(\\hat{\\theta})^{\\prime}\\right]^{-1} g(\\hat{\\theta}) \\ \u0026=\\frac{n\\left(\\hat{\\theta}-\\theta_{0}\\right)^{2}}{\\bar{X}_{n}\\left(1-\\bar{X}_{n}\\right)} \\ \u0026=\\frac{n\\left(\\bar{X}_{n}-\\theta_{0}\\right)^{2}}{\\bar{X}_{n}\\left(1-\\bar{X}_{n}\\right)} \\stackrel{d}{\\rightarrow} \\chi_{1}^{2} \\text { as } n \\rightarrow \\infty \\end{aligned} $$ under $ \\mathbb{H}_{0} . $ Hence, $ \\sqrt{n}\\left(\\bar{X}_{n}-\\theta_{0}\\right) \\stackrel{d}{\\rightarrow} N\\left(0, \\sigma^{2}\\right) $. The LM Test Define the Lagrangian function $$ L(\\theta, \\lambda)=\\hat{l}(\\theta)+\\lambda^{\\prime} g(\\theta)=\\hat{l}(\\theta)+\\lambda\\left(\\theta-\\theta_{0}\\right) $$ where the normalized log-likelihood $$ \\hat{l}(\\theta)=\\bar{X}_{n} \\ln \\theta+\\left(1-\\bar{X}_{n}\\right) \\ln (1-\\theta) $$ The first order conditions for the constrained MLE are $$ \\begin{aligned} \\frac{\\partial L(\\tilde{\\theta}, \\tilde{\\lambda})}{\\partial \\theta} \u0026=\\frac{\\partial \\hat{l}(\\tilde{\\theta})}{\\partial \\theta}+\\tilde{\\lambda}=0 \\ \\frac{\\partial L(\\tilde{\\theta}, \\tilde{\\lambda})}{\\partial \\lambda} \u0026=g(\\tilde{\\theta})=\\tilde{\\theta}-\\theta_{0}=0 \\end{aligned} $$ It follows that $ \\tilde{\\theta}=\\theta_{0}, $ and $$ \\begin{aligned} \\tilde{\\lambda} \u0026=-\\frac{\\partial \\hat{l}(\\tilde{\\theta})}{\\partial \\theta} \\ \u0026=-\\frac{\\bar{X}_{n}}{\\tilde{\\theta}}+\\frac{1-\\bar{X}_{n}}{1-\\tilde{\\theta}} \\ \u0026=-\\frac{\\bar{X}_{n}-\\tilde{\\theta}}{\\tilde{\\theta}(1-\\tilde{\\theta})} \\ \u0026=-\\frac{\\bar{X}_{n}-\\theta_{0}}{\\theta_{0}\\left(1-\\theta_{0}\\right)} \\end{aligned} $$ This indicates that $ \\tilde{\\lambda} $ measures the difference between the unconstrained MLE $ \\hat{\\theta} $ and the constrained MLE $ \\tilde{\\theta}=\\theta_{0} $. Also, the sample Hessian matrix $$ \\begin{aligned} \\hat{H}(\\tilde{\\theta}) \u0026=-\\frac{\\bar{X}{n}}{\\theta{0}^{2}}-\\frac{1-\\bar{X}{n}}{\\left(1-\\theta{0}\\right)^","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:6:1","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.6.2 Hypothesis Testing under the Normal Distribution Suppose $ X^{n} $ is an IID random sample from a $ N\\left(\\mu, \\sigma^{2}\\right) $ population, where $ \\theta=\\left(\\mu, \\sigma^{2}\\right) $ is unknown. We are interested in testing the hypotheses $$ \\mathbb{H}{0}: \\mu=\\mu{0} $$ versus $$ \\mathbb{H}{A}: \\mu \\neq \\mu{0} $$ where $ \\mu_{0} $ is a known number. This is equivalent to choosing the test function $$ g(\\theta)=\\mu-\\mu_{0} $$ It follows that $$ G(\\theta)=\\frac{d g(\\theta)}{d \\theta}=(1,0) $$ is a two-dimensional row vector. Since the PDF of a $ N\\left(\\mu, \\sigma^{2}\\right) $ population is $$ f(x, \\theta)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}} $$ The normalized log-likelihood function of the random sample $ \\mathbf{X}^{n} $ is $$ \\hat{l}\\left(\\theta | \\mathbf{X}^{n}\\right)=-\\frac{1}{2} \\ln (2 \\pi)-\\frac{1}{2} \\ln \\left(\\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}} \\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\mu\\right)^{2} $$ For the unconstrained MLE, we have obtained $ \\hat{\\theta}=\\left(\\hat{\\mu}, \\hat{\\sigma}^{2}\\right), $ where $$ \\begin{aligned} \\hat{\\mu} \u0026=\\bar{X}{n} \\ \\hat{\\sigma}^{2} \u0026=\\frac{1}{n} \\sum{i=1}^{n}\\left(X_{i}-\\bar{X}_{n}\\right)^{2} \\end{aligned} $$ Also, the sample Hessian matrix $$ \\hat{H}(\\theta)=\\left[\\begin{array}{cc} -\\frac{1}{\\sigma^{2}} \u0026 -\\frac{1}{\\sigma^{4}} \\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\mu\\right) \\ -\\frac{1}{\\sigma^{4}} \\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\mu\\right) \u0026 \\frac{1}{2 \\sigma^{4}}-\\frac{1}{\\sigma^{6}} \\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\mu\\right)^{2} \\end{array}\\right] $$ When $ \\theta=\\hat{\\theta}, $ we have $$ \\hat{H}(\\hat{\\theta})=\\left[\\begin{array}{cc} -\\frac{1}{\\hat{\\sigma}^{2}} \u0026 0 \\ 0 \u0026 -\\frac{1}{2 \\hat{\\sigma}^{4}} \\end{array}\\right] $$ The Wald Test Statistic $$ \\begin{aligned} \\hat{W} \u0026=-n g(\\hat{\\theta})^{\\prime}\\left[G(\\hat{\\theta}) \\hat{H}^{-1}(\\hat{\\theta}) G(\\hat{\\theta})^{\\prime}\\right]^{-1} g(\\hat{\\theta}) \\ \u0026=\\frac{n\\left(\\bar{X}_{n}-\\mu_{0}\\right)^{2}}{\\hat{\\sigma}^{2}} \\ \u0026\\stackrel{d}{\\rightarrow} \\chi_{1}^{2} \\end{aligned} $$ under $ \\mathbb{H}_{0} $. The LM Test Statistic Consider the constrained MLE $$ \\tilde{\\theta}=\\max {\\theta \\in \\Theta} \\hat{l}\\left(\\theta | \\mathbf{X}^{n}\\right) $$ subject to the constrain that $ \\mu=\\mu{0} $ Define the Lagrangian function $$ L(\\theta, \\lambda)=\\hat{l}\\left(\\theta | \\mathbf{X}^{n}\\right)+\\lambda\\left(\\mu-\\mu_{0}\\right) $$ The FOCs are: $$ \\begin{aligned} \\frac{\\partial L(\\tilde{\\theta}, \\tilde{\\lambda})}{\\partial \\mu} \u0026=\\frac{1}{\\tilde{\\sigma}^{2}} \\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\tilde{\\mu}\\right)+\\tilde{\\lambda}=0 \\ \\frac{\\partial L(\\tilde{\\theta}, \\tilde{\\lambda})}{\\partial \\sigma^{2}} \u0026=-\\frac{1}{2 \\tilde{\\sigma}^{2}}+\\frac{1}{2 \\tilde{\\sigma}^{4}} \\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\tilde{\\mu}\\right)^{2}=0 \\ \\frac{\\partial L(\\tilde{\\theta}, \\tilde{\\lambda})}{\\partial \\lambda} \u0026=\\tilde{\\mu}-\\mu_{0}=0 \\end{aligned} $$ Solving for these FOCs, we obtain $$ \\begin{aligned} \\tilde{\\mu} \u0026=\\mu_{0} \\ \\tilde{\\sigma}^{2} \u0026=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\mu_{0}\\right)^{2} \\ \\tilde{\\lambda} \u0026=-\\frac{1}{\\tilde{\\sigma}^{2}}\\left(\\bar{X}_{n}-\\mu_{0}\\right) \\end{aligned} $$ and the sample Hessian matrix $$ \\hat{H}(\\tilde{\\theta})=\\left[\\begin{array}{cc} -\\frac{1}{\\tilde{\\sigma}^{2}} \u0026 -\\frac{1}{\\tilde{\\sigma}^{4}}\\left(\\bar{X}_{n}-\\mu_{0}\\right) \\ -\\frac{1}{\\tilde{\\sigma}^{4}}\\left(\\bar{X}_{n}-\\mu_{0}\\right) \u0026 -\\frac{1}{2 \\tilde{\\sigma}^{4}} \\end{array}\\right] $$ It follows that the LM test statistic $$ \\begin{aligned} L M \u0026=-n \\tilde{\\lambda}^{\\prime} G(\\tilde{\\theta}) \\hat{H}^{-1}(\\tilde{\\theta}) G(\\tilde{\\theta})^{\\prime} \\tilde{\\lambda} \\ \u0026=\\frac{n\\left(\\bar{X}_{n}-\\mu_{0}\\right)^{2}}{\\tilde{\\sigma}^{2}-2\\left(\\bar{X}_{n}-\\mu_{0}\\right)^{2}} \\end{aligned} $$ \r The Likelihood Ratio Test Statistic Since $$ \\begin{array}{l} \\hat{l}(\\hat{\\theta})=-\\frac{1}{2} \\ln (2 \\pi)-\\frac{1}{2} \\ln \\left(\\hat{\\sigma}^{2}\\right)-\\frac{1}{2} \\ \\hat{l}(\\tilde{\\theta})=-\\frac{1}{2} \\ln (","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:6:2","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"9.7 Conclusion Hypothesis testing is one of the most important tasks in statistical inference. In this chapter we have introduced basic ideas of hypothesis testing in statistical inference. We introduce the well-known Neyman-Pearman lemma that a likelihood ratio based test will be uniformly most powerful test for simple hypotheses. We discuss three important testing methods, namely the Wald test, the Lagrange Multiplier (LM) test, and the Likelihood Ratio (LR) test, and show they are asymptotically equivalent to each other under the null hypothesis. It is important to note that all hypothesis tests considered in this chapter assume that the population distribution model is correctly specified. When the population distribution model is misspecified, the Wald test statistic and the LM test statistics have to be modified to use a consistent asymptotic variance estimator which is robust to model misspecification. However, it is impossible to modify the Likelihood ratio test statistic. Finally, when testing economic hypotheses, we usually need to transform an economic hypothesis into a statistical hypothesis on model parameters. since some auxiliary conditions are often imposed in such a transformation, there usually exists a gap between the original economic hypothesis and the resulting statistical hypothesis. Caution is needed to interpret the testing results of the statistical hypotheses. ","date":"0001-01-01","objectID":"/9.-hypothesis-testing/:7:0","tags":null,"title":"","uri":"/9.-hypothesis-testing/"},{"categories":null,"content":"##常用分布 PDF/PMF 期望 方差 MGF 均匀分布 $X \\sim U[a,b]$ $E(X)=(a+b)/2$ $var(X)=(b-a)^2/12$ 伯努利分布 $\\begin{aligned} f_X(x)\u0026= \\begin{cases} p, \u0026x=1\\ 1-p, \u0026x=0\\ \\end{cases}\\ \u0026=p^{x}(1-p)^{1-x}, x=0,1\\end{aligned}$ $E(X)=p$ $var(X)=p(1-p)$ $M_X(t)=pe^t+1-p,t \\in(-\\infty,\\infty)$ 正态分布 $f_{X}(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-(x-\\mu)^{2} / 2 \\sigma^{2}},-\\infty\u003cx\u003c\\infty$ $E(X)=\\mu$ $var(X)=\\sigma^2$ $M_X(t)=e^{\\mu t+ \\frac{\\sigma^2}{2}t^2}$ 柯西分布 $f_{X}(x)=\\frac{1}{\\pi \\sigma} \\left[1+\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right]^{-1}, x \\in \\mathbb{R}$ 不存在 不存在 不存在/一阶及以矩上均不存在 $\\chi^2_{\\nu}$ $E(\\chi^2_\\nu)=\\nu$ $var(\\chi^2_\\nu)=2\\nu$ $M_X(t)=(1-2t)^{-\\frac{\\nu }{2}}$ $t_\\nu$ $\\frac{N(0,1)}{\\sqrt{\\chi^2_\\nu/\\nu}}$ 0 $\\nu/(\\nu-2)$ 不存在 $F_{p,q}$ $\\frac{U/p}{V/q}$ ","date":"0001-01-01","objectID":"/%E5%B8%B8%E7%94%A8%E5%88%86%E5%B8%83%E4%B8%8E%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/:0:0","tags":null,"title":"","uri":"/%E5%B8%B8%E7%94%A8%E5%88%86%E5%B8%83%E4%B8%8E%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/"},{"categories":null,"content":"抽样分布 统计量 抽样分布 $\\frac{\\bar{X}_n-\\mu}{\\sigma/\\sqrt{n}}$ $N(0.1)$ $\\frac{\\sum_{i=1}^{n}(X_i-\\mu)^2}{\\sigma^2}$ $\\chi^2_{n} $ $\\frac{(n-1)S^2_n}{\\sigma^2}=\\frac{\\sum^n_{i=1}(X_i-\\bar{X}_n)}{\\sigma^2}$ $\\chi^2_{n-1} $ $\\frac{\\bar{X}{n}-\\mu}{S{n} / \\sqrt{n}} =\\frac{\\frac{X_{n}-\\mu}{\\sigma / \\sqrt{n}}}{\\sqrt{\\frac{(n-1) S^{2}_n}{\\sigma^{2}} /(n-1)}}\\sim \\frac{N(0,1)}{\\sqrt{\\chi_{n-1}^{2} /(n-1)}}$ $t_{n-1}$ $\\frac{\\frac{(n-1) S_{X}^{2} / \\sigma_{X}^{2}}{n-1}}{\\frac{(m-1) S_{Y}^{2} \\sigma_{Y}^{2}}{m-1}} \\sim \\frac{\\chi_{n-1}^{2} /(n-1)}{\\chi_{m-1}^{2} /(m-1)}$ $F_{n-1,m-1}$ ","date":"0001-01-01","objectID":"/%E5%B8%B8%E7%94%A8%E5%88%86%E5%B8%83%E4%B8%8E%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/:1:0","tags":null,"title":"","uri":"/%E5%B8%B8%E7%94%A8%E5%88%86%E5%B8%83%E4%B8%8E%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83/"},{"categories":null,"content":"1. Functions Math review o References o Appendix of MWG o https://www.coursera.org/learn/mathematics-for-economists/ https://mjo.osborne.economics.utoronto.ca/index.php/tutorial/index/1/toc/c o How? read, read, read o Homework o submit to: https://workspace.jianguoyun.com/inbox/collect/283420eeee554c158a4cf4bbe8cf97a6/submit A language without ambiguity Do NOT be afraid! Matrix Notation for Derivatives $ \\mathbf{x} \\in \\mathbb{R}^{\\mathrm{N}}: $ column vector $ \\bullet x \\cdot y=x^{T} y, x \\cdot A_{N \\times M}=x^{T} A\\left(\\because x \\cdot \" \\text { can always be read as } \" x^{T \\prime \\prime}\\right) $ o Given a vector valued differentiable function $ \\mathrm{f}: \\mathbb{R}^{\\mathrm{N}} \\rightarrow \\mathbb{R}^{\\mathrm{M}} $ o denote by Df $ (x) $ the $ M \\times N $ matrix whose mnth entry is $ \\partial f_{m}(x) / \\partial x_{n} $ If $ M=1, $ then $ f(x) \\in \\mathbb{R} $ (real-valued function), and $$ \\operatorname{Df}(\\mathrm{x})=\\left(\\partial \\mathrm{f}(\\mathrm{x}) / \\partial \\mathrm{x}{1}, \\partial \\mathrm{f}(\\mathrm{x}) / \\partial \\mathrm{x}{2}, \\cdots, \\partial \\mathrm{f}(\\mathrm{x}) / \\partial \\mathrm{x}_{\\mathrm{n}}\\right)=[\\nabla \\mathrm{f}(\\mathrm{x})]^{\\mathrm{T}} $$ o and the Hessian matrix $ \\mathrm{D}^{2} \\mathrm{f}(\\mathrm{x})=\\mathrm{D}[\\nabla \\mathrm{f}(\\mathrm{x})] $ Matrix Notation for Derivatives a An exchange economy with $ \\mathrm{N} $ goods, and $ \\mathrm{M} $ consumers; for each agent i, the utility function $ u_{i}: \\mathbb{R}^{N} \\rightarrow \\mathbb{R} $. o Demand function $ x: \\mathbb{R}^{N+1} \\rightarrow \\mathbb{R}^{M}, $ where for each $ i \\in M $ $ \\mathrm{x}_{\\mathrm{i}}\\left(\\mathrm{p}_{1}, \\mathrm{p}_{2}, \\cdots, \\mathrm{p}_{\\mathrm{n}}, \\mathrm{w}\\right), $ is the demand of consumer i. $ \\bullet D_{p} x(p, w)=\\left[\\begin{array}{cccc}\\partial x_{1} / \\partial p_{1} \u0026 \\partial x_{1} / \\partial p_{2} \u0026 \\cdots \u0026 \\partial x_{1} / \\partial p_{n} \\ \\partial x_{2} / \\partial p_{1} \u0026 \\partial x_{2} / \\partial p_{2} \u0026 \\cdots \u0026 \\partial x_{2} / \\partial p_{n} \\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\vdots \\ \\partial x_{n} / \\partial p_{1} \u0026 \\partial x_{n} / \\partial p_{2} \u0026 \\cdots \u0026 \\partial x_{n} / \\partial p_{n}\\end{array}\\right] $ $ \\bullet \\mathrm{M} \\times \\mathrm{N} $ matrix $ (\\text { “Law of demand” }) $ $$ \\equiv, \\quad \\equiv $$ Matrix Notation for Derivatives o Concavity of $ u(\\cdot): $ $$ \\begin{aligned} \\mathrm{D}^{2} \\mathrm{u}{\\mathrm{i}}(\\mathrm{x})=\u0026 \\mathrm{D}\\left[\\nabla \\mathrm{u}{\\mathrm{i}}(\\mathrm{x})\\right]=\\mathrm{D}\\left[\\left(\\partial \\mathrm{u}{\\mathrm{i}} / \\partial \\mathrm{x}{1}, \\partial \\mathrm{u}{\\mathrm{i}} / \\partial \\mathrm{x}{2}, \\cdots, \\partial \\mathrm{u}{\\mathrm{i}} / \\partial \\mathrm{x}{\\mathrm{n}}\\right)^{\\mathrm{T}}\\right] \\ =\u0026\\left[\\begin{array}{cccc} \\partial^{2} \\mathrm{u}{\\mathrm{i}} / \\partial^{2} \\mathrm{x}{1} \u0026 \\partial^{2} \\mathrm{u}{\\mathrm{i}} / \\partial \\mathrm{x}{1} \\partial \\mathrm{x}{2} \u0026 \\cdots \u0026 \\partial^{2} \\mathrm{u}{\\mathrm{i}} / \\partial \\mathrm{x}{1} \\partial \\mathrm{x}{\\mathrm{n}} \\ \\partial^{2} \\mathrm{u}{\\mathrm{i}} / \\partial \\mathrm{x}{2} \\partial \\mathrm{x}{1} \u0026 \\partial^{2} \\mathrm{u}{\\mathrm{i}} / \\partial^{2} \\mathrm{x}{2} \u0026 \\cdots \u0026 \\partial^{2} \\mathrm{u}{\\mathrm{i}} / \\partial \\mathrm{x}{2} \\partial \\mathrm{x}{\\mathrm{n}} \\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\vdots \\ \\partial^{2} \\mathrm{u}{\\mathrm{i}} / \\partial \\mathrm{x}{\\mathrm{n}} \\partial \\mathrm{x}{1} \u0026 \\partial^{2} \\mathrm{u}{\\mathrm{i}} / \\partial \\mathrm{x}{\\mathrm{n}} \\partial \\mathrm{x}{2} \u0026 \\cdots \u0026 \\partial^{2} \\mathrm{u}{\\mathrm{i}} / \\partial^{2} \\mathrm{x}{\\mathrm{n}} \\end{array}\\right] \\end{aligned} $$ Read by yourself o The chain rule The production rule Homogeneous Functions Let $ \\mathrm{f}: \\mathbb{R}{+}^{\\mathrm{N}} \\rightarrow \\mathbb{R} $ Definition. A function $ f\\left(x{1}, x_{2}, \\cdots, x_{N}\\right) $ is homogenous of degree $ r $ (for $ \\mathrm{r}=\\cdots,-1,0,1, \\cdots) $ if for every $ \\mathrm{t}\u003e0 $ we have $$ \\mathrm{f}\\left(\\mathrm{tx}{1}, \\mathrm{tx}{2}, \\cdots,","date":"0001-01-01","objectID":"/1.-functions/:0:0","tags":null,"title":"","uri":"/1.-functions/"},{"categories":null,"content":"Quasi-concave Functions Convex preferences: “averages are better than extremes.” Formally, a preference relation $ \\succeq $ on the consumption set $ X $ is called (strictly) convex if for any $ \\mathrm{x}, \\mathrm{y}, \\mathrm{z} \\in \\mathrm{X} $ where $ \\mathrm{y} \\succeq \\mathrm{x} $ and $ \\mathrm{z} \\succeq \\mathrm{x}(\\text { and } \\mathrm{y} \\neq \\mathrm{z}) $ and for every $ \\left.\\theta \\in[0,1],(\\theta \\in(0,1)) \\theta_{\\mathrm{y}}+(1-\\theta) z \\succeq \\mathrm{x} \\text { (hold with } \\succ\\right) $ A concave utility function represents a convex preference: $$ \\mathrm{u}(\\theta \\mathrm{y}+(1-\\theta) \\mathrm{z}) \\geq \\theta \\mathrm{u}(\\mathrm{y})+(1-\\theta) \\mathrm{u}(\\mathrm{z}) \\geq[\\theta+(1-\\theta)] \\mathrm{u}(\\mathrm{x})=\\mathrm{u}(\\mathrm{x}) $$ However, concavity is a “cardinal” property in that it will not generally be preserved under an increasing transformation of $ \\mathrm{f}(\\cdot) $. E.g. $ \\mathbf{u}(\\mathbf{x})=\\mathbf{x}^{1 / 2}, \\mathbf{f}(\\mathbf{y})=\\mathbf{y}^{4} $, then $ \\mathbf{f}(\\mathbf{u}(\\mathbf{x}))=\\mathbf{x}^{2} $ That is, there are utility functions that are not concave but represent convex preferences, such as quasi-concave functions. Definition The function $ \\mathrm{f}: \\mathrm{A} \\rightarrow \\mathbb{R} $, defined on the convex set $ \\mathrm{A} \\subset \\mathbb{R}^{\\mathrm{N}} $, is quasi-concave if its upper contour sets $ {\\mathrm{s} \\in \\mathrm{A}: \\mathrm{f}(\\mathrm{s}) \\geq \\mathrm{t}} $ are convex sets; that is, if $ \\mathrm{f}(\\mathrm{x}) \\geq \\mathrm{t} $ and $ \\mathrm{f}\\left(\\mathrm{x}^{\\prime}\\right) \\geq \\mathrm{t} $ implies that $ \\mathrm{f}\\left(\\alpha \\mathrm{x}+(1-\\alpha) \\mathrm{x}^{\\prime}\\right) \\geq \\mathrm{t} $ for any $ \\mathrm{t} \\in \\mathbb{R}, \\mathrm{x}, \\mathrm{x}^{\\prime} \\in \\mathrm{A}, $ and $ \\alpha \\in[0,1] . $ If the inequality is strict whenever $ \\mathrm{x} \\neq \\mathrm{x}^{\\prime} $ and $ \\alpha \\in(0,1), $ then we say that $ \\mathrm{f}(\\cdot) $ is strictly quasi-concave. Theorem. The (continuously differentiable) function $ f: A \\rightarrow \\mathbb{R} $ is quasi-concave if and only if $$ \\nabla f(x) \\cdot\\left(x^{\\prime}-x\\right) \\geq 0 \\quad \\text { whenever } \\quad f\\left(x^{\\prime}\\right) \\geq f(x) $$ for all $ \\mathrm{x}, \\mathrm{x}^{\\prime} \\in \\mathrm{A} . $ If $ \\nabla \\mathrm{f}(\\mathrm{x}) \\cdot\\left(\\mathrm{x}^{\\prime}-\\mathrm{x}\\right)\u003e0 $ whenever $ \\mathrm{f}\\left(\\mathrm{x}^{\\prime}\\right) \\geq \\mathrm{f}(\\mathrm{x}) $ and $ \\mathrm{x}^{\\prime} \\neq \\mathrm{x}, $ then $ \\mathrm{f}(\\cdot) $ is strictly quasi-concave. Graph. The gradient vector $ \\nabla f(x) $ and the vector $ \\left(x^{\\prime}-x\\right) $ must form an acute angle(锐角). \r $ \\mathbf{o} \\mathrm{f}\\left(\\mathrm{x}{1}, \\mathrm{x}{2}\\right)=\\mathrm{t} \\Rightarrow \\mathrm{f}{1} \\cdot \\mathrm{dx}{1}+\\mathrm{f}{2} \\cdot \\mathrm{dx}{2}=0 $ (direction of gradient vector.) Comparative Statics and the Implicit Function Theorem We have a system of N equations depending on $ \\mathrm{N} $ endogenous variables $ \\mathrm{x}=\\left(\\mathrm{x}{1}, \\cdots, \\mathrm{x}{\\mathrm{N}}\\right) $ and $ \\mathrm{M} $ parameters $ \\mathrm{q}=\\left(\\mathrm{q}{1}, \\cdots, \\mathrm{q}{\\mathrm{M}}\\right) $ $$ \\begin{array}{l} \\mathrm{f}{1}\\left(\\mathrm{x}{1}, \\cdots, \\mathrm{x}{\\mathrm{N}} ; \\mathrm{q}{1}, \\cdots, \\mathrm{q}{\\mathrm{M}}\\right)=0 \\ \\vdots \\ \\mathrm{f}{\\mathrm{N}}\\left(\\mathrm{x}{1}, \\cdots, \\mathrm{x}{\\mathrm{N}} ; \\mathrm{q}{1}, \\cdots, \\mathrm{q}{\\mathrm{M}}\\right)=0 \\end{array} $$ The domain of the endogenous variables is $ \\mathrm{A} \\subset \\mathbb{R}^{\\mathrm{N}} $ and the domain of the parameters is $ \\mathrm{B} \\subset \\mathbb{R}^{\\mathrm{M}} $ $$ $$ Comparative Statics and the Implicit Function Theorem Suppose that $ \\overline{\\mathrm{x}}=\\left(\\overline{\\mathrm{x}}{1}, \\cdots, \\overline{\\mathrm{x}}{\\mathrm{N}}\\right) \\in \\mathrm{A} $ and $ \\overline{\\mathrm{q}}=\\left(\\overline{\\mathrm{q}}{1}, \\cdots, \\overline{\\mathrm{q}}{\\mathrm{M}}\\right) \\in \\mathrm{B} $ satisfy the above system of equa","date":"0001-01-01","objectID":"/1.-functions/:1:0","tags":null,"title":"","uri":"/1.-functions/"},{"categories":null,"content":"1. Characteristics of financial variables ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:0:0","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"1.1 Prices and asset returns Suppose that we observe financial time series data, e.g., daily stock close prices $ P_t $, for $ t=1,2,…,T $. We define daily simple return as $$ R_t=\\frac{P_t-P_{t-1}}{P_{t-1}}, $$ or $$ R_t=\\frac{\\Delta P_t}{P_{t-1}}, $$ where $\\Delta P_t=P_t-P_{t-1}$ is the increment of price from $t-1$ to $t$. Meanwhile, we define Log return as $$ r_t=\\log(P_t)-\\log(P_{t-1})=\\log\\left(\\frac{P_t}{P_{t-1}}\\right). $$ $r_t$ is also called continuously compounded return because $$ \\underset{m\\rightarrow\\infty}{\\lim}\\left(1+\\frac{r}{m}\\right)^m=e^r. $$ Note that we could establish the relationship between $R_t$ and $r_t$ as $$ r_t=\\log\\left(1+\\frac{P_t-P_{t-1}}{P_{t-1}}\\right)=\\log(1+R_t) $$ Question: What is the difference between $R_t$ and $r_t$? Are their values very close? Which one is always larger? (Hint: Taylor expansion of $\\log(1+x)$ with $x$ close to 0.) In reality, simple return is more realistic, while log return is more like a academic item. ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:1:0","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"Multiperiod returns We consider the stock price from period $t-k$ to $t$.\\ The simple return from $t-k$ to $t$ is $$ \\begin{aligned} 1+R_t(k)=\u0026\\frac{P_{t}}{P_{t-k}}=\\frac{P_{t}}{P_{t-1}}\\frac{P_{t-1}}{P_{t-2}}\\cdots\\frac{P_{t-k+1}}{P_{t-k}}\\ =\u0026(1+R_{t})(1+R_{t-1})\\cdots(1+R_{t-k+1}) \\end{aligned} $$ While the log return is $$ \\begin{aligned} r_t(k)=\u0026\\log\\left(\\frac{P_{t}}{P_{t-k}}\\right)=\\log\\left(\\frac{P_{t}}{P_{t-1}}\\frac{P_{t-1}}{P_{t-2}}\\cdots\\frac{P_{t-k+1}}{P_{t-k}}\\right)\\ =\u0026r_{t}+r_{t-1}+\\cdots+r_{t-k+1} \\end{aligned} $$ ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:1:1","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"Adjustment for Dividends If a dividend $D_t$ is paid prior to time $t$, we define daily simple return as $$ R_t=\\frac{P_t+D_t-P_{t-1}}{P_{t-1}}, $$ and the log return is $$ r_t=log(1+R_t)=log(P_t+D_t)-log(P_{t-1}), $$ Similarly, one can define multiple-period returns with dividend adjustments. ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:1:2","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"1.2 Random Walk Model ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:2:0","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"1.2.1 Brownian motion 布朗运动(Brownian motion), 一种正态分布的独立增量连续随机过程($ B(t+2)-B(t+1) \\perp B(t+1)-B(t) $)，可以证明布朗运动是马尔可夫过程、鞅过程和伊藤过程。布朗运动是在1827年英国植物学罗伯特·布朗(Robert Brown)利用一般的显微镜观察悬浮于水中由花粉所迸裂出之微粒时，发现微粒会呈现不规则状的运动，因而称它布朗运动。 Random walk: $ \\varepsilon_t = log(P_t) - log(P_{t-1})$ is i.i.d, then logarithm of stock price is a random walk, which is a strong condition for financial analysis. The $ \\textcolor{blue}{\\text{Random walk hypothesis}} $ states that the single-period log returns, $r_t$ are independent, then $$ \\begin{aligned} 1+R_t(k)=\u0026\\frac{P_{t}}{P_{t-k}}=\\frac{P_{t}}{P_{t-1}}\\frac{P_{t-1}}{P_{t-2}}\\cdots\\frac{P_{t-k+1}}{P_{t-k}}\\ =\u0026(1+R_{t})(1+R_{t-1})\\cdots(1+R_{t-k+1})\\ =\u0026exp(r_t+r_{t-1}+\\cdots+r_{t-k+1}) \\end{aligned} $$ and we have $$ log(1+R_t(k))=r_t+r_{t-1}+\\cdots+r_{t-k+1}. $$ It is sometimes assumed further that the log returns $r_t$ are i.i.d., $N(\\mu,\\sigma^2)$. ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:2:1","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"1.2.2 Geometric Random Walks It is easy to show $$ \\frac{P_{t}}{P_{t-k}}=1+R_t(k)=exp(r_t+r_{t-1}+\\cdots+r_{t-k+1}) $$ Taking $ k=t $, we have $$ P_t=P_0exp(r_t+r_{t-1}+\\cdots+r_{1}) $$ We call such a process whose logarithm is a random walk (in this case $ P_t$ ) a $ \\textcolor{blue}{\\text{geometric random walk}} $ or an $ \\textcolor{red}{\\text{exponential random walk}} $. If add the assumption that $r_t$ are i.i.d., normal distribution, the process is a $ \\textcolor{blue}{\\text{lognormal geometric random walk}} $. Simulating a stock price process \r ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:2:2","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"1.2.3 Descriptive statistics For stationary time series $X_t$, we define its descriptive statistics as Mean $\\mu=E $; Variance $\\sigma^2=E[(X-E )^2]$; Skewness $sk=\\frac{E[(X-E )^3]}{\\sigma^3}$; Kurtosis $K=\\frac{E[(X-E )^4]}{\\sigma^4}$. For normal distributions, $sk=0$ and $K=3$. Taking Normal distribution as a benchmark, we define $K-3$ as the excess kurtosis. With $ T $ realizations of $ X_t $, $ t=1,2,…,T $, we could compute Sample mean $\\widehat\\mu=T^{-1}\\sum_{t=1}^TX_t$; Sample variance $\\widehat\\sigma^2=(T-1)^{-1}\\sum_{t=1}^T(X_t-\\widehat\\mu)^2$; Sample skewness $\\widehat{sk}=T^{-1}\\sum_{t=1}^T(X_t-\\widehat\\mu)^3/\\widehat\\sigma^3$; Sample kurtosis $\\widehat K=T^{-1}\\sum_{t=1}^T(X_t-\\widehat\\mu)^4/\\widehat\\sigma^4$. If $X$ follows normal distribution, then $\\widehat{sk}\\approx0$ and $\\widehat K\\approx 3$. Why do we care about these moments? Sample mean: telling us the return performance, cared most by investors, studied most in the literature as well Sample variance: the typical measurement for volatility (standard error); see Sharpe ratio; and covariance matrix could be applied for portfolio construction. Sample skewness: the measurement for crash risk Sample kurtosis: the measurement for heavy tail, needed for Central Limit Theorem ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:2:3","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"1.3 Testing for Normality ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:3:0","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"1.3.1 Jarque-Bera test As $T\\rightarrow\\infty$, we have $$ \\sqrt T\\widehat{sk}\\overset{D}{\\longrightarrow} N(0, 6), $$ and $$ \\sqrt T(\\widehat K-3)\\overset{D}{\\longrightarrow} N(0, 24). $$ Jarque-Bera test statistic $$ \\qquad J=T\\left(\\frac{\\widehat{sk^2}}{6}+\\frac{(\\widehat K-3)^2}{24}\\right). $$ In the Jaque-Bera test, we specify $$ \\begin{aligned} \\mathbb H_0:\u0026, X \\mbox{ follows normal distribution}\\ \\mathbb H_1:\u0026, X \\mbox{ does NOT follow normal distribution.} \\end{aligned} $$ Under the null hypothesis, $\\widehat{sk}$ converge to a normal random variable with mean zero, and the same for $\\widehat K-3$. Therefore, $ J\\sim \\chi^2_{(2)}. $ Under the alternative hypothesis, we would get a large value of $J$. Decision rule: we reject the null hypothesis when $J$ is larger than the critical value, for example, $ \\chi^2_{(2)}(0.95) $ under $5%$ significance level. \r ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:3:1","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"A real example: Shanghai Composite Index} We consider the daily close prices of the Shanghai Stock Exchange (SSE) Composite Index from 1 Jan 2000 to 31 Dec 2016. \r \r Phenomena: the probability of large shock is higher than normal distribution \r \r Simple return and Log return are very close when they are small. \r Table: Descriptive statistics of Daily Simple and Log returns Simple Return Log Return Mean 0.000326 0.000192 Standard Deviation 0.016328 0.016362 Skewness -0.184995 -0.341840 Kurtosis 7.467776 7.488846 Jarque-Bera Statistic 3445.118 3534.125 P-value of the JB test 0.0000 0.0000 Negative skewness, higher crash risk for log return Huge kurtosis, heavy tail Refuse JB test’s null hypothesis, not normal distributed \r Positive or negative correlation may depends on some pattern like trade volunm. \r ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:3:2","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"Quantile-Quantile (QQ) plot A simpler way to examine Normality: We compare the quantiles of the log return data with the normal distribution that has the same mean and variance. If log return follows normal distribution, then they should have the same quantiles (on the 45 degree line). Compare the distribution of LR with Normal distribution. Heavier tail. \r How about $ t $-distribution? Thinner tail. \r Measuring serial autocorrelation We define $k$-th order autocovariance as $$ \\gamma(k)=Cov(r_t, r_{t+k})=E[r_tr_{t+k}]-E[r_t]E[r_{t+k}], $$ and $k$-th order serial autocorrelation as $$ \\rho(k)=\\frac{\\gamma(k)}{\\gamma(0)}. $$ We estimate $\\gamma(k)$ by $$ \\widehat\\gamma(k)=T^{-1}\\sum_{t=1}^{T-k}(r_t-\\bar r)(r_{t+k}-\\bar r), $$ where $\\bar r=T^{-1}\\sum_{t=1}^nr_t$. Therefore, $\\widehat\\rho(k)=\\widehat\\gamma(k)/\\widehat\\gamma(0)$. Suppose that $$ X_t=\\mu +X_{t-1}+\\epsilon_t, $$ where $X_t=P_t$ or $X_t=p_t$. Historically, $\\mu$(drift) was often assumed to be zero RW1: $\\epsilon_t \\sim IID;, E \\epsilon_t=0 $ RW2: $\\epsilon_t $ is independent over time; $ E \\epsilon_t=0 $ RW3: For all $k$, $cov(\\epsilon_t, \\epsilon_{t-k} )=0$ Then, provided $E\\epsilon_t^2 \\leq C \u003c \\infty, RW1\\Rightarrow RW2 \\Rightarrow RW3 $ 【Martingale】A martingale is a time-series process $X_t$ obeying $$ E[X_{t+1} | X_t, X_{t-1}, \\dots ]=X_t $$ or equivalently, call $\\epsilon_{t+1}=X_{t+1}-X_t$ a martingale difference sequence $$ E[\\epsilon_t | X_{t-1}, X_{t-2}, \\dots ]=0 $$ This corresponds with the notion of a fair game: If you toss a coin against opponent and bet successively at fair odds with initial capital $X_0$, current capital $X_t$ is martingale. This is the case that $\\mu=0$; More generally, by transforming conditional mean to unconditional, we might assume that 【RW2.5 (Fama)】: $ \\epsilon_{t+1}=X_{t+1}-X_t-\\mu $, is a martingale difference (MDS). Martingale property implies that $$ cov(\\epsilon_t, g(X_{t-1}, X_{t-2}, \\dots) )=0 $$ for any (measurable) function $g.$ In particular $$ g(X_{t-1}, X_{t-2}, \\dots)=X_{t-k}-X_{t-k-1}-\\mu =\\epsilon_{t-k} $$ so stronger condition than RW3. ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:3:3","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"1.4 Efficient Markets Hypothesis Fama (1970, JoF): A market in which prices always ‘fully reflect’ available information is called ‘efficient’ $\\Rightarrow$ EMH If prices are predictable $\\Rightarrow$ opportunities for superior returns (free lunch) $\\Rightarrow$ will be competed away immediately by a lot of hungry traders $\\Rightarrow$ unpredictable random walk If a security believed to be underpriced, buying pressure $\\Rightarrow$ jump up to a level where no longer thought a bargain If a security believed to be overpriced, (short-)selling pressure $\\Rightarrow$ jump down to a level where no longer thought too expensive As a result, market forces respond to news quickly and make prices the best available estimates of fundamental values, i.e. values justified by likely future cash flows and preferences of investors/consumers We distinguish among three forms of market efficiency depending on the information set with respect to which efficiency is defined Weak form - historical prices fully reflected in current price: Can’t profit from strategies based on past prices Semi strong form - all public information (past prices, annual reports, quality of management, earnings forecasts, macroeconomic news, etc.) fully reflected in current prices: Can’t outsmart others since they read newspapers too Strong form - all private and public information fully reflected in current prices: Can’t profit from knowledge of tomorrow’s news (bonds downgraded by rating agencies, new drug failed to be approved, book-cooking by Enron, last-quarter earnings, M\u0026A deals, etc.) Econometric Issue Joint Hypothesis Problem. Any test of weak form EMH must assume an equilibrium asset pricing model that defines ‘normal’ security returns against which investor returns are measured. If we reject the hypothesis that investors can’t achieve superior risk-adjusted returns, we don’t know if markets are inefficient or if the underlying model is misspecified. we should allow $$ r_t=\\mu_t+\\epsilon_t, $$ mostly assuming $\\mu_t=\\mu$ is constant or small, where $\\epsilon_t$ is a martingale difference sequence with respect to some information set $\\mathcal{F}{t-1},$ i.e., $E(\\epsilon | \\mathcal{F}{t-1} )=0$ and $\\mu_t$ is the risk premium that is given from some asset pricing model. ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:4:0","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"Testing of EMH under RW1 The population autocovariance and autocorrelation functions of a stationary series $Y_t$ $$ \\gamma_s=cov(Y_t, Y_{t-s})=E[(Y_t-EY_t)(Y_{t-s}-EY_{t-s})] \\ \\rho_s=\\frac{\\gamma_s}{\\gamma_0} $$ for $s=0, \\pm 1, \\pm 2, \\dots$ Take $Y_t=r_t$ or $R_t$ can estimate these by the sample equivalents $$ \\hat{\\gamma_s}=\\frac{1}{T} \\mathop{\\sum}\\limits_{t=s+1}^T (Y_t-\\bar{Y})(Y_{t-s}-\\bar{Y}) \\ \\hat{\\rho_s}=\\frac{\\hat{\\gamma_s}}{\\hat{\\gamma_0}}. $$ The efficient markets hypothesis says that $\\gamma_s, \\rho_s=0$ for all $s \\neq 0$ 检验单个ACF Assume further that $Y_t \\text{is i.i.d.}$ It can be shown that for any $k,$ $$ \\sqrt{T} \\hat{\\rho_k} \\Rightarrow N(0,1) $$ under the null hypothesis of no correlation. Therefore, you can test the null hypothesis by comparing $\\hat{\\rho_k}$ with the so-called ‘Bartlett intervals’ $$ [-z_{\\alpha/2}/\\sqrt{T}, z_{\\alpha/2}/\\sqrt{T}], $$ where $z_\\alpha$ are normal critical values. Values of $\\hat{\\rho_k}$ lying outside this interval are inconsistent with the null hypothesis. Literally, **this is testing the hypothesis that $\\rho_k=0$ versus $\\rho_k \\neq 0$ for a given $ k $.** Under the alternative hypothesis $$ \\sqrt{T} \\hat{\\rho_k} \\mathop{\\rightarrow}\\limits^{P} \\infty $$ for at least one $k.$ 检验联合ACF / Ljung-Box白噪声检验 In fact, under this assumption we have $$ \\sqrt{T} (\\hat{\\rho_1}, \\dots, \\hat{\\rho_P})^T \\Rightarrow N(0,I_P). $$ The Box-Pierce $Q$ statistic $$ Q=T\\mathop{\\sum}\\limits_{j=1}^P \\hat{\\rho}_j^2 $$ can be used to **test the joint hypothesis** that $\\rho_1=0, \\dots, \\rho_P=0$ versus the general alternative. We have $$ Q\\Rightarrow \\chi_P^2 $$ Under the null hypothesis, so reject when $Q\u003e\\chi_P^2(\\alpha)$ for an $\\alpha$-level test. Box-Ljung version is known to have better finite sample performance, which can be used test white noise, $$ Q=T(T+2)\\mathop{\\sum}^{p}_{j=1} \\frac{\\hat{\\rho}_j^2}{T-j} $$ 实际上，由于p的选择会影响Q的表现，所以经常选用多个p值模拟。研究表明$ p \\approx \\ln(T) $会有较好的功效。在分析季节性时间序列时，由于时间隔为周期的倍数的自相关系数更加重要，所以这个一般性的规则需要加以修正。 ","date":"0001-01-01","objectID":"/1.-characteristics-of-financial-variables/:4:1","tags":null,"title":"","uri":"/1.-characteristics-of-financial-variables/"},{"categories":null,"content":"2. Linear Time Series Models ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:0:0","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"2.1 Introduction A time series is a sequence of data points, measured typically at successive points in time spaced at uniform time intervals. Examples: tick data of stock prices, daily exchange rates, monthly interest rates, annual GDP growth rate etc.; Time series analysis provides econometrics tools for those time series data; Famous time series models/tools: ARIMA model, random walk, cointegration, error correction model, vector autoregressive model and ARCH/GARCH etc.. The Importance of Time Series to Economics Time and uncertainty are two most important factors when economic agents make a decision; Time series econometrics provides statistical methods and tools to investigate dynamic relations in economics/finance; Time series data are attractive to researchers as they show many interesting phenomena: asymmetry(牛短熊长), time irreversibility, regime-shifts, volatility clustering, and jumps or outliers; In past two decades, a fast development of Time series from linear to nonlinear, stationary to nonstationary, univariate to multivariate. The objective of time series econometrics Examine how well economic and financial theory/models can explain the stylized facts of economic/financial phenomena (e.g., volatility clustering, seasonal effects) Test the validity of economic and financial hypotheses (e.g., market Efficient Hypothesis) Predict the future evolution of economic systems and financial markets using historical data (e.g., the prediction of business cycle turning points). Make policy recommendations (e.g., program evaluations) General Methodology of Time Series Analysis Survey/data collection and summary of stylized facts; Model specification: propose an economic/financial model using economic theory, empirical experience, and etc; Model estimation: estimate the proposed model using historical data. Model evaluation (in-sample and out-of-sample evaluation); Explain the stylized facts, test economic hypotheses/theories, and forecast futures. ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:1:0","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"2.2 Linear time series models ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:2:0","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"2.2.0 Basic 【strictly stationary】A time series $ \\left{X_{t}\\right} $ is said to be strictly stationary (or strongly stationary) if the $ k $ -dimensional distribution of $ \\left(X_{1}, \\ldots, X_{k}\\right) $ is the same as that of $ \\left(X_{t+1}, X_{t+2}, \\ldots, X_{t+k}\\right) $ for any integer $ k \\geq 1 $ and $ t $. 【weakly stationary】A time series $ \\left{X_{t}\\right} $ is said to be weakly stationary (or second order stationary or covariance stationary) if $ E\\left[X_{t}^{2}\\right]\u003c\\infty $ and both $ E X_{t} $ and $ \\operatorname{cov}\\left(X_{t}, X_{t-k}\\right), $ for any integer $ k, $ do not depend on $ t $. ACVFs and ACFs For weakly stationary time series $ \\left{X_{t}\\right}, $ let $ \\mu=E X_{t} $ denote its common mean. We define the **autocovariance function** (ACVF) as $$ \\gamma(k)=\\operatorname{cov}\\left(X_{t}, X_{t+k}\\right)=E\\left{\\left(X_{t}-\\mu\\right)\\left(X_{t+k}-\\mu\\right)\\right} $$ and the **autocorrelation function** (ACF) as $$ \\rho(k)=\\operatorname{Corr}\\left(X_{t}, X_{t+k}\\right)=\\gamma(k) / \\gamma(0) $$ for $ k=0,\\pm 1,\\pm 2, \\cdots . $ Note that $ \\gamma(0) $ is the variance of $ X_{t}, $ i.e. $ \\gamma(0)=\\operatorname{var}\\left(X_{t}\\right), $ and $ \\rho(k)=\\rho(-k) $ 【white noise】$ \\left{X_{t}\\right} $ is called a white noise process when $ \\rho(k)=0 $ for any $ k \\neq 0, $ and is denoted by $ X_{t} \\sim W N\\left(\\mu, \\sigma^{2}\\right), $ where $ \\sigma^{2}=\\gamma(0)=\\operatorname{var}\\left(X_{t}\\right) $ 如果时间序列$ x_t $是一个具有有限均值和有限方差的独立同分布随机变量序列，则$ {x_t} $称为一个白噪声序列(white noise). 特别地，若$ x_t $，还服从均值为0、方差为$ \\sigma^2 $的正态分布，则称这个序列为高斯白噪声(Gaussian white noise)。 对于白噪声序列，所有自相关函数为零。在实际应用中，如果所有样本自相关函数接近于零，则认为该序列是白噪声序列。 In practice, we observe $ X_{1}, \\ldots, X_{T} $ and estimate ACVF and ACF by the sample ACVF and sample ACF. $$ \\hat{\\gamma}(k)=\\frac{1}{T} \\sum_{t=k+1}^{T}\\left(X_{t}-\\bar{X}\\right)\\left(X_{t-k}-\\bar{X}\\right), \\quad \\hat{\\rho}(k)=\\hat{\\gamma}(k) / \\widehat{\\gamma}(0) $$ where $ \\bar{X}=T^{-1} \\sum_{t=1}^{T} X_{t} $. In time series analysis, we use stationary autoregressive moving average (ARMA) models to reveal the autocorrelation structures in the data. 在时间序列分析中，单变量是否显著不重要，重要的是残差项接近白噪声(不包含有效信息)。 ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:2:1","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"2.2.1 Autoregressive (AR) models An autoregressive model of order $ p(\\mathrm{AR}(\\mathrm{p})) $ is defined as $$ X_{t}=c+\\rho_{1} X_{t-1}+\\cdots+\\rho_{p} X_{t-p}+\\epsilon_{t} $$ where $ \\epsilon_{t} \\sim W N\\left(0, \\sigma^{2}\\right), $ and $ c, \\rho_{1}, \\ldots, \\rho_{p} $ are parameters. In particular, an AR(1) model is written as $$ X_{t}=c+\\rho_{1} X_{t-1}+\\epsilon_{t} $$ where $ \\epsilon_{t} \\sim W N\\left(0, \\sigma^{2}\\right) $. Then $ \\left{X_{t}\\right} $ is stationary if and only if $ |\\rho_1|\u003c1 $. Question: Compute the mean, variance, ACVFs and ACFs of $ \\left{X_{t}\\right} $. AR(1) model Taking expectations on both sides of the $ \\mathrm{AR}(1) $ equation, we obtain $$ \\begin{aligned} E\\left[X_{t}\\right] \u0026=c+E\\left[\\rho X_{t-1}\\right]+E\\left[\\epsilon_{t}\\right] \\ \u0026=c+\\rho E\\left[X_{t-1}\\right]+0 \\ \u0026=c+\\rho E\\left[X_{t}\\right] \\quad (\\text{by stationary}) \\end{aligned} $$ since $ |\\rho|\u003c1, $ we have $$ E\\left[X_{t}\\right]=\\frac{c}{1-\\rho} $$ Note that $ E\\left[X_{t}\\right]=0 $ if and only if $ c=0 $. We compute the variances of both sides of the AR(1) equation, and obtain $$ \\begin{aligned} \\operatorname{var}\\left[X_{t}\\right] \u0026=\\operatorname{var}\\left[c+\\rho X_{t-1}+\\epsilon_{t}\\right] \\ \u0026=\\operatorname{var}\\left[\\rho X_{t-1}+\\epsilon_{t}\\right] \\ \u0026=\\operatorname{var}\\left[\\rho X_{t-1}\\right]+\\operatorname{var}\\left[\\epsilon_{t}\\right]+2 \\operatorname{cov}\\left(\\rho X_{t-1}, \\epsilon_{t}\\right) \\ \u0026=\\rho^{2} \\operatorname{var}\\left[X_{t-1}\\right]+\\sigma^{2}+0 \\ \u0026=\\rho^{2} \\operatorname{var}\\left[X_{t}\\right]+\\sigma^{2} \\end{aligned} $$ since $ |\\rho|\u003c1, $ we have $$ \\operatorname{var}\\left[X_{t}\\right]=\\frac{\\sigma^{2}}{1-\\rho^{2}} $$ Finally, we compute the autocovariance of $ X_{t} $ for any integer $ k . $ First, when $ k=1 $ $$ \\begin{aligned} \\gamma(1) \u0026=\\operatorname{cov}\\left(X_{t}, X_{t-1}\\right)=\\operatorname{cov}\\left(c+\\rho X_{t-1}+\\epsilon_{t}, X_{t-1}\\right) \\ \u0026=\\operatorname{cov}\\left(c, X_{t-1}\\right)+\\operatorname{cov}\\left(\\rho X_{t-1}, X_{t-1}\\right)+\\operatorname{cov}\\left(\\epsilon_{t}, X_{t-1}\\right) \\ \u0026=0+\\rho \\operatorname{cov}\\left(X_{t-1}, X_{t-1}\\right)+0=\\rho \\operatorname{var}\\left(X_{t-1}\\right)=\\rho \\gamma(0) \\end{aligned} $$ i.e., $ \\gamma(1)=\\rho \\sigma^{2} /\\left(1-\\rho^{2}\\right) $. Using the same method, we can show that $ \\gamma(k)=\\operatorname{cov}\\left(X_{t}, X_{t-k}\\right)=\\rho^{k} \\gamma(0) $ for all $ k\u003e1 $ when $ X_{t} \\sim \\operatorname{AR}(1) $. Therefore, $ \\rho(1)=\\gamma(1) / \\gamma(0)=\\rho $ and $ \\rho(k)=\\rho^{k} $ for $ k\u003e1 $ To summarize, if $ X_{t}=c+\\rho X_{t-1}+\\epsilon_{t} $ is a stationary process, we have $$ \\begin{array}{c} E\\left[X_{t}\\right]=\\frac{c}{1-\\rho} \\ \\operatorname{var}\\left[X_{t}\\right]=\\frac{\\sigma^{2}}{1-\\rho^{2}} \\ \\gamma(k)=\\left{\\begin{array}{ll} \\rho \\sigma^{2} /\\left(1-\\rho^{2}\\right) \u0026 \\text { for } k=1 \\ \\rho^{k} \\sigma^{2} /\\left(1-\\rho^{2}\\right) \u0026 \\text { for } k\u003e1 \\end{array}\\right. \\ \\rho(k)=\\left{\\begin{array}{cc} \\rho \u0026 \\text { for } k=1 \\ \\rho^{k} \u0026 \\text { for } k\u003e1 \\end{array}\\right. \\end{array} $$ Autoregressive Process with Order p $$ Y_{t}=c+\\phi_{1} Y_{t-1}+\\phi_{2} Y_{t-2}+\\ldots+\\phi_{p} Y_{t-p}+\\varepsilon_{t}, \\text { where } \\varepsilon_{t} \\sim W N\\left(0, \\sigma^{2}\\right) $$ Provided that the roots of $ 1-\\phi_{1} z-\\phi_{2} z^{2}-\\ldots, \\phi_{p} z^{p}=0 $ are outside of the unit circle, then $ Y_{t} $ is stationary. $$ \\mu=\\frac{c}{1-\\phi_{1}-\\phi_{2}-\\ldots-\\phi_{p}} $$ After some adjustment: $$ Y_{t}-\\mu=\\phi_{1}\\left(Y_{t-1}-\\mu\\right)+\\phi_{2}\\left(Y_{t-2}-\\mu\\right)+\\ldots+\\phi_{p}\\left(Y_{t-p}-\\mu\\right)+\\varepsilon_{t} $$ Multiple $ Y_{t-j}-\\mu $ on both sides and take expectation, we have $$ \\gamma_{j}=\\left{\\begin{array}{r} \\phi_{1} \\gamma_{j-1}+\\phi_{2} \\gamma_{j-2}+\\ldots+\\phi_{p} \\gamma_{j-p}, \\text { for all } j=1,2,3, \\ldots \\ \\phi_{1} \\gamma_{1}+\\phi_{2} \\gamma_{2}+\\ldots+\\phi_{p} \\gamma_{p}+\\sigma^{2}, \\text { for } j=0 \\end{array}\\right} $$ ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:2:2","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"2.2.2 Moving average (MA) models A moving average model of order $ q(\\mathrm{MA}(\\mathrm{q})) $ is defined as $$ X_{t}=\\mu+\\epsilon_{t}+\\theta_{1} \\epsilon_{t-1}+\\cdots+\\theta_{q} \\epsilon_{t-q} $$ where $ \\epsilon_{t} \\sim W N\\left(0, \\sigma^{2}\\right), $ and $ \\mu, \\theta_{1}, \\ldots, \\theta_{q} $ are parameters. Note that a MA(q) process is always stationary if the coefficients do not vary over time. In particular, a MA(1) models is written as $$ X_{t}=\\mu+\\epsilon_{t}+\\theta_{1} \\epsilon_{t-1} $$ where $ \\epsilon_{t} \\sim W N\\left(0, \\sigma^{2}\\right) $ Question: Compute the mean, variance, ACVFs and ACFs of $ \\left{X_{t}\\right} $. Suppose that $ X_{t}=\\mu+\\epsilon_{t}+\\theta \\epsilon_{t-1}, $ where $ \\epsilon_{t} \\sim W N\\left(0, \\sigma^{2}\\right), $ we have $$ \\begin{aligned} E\\left[X_{t}\\right]\u0026=E\\left[\\mu+\\epsilon_{t}+\\theta \\epsilon_{t-1}\\right] \\ \u0026=\\mu+E\\left[\\epsilon_{t}\\right]+E\\left[\\theta \\epsilon_{t-1}\\right] \\ \u0026=\\mu \\ \\operatorname{var}\\left[X_{t}\\right]\u0026=\\operatorname{var}\\left[\\mu+\\epsilon_{t}+\\theta \\epsilon_{t-1}\\right]\\ \u0026=\\operatorname{var}\\left[\\epsilon_{t}+\\theta \\epsilon_{t-1}\\right] \\ \u0026=\\operatorname{var}\\left[\\epsilon_{t}\\right]+\\operatorname{var}\\left[\\theta \\epsilon_{t-1}\\right]+2 \\operatorname{cov}\\left[\\epsilon_{t}, \\theta \\epsilon_{t-1}\\right] \\ \u0026=\\sigma^{2}+\\theta^{2} \\sigma^{2}+0\\ \u0026=\\left(1+\\theta^{2}\\right) \\sigma^{2} \\end{aligned} $$ For the $ k- $th order autocovariance, when $ k=1 $, $$ \\begin{aligned} \\gamma(1)=\u0026 \\operatorname{cov}\\left(X_{t}, X_{t-1}\\right)=\\operatorname{cov}\\left(\\mu+\\epsilon_{t}+\\theta \\epsilon_{t-1}, \\mu+\\epsilon_{t-1}+\\theta \\epsilon_{t-2}\\right) \\ =\u0026 \\operatorname{cov}\\left(\\epsilon_{t}+\\theta \\epsilon_{t-1}, \\epsilon_{t-1}+\\theta \\epsilon_{t-2}\\right) \\ =\u0026 \\operatorname{cov}\\left(\\epsilon_{t}, \\epsilon_{t-1}\\right)+\\theta \\operatorname{cov}\\left(\\epsilon_{t}, \\epsilon_{t-2}\\right)+\\theta \\operatorname{cov}\\left(\\epsilon_{t-1}, \\epsilon_{t-1}\\right) \\ \u0026+\\theta^{2} \\operatorname{cov}\\left(\\epsilon_{t-1}, \\epsilon_{t-2}\\right)=\\theta \\sigma^{2} \\end{aligned} $$ It is easy to show that $ \\gamma(k)=0 $ for $ k\u003e1 $. Therefore, $ \\rho(1)=\\frac{\\theta}{1+\\theta^{2}} $ and $ \\rho(k)=0 $ for $ k\u003e1 $. To summarize, if $ X_{t}=\\mu+\\epsilon_{t}+\\theta \\epsilon_{t-1} $ is a stationary process, we have $$ \\begin{array}{c} E\\left[X_{t}\\right]=\\mu \\ \\operatorname{var}\\left[X_{t}\\right]=\\left(1+\\theta^{2}\\right) \\sigma^{2} \\ \\gamma(k)=\\left{\\begin{array}{cl} \\theta \\sigma^{2} \u0026 \\text { for } k=1 \\ 0 \u0026 \\text { for } k\u003e1 \\end{array}\\right. \\ \\rho(k)=\\left{\\begin{array}{cc} \\frac{\\theta}{1+\\theta^{2}} \u0026 \\text { for } k=1 \\ 0 \u0026 \\text { for } k\u003e1 \\end{array}\\right. \\end{array} $$ MA(q) process: $$ Y_{t}=c+\\varepsilon_{t}+\\theta_{1} \\varepsilon_{t-1}+\\theta_{2} \\varepsilon_{t-2}+\\ldots+\\theta_{q} \\varepsilon_{t-q} \\text{, where } \\varepsilon_{t} \\text{ is WN}\\left(0, \\sigma^{2}\\right) $$ Mean and autocovariance functions: $$ \\begin{aligned} \\mu \u0026=E\\left(Y_{t}\\right)=E\\left(c+\\varepsilon_{t}+\\theta_{1} \\varepsilon_{t-1}+\\theta_{2} \\varepsilon_{t-2}+\\ldots+\\theta_{q} \\varepsilon_{t-q}\\right)=c \\ \\gamma_{0} \u0026=E\\left[\\left(Y_{t}-\\mu\\right)\\left(Y_{t}-\\mu\\right)\\right] \\ \u0026=E\\left(\\varepsilon_{t}^{2}+\\theta_{1}^{2} \\varepsilon_{t-1}^{2}+\\ldots+\\theta_{q}^{2} \\varepsilon_{t-q}^{2}\\right)=\\left(1+\\theta_{1}^{2}+, \\ldots+\\theta_{q}^{2}\\right) \\sigma^{2} \\ \\gamma_{j} \u0026=E\\left[\\left(Y_{t}-\\mu\\right)\\left(Y_{t-j}-\\mu\\right)\\right] \\ \u0026=\\left(\\theta_{j}+\\theta_{j+1} \\theta_{1}+\\theta_{j+2} \\theta_{2}+\\ldots+\\theta_{q} \\theta_{q-j}\\right) \\sigma^{2}, \\text { for } j=1,2, \\ldots, q \\ \\gamma_{j} \u0026=0 \\text { for } j\u003eq \\end{aligned} $$ $ \\mathrm{MA}(\\infty) $ process: $$ Y_{t}=\\mu+\\psi_{0} \\varepsilon_{t}+\\psi_{1} \\varepsilon_{t-1}+\\psi_{2} \\varepsilon_{t-2}+\\ldots, \\text { where } \\varepsilon_{t} \\text { is } W N\\left(0, \\sigma^{2}\\right) $$ $ Y_{t} $ is weakly stationary if $ \\left{\\psi_{j}\\right}_{j=0}^{\\infty} $ is square summable: $$ \\sum_{j=0}^{\\infty} \\psi_{j}^{2}\u003c\\infty $$ a slightly stronger conditi","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:2:3","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"2.3 ARMA(p,q) model In time series analysis, we use stationary autoregressive moving average (ARMA) models to reveal the autocorrelation structures in the data. An typical ARMA(p,q) model could be established as $$ X_{t}=c+\\underbrace{\\rho_{1} X_{t-1}+\\cdots+\\rho_{p} X_{t-p}}_{\\text {AR component }}+\\epsilon_{t}+\\underbrace{\\theta_{1} \\epsilon_{t-1}+\\cdots+\\theta_{q} \\epsilon_{t-q}}_{\\text {MA component }} $$ Stationary Condition: the roots of $ 1-\\rho_{1} z-\\rho_{2} z^{2}-\\ldots, \\rho_{p} z^{p}=0 $ are outside of the unit circle. While in practice, we usually use ARMA (1,1) model to capture the dynamics in empirical time series data. ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:3:0","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"ARMA model construction \r Box-Jenkins Modeling Philosophy Four-steps procedure: Transform the data to be stationary (taking log or difference); Make an initial guess of small values for $ p $ and $ q $ for an $ A R M A(p, q) $; Estimate the parameters in $ \\Phi(L) $ (AR parameters) and $ \\theta(L) $ (MA parameters); Perform a diagnostic analysis to confirm the model; Identify the degree of MA process Sample Autocorrelation is given by $$ \\widehat{\\rho}{j}=\\frac{\\widehat{\\gamma}{j}}{\\hat{\\gamma}{0}} $$ For a pure $ M A(q) $ process, for $ j\u003eq, $ we have $$ E\\left(\\widehat{\\rho}{j}\\right)=0, \\quad \\operatorname{Var}\\left(\\widehat{\\rho}{j}\\right)=\\frac{1}{T}\\left(1+2 \\sum{i=1}^{q} \\rho_{i}^{2}\\right) $$ One can identify the degree of MA process by checking for the smallest value of $ h $ such as that all $ \\hat{\\rho}_{j} $ for any $ j\u003eh, $ stays within $$ \\pm \\frac{1.96}{\\sqrt{T}} \\sqrt{\\left(1+2 \\sum_{i=1}^{h-1} \\widehat{\\rho}_{i}^{2}\\right)} $$ It means that its ACF cuts off at lag q. Identify the degree of AR process Consider the regression $$ y_{t+1}=\\widehat{c}+\\widehat{\\alpha}_{1}^{(m)} y_{t}++\\widehat{\\alpha}_{2}^{(m)} y_{t-1}+\\ldots++\\widehat{\\alpha}_{m}^{(m)} y_{t-m+1}+\\widehat{e}_{t} $$ where $ \\hat{\\alpha}_{m}^{(m)} $ is called $ m- $ th **partial autocorrelation (PACF)**. The estimation formula is $$ \\widehat{\\alpha}^{(m)}=\\left(\\begin{array}{r} \\gamma_{0}, \\gamma_{1}, \\gamma_{2}, \\ldots \\ldots \\gamma_{m-1} \\ \\gamma_{1}, \\gamma_{0}, \\gamma_{1}, \\ldots \\ldots \\gamma_{m-2} \\ \\ldots \\ \\gamma_{m-1}, \\gamma_{m-2}, \\gamma_{m-3}, \\ldots \\ldots \\gamma_{0} \\end{array}\\right)^{-1}\\left(\\begin{array}{c} \\gamma_{1} \\ \\gamma_{2} \\ \\ldots \\ \\gamma_{m} \\end{array}\\right) $$ If the data were really generated by an $ A R(p) $ process, for $ m\u003ep, $ we have $$ E\\left(\\widehat{\\alpha}_{m}^{(m)}\\right)=0, \\quad \\operatorname{Var}\\left(\\widehat{\\alpha}_{m}^{(m)}\\right)=\\frac{1}{T} $$ One can use this result to identify the degree of AR process by checking for which value of $ h $ such as that $ \\widehat{\\alpha}_{h}^{(h)} $ stays within $$ \\pm \\frac{1.96}{\\sqrt{T}} $$ It means that its PACF cuts off at lag q. Summary: In practice, we use Box-Jenkins method to determine the orders in AR(p) and $ \\mathrm{MA}(\\mathrm{q}) $ For a stationary $ \\mathrm{AR}(\\mathrm{p}) $ model, its ACF decays, while its PACF cuts off at lag $ p $. For a stationary MA(q) model, its PACF decays, while its ACF cuts off at lag $ q $ \r \r \r Ljung-Box test for white noise $ \\mathrm{H}0 : r{t} $ is a white noise process; $ \\mathrm{H}1: r{t} $ is not a white noise process. The Ljung-Box test statistic is defined as $$ Q_{m}=T(T+2) \\sum_{j=1}^{m} \\frac{1}{T-j} \\widehat{\\rho}_{j}^{2} $$ where $ m \\geq 1 $ is a prescribed integer. We reject the null hypothesis at significant level $ \\alpha $ if $ Q_{m}\u003e\\chi_{\\alpha, m}^{2}, $ where $ \\chi_{\\alpha, m}^{2} $ is the top $ \\alpha $ -th percentile of the $ \\chi^{2} $ distribution with $ m $ degrees of freedom. Residual diagnostics - An adequate model $$ X_{t}=\\alpha+\\rho_{1} X_{t-1}+\\epsilon_{t}+\\theta_{1} \\epsilon_{t-1} $$ An ARMA $ (p, q) $ model is said to be **adequate** if there is no serial correlation in the residuals. In other words, if you can still find significant autocorrelation in the residuals, the model is not adequate. Then, you should extend your model to capture remaining structures of autocorrelations. Note that when we test for white noise in the residuals of an estimate ARMA(p,q) model, we should adjust the distribution in the Ljung-Box test from $ \\chi_{m}^{2} $ to $ \\chi_{m-p-q}^{2} $. \r Model identification based on information criteria The previous method is powerless in detecting overfitting, which leads to an unnecessarily complicated model with some redundant parameters. Therefore, they increase errors in the estimated parameters. To solve this problem, we use AlC and BIC to combine the consideration on both the goodness of the fit and the simplicity of the model by penalizing extra terms in the model. ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:3:1","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"Forecast based on ARMA models Suppose that stock market return follows a stationary AR(1) process $$ r_{t}=c+\\rho r_{t-1}+\\epsilon_{t} $$ where $ \\epsilon_{t} \\sim W N\\left(0, \\sigma^{2}\\right) . $ Let $ F_{t}=\\left{r_{1}, r_{2}, \\ldots, r_{t}\\right} $ denote the information set up to time $ t $. Compute the 1-step ahead forecast of $ r_{T} $ based on $ F_{T}, $ i.e., $ E\\left[r_{T+1} \\mid F_{T}\\right] $ Compute the 2 -step ahead forecast of $ r_{T} $ based on $ F_{T}, $ i.e., $ E\\left[r_{T+2} \\mid F_{T}\\right] $ Compute the k-step ahead forecast of $ r_{T} $ based on $ F_{T}, $ i.e., $ E\\left[r_{T+k} \\mid F_{T}\\right] $ What happens if $ k \\rightarrow \\infty ? $ ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:3:2","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"2.4 Unit root process ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:4:0","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"2.4.1 Random walks Suppose that $ X_{t} $ follows a random walk process $$ X_{t}=c+X_{t-1}+e_{t} $$ for $ t=1,2, \\ldots, T, $ where $ c \\neq 0, e_{t} \\sim W N\\left(0, \\sigma^{2}\\right), $ and $ X_{0}=0 $. We can show that $ X_{t}=c t+\\sum_{s=1}^{t} e_{s} $ for $ t=1,2, \\ldots, T $. Then we can show that $ E\\left[X_{t}\\right]=c t, \\operatorname{Var}\\left[X_{t}\\right]=t \\sigma^{2} $. $ \\operatorname{Cov}\\left(X_{t}, X_{t-k}\\right)=(t-k) \\sigma^{2} $ \r Predicting R.W. processes Suppose that stock price follows a random walk process $$ P_{t}=c+P_{t-1}+\\epsilon_{t} $$ where $ \\epsilon_{t} \\sim W N\\left(0, \\sigma^{2}\\right) $ and $ P_{0} $ is a constant. Let $ F_{t}=\\left{P_{0}, P_{1}, \\ldots, P_{t}\\right} $ be the information set up to time $ t $. For some integer $ k\u003e1 $, compute the forecast value and variance of $ P_{t+k} $ based on $ F_{T}, $ i.e., $ E\\left[P_{T+k} \\mid F_{T}\\right] $ and $ \\operatorname{Var}\\left[P_{T+k} \\mid F_{T}\\right] . $ What happens if $ k \\rightarrow \\infty $ ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:4:1","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"2.4.2 Trend-stationary process A trend-stationary process is a deterministic time trend plus a stationary process. For example, $$ Y_{t}=\\alpha+\\delta t+e_{t} $$ where $ \\alpha+\\delta t $ is a linear time trend, $ e_{t} $ is a stationary process, e.g. stationary process like $ e_{t}=\\theta(L) \\epsilon_{t}, $ in which $ \\epsilon_{t} \\sim W N\\left(0, \\sigma^{2}\\right) . $ Then, $ \\epsilon_{t} $ only causes transitory shocks to $ Y_{t}, $ i.e., the effect caused by $ \\epsilon_{t} $ to $ Y_{t+k} $ decays with $ k $. Question: Suppose that $ Y_{t}=\\alpha+\\delta t+e_{t}, e_{t}=0.5 e_{t-1}+\\epsilon_{t}, $ where $ \\epsilon_{t} \\sim W N\\left(0, \\sigma^{2}\\right) . $ Compute the effect caused by $ \\epsilon_{t} $ to $ Y_{t+k}, $ i.e., the change of $ Y_{t+k} $ for one unit change in $ \\epsilon_{t} $. Suppose that $ \\Phi(L) X_{t}=\\theta(L) \\epsilon_{t}, $ then $ X_{t} $ is called a unit root process if $ \\Phi(z)=0 $ has a root of unity $ (z=1) . $ e.g., random walk $$ X_{t}=X_{t-1}+e_{t} $$ is a unit root process, where $ e_{t} $ is a stationary process. Question: Suppose that $ X_{t}=X_{t-1}+e_{t}, e_{t}=0.5 e_{t-1}+\\epsilon_{t}, $ where $ \\epsilon_{t} \\sim W N\\left(0, \\sigma^{2}\\right), $ compute the effect caused by $ \\epsilon_{t} $ to $ X_{t+k} $ \r \r The Dickey-Fuller (DF) unit root test $ \\mathbb{H}{0}: X{t} $ has a unit root $ \\mathbb{H}{1}: X{t} $ is (trend)-stationary Test equations: (None) $ \\Delta X_{t}=\\rho X_{t-1}+e_{t} $ (Constant) $ \\Delta X_{t}=\\alpha+\\rho X_{t-1}+e_{t} $ (Constant+Trend) $ \\Delta X_{t}=\\alpha+\\delta t+\\rho X_{t-1}+e_{t} $ Equivalently, we need to test $$ \\mathbb{H}{0}: \\rho=0 \\quad \\text { v.s. } \\quad \\mathbb{H}{1}: \\rho\u003c0 $$ ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:4:2","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"2.4.2 Stationary linear process 【Linear I(0) processes】A linear I(0) process can be written as a constant plus a zero-mean linear process $ \\left{u_{t}\\right} $ such that $$ u_{t}=\\psi(L) \\varepsilon_{t}, \\psi(L) \\equiv \\psi_{0}+\\psi_{1} L+\\psi_{2} L^{2}+\\cdots \\text { for } t=0,\\pm 1,\\pm 2, \\ldots $$ $ \\left{\\varepsilon_{t}\\right} $ is independent white noise (i.i.d. with mean 0 and $ \\mathrm{E}\\left(\\varepsilon_{t}^{2}\\right) \\equiv \\sigma^{2}\u003e0 $） $$ \\begin{array}{c} \\sum_{j=0}^{\\infty} j\\left|\\psi_{j}\\right|\u003c\\infty \\ \\psi(1) \\neq 0 \\end{array} $$ Beveridge-Nelson decomposition Approximating I(1) by a Random Walk: Let $ \\left{\\xi_{t}\\right} $ be I(1) so that $ \\Delta \\xi_{t}=\\delta+u_{t} $ where $ u_{t} \\equiv \\psi(L) \\varepsilon_{t} $ is a zero-mean $ \\mathrm{I}(0) $ process with $ \\mathrm{E}\\left(\\xi_{0}^{2}\\right)\u003c\\infty . $ Using the following identity: $$ \\begin{array}{c} \\psi(L)=\\psi(1)+\\Delta \\alpha(L), \\quad \\Delta \\equiv 1-L \\ \\alpha(L) \\equiv \\sum_{j=0}^{\\infty} \\alpha_{j} L^{j}, \\quad \\alpha_{j}=-\\left(\\psi_{j+1}+\\psi_{j+2}+\\cdots\\right) \\quad(j=0,1,2, \\ldots) \\end{array} $$ we can write $ u_{t} $ as $$ u_{t} \\equiv \\psi(L) \\varepsilon_{t}=\\psi(1) \\cdot \\varepsilon_{t}+\\eta_{t}-\\eta_{t-1} \\text { with } \\eta_{t} \\equiv \\alpha(L) \\varepsilon_{t} $$ It can be shown that $ \\alpha(L) $ is absolutely summable. So, by Proposition $ 6.1(\\mathrm{a}),\\left{\\eta_{t}\\right} $ is a well-defined zero-mean covariance-stationary process (it is actually ergodic stationary by Proposition $ 6.1(\\mathrm{d}) $ ) . Substituting $ (9.2 .6) $ into $ (9.1 .4), $ we obtain (what is known in econometrics as) the Beveridge-Nelson decomposition: $$ \\begin{aligned} \\xi_{t} \u0026=\\delta \\cdot t+\\sum_{s=1}^{t}\\left[\\psi(1) \\cdot \\varepsilon_{s}+\\eta_{s}-\\eta_{s-1}\\right]+\\xi_{0} \\ \u0026=\\delta \\cdot t+\\psi(1) \\sum_{s=1}^{t} \\varepsilon_{s}+\\eta_{t}+\\left(\\xi_{0}-\\eta_{0}\\right)\\left(\\text { since } \\sum_{s=1}^{t}\\left(\\eta_{s}-\\eta_{s-1}\\right)=\\eta_{t}-\\eta_{0}\\right) \\end{aligned} $$ Wiener process 【The Wiener Process】The next two sections will present a variety of unit-root tests. The limiting distributions of their test statistics will be written in terms of Wiener processes (also called Brownian motion processes). Some of you may already be familiar with this from continuous-time finance, but to refresh your memory, 【Standard Wiener processes】A standard Wiener (Brownian motion) process $ W(\\cdot) $ is a continuous-time stochastic process, associating each date $ t \\in[0,1] $ with the scalar random variable $ W(t), $ such that $ W(0)=0 $ for any dates $ 0 \\leq t_{1}\u003ct_{2}\u003c\\cdots\u003ct_{k} \\leq 1 $, the changes $$ W\\left(t_{2}\\right)-W\\left(t_{1}\\right), W\\left(t_{3}\\right)-W\\left(t_{2}\\right), \\ldots, W\\left(t_{k}\\right)-W\\left(t_{k-1}\\right) $$ are independent multivariate normal with $ W(s)-W(t) \\sim N(0,(s-t)) $ (so in $ \\text { particular } W(1) \\sim N(0,1)$). 3. for any realization, $ W(t) $ is continuous in $ t $ with probability 1. ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:4:3","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"2.4.3 Dickey-Fuller unit root test In the first case with no intercept and no trend, we construct two test statistics as $$ T \\cdot \\widehat{\\rho} \\rightarrow_{d} \\frac{\\frac{1}{2}\\left(W(1)^{2}-1\\right)}{\\int_{0}^{1} W(r)^{2} d r} \\triangleq D F_{\\rho} $$ and $$ t=\\frac{\\widehat{\\rho}}{\\operatorname{se}(\\widehat{\\rho})} \\rightarrow_{d} \\frac{\\frac{1}{2}\\left(W(1)^{2}-1\\right)}{\\sqrt{\\int_{0}^{1} W(r)^{2} d r}} \\triangleq D F_{t} $$ in which $ D F_{\\rho} $ and $ D F_{t} $ are not the usual $ t $ -distribution. \r The Augmented Dickey-Fuller (ADF) unit root test $ \\mathbb{H}{0}: X{t} $ has a unit root $ \\mathbb{H}{1}: X{t} $ is (trend)-stationary Test equations: (None) $ \\Delta X_{t}=\\rho X_{t-1}+\\theta_{1} \\Delta y_{t-1}+\\ldots+\\theta_{p} \\Delta y_{t-p}+e_{t} $ (Constant)$ \\Delta X_{t}=\\alpha+\\rho X_{t-1}+\\theta_{1} \\Delta y_{t-1}+\\ldots+\\theta_{p} \\Delta y_{t-p}+e_{t} $ (Constant+Trend) $ \\Delta X_{t}=\\alpha+\\delta t+\\rho X_{t-1}+\\theta_{1} \\Delta y_{t-1}+\\ldots+\\theta_{p} \\Delta y_{t-p}+e_{t} $ Equivalently, we need to test $$ \\mathbb{H}{0}: \\rho=0 \\quad \\text { v.s. } \\quad \\mathbb{H}{1}: \\rho\u003c0 $$ \r \r ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:4:4","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"2.5 Order of integration 【integration】If time series $ X_{t} $ is a stationary process, then $ X_{t} \\sim I(0) $; If time series $ X_{t} $ is an $ I(\\mathrm{d}) $ process, if and only if $ \\Delta^{j} X_{t} $ for $ 1 \\leq j\u003cd $ contain unit roots, and $ \\Delta^{d} X_{t} \\sim I(0) $ (stationary). 【Cointegration】Suppose that $ X_{t} $ and $ Y_{t} $ are $ I(\\mathrm{d}) $ time series, and there exists some linear combination such that $$ Y_{t}-X_{t}^{\\prime} \\beta \\sim 1(0) $$ then, $ X_{t} $ and $ Y_{t} $ are said to be **cointegrated**, and $ \\beta $ is called the cointegrating vector (coefficient). 2003 Nobel Prize winner: Clive W.J. Granger and Robert F. Engle for their research on ‘cointegration’. \r Relationship between 1 year and 3 month interest rates. \r The spread (difference) between 1 year and 3 month interest rates. \r Trend removal methods In classical time series models, the data are required to be stationary. If in practice, the data is nonstaitonary with trends, then one needs to remove the trend first to stationarize the data. Common trend-removal methods include Taking difference; Trend estimation and subtraction; HP filter; … HodrickPrescott filter Suppose that $$ y_{t}=\\tau_{t}+c_{t}+\\epsilon_{t} $$ where $ \\tau_{t} $ is trend, $ c_{t} $ is cycles, and $ \\epsilon_{t} $ is disturbance. Given an adequately chosen, positive value of $ \\lambda $, there is a trend component that will solve $$ \\min {\\tau}\\left(\\sum{t=1}^{T}\\left(y_{t}-\\tau_{t}\\right)^{2}+\\lambda \\sum_{t=2}^{T-1}\\left[\\left(\\tau_{t+1}-\\tau_{t}\\right)-\\left(\\tau_{t}-\\tau_{t-1}\\right)\\right]^{2}\\right) $$ ii.e. 拟合程度+惩罚系数*二阶导的大小(平滑程度) $ \\lambda $ should equal 6.25 for annual data, 1,600 for quarterly data, and 129,600 for monthly data. An example: Global temperature anomalies \r \r \r library(mFilter) f1gta=hpfilter(GTA,freq=120,type=c(\"lambda\")) plot(f1gta) f2gta=hpfilter(GTA,freq=1200,type=c(\"lambda\")) plot(f2gta) f3gta=hpfilter(GTA,freq=12000,type=c(\"lambda\")) plot(f3gta) f1gta ## Title: ## Hodrick-Prescott Filter ## Call: ## hpfilter(x = GTA, freq = 120, type = c(\"lambda\")) ## Method: ## hpfilter ## Filter Type: ## lambda ## Series: ## GTA ## GTA Trend Cycle ## 1 -0.255273 -0.285225 0.029952 ## 2 -0.325364 -0.285402 -0.039962 ## 3 -0.239818 -0.285330 0.045511 ## 4 -0.270364 -0.285091 0.014727 ## 5 -0.280818 -0.284390 0.003572 ## 6 -0.337727 -0.282809 -0.054918 ## 7 -0.233364 -0.279899 0.046535 \r \r \r \r \r ","date":"0001-01-01","objectID":"/2.-linear-time-series-models/:5:0","tags":null,"title":"","uri":"/2.-linear-time-series-models/"},{"categories":null,"content":"3. GARCH Type Models ","date":"0001-01-01","objectID":"/3.-garch-type-models/:0:0","tags":null,"title":"","uri":"/3.-garch-type-models/"},{"categories":null,"content":"3.1 ARCH effect - volatility clustering ARCH: autoregressive conditional heteroskedasticity. \r ARCH effect - ACFs of r \r ARCH effect - ACFs of $ r^2 $ \r ARCH effect - ACFs of $ | r | $ \r ARCH models Let $ P_{t} $ denote the price and $ r_{t}=\\log \\left(P_{t} / P_{t-1}\\right) $ be the log-return at time $ t $. Then $$ r_{t}=\\mu_{t}+e_{t} $$ where $ \\mu_{t}=E\\left[r_{t} \\mid F_{t-1}\\right] $ denotes the conditional mean of the return, $ e_{t} $ is a diffusion term which may be modeled as $$ e_{t}=\\sigma_{t} \\epsilon_{t}, \\quad \\epsilon_{t} \\sim i . i . d .(0,1) $$ where $ \\sigma_{t}=\\operatorname{Var}\\left[r_{t} \\mid F_{t-1}\\right]\u003e0 $ is determined by the information available before time $ t, $ and $ \\epsilon_{t} $ is assumed to be independent of $ \\sigma_{t} $. 20 ARCH models A simple specification for the volatility function $ \\sigma_{t} $ is the autoregressive conditional heteroskedastic (ARCH) model $$ \\sigma_{t}^{2}=a_{0}+a_{1} e_{t-1}^{2}+a_{2} e_{t-2}^{2}+\\ldots+a_{p} e_{t-p}^{2} $$ where $ a_{0}\u003e0, a_{j} \\geq 0(1 \\leq j \\leq p) $ are constants, and $ p $ is a positive integer. This model allows for capturing the effect of volatility clustering. ARCH(1) model We consider the ARCH(1) model as a special example. $$ \\begin{aligned} e_{t} \u0026=\\sigma_{t} \\epsilon_{t} \\ \\sigma_{t}^{2} \u0026=a_{0}+a_{1} e_{t-1}^{2} \\end{aligned} $$ where $ a_{0}\u003e0, a_{1} \\geq 0 . $ In particular, by the law of iterated expectation (LIE), we have $$ \\begin{aligned} E\\left[e_{t}\\right] \u0026=0 \\ \\operatorname{Var}\\left[e_{t}\\right] \u0026=\\frac{a_{0}}{1-a_{1}} \\end{aligned} $$ where $ 0 \\leq a_{1}\u003c1 $ Suppose that $ \\epsilon_{t} \\sim i . i . d . N(0,1), $ then $$ E\\left(e_{t}^{4} \\mid F_{t-1}\\right)=3\\left[E\\left(e_{t}^{2} \\mid F_{t-1}\\right)\\right]^{2}=3\\left(a_{0}+a_{1} e_{t-1}^{2}\\right)^{2} $$ Therefore, $$ E\\left(e_{t}^{4}\\right)=E\\left[E\\left(e_{t}^{4} \\mid F_{t-1}\\right)\\right]=3 E\\left(a_{0}+a_{1} e_{t-1}^{2}\\right)^{2}=3 E\\left(a_{0}^{2}+2 a_{0} a_{1} e_{t-1}^{2}+a_{1}^{2} e_{t-1}^{4}\\right) $$ Assume that $ m_{4}=E\\left(e_{t}^{4}\\right) $ is stationary, then $$ m_{4}=\\frac{3 a_{0}^{2}\\left(1+a_{1}\\right)}{\\left(1-a_{1}\\right)\\left(1-3 a_{1}^{2}\\right)} $$ ARCH (1) model To ensure that $ m_{4}\u003e0, $ we have $ 0 \\leq a_{1}^{2}\u003c\\frac{1}{3} . $ Then, the Kurtosis of $ e_{t} $ is $$ \\frac{E\\left(e_{t}^{4}\\right)}{\\left[\\operatorname{Var}\\left(e_{t}\\right)^{2}\\right]^{2}}=3 \\frac{a_{0}^{2}\\left(1+a_{1}\\right)}{\\left(1-a_{1}\\right)\\left(1-3 a_{1}^{2}\\right)} \\times \\frac{\\left(1-a_{1}\\right)^{2}}{a_{0}^{2}}=3 \\frac{1-a_{1}^{2}}{1-3 a_{1}^{2}}\u003e3 $$ This result implies that $ e_{t} $ has positive excess kurtosis, and it is more likely to see outliers for $ e_{t} $. ARCH (1) model How to forecast volatility based on an estimated ARCH(1) model? Suppose that we had estimated an ARCH(1) model $$ \\begin{aligned} e_{t} \u0026=\\sigma_{t} \\epsilon_{t} \\ \\sigma_{t}^{2} \u0026=a_{0}+a_{1} e_{t-1}^{2} \\end{aligned} $$ using $ F_{T}, $ what are the one-step ahead forecast $ E\\left[\\sigma_{T+1}^{2} \\mid F_{T}\\right] $ and two-step ahead forecast $ E\\left[\\sigma_{T+2}^{2} \\mid F_{T}\\right] $ given the information set $ F_{T} ? $ GARCH models Bollerslev (1986) developed the $ \\mathrm{GARCH}(\\mathrm{p}, \\mathrm{q}) $ model as $$ \\begin{aligned} r_{t} \u0026=\\mu+e_{t} \\ e_{t} \u0026=\\sigma_{t} \\epsilon_{t}, \\quad \\epsilon_{t} \\sim i . i . d .(0,1) \\ \\sigma_{t}^{2} \u0026=a_{0}+\\underbrace{a_{1} e_{t-1}^{2}+\\ldots+a_{p} e_{t-p}^{2}}_{\\text {ARCH components }}+\\underbrace{b_{1} \\sigma_{t-1}^{2}+\\ldots+b_{q} \\sigma_{t-q}^{2}}_{\\text {GARCH components }} \\end{aligned} $$ where $ a_{0}\u003e0, a_{i} \\geq 0, b_{j} \\geq 0, $ for $ 1 \\leq i \\leq p, 1 \\leq j \\leq q, $ and $$ \\sum_{i=1}^{p} a_{i}+\\sum_{j=1}^{q} b_{j}\u003c1 $$ GARCH models It is easy to show that $$ \\begin{aligned} E\\left[e_{t}\\right] \u0026=0 \\ \\operatorname{Var}\\left[e_{t}\\right] \u0026=\\frac{a_{0}}{1-a_{1}-\\ldots-a_{p}-b_{1}-\\ldots-b_{q}} \\ \u0026=\\frac{a_{0}}{1-\\sum_{i=1}^{p} a_{i}-\\sum_{j=1}^{q} b_{j}} \\end{aligned} $$ which denote the long-run variance. Residual ","date":"0001-01-01","objectID":"/3.-garch-type-models/:1:0","tags":null,"title":"","uri":"/3.-garch-type-models/"},{"categories":null,"content":"Threshold Autoregressive (TAR) Model To capture asymmetric features in financial data, we need to establish Threshold Autoregressive (TAR) models. For example, a TAR(1) model is constructed as $$ X_{t}=\\left{\\begin{array}{ll} \\alpha_{1} X_{t-1}+u_{t}, \u0026 X_{t-1}\u003cc \\ \\alpha_{2} X_{t-1}+u_{t}, \u0026 X_{t-1} \\geq c \\end{array}\\right. $$ for some threshold value $ c $. In a compact form, we write the model as $$ X_{t}=\\alpha_{1} X_{t-1} 1\\left(X_{t-1}\u003cc\\right)+\\alpha_{2} X_{t-1} 1\\left(X_{t-1} \\geq c\\right)+u_{t} $$ Question: How to estimate $ \\alpha_{1}, \\alpha_{2} $ and $ c ? $ Why TAR model is able to capture asymmetric features in financial data? We consider the following example. $$ X_{t}=\\left{\\begin{array}{cl} -1.5 X_{t-1}+u_{t}, \u0026 X_{t-1}\u003c0 \\ 0.5 X_{t-1}+u_{t}, \u0026 X_{t-1} \\geq 0 \\end{array}\\right. $$ If $ X_{t-1}\u003c0, $ then there is a very strong tendency for $ X_{t} $ to jump to a positive value. If $ X_{t-1}\u003e0, $ then it will tend to take multiple time periods for $ X_{t} $ to reduce to a negative value. Therefore, there are more positive observations than negative observations. The Momentum Threshold Autoregressive (MTAR) Model $$ X_{t}=\\left{\\begin{array}{ll} \\alpha_{1} X_{t-1}+u_{t}, \u0026 \\Delta X_{t-1}\u003c0 \\ \\alpha_{2} X_{t-1}+u_{t}, \u0026 \\Delta X_{t-1} \\geq 0 \\end{array}\\right. $$ Self-Exciting Threshold Autoregressive (SETAR) Model We consider a more general case as $$ X_{t}=\\alpha_{0}^{(s)}+\\sum_{j=1}^{p} \\alpha_{j}^{(s)} X_{t-j}+h^{(s)} u_{t} \\text { if } X_{t-d} \\in\\left(r_{s-1}, r_{s}\\right] $$ for $ s=1,2, \\ldots, M, $ where $ {s}_{s=1}^{M} $ are different states. Since **the threshold or regime change is generated by $ \\left{X_{t}\\right} $ itself** rather than by other variables, we call it a Self-Exciting Threshold Autoregressive model. Example: Managed Foreign Exchange Rates with Floors and Ceilings $$ X_{t}=\\left{\\begin{array}{cc} a+u_{t}^{(1)} \u0026 \\text { if } X_{t-1} \\leq a \\ X_{t-1}+u_{t}^{(2)} \u0026 \\text { if } a\u003cX_{t-1}\u003cb \\ b+u_{t}^{(3)} \u0026 \\text { if } X_{t-1} \\geq b \\end{array}\\right. $$ If-Exciting Threshold Autoregressive (SETAR) Model Example: U.S. quarterly real GNP from 1947:2 to 1991:1. $$ X_{t}=\\left{\\begin{array}{cc} -0.015-1.076 X_{t-1}+\\epsilon_{1 t}, \u0026 X_{t-1} \\leq X_{t-2} \\leq 0 \\ 0.630 X_{t-1}-0.756 X_{t-2}+\\epsilon_{2 t}, \u0026 X_{t-1}\u003eX_{t-2}, X_{t-2} \\leq 0 \\ 0.006+0.438 X_{t-1}+\\epsilon_{3 t}, \u0026 X_{t-1} \\leq X_{t-2}, X_{t-2}\u003e0 \\ 0.433 X_{t-1}+\\epsilon_{4 t}, \u0026 X_{t-1}\u003eX_{t-2}\u003e0 \\end{array}\\right. $$ Remarks: Regime 1 : a recession period in which the economy changed from a contraction to an even worse one. The negative explosive nature of the regression function in this regime indicates the economy usually recovers quickly from the recession. Regime 2: a period in which the economy was in contraction but improving. Here, the regression function tends to be positive, suggesting that the economy is more likely to grow continuously out of recession once a recovery has started. Regime 3: a period in which the economy was reasonable but the growth declined. Regime 4: an expansion period in which the economy became stronger. Teräsvirta (1994, JASA) $$ X_{t}=\\left(\\alpha_{00}+\\sum_{j=1}^{p} \\alpha_{0 j} X_{t-j}\\right)+F\\left(\\frac{X_{t-d}-c}{\\gamma}\\right)\\left(\\alpha_{10}+\\sum_{j=1}^{p} \\alpha_{1 j} X_{t-j}\\right)+u_{t} $$ where $ d $ is the delay parameter, $ c $ and $ \\gamma $ are parameters representing the location and scale of regime transition, and $ F(.) $ is a smooth transition function, which is usually a continuous cumulative distribution function. Examples of $ F(.) $ include the $ \\mathrm{N}(0,1) \\mathrm{CDF}, $ the logistic function, and the exponential CDF. Smooth Transition AR (STAR) Model Logistic function $$ F(z)=\\frac{1}{1+e^{-\\gamma(z-c)}}, \\quad-\\infty\u003cz\u003c\\infty $$ where $ c $ determines the location of regime transition, while $ \\gamma $ determines the smoothness of the regime shift. Exponential function $$ F(z)=1-\\frac{1}{e^{-\\gamma(z-c)^{2}}}, \\quad-\\infty\u003cz\u003c\\infty $$ where $ c $ determines ","date":"0001-01-01","objectID":"/4.-nonlinear-autoregressive-models/:0:0","tags":null,"title":"","uri":"/4.-nonlinear-autoregressive-models/"},{"categories":null,"content":"Factor Model Three Types of Asset Return Factor Models Macroeconomic factor model: Factors are observable economic and financial time series Fundamental factor model: Factors are created from observerable asset characteristics Statistical factor model: Factors are unobservable and extracted from asset returns Suppose that there are $ k $ assets and $ T $ time periods. Let $ r_{i t} $ be the return of asset $ i $ in the time period $ t $. A general form for the factor model is $$ r_{i t}=\\alpha_{i}+\\beta_{i 1} f_{1 t}+\\ldots+\\beta_{i m} f_{m t}+\\epsilon_{i t}=\\alpha_{i}+b_{i}^{\\prime} f_{t}+\\epsilon_{i t} $$ for $ t=1,2, \\ldots, T, i=1,2, \\ldots, k, $ where $ \\alpha_{i} $ is the intercept term, $ f_{t}=\\left(f_{1 t}, \\ldots, f_{m t}\\right)^{\\prime} $ are $ m $ common factors, note that $ f_t $ only changing with time $ t $ not stock $ m $, $ b_{i}=\\left(\\beta_{i 1}, \\ldots, \\beta_{i m}\\right)^{\\prime} $ is called the vector of factor loadings for asset $ i, \\epsilon_{i t} $ is the specific factor of asset $ i . $ The factor vector $ f_{t}=\\left(f_{1 t}, \\ldots, f_{m t}\\right)^{\\prime} $ is assumed to be an $ m $ -dimensional stationary process such that $$ \\begin{aligned} E\\left(f_{t}\\right) \u0026=\\mu_{f} \\ \\operatorname{Cov}\\left(f_{t}\\right) \u0026=\\Sigma_{f} \\end{aligned} $$ At time $ t, $ we can write the model as $$ r_{t}=\\alpha+B f_{t}+\\epsilon_{t} $$ where $ r_{t}=\\left(r_{1 t}, \\ldots, r_{k t}\\right)^{\\prime} $ is a $ k \\times 1 $ vector, $ \\alpha=\\left(\\alpha_{1}, \\ldots, \\alpha_{k}\\right)^{\\prime}, B $ is a $ k \\times m $ factor loading matrix, and $ \\epsilon_{t}=\\left(\\epsilon_{1 t}, \\ldots, \\epsilon_{k t}\\right)^{\\prime} $ for which $ \\operatorname{Cov}\\left(\\epsilon_{t}\\right)=D=\\operatorname{diag}\\left(\\sigma_{1}^{2}, \\ldots, \\sigma_{k}^{2}\\right) . $ Then, we have the covariance matrix $$ \\operatorname{Cov}\\left(r_{t}\\right)=B \\Sigma_{f} B^{\\prime}+D $$ For fixed $ i, $ we can write the model as $$ R_{i}=\\alpha_{i} 1_{T}+F b_{i}^{\\prime}+E_{i} $$ where $ R_{t}=\\left(r_{11}, \\ldots, r_{1} T\\right)^{\\prime} $ is a $ T \\times 1 $ vector, $ 1_{T}=(1, \\ldots, 1)^{\\prime} $ is a $ T $ -dimensional vector of ones, $ F $ is a $ T \\times m $ matrix whose $ t $ th row is $ f_{t}^{\\prime}, $ and $ E_{i}=\\left(\\epsilon_{i 1}, \\ldots, \\epsilon_{i T}\\right)^{\\prime} $ for which $ \\operatorname{Cov}\\left(E_{i}\\right)=\\sigma^{2} I_{T} $ Capital Asset Pricing Model Capital Asset Pricing Model (CAPM) $$ r_{i t}=\\alpha_{i}+\\beta_{i} r_{m t}+\\epsilon_{i t} $$ for $ i=1,2, \\ldots, m, t=1,2, \\ldots, T . $ In the model, $ r_{i t} $ is the excess return of as set $ i, r_{m t} $ is the excess return of the market, and $ \\beta_{i} $ is the well-known $ \\beta $ (Beta) for the asset. Question: How to interpret $ \\alpha_{i} $ and $ \\beta_{i} $ in the CAPM? $ \\beta \u003e1$: Aggressive stock, Exhibits greater volatility than the market portfolio. $ \\beta=1 $ : Tracking stock, Tracks the market portfolio exactly. $ 0\u003c\\beta\u003c1 $ : Conservative stock: Exhibits less volatility than the market portfolio. $ \\beta=0 $ : Independence: Independent of the market. $ -1\u003c\\beta\u003c0 $ : Imperfect hedge: Moves in the opposite direction to the market portfolio. $ \\beta=-1 $ : Perfect hedge: Moves in the exact opposite direction to the market portfolio. Fama-French three Factor Model In the Fama-French (1992,1993) method, combinations of portfolios are constructed to take account of the observed fact that Small stocks have higher average returns than large stocks; Value stocks have higher average returns than growth stocks; Therefore, they established the Fama-French three Factor Model (FF3FM) as $$ r_{i t}=\\alpha_{i}+\\beta_{i} r_{m t}+\\gamma_{i} S M B_{t}+\\theta_{i} H M L_{t}+\\epsilon_{i t} $$ for $ i=1,2, \\ldots, m, t=1,2, \\ldots, T $ In the model, $ r_{i t} $ is the excess return of asset $ i $ $ r_{m t} $ is the excess return of the market; $ SMB_t $ is the performance of small stocks relative to large stocks Small firms are more susceptible to changes in economic conditio","date":"0001-01-01","objectID":"/5.-factor-model-in-finance/:0:0","tags":null,"title":"","uri":"/5.-factor-model-in-finance/"},{"categories":null,"content":"A derivative can be defined as a financial instrument whose value depends on (or derives from) the values of other, more basic, underlying assets. Financial assets: stocks, interest rate, foreign exchange Others: weather, electricity, corn, and so on Markets: Exchanges and Over-the-Counter market (OTC) Exchanges: CME, CBOE, SHFE, CFFE, DCE, CZCE OTC markets \u0026 Central Counterparty Forward Contract （远期合约） Contract: two parties to buy or sell an asset at a specified price (forward price) on a future date. Maturity（到期日）or delivery date （交割日期）: the future date Underlying asset （标的资产） Forward price （远期价格） the nominal amount（名义金额、面额） Over-the-Counter (OTC，场外交易衍生品) Obligations for two parties, but with default risk （违约风险） Futures（期货） Legal Agreement: buy or sell a particular commodity or financial instrument at a predetermined price at a specified time in the future. quality and quantity of the commodity. Standardized and Exchanged-traded Similar, but different from Forward contracts. Options （期权） Call options （看涨期权）vs. Put Options （看跌期权） call option：to buy a certain asset by a certain date for a certain price, i.e. the strike price（敲定价格） put option：to sell a certain asset by a certain date for a certain price, i.e. the strike price European （欧式）vs. American （美式） A European option can be exercised（执行）only at maturity An American option can be exercised at any time during its life Options vs. Futures / Forwards A futures/forward contract gives the holder the obligation to buy or sell at a certain price An option gives the holder the right to buy or sell at a certain price 衍生工具的主要用途 To hedge （对冲）risks To speculate（投机）: take a view on the future direction of the market) To lock in an arbitrage（套利）profit To change the nature of a liability To change the nature of an investment without incurring the costs of selling one portfolio and buying another Positions （头寸） 头寸(Position)：投资人根据其对某一项资产未来价值走势的判断而持有的买入或者卖出该资产的立场 多方头寸(Long Position)：如果投资人看涨某项资产，那么投资人会买入该项资产，以期在未来获利 空方头寸(Short Position)：投资人认为某项资产的价格未来可能下跌，因而卖出了该项资产，以期在未来获利 卖空(Short Sell)：（通过融券）卖出自己不拥有的资产 衍生工具市场的参与者 按机构类别分；银行、证券公司、基金公司、保险公司、工商企业、个人 按参与者的作用划分：交易所、交易结算机构、经纪公司、经纪人、交易商、客户 按交易动机划分：套期保值者(Hedgers)、投机者(Speculators)、套利者(Arbitrageurs) Arbitrage （套利） 套利机会存在的条件 存在两个不同的资产组合，它们的未来现金流相同，但它们 的成本却不同 存在两个成本相同的组合，第一个组合在所有可能状态下的 现金流都不低于第二个组合，而且至少存在一种状态，在此 状态下第一个组合的现金流大于第二个组合 一个组合的成本为零，但在所有可能状态下，这个组合的损 益都不小于零，而且至少存在一种状态，在此状态下这个组 合的损益要大于零 套利的实现 买入被低估的资产（组合），卖出被高估的资产（组合） 通过融资/融券使初始投资为零 在期末获得正的利润 Futures Markets ","date":"0001-01-01","objectID":"/1.-introduction/:0:0","tags":null,"title":"","uri":"/1.-introduction/"},{"categories":null,"content":"1. 导论 [Toc] 债券(bond)是要求发行人(issuer)，也称债务人(debtor)或借款人(borrower)在规定期限内向债权人/投资者偿还借入款项并支付利息的债务工具。 如果持有至到期，则现金流模式是已知的(没有违约或赎回)； 在到期之前，债券市场价格会发生变化； 债务偿还的优先级高于股票。 ","date":"0001-01-01","objectID":"/1.-%E5%AF%BC%E8%AE%BA/:0:0","tags":null,"title":"","uri":"/1.-%E5%AF%BC%E8%AE%BA/"},{"categories":null,"content":"1.1 债券市场 ","date":"0001-01-01","objectID":"/1.-%E5%AF%BC%E8%AE%BA/:1:0","tags":null,"title":"","uri":"/1.-%E5%AF%BC%E8%AE%BA/"},{"categories":null,"content":"1.1.1 一级市场 v.s. 二级市场 一级市场：债券发行市场，借款人发行债券+投资者提供资金。 二级市场：债券交易市场，将一级市场发行的债券进行交易。 场外市场(Over-the-Counter, OTC)为主 做市商(dealers)提供报价，买价(bid price)和卖价(ask price) 我国场内市场主要以上海证券交易所(上交所)和深圳证券交易所(深交所)为主，交易绝大部分集中在上交所。场外市场主要包括银行间债券市场和商业银行国债柜台市场，交易绝大部分集中在银行间债券市场。 ","date":"0001-01-01","objectID":"/1.-%E5%AF%BC%E8%AE%BA/:1:1","tags":null,"title":"","uri":"/1.-%E5%AF%BC%E8%AE%BA/"},{"categories":null,"content":"1.1.1 美国债券市场的组成 【国债(Treasury bond)】美国政府发行的债券，包括国库券、中期债券和长期债券。美国财政部是全球最大的证券发行人。该市场对全球范围内的证券估值和利率决定起着至关重要的作用。 【政府机构债券(Agency bond)】联邦政府相关机构和政府资助企业发行的证券。 【市政债券(Municipal bond)】州和地方政府发行的债券。市政债券市场上发行的债券通常可以免缴联邦所得税，因此通常被称为免税债券(tax-exempt bond) 【公司债券(Corporate bond)】美国公司和非美国公司在美国发行的债券。 【资产支持证券(Asset-backed securities, ABS)】用资产池为抵押发行的债券。 煎饼摊证券化的交易结构 \r 【抵押贷款支持证券(Mortgage-backed securities, MBS)】由抵押贷款支持的债券。按住房类型分类： 住房抵押贷款支持证券(residential mortgage-backed securi-ties) 商业房地产抵押贷款支持证券(commercial mortgage-backed securities) 按借款人信用质量分类： 优质贷款 次级贷款：向信用评级较差的借款人发放的贷款。 ","date":"0001-01-01","objectID":"/1.-%E5%AF%BC%E8%AE%BA/:1:2","tags":null,"title":"","uri":"/1.-%E5%AF%BC%E8%AE%BA/"},{"categories":null,"content":"1.1.2 中国债券市场的组成 根据发行人的信用情况，发行人可以分两大 类，对应的债券可以分为两大类： 利率债，发行人为国家或信用等级与国家相当的机构，因而债券信用风险极低(一般不存在违约风险)，债券收益率主要受到利率变动影响； 信用债，即发行人没有国家信用背书，发行人信用情况是影响债券收益率的重要因素。 \r 来源：国联证券研究报告《债券分类解读》，2016-08-08 ","date":"0001-01-01","objectID":"/1.-%E5%AF%BC%E8%AE%BA/:1:3","tags":null,"title":"","uri":"/1.-%E5%AF%BC%E8%AE%BA/"},{"categories":null,"content":"1.2 债券特征 【发行人(issuer)】发行债券筹集资金。 联邦政府及其机构 地方政府(municipal governments) (国内和国外)公司 【到期期限(Term to Maturity)】债券发行人承诺履行偿债条款的日期。 有些条款可能允许债券发行人或债券持有人更改债券的到期期限 【本金价值(Principal value)】债券发行人同意在到期日向债券持有人偿还的金额，也称赎回价值(redemption value)、到期价值(maturity value)、面值(par value/face value)。 【息票利率(coupon rate)】也被称为名义利率(nominal rate)，是债券发行人同意每年支付的利率。在债券存续期内，每年向债券持有人支付的利息被称为息票(coupon)，一般6个月支付一次。 【零息债券(Zero-Coupon Bond)】持有人购买债券的价格远远低于其本金价值，以此获得利息。在到期日支持利息，所支付利息的确切金额等于本金价值与债券购买价格之差。 【浮动利率债券(Floating-rate bonds)】根据公式定期(在调息日)重新设定息票利率的债券。调息公式(coupon reset formula): $$ \\text{参考利率(reference rate) + 报价利差(quoted margin)} $$ 报价利差为债券发行人同意在参考利率之外额外支付的利率。例如，假设参考利率为1个月期伦敦银行同业拆借利率(LIBOR)，报价利差为150个基点(1个基点表示0.0001或0.01%)。那么调息公式为，1个月期 LIBOR+150个基点。 【反向浮动利率债券( inverse-floating-rate bonds或 inverse floaters)】息票利率与基准利率的变化方向相反。 【分期偿还特征】债券的本金偿还有两种方式： 到期时一次性偿还全部本金 在债券存续期内分期偿还本金 具有本金偿还时间表，称为分期偿还时间表(amortization schedule)； 属于分期偿还证券，债券到期期限没有实际意义，需要计算加权平均存续期(weighted average life )或称平均存续期(average life). 【嵌入式期权(Embedded Options)】债券契约中常常加入一种条款，给予债券所有人或/和债券发行人对另一方采取某种措施的期权 【赎回条款(Call provision) 】赋予发行人在规定到期日前全部或部分赎回债务的权利。 对债券发行人而言，赎回条款的好处在于：当市场利率下降时，债券发行人可以用息票利率较低的新债券代替息票利率较高的已发行债券(降低了融资成本)。 多数银行贷款也包含债务的提前赎回权 【回售条款(Put provision) 】给予债券持有人在指定日期按票面价值回售给发行人的权利。对于投资者而言，回售条款的优点是，如果在发行日后市场利率上升，因而使债券价格下降，那么投资者可以要求发行人以本金价值赎回债券。 【可转换债券(Convertible bond)】给予债券持有人用债券交换特定普通股的权利的债券。这种特征使债券持有人可以利用债券发行人普通股股价的有利变动 【可交换债券(Exchangeable bond)】允许债券持有人用债券交换特定数量的债券发行人以外的企业发行的普通股 嵌入式期权的存在使债券定价变得复杂，计算附有嵌人式期权的债券的价值时，由于给定债券中往往存在多个期权，因此定价过程会变得更加复杂。例如，一种债券中可能包括赎回条款、回售条款和可转换条款，而所有这些条款在不同情况下的重要性也不同。 ","date":"0001-01-01","objectID":"/1.-%E5%AF%BC%E8%AE%BA/:2:0","tags":null,"title":"","uri":"/1.-%E5%AF%BC%E8%AE%BA/"},{"categories":null,"content":"1.3 与债券投资有关的风险 利率风险 利率风险(Interest-rate risk)或市场风险(market risk)指利率上升带来的债券价格下跌的风险。 利率风险是迄今为止债券市场中投资者面临的主要风险。 票面利率越高，利率风险越大；到期日越远，利率风险越大；到期收益率(YTM)越高，利率风险越小 再投资风险 再投资风险(Reinvestment risk)指在期间现金流以通行市场利率再投资的时期内，通行市场利率下降的风险. 持有债券时间越长，再投资风险越高；同时，现金流越大、越早的债券(息票率越高的债券)的再投资风险也越高。 注意：利率风险和再投资风险之间具有此消彼长的关系. 赎回风险 赎回风险(Call risk): 当利率下跌时赎回债券被赎回的风险. 许多债券包含允许发行人在到期日之前提前偿还或赎回全部或部分债券的条款。从投资者的角度看，赎回条款有三个缺点: 投资者无法确知可赎回债券的现金流模式 投资者面临再投资风险，债券被赎回时，投资者不得不将债券收益进行再投资 由于可赎回债券的价格不会升到发行人赎回债券的价格之上很多，债券资本增值的可能性将会降低 信用风险 违约风险(Credit risk)指债券发行人无法履行及时支付利息和偿还所借款项的义务的风险。 信用利差风险(Credit spread )由于信用利差增加而使债券价格下降的风险. 债券收益率 = 无风险利率 + 风险溢价/信用利差 信用利差(Credit spread) 因信用风险产生的风险溢价或利差的部分. 信用降级风险(downgrade risk)，未预期到的债券或债券发行人的信用降级将会增加其在市场上的信用利差，导致债券价格下降。 三大评级公司：标准普尔、穆迪和惠誉 通货膨胀风险 通货膨胀风险(Inflation risk)指通货膨胀引起以购买力衡量的证券现金流价值波动而形成的风险。 除了浮动利率债券外，所有债券投资者都面临通货膨胀风险，如果反映了预期通货膨胀率，那么浮动利率债券的通货膨胀风险水平就较低。 汇率风险 汇率风险(Exchange-rate risk)指一种货币相对于另一种货币的非预期变动带来的风险。 例如，假设资者购买了用日元进行支付的债券，如果日元相对于美元贬值，那么投资者收到的美元将减少。 流动性风险 流动性风险(Liquidity risk)或可销性风险(marketability risk) 取决于债券以等于或接近债券价值的价格出售的难易程度。 对于计划持有债券到期并有能力这样做的个人投资者而言，流动性风险并不重要。 相反，机构投资者必须定期按市场价格调整头寸。按市价调整头寸(marking a position to market)，简称盯市(marking to market.)，是指投资组合管理者必须定期确定投资组合中各种债券的市场价值。为了获得反映市场价值的价格，债券的交易必须足够频繁。 衡量流动性的基本指标是买卖价差(bid-ask spread)，即交易商报出的买价和卖价之间的价差大小。交易商报出的价差越大，流动性风险越高。 波动性风险 波动性风险(Volatility risk)指波动性的变化对债券价格产生不利影响的风险。 比如，在附有嵌入式期权的可赎回债券中，利率向下的波动性增加，赎回期权的价格随之增加，因此债券价格将下跌。 模型风险 定价模型不准确带来的风险，利率衍生品中模型风险更大 风险的风险 风险的风险(Risk risk)指不知道证券的风险是什么的风险. 减轻或消除风险的风险的两种方法: 密切关注证券分析前沿方法的文献 远离未清楚了解的证券 ","date":"0001-01-01","objectID":"/1.-%E5%AF%BC%E8%AE%BA/:3:0","tags":null,"title":"","uri":"/1.-%E5%AF%BC%E8%AE%BA/"},{"categories":null,"content":"2. 债券定价 [Toc] ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:0:0","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.1 货币时间价值 ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:1:0","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.1.1 终值 【终值(Future Value)】：现在投资的任意一笔资金的终值 (Pn) 是: $$ P_n = P_0(1+r)^n $$ n = 期数 $ P_n $ = 从现在起n 期后的终值 $ P_0 $ = 初始本金 r = 每期利率 $ (1+r)^n $ 表示现在投资的$1在n期后以复利r计算的终值 Excel: Pn=P0*(1+r)^n 当每年的付息次数超过一次时，用于计算终值的利率和期数作如下调整后，代入上述公式计算即可: r = 年利率/ 每年付息的次数 n = 每年付息的次数*年数 与每年付息相比，每半年付息一次的终值更高，反映将所得利息再投资的机会也更好. 普通年金的终值 【年金】如果定期投资相同金额的资金，则该资金称为年金(annuity). 如果第一笔投资在从现在起一期时发生，则这笔投资称为普通年金(ordinary annuity). 如果立即收到第一笔支付，则这种年金称为即付年金(annuity due) 普通年金 终值(Pn) 的计算公式: $$ P_{n}=A(1+r)^{(n-1)}+A(1+r)^{(n-2)}+\\ldots+A=A\\left[\\frac{(1+r)^{n}-1}{r}\\right] $$ A = 年金的金额 . r = 年利率/ 每年支付利息的次数 n =每年付息的次数*年数 Excel: =FV(rate, nper, pmt, , 0) 其中0表示期末支付/普通年金，1则表示期初支付/即付年金 例：使用年利息计算普通年金的终值，如果年金A = $2,000,000, 利率r = 0.08, 并且 n = 15, 则Pn = ? $$ \\begin{aligned} \u0026P_{n}=A\\left[\\frac{(1+r)^{n}-1}{r}\\right] \\ \u0026P_{\\mathrm{n}}=$ 2,000,000\\left[\\frac{(1+0.08)^{15}-1}{0.08}\\right] \\end{aligned} $$ Excel: =FV(0.08,15,-2000000, ,0) 例：使用半年利息计息普遍年金，如果A = $ 2,000,000/2 = $1,000,000, r = 0.08/2 = 0.04, and n = 15*2 = 30, 那么 Pn = ? $$ \\begin{aligned} \u0026P_{n}=A\\left[\\frac{(1+r)^{n}-1}{r}\\right] \\ \u0026 P_{n}=$ 1,000,000\\left[\\frac{(1+0.04)^{30}-1}{0.04}\\right] \\end{aligned} $$ Excel: =FV(0.08/2,30,-2000000/2,,0) ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:1:1","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.1.2 现值 现值(present value)是终值的反向计算，即为了实现特定终值现在必须投资的货市金额: $$ P_{0}=P_{n} \\frac{1}{(1+r)^{n}} $$ r = 年利率/ 每年支付利息的次数 n =每年付息的次数*年数 对于未来特定时间的给定终值，利率(或贴现率)越高，现值越低. 对于给定利率，收到终值的时间离现在越久，现值越低. 系列终值的现值：为了确定一系列终值的现值，必须先计算每个终值的现值；然后，将这些现值加总以获得整个系列终值的现值。 $$ P V=\\sum_{i=1}^{n} \\frac{P_{t}}{(1+r)^{t}} $$ 普通年金的现值 普通年金的现值, $$ P V=P_n/(1+r)^n = A\\left[\\frac{1-1 /(1+r)^{n}}{r}\\right] $$ A = 年金的金额 (in dollars). r = 年利率÷ 每年支付利息的次数 n =每年付息的次数*年数 Excel: =PV(r,n,-A,0,0) 例：使用年利息计算普通年金的现值，如果A = $100, r = 0.09, n = 8, 那么PV = ? $$ \\begin{aligned} \u0026 P V=A\\left[\\frac{1-1 /(1+r)^{n}}{r}\\right]\\ \u0026 P V=$ 100\\left[\\frac{1-1 /(1+0.09)^{8}}{0.09}\\right] \\end{aligned} $$ Excel: =PV(0.09,8,-100,0,0) ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:1:2","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.2 债券定价 确定任何一种金融工具的价格都需要估算： 预期现金流 适当的必要收益率：必要收益率反映了在风险方法具有可比性(comparable risk)或可以作为替代投资(alternative investments)的金融工具的收益率，作为贴现率 发行人在规定到期日前不可赎回(未付期权)的债券的现金流包括： 截至到期日的定期息票利息支付额 到期日的票面价值 为了简化分析，首先作出了以下三个假设： 每6个月支付一次利息，对于多数美国国内债券而言，实际上是半年支付次利息。 下一次支付利息的时间恰好是从现在起6个月后 在债券期限内，利率是固定的。 通常，可以用以下公式计算债券价格: $$ P=\\sum_{t=1}^{n} \\frac{C}{(1+r)^{t}}+\\frac{M}{(1+r)^{n}} $$ P = 价格 n = 期数 (年数乘以2) t = 收取利息支付的时期 C = 半年息票支付额 r = 期间利率 (必要年收益率除以2) M = 到期价值 Excel: =PV(r,n,C,M,0) 例子：考虑利率为10% 、面值为 $1,000的20年期债券，假设该债券的必要收益率为 11%。给定C = 0.1*$1,000 / 2 = $50, n = 2*20 = 40 , r = 0.11 / 2 = 0.055, 息票支付额现值 (P) 为: $$ \\begin{aligned} \u0026P=C\\left[\\frac{1-1 /(1+r)^{n}}{r}\\right] \\ \u0026P=$ 50\\left[\\frac{1-1 /(1+0.055)^{40}}{0.055}\\right] \\end{aligned} $$ 面值(到期价值)$1,000的现值: $$ \\left[\\frac{M}{(1+r)^{n}}\\right]=\\left[\\frac{$ 1,000}{(1+0.055)^{40}}\\right]=$ 117.46 $$ 债券价格 (P) =息票支付额现值+面值(到期价值)现值 =$802.31 + $117.46 = $919.77. Excel: =PV(0.11/2, 2*20, 0.1*1000/2, 1000,0) 零息债券 对于零息债券(zero-coupon bonds), 投资者的收益是通过债券到期价值和购买价格之差实现的. 计算公式是: $$ \\mathrm{P}=\\frac{M_{t}}{(1+r)^{n}} $$ P = 价格(美元) M = 到期价值 r = 期间利率(必要收益率的一半) n =期数 (年数乘以2) 例：考虑从现在起15年到期的零息债券的价格，如果到期价值为$1,000，必要收益率为 9.4%。给定M = $1,000, r = 0.094 / 2 = 0.047, n = 2(15) = 30： $$ p=\\frac{M}{(1+r)^{n}}=\\frac{$ 1,000}{(1+0.047)^{30}}=$ 252.12 $$ ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:2:0","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.3 债券价格变化规律 ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:3:0","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.3.1 价格与收益率的关系 债券的一个基本特征是其价格与必要收益率呈反向变动(如图表2-1)。原因是：债券价格是现金流的现值。当必要收益率上升时，现金流的现值减少，债券价格也随之下降；反之则反是。 图表2-1 息票利率为10%的20年债券的价格-收益率关系 收益率 价格 ($) 收益率 价格 ($) 0.055 1,541.76 0.085 1,143.08 0.060 1,462.30 0.090 1,092.01 0.065 1,388.65 0.095 1,044.41 0.050 1,627.57 0.100 1,000.00 0.070 1,320.33 0.110 $919.77 0.075 1,256.89 0.115 883.50 0.080 1,197.93 0.120 849.54 如果我们将任意未附期权债券的价格-收益率关系用图形描绘出来，将发现其形状是“弓形” (如图表2-2，类似边际替代率递减)，曲线凸向原点，称之为凸性。 图表2-2 未附期权债券的价格-收益率关系曲线的形状 \r 注意，价格-收益率关系曲线与价格轴(即纵轴)相交那点的价格是债券的最高价格，对应于债券未贴现现金流的价值，即所有息票支付额和面值之和。 ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:3:1","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.3.2 息票利率、必要收益率和价格之间的关系 当必要收益率=息票利率，债券售价=面值，称为平价出售 当必要收益率\u003e息票利率债券，债券售价\u003c面值，称为折价( discount )出售。 当市场收益率在某个给定时点升到息票利率以上时，债券价格将会进行下降，以使投资者预期购买债券能获得额外利息。 表示由于息票利率低于必要收益率而补偿给新投资者的一种利息.。 当必要收益率\u003c息票利率债券，债券售价\u003e面值，称为溢价(premium)出售。 证明： 记息票率为$ Cr = C/M $，则债券价值 $$ PV=\\sum_{i=1}^{n} \\frac{Cr \\times M}{(1+y)^{i}}+\\frac{M}{(1+y)^{n}}=C_{r} \\times M \\times \\frac{1-\\left(\\frac{1}{1+y}\\right)^{n}}{y}+\\frac{M}{(1+y)^{n}}=\\frac{G}{y} M-\\left(\\frac{C_{r}}{y}-1\\right) \\frac{M}{(1+y)^{n}} $$ 当$ Cr = y $时， $$ PV=M-(1-1) \\cdot \\frac{M}{(1+y)^{n}}=M-0=M $$ 也即债券售价=面值，平价出售。 当$ Cr \u003c y $时， $$ PV-m=\\frac{C_{r}}{y} M-\\left(\\frac{C_{r}}{y}-1\\right) \\frac{M}{(1+y)^{n}}-M=\\left(\\frac{G_{r}}{y}-1\\right)\\left(M-\\frac{M}{(1+y)^{n}}\\right)\u003c0 $$ 也即债券售价\u003c面值，折价出售。 当$ Cr \u003e y $时， $$ PV-m=\\frac{C_{r}}{y} M-\\left(\\frac{C_{r}}{y}-1\\right) \\frac{M}{(1+y)^{n}}-M=\\left(\\frac{G_{r}}{y}-1\\right)\\left(M-\\frac{M}{(1+y)^{n}}\\right)\u003e0 $$ 也即债券售价\u003e面值，溢价出售。 ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:3:2","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.3.3 利率不变时债券价格与时间的关系 债券以面值出售，息票利率等于必要收益率. 当债券临近到期时，债券将继续以面值出售. 当债券接近到期日时，债券价格将保持不变. 以溢价或折价出售的债券，债券价格不会保持不变. 假设必要收益率不变，折价债券的价格随着到期日的临近而升高；溢价债券的变动情况恰好相反. 在到期日时，两种债券的价格都等于其面值. 下图显示了息票利率为10%的20年期债券以折价和溢价出售时，其价格随着到期日的临近而变化的路径(用债券定价公式计算)。 距离到期日的年数 以12%的收益率折价出售的债券价格（美元） 以7.8%的收益率溢价出售的债券价格（美元） 20.0 $ 849.54 $1,221.00 16.0 859.16 1,199.14 12.0 874.50 1,169.45 10.0 885.30 1,150.83 8.0 898.94 1,129.13 4.0 937.90 1,074.37 0.0 1,000.00 1,000.00 ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:3:3","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.3.4 债券价格变动的原因 发行人信用质量的变化导致必要收益率发生变化 即使必要收益率不变，但只要债券临近到期，溢价或折价债券的价格就会发生变化 可比债券收益率的变化(即市场必要收益率的变化)导致必要收益率发生变化 ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:3:4","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.4 复杂的定价情况 此前讨论的债券定价框架基于以下简单假设: 下一次支付利息恰好在6个月后 现金流已知 可以确定恰当的必要收益率 所有现金流都用同一个贴现率贴现 下面分别放松以上假设： 距离下一次付息时间少于6个月的情况 当投资者购买了距离下一次付息时间少于6个月的债券，计算债券价格的公认方法如下: $$ p=\\sum_{t=1}^{n} \\frac{C}{(1+r)^{v}(1+r)^{t-1}}+\\frac{M}{(1+r)^{v}(1+r)^{t-1}} $$ 其中： $$ v=\\frac{结算日和下次付息日之间的天数}{6个月期间的天数} $$ 普通债券的定价公式可以看作$ v= 1 $时的特例。 现金流未知的情况 对于未附期权的债券，假设债券发行人没有违约，那么现金流就是已知的。然而，对于多数债券而言，现金流是无法确知的。这是因为债券发行人可能在规定到期日前赎回债券。 可赎回债券的现金流实际上取决于当期利率水平与息票利率之间的对比。当市场利率跌到息票利率以下很多时，债券发行人通常会赎回债券，因为在到期日之前还清债券，并以更低的息票利率发行新债券是合算的。因此，可能在到期之前被提前赎回的债券的现金流取决于市场即期利率。 确定适当的必要收益率 所有必要收益率都是以国债提供的收益率为基准确定的。后续我们需要将债券的必要收益率分解成几个组成部分。 运用不同贴现率对所有现金流贴现 债券可以视为一揽子零息债券的组合，其中每笔现金流都应该用特有的贴现率贴现，从而确定其现值. ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:4:0","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.4 浮动利率债券和反向浮动利率债券的定价 浮动利率证券和反向浮动利率证券的现金流都是未知的，它们的现金流取决于未来的参考利率。 ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:5:0","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.4.1 浮动利率证券的价格 浮动利率证券( floating-rate security, or floater )的息票利率等于参考利率加上某个利差或价差。 浮动利率证券的价格取决于 高于参考利率的价差 对重新调整息票利率可能施加的约束： 比如，浮动利率证券可能有最高息票利率，称为利率上限(cap)； 也可能有最低息票利率，称为利率下限(floor). 只要满足下列条件，浮动利率的价格将接近其面值 市场要求的高于参考汇率的价差不变 息票利率在利率上下限之间浮动，但没有达到上限或下限， 如果市场要求的浮动息票利率高于利率上限，则将以利率上限计算息票，此时债券价格将低于债券面值。 ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:5:1","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.4.2 反向浮动利率债券的价格 通常，反向浮动利率证券是由固定利率债券派生出来的 派生出反向浮动利率证券的证券称为担保品(collateral). 根据担保品可以派生出两种证券：浮动利率证券和反向浮动利率证券. 如图表2-4所示。 表2-4 反向浮动利率债券创建 \r 浮动利率债券和反向浮动利率债券需要满足以下条件，使担保品产生的现金流足以偿还这两种债券的债务： 每期支付给两种债券的总利息小于或等于每期的担保品利息； 两种债券的总面值小于或等于担保品的总面值。 例如，考虑一只息票利率为7.5%、半年付息一次的10年期债券。假设将1亿美元的该债券用作担保品，派生出面值为5000万美元的浮动利率债券和面值为5000万美元的反向浮动利率债券。假设每6个月根据下列公式调整一次息票利率： $$ 浮动利率债券的息票利率：参考利率+1% \\ 反向浮动利率債券的息票利率：14%一参考利率 $$ 注意，浮动利率债券和反向浮动利率债券的面值之和等于担保品的面值，1亿美元。两种债券的加权平均息票利率为： $$ 0.5×(参考利率+1%)+0.5×(14%一参考利率)=7.5% $$ 因此，不管参考利率的水平为多少，这两种债券的加权平均息票利率都等于担保品的息票利率，7.5%。 考虑利率上下限，假设参考利率高于14%，那么根据该息票利率公式求出的反向浮动利率债券的息票利率将为负值。为了防止发生这种情况，对反向浮动利率债券的息票利率设定了下限。通常，下限被设为0。由于存在下限，因此浮动利率债券的息票利率必定受到限制，因此对这两种债券支付的息票利息不会超过担保品的息票利息。在我们的假设结构下，必须对浮动利率债券设定的最高息票利率为15%。因此，由担保品派生出浮动利率债券和反向浮动利率债券时，对反向浮动利率债券的息票利率设定了下限，对浮动利率债券的息票利率设定了上限。 上限和下限的估值超出了这里的讨论范围。这里指出以下这点就足够了：反向浮动利率债券的价格是通过确定担保品和浮动利率债券的价格得出的， $$ 反向浮动利率债券的价格 = 担保品的价格-浮动利率债券的价格 $$ 注意，反向浮动利率债券价格的影响因素受参考利率的影响程度以参考利率对浮动利率债券利率限制的影响程度为限。这是一个十分重要的结论。有些投资者错误地认为，由于息票利率上升，当参考利率下降时，反向浮动利率债券的价格应上升。这是不正确的。反向浮动利率债券定价的关键在于利率变化如何影响担保品的价格。参考利率的重要性取决于参考利率对浮动利率债券息票利率的限制。 原文：The factors that affect the price of an inverse floater are affected by the reference rate only to the extent that it affects the restrictions on the floater’s rate. This is quite an important result. Some investors mistakenly believe that because the coupon rate rises, the price of an inverse floater should increase if the reference rate decreases. This is not true. The key in pricing an inverse floater is how changes in interest rates affect the price of the collateral. The reference rate is important only to the extent that it restricts the coupon rate of the floater. More details are given below. ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:5:2","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.5 报价和应计利息 ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:6:0","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.5.1 报价( Price Quotes ) 对债券报价时，交易商是以面值的百分比报价的。 以面值出售的债券的报价为100，表示其面值的100%. 以折价出售的债券的报价低于100. 以溢价出售的债券的价格高于100. 下表显示了如何将报价转换为美元价格。 (1) 报价(类似百分比) (2) 转换为小数形式 [= (1)/100] (3) 面值 （美元） (4) 美元价格 [= (2) × (3)] 80 1/8 0.8012500 10,000 8,012.50 76 5/32 0.7615625 1,000,000 761,562.50 86 11/64 0.8617188 100,000 86,171.88 100 1.0000000 50,000 50,000.00 109 1.0900000 1,000 1,090.00 103 3/4 1.0375000 100,000 103,750.00 105 3/8 1.0537500 25,000 26,343.75 ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:6:1","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"2.5.2 应计利息 当投资者在两个付息日之间购买债券时，投资者必须向债券出售者补偿从上一次付息到债券结算日之间应获得的利息，这部分利息称为应计利息(accrued interest)。对公司债券和市政债券来说，应计利息以每年360天、每月30天为标准计算. 全价( full price )或脏价(dirty price)是买方向卖方支付的金额，即协议价格加上应计利息。 净价(clean price)，不含应计利息的债券价格。 ","date":"0001-01-01","objectID":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/:6:2","tags":null,"title":"","uri":"/2.-%E5%80%BA%E5%88%B8%E5%AE%9A%E4%BB%B7/"},{"categories":null,"content":"3. 债券收益率的衡量 [Toc] ","date":"0001-01-01","objectID":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/:0:0","tags":null,"title":"","uri":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/"},{"categories":null,"content":"3.1 内部收益率 投资收益率，也称为内部收益率(internal rate of return, IRR)是使投资获取的现金流现值等于投资价格(或成本)的利率。从数学角度看，投资收益率y是满足下式的利率： $$ P=\\frac{C F_{1}}{1+y}+\\frac{C F_{2}}{(1+y)^{2}}+\\frac{C F_{3}}{(1+y)^{3}}+\\cdots+\\frac{C F_{N}}{(1+y)^{N}}= \\sum_{t=1}^{N} \\frac{C F_{i}}{(1+y)^{t}} $$ 其中P = 投资的价格, CF = 第t年的现金流，N = 年数。 若没有金融计算器或计算机软件，求解收益率y需要一个试错(trial-and-error)过程。 所计算的收益率是期间收益率,也即，如果现金流是半年期的，则计算出的收益率是半年期收益率。为了计算简单年利率，应当用每期的收益率乘以每年的期数。 当只有一笔未来现金流时，上述公式经过简化变形后得到: $$ \\boldsymbol{y}=\\left[\\frac{\\boldsymbol{C F}_{\\boldsymbol{n}}}{\\boldsymbol{P}}\\right]^{\\frac{1}{n}}-1 $$ 收益率年化 年化利率默认采用简单年利率(simple annual interest rate)： $$ 简单年利率 = 期间利率 \\times m $$ 其中，m是每年的付息次数。 简单年利率低估了实际年收益率，用下式可以求出与期间利率有关的实际年收益率: $$ 实际年收益率= (1 +期间利率)^m – 1 $$ 【例子】如果每季付息一次，期间利率为2%(8%/4)，则: $$ 实际收益率 = (1.02)^4 – 1 = 1.0824 – 1 = 0.0824 或 8.24% $$ 反过来通过实际年收益率公式求出可产生给定年利率的期间利率， $$ 期间利率= (1 +实际年收益率)^{1/m} – 1 $$ 例子：可产生12%的实际年收益率的季度期间利率为: $$ 期间利率 = (1.12)^{1/4} – 1 = 1.0287 – 1 = 0.0287 或 2.87% $$ ","date":"0001-01-01","objectID":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/:1:0","tags":null,"title":"","uri":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/"},{"categories":null,"content":"3.2 债券收益率衡量指标 当期收益率 当期收益率(Current yield)等于年息票利息与当前市场价格之比： $$ 当期收益率=年息票利率/价格 $$ 计算当期收益率时，只考虑息票利息而不考虑投资者收益率的其他收益来源(如资本利得)，货币的时间价值也被忽略了。 到期收益率 到期收益率(yield to maturity)是使现金流现值等于价格(或初始投资金额)的利率，计算方法与IRR相同。对于半年付息一次的债券而言，到期收益率y的计算方法是: $$ \\begin{aligned} P\u0026=\\frac{C}{1+y}+\\frac{C}{(1+y)^{2}}+\\frac{C}{(1+y)^{3}}+\\cdots+\\frac{C}{(1+y)^{n}}+\\frac{M}{(1+y)^{n}} \\ P\u0026=\\sum_{t=1}^{n} \\frac{C}{(1+y)^{t}}+\\frac{M}{(1+y)^{n}} \\end{aligned} $$ 其中 P = 债券价格, C = 半年期的息票利息(美元), M = 到期价值(美元), n =期数(年数*2)。 Excel: =IRR(Cash flow) 对于半年付息一次的债券而言，将期间利率或贴现率(y)乘以2就得出年化到期收益率，也称为债券等值收益率(bond-equivalent yield)。 到期收益率不仅考虑了当期息票收入，还考虑了投资者持有债券到期将实现的资本损益、现金流产生的时间。 债券售价 关系 平价 息票利率=当期收益率=到期收益率 折价 息票利率\u003c当期收益率\u003c到期收益率 溢价 息票利率\u003e当期收益率\u003e到期收益率 赎回收益率 赎回价格(call price)是发行人赎回债券的价格。 赎回时间表(call schedule)规定了每个赎回日的赎回价格。 赎回收益率(yield to call)，假设债券发行人在某个规定的赎回日赎回债券，赎回价格为赎回时间表中规定的赎回价格。通常，投资者需要计算 首次赎回收益率(yield to first call)，当前到第一个可赎回日的收益率； 下次赎回收益率(yield to next call)，当前赎回日到下一个赎回日的收益率； 首次按面值赎回收益率(yield to first par call)，在赎回日按面值赎回的收益率； 转期收益率(yield to refunding)，计算的是假设债券在债券可转期的第一天赎回时的收益率。原文：The yield to refunding is computed assuming the issue will be called on the first date the issue is refundable. 与到期收益率类似，赎回收益率的计算公式如下 $$ \\begin{aligned} P\u0026=\\frac{C}{1+y}+\\frac{C}{(1+y)^{2}}+\\frac{C}{(1+y)^{3}}+\\ldots+\\frac{C}{(1+y)^{n^{}}}+\\frac{M^{}}{(1+y)^{n^{*}}} \\ P\u0026=\\sum_{t=1}^{n^{*}} \\frac{C}{(1+y)^{t}}+\\frac{M}{(1+y)^{n^{*}}} \\end{aligned} $$ 其中M* = 赎回价格(美元)，n* = 截至假定赎回日的期数(年数*2) 对于半年付息一次的债券而言，用期间利率(y)乘以2，即可得出债券等值赎回收益率。 偿债基金收益率 一些债券有偿债基金条款，这要求发行人在预定日期赎回特定金额的债券。 假设债券将在特定偿债基金支付日被赎回，则计算出的收益率称为偿债基金收益率(yield to sinker)。 计算方法和到期收益率与赎回收益率相同：使用偿债基金支付日和偿债基金价格*。* 回售收益率 债券是可回售的，意味着债券持有者可以要求债券发行人以规定价格买回债券。回售时间表规定了债券被回售的时间和价格，称为回售价格(put price)。 当债券可回售时，可以算出回售收益率。回售收益率(yield to put)是使截至假定回售日的现金流现值加上回售时间表规定的当日回售价格等于债券价格的利率。 回售收益率的计算公式和赎回收益率的计算公式相同，只是现在M*被定义为回售价格，n*被定义为截至假定回售日的期数。除此以外，其计算过程与到期收益率和赎回收益率的计算过程相同。 最差收益率 在行业实践中，投资者需要计算到期收益率、每个可能赎回日的收益率以及每个可能回售日的收益率。所有这些收益率中最低的收益率称为最差收益率(yield to worst). 现金流收益率 分期偿付证券(amortizing securities)的现金流包括在到期日前定期偿还的本金，例如抵押货款支持证券和资产支持证券。其每期的现金流包括三个部分: 息票利息； 分期偿付时间表中的规定的本金偿付额； 提前偿付额：超出规定金额的本金偿付额。 市场参与者需要计算分期偿付证券的现金流收益率(cash flow yield)，这是使预期现金流现值等于市场价格的利率，困难在于如何预测每期的提前偿付额。 投资组合收益率(内部收益率) 债券投资组合的收益率(yield for a portfolio of bonds)并不只是单只债券到期收益率的简单平均或加权平均。 计算方法是：首先确定投资组合的现金流，然后确定使现金流现值等于投资组合市场价值的利率。 例：考虑下面三只债券构成的投资组合： \r 为了简化说明，假设每只债券的付息日相同。投资组合的市场价值总额为57,259,000美元。投资组合中每只债券的现金流和整个投资组合的现金流如下： \r 为了确定由这三只债券组成的投资组合的收益率（内部收益率），必须找出使上表中最后一列所示的所有现金流现值等于57,259,000美元（该投资组合的市场价值总额）的利率。如果采用4.77%的利率，则现金流现值将等于57259000美元。4.77%乘以2得到9.54%,这就是该投资组合的债券等值收益率。 浮动利率证券的收益率利差衡量指标 浮动利率证券的息票利率根据调息公式定期变化： $$ 息票利率=参考利率(\\text{reference rate})+报价利差(\\text{quoted margin}) $$ 由于参考利率的是浮动的，因此不可能确定现金流.这意味着无法计算到期收益率。 浮动利率证券的市场参与者采用几种传统指标作为利差的衡量指标，包括：存续期利差(spread for life)(或简单利差simple margin)、调整后的简单利差(adjusted simple margin)、调整后的总利差(adjusted total margin)和贴现利差(discount margin)。 其中，最常用的是贴现利差(discount margin)，它估计的是投资者在证券存续期内的预期收益率与参考利率的平均利差。计算步骤为： 假设证券存续期内的参考利率保持在不变，确定债券的现金流 选择一个利差，加上参考利率后得到一个息票利率 第2步中选择的息票利率贴现第1步中算出的现金流 用试错法求使现金流现值=债券价格的息票利率：将第3步算出的现金流与价格进行比较，如果现金流现值等于证券价格，那么贴现利差就等于第2步中选定的利差。如果现金流现值不等于证券价格，那么就返回第2步试用不同的利差进行计算。 平价出售债券的贴现率=息票利率，因此贴现利差=第4步中得到的息票利率-参考利率 例：假设有一只售价为99.3098的6年期浮动利率证券、利率为参考利率加80个基点、每6个月重新调整一次息票利率。假设当前的参考利率为10%。图表3-1显示了该证券贴现利差的计算结果。表中第一列显示了当前的参考利率。第二列给出了证券的现金流。前11期的现金流等于当前参考利率的一半(5%)加上40个基点的半年期利差再乘以100。在第20个6个月期，现金流为5.4加上到期价值100。最后五列的第一行显示了假定的利差基点，下面各行显示了按假定利差贴现的各笔现金流的现值。最后一行给出了现金流的现值总额。 图表3-1 计算浮动利率证券的贴现利差 时期 参考利率% 现金流 80 84 88 96 100 1 10 5.4 5.1233 5.1224 5.1214 5.1195 5.1185 2 10 5.4 4.8609 4.8590 4.8572 4.8535 4.8516 3 10 5.4 4.6118 4.6092 4.6066 4.6013 4.5987 4 10 5.4 4.3755 4.3722 4.3689 4.3623 4.3590 5 10 5.4 4.1514 4.1474 4.1435 4.1356 4.1317 6 10 5.4 3.9387 3.9342 3.9297 3.9208 3.9163 7 10 5.4 3.7369 3.7319 3.7270 3.7171 3.7122 8 10 5.4 3.5454 3.5401 3.5347 3.5240 3.5186 9 10 5.4 3.3638 3.3580 3.3523 3.3409 3.3352 10 10 5.4 3.1914 3.1854 3.1794 3.1673 3.1613 11 10 5.4 3.0279 3.0216 3.0153 3.0028 2.9965 12 10 105.4 56.0729 55.9454 55.8182 55.5647 55.4385 现值 100.0000 99.8269 99.6541 99.3098 9","date":"0001-01-01","objectID":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/:2:0","tags":null,"title":"","uri":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/"},{"categories":null,"content":"3.3 债券收益的来源 购买债券的投资者可能从以下一个或多个来源中获得收益： 债券发行人定期支付的利息 当债券到期、被赎回或出售时的资本利得(capital gain)或资本损失(capital loss) 再投资收益：将定期现金流进行再投资产生的利息收入 对于非分期偿付债券，期间现金流只是息票利息支付，因此再投资收益只是将息票利息进行再投资获得的利息，称为利息的利息(Interest-on-interest component)。对于分期偿付债券而言，再投资 收益包括对息票利息和到期日之前定期偿还的本金进行再投资所得到的利息收入。下面考察非分期偿付证券的收益来源。 当期收益率(current yield)只考虑了息票利息支付。 到期收益率(yield to maturity)、赎回收益率(yield to call)、回收收益率(yield to put)和现金流收益率(cash flow yield)都考虑了所有三个潜在收益来源。这些收益率都是一种承诺收益率，即只有在：(1)持有债券至相应期限(到期日、回售日、赎回日)；(2)并按该收益率对息票利息进行再投资时，才能实现该收益率。 ","date":"0001-01-01","objectID":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/:3:0","tags":null,"title":"","uri":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/"},{"categories":null,"content":"3.3.1 利息的利息 对于非分期偿付债券，利息的利息(interest-on-interest)是债券的再投资收益。根据普通年金的终值计算公式，利息的利息与全部息票利息之和可以从下式得出: $$ 息票利息+利息的利息 = C\\left[\\frac{(1+r)^{n}-1}{r}\\right] $$ 其中，C是息票利息，r是半年期再投资利率，n是期数。 息票利息总额(total dollar coupon interest)可以用半年期息票利息乘以期数: $$ 息票利息总额= nC $$ 因此利息的利息等于 $$ 利息的利息=C\\left[\\frac{(1+r)^{n}-1}{r}\\right]-n C $$ 例：考虑一只息票利率为7%的15年期债券。如果该债券每1000美元面值的价格为769.40美元，则该债券的到期收益率为10%。假设年再投资利率也为10%，半年期再投资利率为5%，那么利息的利息为： $$ IOI=\\boldsymbol{C}\\left[\\frac{(1+\\boldsymbol{r})^{\\boldsymbol{n}}-1}{\\boldsymbol{r}}\\right]-\\boldsymbol{n} \\boldsymbol{C}= 35\\left[\\frac{(1+0.05)^{30}-1}{0.05}\\right]-30 \\times 35=$ 1275.36 $$ 该债券的收益总额来源于以下三个方面： 1050美元的息票利息总额(每6个月支付35美元的利息，期限为15年)。 对每6个月获得的半年期息票利息按5%的利率进行再投资，获得的利息的利息为1275.36美元。 230.60美元的资本利得(1000美元减去769.40美元)。 如果息票利息可以按10%的到期收益率再投资，那么潜在收益总额为 $$ 1050+1275.36+230.60=$2,555.96 $$ 注意，如果投资者将本可以用来购买该债券的769.40美元存在储蓄账户中，在未来15年内每半年收取5%的利率，那么该储蓄账户的终值为： $$ $769.40 \\times (1.05)^{20}=$3325.30 $$ 769.40美元的初始投资的收益总额为 $$ $3325.30-769.40 = $ 2255.90 $$ 如果忽略四舍五入的误差，那么这就是我们假设再投资收益率等于到期收益率(10%)时，通过分解债券收益率得到的结果。由此可见，为了使债券收益率达到10%，投资者必须通过将息票利息进行再投资，产生1275.36美元的利息收入。这意味着，为了创造出10%的到期收益率，大约一半的债券收益总额(1275.36美元/2255.96美元)必须来自于息票利息的再投资。 ","date":"0001-01-01","objectID":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/:3:1","tags":null,"title":"","uri":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/"},{"categories":null,"content":"3.3.2 到期收益率与再投资风险 只有在持有债券至到期，且按计算出的到期收益率将息票利息进行再投资时，投资者才能实现购买债券时的到期收益率。再投资风险(Reinvestment risk)是未来的再投资收益率低于购买债券时的到期收益率的风险。 债券的两个特征决定了利息的利息的重要性和再投资风险的大小 对于给定的到期收益率和息票利率而言，期限越长，要实现购买时的到期收益率，债券收益总额就越依赖于利息的利息，在投资风险越大。 对于给定期限、给定到期收益率的债券，息票利率越高，为了实现购买债券时预期的到期收益率，债券收益总额越依赖于息票利息再投资，再投资风险越大。 利息的利息/再投资风险：溢价债券\u003e平价出售\u003e折价债券 零息债券的债券收益总额不依赖于利息的利息，因此零息债券的再投资风险为零。持有至到期的零息债券实现的收益率等于承诺的到期收益率。 ","date":"0001-01-01","objectID":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/:3:2","tags":null,"title":"","uri":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/"},{"categories":null,"content":"3.3.3 现金流收益率与再投资风险 分期偿付证券除了定期支付的息票利息，投资者现在还必须将定期偿付的本金再投资，因此再投资风险比高于非分期偿付证券。 另外，对于分期偿付证券而言，借款人可以加速定期偿还本金，且通常只在利率下降时提前偿付。如果借款人在利率下降时提前偿付，投资者就会面临更高的再投资风险，因为投资者必须将提前偿付的本金以较低的利率进行再投资。 ","date":"0001-01-01","objectID":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/:3:3","tags":null,"title":"","uri":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/"},{"categories":null,"content":"3.4 总收益率 到期收益率是一种承诺收益率( promised yield)，在购买债券时，如果以下两个条件满足，则投资者将获得某个用到期收益率衡量的承诺收益率: 持有债券至到期。 • 按到期收益率将所有息票利息进行再投资  总收益率(total return)是一种包含关于再投资收益率的明确假设的收益率指标.  赎回收益率指标和到期收益率有同样问题: • 它假设投资者持有债券至首次赎回日 • 它假设投资者将息票利息按赎回收益率进行再投资 ","date":"0001-01-01","objectID":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/:4:0","tags":null,"title":"","uri":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/"},{"categories":null,"content":"3.4.1 计算债券的总收益率 其目的是在特定的再投资收益率下，计算债券投资的未来收入总额. • 然后，计算出使债券初始投资增长为计算出的未来收入总额的利率，即总收益率. 假设一位投资期为3年的投资者正在考虑以828.40美元的价格购买一只息票利率为8%的20年期债券，该债券的到期收益率(YTM)为10%，投资者预期能以6%的年利率将息票利率进行再投资，并预期在计划投资期期末，还剩17年到期的债券能按7%的到期收益率出售。 首先计算三年后的利息终值 =FV(0.06,6,-40,0) 计算三年后的债券价值 =priceofbond(DATE(1,1,1),DATE(17,12,31),8%,7%,1000,2,) 加总后折现 $$ P_0 = [(\\frac{FV+P_3}{P_0})^{1/6}-1]^2 $$ ","date":"0001-01-01","objectID":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/:4:1","tags":null,"title":"","uri":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/"},{"categories":null,"content":"3.4.2 总收益率的应用(投资期分析) 投资期分析(horizon analysis) 指的是用总收益率评估债券在某段投资期内的表现.  投资期收益率(horizon return )指所计算的是某段投资期的总收益率.  总收益率常被提及的一个缺点是，它要求投资组合经理对再投资收益率和未来收益率作出假设，同时还要考虑投资期方面的问题. 计算收益率的变动 绝对收益率变动(absolute yield change), 或绝对利率变动(absolute rate change)，是用基点衡量的，它是两种收益率之差的绝对值。 $$ 绝对收益率变动= │初始收益率– 新收益率│ × 100. $$ 收益率变动百分比是收益率变动比率的自然对数 $$ 收益率变动百分比= 100 × \\ln (新收益率/初始收益率) $$ 其中，ln表示自然对数. $$ \\frac{\\Delta y}{y}=\\frac{\\frac{d y_{t}}{d t}}{y_{t}}=\\frac{d\\left(\\ln y_{t}\\right)}{d t} =\\frac{\\ln y_{2}-\\ln y_{1}}{2-1}=\\ln \\left(\\frac{y_{2}}{y_{1}}\\right) $$ 要点 任何投资的收益率和内部收益率都是令现金流现值等于投资价格(或成本)的利率。也可以用相同方法来计算债券收益率。 债券市场参与者常用的传统收益率指标是当期收益率、到期收益率、赎回收益率、回售收益率、最差收益率和现金流收益率。 债券投资的三种潜在收益来源为息票利息、再投资收益和资本利得(或资本亏损)。 传统收益率指标无法令人满意地衡量所有这些收益来源。当期收益率指标没有考虑再投资收益和资本利得(或资本亏损)。到期收益率考虑了全部三种来源，但它的不足是假设所有息票利息都能以到期收益率进行再投资。以低于到期收益率的收益率对息票利息进行再投资的风险称为再投资风险。 赎回收益率有相同的缺点：它假设息票利息可以按赎回收益率再投资。现金流收益率做出了和到期收益率相同的假设，并假设可以定期按计算出的现金流收益率对本金进行再投资，且实际上实现了提前偿付。 给定投资者或投资组合经理的预期和计划投资期，总收益率是更有意义的债券吸引力评价指标 两期之间的收益率变化可以用绝对收益率变化或相对收益率变化(或收益率变化百分比)计算。 本章作业 • (pp.52~54) 5,6,7,12,16,18 ","date":"0001-01-01","objectID":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/:4:2","tags":null,"title":"","uri":"/3.-%E5%80%BA%E5%88%B8%E6%94%B6%E7%9B%8A%E7%8E%87%E7%9A%84%E8%A1%A1%E9%87%8F/"},{"categories":null,"content":"4. 债券价格的波动性 [Toc] 未附期权债券的价格与收益率之间的关系综述 随着必要收益率的上升（下降），预期现金流的现值将下降（上升），因而会降低提高债券的价格，如图表4-1。 **图表 4-**1 6只假设债券的价格与收益率之间的关系 按必要收益率计算的价格 (息票利率/距离到期日的年数) 必要收益率 (%) 9% / 5 9% / 25 6% / 5 6% / 25 0% / 5 0% / 25 6.00 112.795 138.594 100.0000 100.000 74.4094 22.8107 7.00 108.316 123.455 95.8417 88.2722 70.8919 17.9053 8.00 104.055 110.741 91.8891 78.5178 67.5564 14.0713 8.50 102.002 105.148 89.9864 74.2587 65.9537 12.4795 8.90 100.396 100.996 88.4983 71.1105 64.7017 11.3391 8.99 100.039 100.098 88.1676 70.4318 64.4236 11.0975 9.00 100.000 100.000 88.1309 70.3570 64.3928 11.0710 9.01 99.9604 99.9013 88.0943 70.2824 64.3620 11.0445 9.10 99.6053 99.0199 87.7654 69.6164 64.0855 10.8093 9.50 98.0459 95.2539 86.3214 66.7773 62.8723 9.8242 10.00 96.1391 90.8720 84.5565 63.4881 61.3913 8.7204 11.00 92.4624 83.0685 81.1559 57.6712 58.5431 6.8767 12.00 88.9599 76.3572 77.9197 52.7144 55.8395 5.4288 价格与收益率关系不是线性的，如图表4-2. 图表4-2 未附期权债券的价格-收益率关系曲线 \r 未付期权债券的价格与收益率关系曲线形状称为凸性（convex）. 价格-收益率关系曲线的凸性说明了债券价格波动性的这四个特征: 尽管所有未附期权债券的价格都与必要收益率反向变动，但并非所有债券的价格变动百分比都相同. 当必要收益率的变动较小时，不管必要收益率是上升还是下降，给定债券的价格变动百分比都大致相同. 当必要收益率的变动较大时，必要收益率上升和下降导致的债券价格变动百分比不同. 当基点变动幅度较大时，价格上升的百分比大于价格下降的百分比. 影响债券价格波动性的债券特征 未附期权债券的价格波动性是由它的两个特征决定的：息票利率和期限. • 当给定期限和初始收益率时，债券价格的波动性越大，息票利率越低。 – 通过比较期限相同、利率分别为9%、6%的债券和零息债券就可以看出这一特征. • 当给定息票利率和初始收益率时，距离到期的期限越长，价格波动性就越大. – 通过比较图表4-3中息票利率相同的5年期和25年期债券，就可以看出这一特征。 未附期权债券的价格波动性特征(续)  到期收益率的影响  我们无法忽视一个事实即使债券的息票利率和期限相同，信用因素也会导致债券交易时的收益率不同.  如果保持其他因素不变，债券交易时的到期收益率越高，价格波动性就越低.  为了说明这一结论，比较息票利率为9%的25年期债券在图表4-4中各种收益率水平下的交易情况.  第一列显示了债券交易时的收益率水平，第二列给出了初始价格. 未附期权债券的价格波动性特征(续)  到期收益率的影响  第三列显示了收益率变动100个基点时的债券价格.  第四点和第五列显示了价格变动额和价格变动百分比.  第四列的第五列表明：初始收益率越高，价格波动性就越低.  这意味着，对于给定的收益率变动而言，市场收益率水平越低（高），价格波动性就越高（低）. 债券价格波动性的衡量指标  货币经理、套利商和交易商需要找到一种衡量债券价格波动性的方法.  常用的三个衡量指标: • »ùµãµÄ¼ÛÖµ • ¼Û¸ñ±ä¶¯µÄÊÕÒæÂÊ • ¾ÃÆÚ 债券价格波动性的衡量指标(续) 基点的价值 基点的价值（price value of a basis point, PVBP），也称为01的美元价值（dollar value of an 01）, 是指必要收益率变化1个基点时的债券价格变动额. 注意，这种债券波动性指标衡量的是以货币表示的价格波动性，而不是价格波动百分比（价格变动占初始价格的百分比）。 通常，一个基点的价格用价格变动的绝对值表示。 根据价格与收益率关系的第二个特征，收益率上升或下降1个基点时，价格波动性相同。 债券价格波动性的衡量指标(续)  价格变动的收益率  Í¶×ÊÕßÊ¹ÓÃµÄÁíÒ»¸öÕ®È¯¼Û¸ñ²¨¶¯ÐÔºâÁ¿Ö¸±êÊÇÌØ¶¨¼Û¸ñ±ä»¯ÏÂµÄÊÕÒæÂÊ.  ¹ÀËã¸ÃÖ¸±êÊ±£¬Ê×ÏÈÓ¦¼ÆËã³öÕ®È¯¼Û¸ñ½µµÍ ÃÀÔªÊ±µÄµ½ÆÚÊÕÒæÂÊ¡£  È»ºóÇó³ö³õÊ¼ÊÕÒæÂÊºÍÐÂÊÕÒæÂÊÖ®²î£¬¼´Õ®È¯¼Û¸ñ±ä¶¯ ÃÀÔªÊ±µÄÊÕÒæÂÊ¡£  ¸ÃÊÕÒæÂÊµÄÖµÔ½Ð¡£¬¼Û¸ñ²¨¶¯ÐÔÔ½´ó£¬ÒòÎª½ÏÐ¡µÄÊÕÒæÂÊ±ä»¯£¬¼´¿Éµ¼ÖÂ ÃÀÔªµÄ¼Û¸ñ±ä¶¯¡£ 债券价格波动性的衡量指标(续) 久期 麦考利久期（Macaulay duration ）衡量必要收益率小幅变动所导致的价格变动近似值: $$ \\operatorname{Mac} D \\equiv \\frac{\\sum_{t=1}^{n} \\frac{C F_{t}}{(1+y)^{t}} \\cdot t}{P}=\\frac{\\frac{CF_1}{P_0}}{(1+y)} \\cdot 1+\\frac{\\frac{CF_2}{P_0}}{(1+y)^2} \\cdot 2+ \\ldots+ \\frac{\\frac{CF_n}{P_0}}{(1+y)^n} \\cdot n $$ 其中： P =债券价格 C = 半年息票利息额（美元） y = 到期收益率或必要收益率的一半 n = 半年期的期数 (年数*2) M = 到期价值 (美元) 含义：每期现金流占债券价格的比值的贴现值的加权平均。 债券价格波动性的衡量指标(续)  久期  投资者通常将麦考利久期与（1 + y）之比称为修正久期（modified duration）. 公式是: 其中： y = 到期收益率或必要收益率的一半  修正久期与给定收益率变化下的价格变动百分比近似值有关: 其中： dP =价格变化, dy = 收益率变动, P =债券价格. $$ \\begin{aligned} P \u0026=\\sum_{t=1}^{n} \\frac{C F_{t}}{(1+y)^{t}} \\ \\Rightarrow \\frac{d P}{d y} \u0026=-\\sum_{t=1}^{n} \\frac{C F_{t} \\cdot t}{(1+y)^{t+1}} \\ \\square \u0026=-\\frac{1}{1+y} \\sum_{t=1}^{n} \\frac{C F_{t}}{(1+y)^{t}} \\cdot t \\ \\Rightarrow \\frac{\\frac{d P}{p}}{d y} \u0026=-\\frac{1}{1+y} \\frac{\\sum_{t=1}^{n} \\frac{C F_{t}}{(1+y)^{t}} \\cdot t}{P} \\ \\square \u0026=-\\frac{1}{1+y} \\operatorname{Mac} D \\ \\operatorname{Mac} D \u0026 \\equiv \\frac{\\sum_{t=1}^{n} \\frac{C F_{t}}{(1+y)^{t}} \\cdot t}{P} \\ M D \u0026 \\equiv-\\frac{d P}{d y} \\frac{1}{P}=\\frac{1}{1+y} \\operatorname{Mac} D \\end{aligned} $$ 债券价格波动性的衡量指标(续)  久期  由于所有未附期权债券的修正久期均为正值，因此修正久期公式 (dP/dy )(1/P)说明修正久期和给定收益率变化下的价格变动百分比近似值存在反向关系.  这一点可以根据债券价格与利率的变化方向相反的基本原理得出.  图表4-5和图表4-6说明了两只5年期附息债券的麦考利久期和修正久期的计算过程. • 这里求出的久期是每一期的久期. 债券价格波动性的衡量指标(续)  久期  Í¨³£Èç¹ûÏÖ½ðÁ÷Ã¿Äê²úÉúm´Î£¬ÐèÒª½«¾ÃÆÚ³ýÒÔm½øÐÐµ÷Õû£º  ÎÒÃÇ¿ÉÒÔµ¼³öÒ»¸ö²»±ØÐèÒª´óÁ¿¼ÆËãÂó¿¼Àû¾ÃÆÚºÍÐÞÕý¾ÃÆÚµÄÌæ´ú¹«Ê½. – Í¨¹ý¸ÄÐ´Õ®È¯¼Û¸ñµÄÁ½¸öÒªËØ——Äê½ðµÄÏÖÖµºÍÃæÖµµÄÏÖÖµ£¬ÇóÒ»½×µ¼Êý£¬³ýÒÔP £¬¼´¿ÉÊµÏÖÉÏÊöÄ¿±ê:  ÆäÖÐ£¬¼Û¸ñ±íÊ¾ÎªÃæÖµµÄ°Ù·Ö±È¡£ 债券价格波动性的衡量指标(续) 久期的特征 附息债券的修正久期和麦考利久","date":"0001-01-01","objectID":"/4.-%E5%80%BA%E5%88%B8%E4%BB%B7%E6%A0%BC%E7%9A%84%E6%B3%A2%E5%8A%A8%E6%80%A7/:0:0","tags":null,"title":"","uri":"/4.-%E5%80%BA%E5%88%B8%E4%BB%B7%E6%A0%BC%E7%9A%84%E6%B3%A2%E5%8A%A8%E6%80%A7/"},{"categories":null,"content":"必考：根据到期收益率计算即期利率，参考ch5.excel ","date":"0001-01-01","objectID":"/5.-%E5%88%A9%E7%8E%87%E7%9A%84%E6%9C%9F%E9%99%90%E7%BB%93%E6%9E%84/:0:0","tags":null,"title":"","uri":"/5.-%E5%88%A9%E7%8E%87%E7%9A%84%E6%9C%9F%E9%99%90%E7%BB%93%E6%9E%84/"},{"categories":null,"content":"jj $$ 可赎回债券的价格 = 不可赎回债券的价格 – 赎回期权的价格 $$ \r 可赎回债券的价格—收益率关系 负凸性（negative convexity ）是指波幅为给定基点数的收益率大幅波动下，债券价格上升的幅度小于价格下降的幅度. 未付期权的债券具有正凸性，即对于给定的收益率大幅波动，债券价格上升幅度大于价格下降幅度. 因债券成正凸性和负凸性而导致的价格变化如图表18-5所示 . 即使债券很有可能被赎回，但是债券仍然可以按高于赎回价格的价格进行交易，理解这一点是很重要的. 【例】考虑一只息票利率为13%的10年期可赎回债券，半年付息一次，该债券1年后将以104美元的赎回价格被赎回。假设10年期债券的收益率为6%，而1年期的收益率为5%。 $$ \\frac{6.5}{1.025}+\\frac{110.5}{(1.025)^{2}}=111.52\u003e104 $$ 如果债券可赎回，价格将受到抑制。 ","date":"0001-01-01","objectID":"/6.-%E9%99%84%E6%9C%89%E5%B5%8C%E5%85%A5%E5%BC%8F%E6%9C%9F%E6%9D%83%E5%80%BA%E5%88%B8/:0:0","tags":null,"title":"","uri":"/6.-%E9%99%84%E6%9C%89%E5%B5%8C%E5%85%A5%E5%BC%8F%E6%9C%9F%E6%9D%83%E5%80%BA%E5%88%B8/"},{"categories":null,"content":"可回售债券 可赎回债券的原理同样可以应用于可回售债券 （putable bonds）. 【可回售债券】债券持有者有权在指定的时间，按指定的价格将债券回售给债券发行人. 可回售债券可以分解为两笔独立的交易. 投资者购买不可回售债券. 投资者从发行人那里购买一项回售期权，该期权允许投资者将债券回售给债券发行人. 可回售债券的价格可以表示为 $$ 可回售债券的价格 =不可回售债券的价格+回售期权的价格 $$ ","date":"0001-01-01","objectID":"/6.-%E9%99%84%E6%9C%89%E5%B5%8C%E5%85%A5%E5%BC%8F%E6%9C%9F%E6%9D%83%E5%80%BA%E5%88%B8/:1:0","tags":null,"title":"","uri":"/6.-%E9%99%84%E6%9C%89%E5%B5%8C%E5%85%A5%E5%BC%8F%E6%9C%9F%E6%9D%83%E5%80%BA%E5%88%B8/"},{"categories":null,"content":"估值 采用以下假设的收益率曲线 期限 到期收益率 即期利率 1年期远期利率 1 3.50 3.500 3.500 2 4.00 4.010 4.523 3 4.50 4.541 5.580 其中，假设债券为每年偿付一次，根据自助法，我们可以由到期收益率得出即期利率和一年期远期利率。 【未付期权债券估值】现在考虑一只息票利率为5.25%，距离到期日还剩三年的未付期权债券。该债券价格的计算方法有两种，两种方法都可以得出相同的结果。 用零息债券的利率贴现息票支付额: $$ \\frac{$ 5.25}{1.035}+\\frac{$ 5.25}{(1.0401)^{2}}+\\frac{$ 100+$ 5.25}{(1.04541)^{3}}=$ 102.075 $$ 以1年期远期利率贴现息票支付额: $$ \\frac{$ 5.25}{1.035}+\\frac{$ 5.25}{(1.035)(1.04523)}+\\frac{$ 100+$ 5.25}{(1.035)(1.04523)(1.05580)}=$ 102.075 $$ 引入利率波动性 当存在嵌入式期权时，就必须考虑利率波动性. 常用的方法是利率树（interest-rate tree），也称为利率树状图（interest-rate lattice）. 利率树是以某种利率模型和利率波动性假设为基础，描述某个期间内的远期利率如何随时间推移而变化的图形. \r ","date":"0001-01-01","objectID":"/6.-%E9%99%84%E6%9C%89%E5%B5%8C%E5%85%A5%E5%BC%8F%E6%9C%9F%E6%9D%83%E5%80%BA%E5%88%B8/:2:0","tags":null,"title":"","uri":"/6.-%E9%99%84%E6%9C%89%E5%B5%8C%E5%85%A5%E5%BC%8F%E6%9C%9F%E6%9D%83%E5%80%BA%E5%88%B8/"},{"categories":null,"content":"可转换债券(convertible bond)是一种对债券发行人的普通股拥有买入期权的公司债券。 可交换债券(exchangeable bonds)授予债券持有者将债券交换为债券发行人之外的其他公司的普通股的权利。 结算方式 在债券转换为普通股时债券持有者通常会从发行人手中得到标的股票，这称为实物结算(physical settle). 发行人也可以选择向债券持有者支付与标的股票价值相当的现金，这称为现金结算（cash settle*）*. 在执行可转换债券或可转换债券的买入期权时，债券持有者获得的普通股股数称为转换比率(conversion ratio). 在发行可转换债券时，发行人实际上授予了债券持有者按照以下价格购买普通股的权利，称为转换价格 $$ 转换价格 = \\frac{可转换债券的面值}{转换比率} $$ 赎回条款 在授予债券持有者转换期权的同时，多数可转换债券都可由债券发行人选择在特定时期赎回.可转换债券的这种标准赎回期权称为无保护赎回(unprotected call). 某些可转换债券还包括另一种赎回特征：债券可能只能在标的股票的价格(或某些天数内的平均股价)超过特定触发价格时被赎回，这种赎回称为保护赎回(protected call). 某些可转换债券是可回售的，回售期权可分为硬回售期权和软回售期权： 硬回售期权(hard put)是指债券发行人必须用现金赎回可转换债券. 软回售期权(soft put)是指债券发行人可选择用现金、普通股、次级票据或上述三者的组合赎回可转换债券. 另一种曾因有税收优惠而发行的债券附有或有偿付条款(contingent payment provision)，昵称为 “CoPa”债券，和息票利率在债券存续期中固定的传统可转换债券不同，Copa在现在基础股价达到特定限值利率(例如，转换价格的125%)时支付更高的息票利率. 类型****Type •现金支付可转换债券Cash-pay bonds •零息/初始发行折价可转换债券Zero-coupon/original issue discount •优先可转换债券Preferreds •强制可转换债券Mandatories 基础市值: 未清偿的普通股和普通股每股价格的乘积. 小市值可转换债券Small cap 中市值可转换债券Mid cap 大市值可转换债券Large cap 信用质量：基于可转换债券的信用评级 投资级可转换债券Investment grade 中级可转换债券Intermediate grade 垃圾级可转换债券Junk 未评级可转换债券Nonrated 特征****Profile 典型可转换债券Typical 股权敏感型可转换债券Equity sensitive 丧失转换价值的可转换债券Busted 不良债券Distressed ","date":"0001-01-01","objectID":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/:0:0","tags":null,"title":"","uri":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/"},{"categories":null,"content":"可转换债券的最低价值 可转换债券的转换价值(conversion value)是指债券被立即转换时的价值，表示为: $$ 转换价值 =普通股的市场价格 \\times 转换比率 $$ 可转换债券的最低价格为以下两个价值中的较高者 可转换债券的转换价值 其作为不含转换期权的公司债券的价值，即在假设债券不可转换的情况下（即普通债券），根据可转换债券的现金流确定的债券价值，称为直接价值(straight value). 为了估算直接价值我们必须确定具有相同信用评级和类似投资特征的不可转换债券的必要收益率。在给定估算的必要收益率后，直接价值就等于使用该收益率对债券现金流进行贴现所得到的价值。 如果可转换债券的销售价格并非是以上两个价格的较高者，那么，投资者将能获得套利利润。例如，假设转换价值高于直接价值，并且债券以直接价值交易，那么投资者可以按照直接价值购买可转换债券，然后将其转换为普通股，通过上述交易，投资者可以从中获利。  例  对于XYZ可转换债券： 转换价值 =普通股市场价格×转换比率 =$17 × 50 = $850 可转换债券分析的基础方法和概念(续)  例 (续) 为了确定直接价值，需要确定在市场上交易的可比债券.  假设可比债券交易时的收益率为14%.  那么直接价值就等于销售时收益率为14%、息票利率为10%的10期债券的价格.  这种债券的价格为$788. 可转换债券分析的基础方法和概念(续)  例(续)  如何转换价值为 $850，直接价值为 $788,那么可转换债券的最低价格为 $850.  为了说明这一点，注意，如果债券以直接价值而非转换价值出售，那么投资者就可以按$788价格购买该债券，同时按每股$17的价格出售50股XYZ股票.  将债券转换为普通股时，就是投资者补进卖空的股票时。通过这笔交易，投资者购买的每只XYZ债券会产生$62的套利利润.  以等于转换价值的价格（$850）出售债券XYZ是消除套利利润的唯一方法. 可转换债券分析的基础方法和概念(续)  例 (续)  相反，假设可比的不可转换债券以11.8%的收益率进行交易.  那么，XYZ债券的直接价值为$896，在这种情况下，债券的最低价格必定等于其直接价值，因为该价格高于$850的转换价值.  为了说明这一点，假设债券的市场价格为$850.  在这个价格水平上，债券收益率大约为12.7%,比可比的不可转换债券的收益率高90个基点.  投资者会发现该债券很有吸引力.  当投资者购买该债券时，他们的相互竞价会使债券收益率达到11.8%的水平. ","date":"0001-01-01","objectID":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/:0:1","tags":null,"title":"","uri":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/"},{"categories":null,"content":"市场转换价格 如果投资者购买的可转换债券，并将其转换为普通股那么投资者为普通股，实际支付的价格称为市场转换价格(market conversion price)。计算公式如下: $$ 市场转换价格 = \\frac{可转换债券的市场价格}{转换比率} $$ 如果投资者购买了可转换债券而非标的股票，那么，投资者通常要支付高于当前股票市场价格的溢价。每股溢价等于市场转换价格与当前的普通股市场价格之差，即: $$ 每股市场转换溢价 = 市场转换价格-当前市场价格 $$ 每股市场转换溢价通常表示为当前市场价格的百分比： $$ 市场转换溢价率 = \\frac{每股市场转换溢价}{普通股的市场价格} $$ 每股市场转换溢价可以视为看涨期权的价格。 看涨期权购买者与可转换债券购买者之间的区别在于，前者准确地知道下跌风险的大小，而后者只知道损失最高不超过可转换债券价格与直接价值之差. 【例】已知债券市场价格$950，股票价格为$17，转换比率为50，那么XYZ可转换债券的市场转换价格、每股市场转换溢价和市场转换溢价率的计算结果如下： ","date":"0001-01-01","objectID":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/:0:2","tags":null,"title":"","uri":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/"},{"categories":null,"content":"可转换债券与股票的当期收入比较 如果投资者选择投资与可转换债券而不是直接购买股票，那么通常意味着从实现的当期收入的角度看，投资者从可转换债券中获得的息票利息要高于从按转换比率换算的股票中获得的股利，两者之间的差额可以计算每股市场转换溢价. 在对可转换债券估值时，分析师通常通过计算溢价偿还期（premium per share）（或盈亏平衡点break-even time）来求出收回每股溢价所需的时间. 可转换债券与股票的当期收入比较(续)  溢价偿还期(premium payback period)的计算公式如下: 其中，每股收入顺差(favorable income differential)等于： 注意，溢价偿还期没有考虑货币的时间价值. 可转换债券与股票的当期收入比较(续)  例. XYZ可转换债券的息票利率为10%, 转换比率为50, 每股股利为 $1, 每股市场转换溢价为$2, 投资者将在多久后收回每股市场转换溢价?  首先计算每股收入顺差  债券的息票利息 = 息票利率面值 = 0.101,000 = $100.  每股收入顺差: 可转换债券与股票的当期收入比较(续)  示例 (续)  现在可以计算溢价偿还期:  因此,在不考虑货币时间价值的情况下,投资者将在两年后收回每股市场转换溢价. ","date":"0001-01-01","objectID":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/:0:3","tags":null,"title":"","uri":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/"},{"categories":null,"content":"可转换债券的跌价风险 投资者通常将债券的直接价值用作衡量可转换债券跌价风险的指标。因此，直接价值可以作为当前可转换债券价格的下限，跌价风险可以表示为直接价值的百分比。计算方法如下： $$ 超过直接价值的溢价= \\frac{可转换债券的市场价格}{直接价值}-1 $$ 在其他因素保持不变的情况下，超过直接价值的溢价越高，下跌风险越大，可转换债券的吸引力就越低。 示例.如前所述，如果可比的不可转换债券以14%的收益率交易，那么XYZ可转换债券的直接价值为$788。因此，超过直接价值的溢价为： 如果可比的不可转换债券的收益率为11.8%而非14%, 则直接价值为 $896. 超过直接价值的溢价为： ","date":"0001-01-01","objectID":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/:0:4","tags":null,"title":"","uri":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/"},{"categories":null,"content":"期权指标 由于可转换债券中嵌入了基础普通股的买入期权，因此我们可以根据期权理论中使用的指标估计可转换债券价格的敏感性。 我们介绍的指标是：delta, gamma, vega, 和隐含波动率. 基本上，这些指标都与影响期权价值的因素相关. 前三个指标显示了期权价格对于只会影响价格的某个特定因素的变化的敏感性. 以下是股票期权中此类因素的几个例子 – 标的股票的价格 – 标的股票价格的预期波动率 – 距期权到期日所剩的时间.  这些指标是通过运用期权定价模型（最常用的普通股期权估值模型，是第33章介绍的著名的Black-Scholes期权定价模型）和确定某个因素变化时（保持其他所有因素不变）理论价值如何改变而计算出来的。 期权的Delta衡量了期权价格对标的工具价格变化的敏感性。 对于普通股期权而言，标的工具为普通股。 对于可转换债券而言，标的工具为债券发行人的普通股。 因此，可转换债券的Delta为可转换债券的价值对标的股票价格变动的敏感性。 Delta的另一个名称是对冲比率(hedge ratio)或中性对冲比率(neutral hedge ratio)具体而言，Delta是可转换债券的价值变动与标的股票价格变动之比。 Delta可以用来估计标的股票每股价格变动对可转换债券价值的影响，如下： $$ 可转换债券价值的近似变动=每股股价变动×转换比率×Delta $$ 【例】考虑可转换债券XYZ,转换比率为50.假设delta为0.60 .当每股股价变动0.125美元时,可转换价值的近似变动为$ 0.125 × 50 × 0.60 = $ 3.75 $. delta的变化范围为0~1。delta可以帮助描述可转换债券的特征。 如果delta为0，那么基本上这只债券就是直接债券，因为基础股票的价格变动对可转换债券的价格没有影响。 在另一个极端，当delta为1时，可转换债券的价格变动等于基 础股票的价格变动。 用delta乘以转换比率50，结果为30，这意味着，为了获得中性头寸，必须卖空30股。比如，假设标的股票的价格增加$0.125，这意味着由30股股票组成的空头将损失3.75美元。 关于delta，有两点重要内容需要了解： delta只是一个近似指标，也就是说， delta是股票价格小幅变动时可转换债券价值变动的近似值. 期权的delta随着时间的推移而改变，它由于标的股票价格的变动以及已知影响期权价值的其他因素（如期权到期前的剩余时间）的变化而改变。因此，如果投资者希望在可转换债券中维持对冲头寸（即市场中性头寸）和股票空头，那么delta变化时空头就必须变动。 Gamma 久期是衡量利率变动时的债券价格变动的一级近似值，凸性指标显示了利率大幅变动时，债券价格的额外变动是多少  基本上，凸性与较大的利率变动和利率波动率的相关收益有关.  在期权理论中，gamma与凸性所起的作用相同.  对可转换债券而言， gamma 是标的股票价格大幅变动时，可转换债券价值的额外变动. Vega  期权的vega是期权价格对标的工具预期波动率变化的敏感性.  对可转换债券而言，它是可转换债券价格，对预期股票价格波动率变化的敏感性. ","date":"0001-01-01","objectID":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/:1:0","tags":null,"title":"","uri":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/"},{"categories":null,"content":"隐含波动率 在期权定价模型中，唯一必须估计的未知输入值就是预期波动率. 在期权市场中，通常的做法是，给定期权的市场观测价格和期权定价模型，倒算出(back out)预计波动率，用这种方法得到的波动率称为隐含波动率(implied volatility). 隐含波动率和历史波动率之差通常是期权市场上交易策略的基础,而在可转换债券市场上也被用于相同的用途。如果隐含波动性小于历史波动性，那么就认为期权是便宜的。对于可转换债券而言，如果隐含波动性小于历史波动性，那么债券就是便宜的。 ","date":"0001-01-01","objectID":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/:1:1","tags":null,"title":"","uri":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/"},{"categories":null,"content":"可转换债券的特征 在本章开头介绍如何对可转换债券分类时,一种方法是根据可转换债券的特征分类。基本上,所根据的特征是指决定可转换债券表现的因素，如发行人的股价或利率和利差水平. 根据雷曼兄弟的分类方法，可转换债券可以分为： 典型可转换债券(typical) 股权敏感型可转换债券(equity sensitive) 丧失转换价值的可转换债券(busted) 不良可转换债券(distressed) 一种典型的可转换债券指平衡可转换债券（balanced convertible），其对冲比率，即平衡可转换债券价格与股价变动的相关系数从约55%到80%不等。 股权敏感型可转换债券(equity sensitive convertible)，也称为股权替代可转换债券(equity substitute convertible)，是标的股票价格超过股票转换价格的可转换债券。 当标的股票价格远低于转换价格时，可转换债券称为丧失转换价值的可转换债券(busted convertible). 不良可转换债券(distressed convertible)可以视为一种特殊的丧失转换价值的可转换债券，在这种可转换债券中，标的股票价格跌至远远低于转换价格的水平，以至于发行人很可能被迫破产. ","date":"0001-01-01","objectID":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/:2:0","tags":null,"title":"","uri":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/"},{"categories":null,"content":"可转换债券投资的利弊 投资可转换债券的不利之处是必须支付每股溢价，而牺牲了价格上升潜力。  有利之处是降低了跌价风险（由直接价值决定），并有机会通过持有可转换债券获取更高的当期收入来补偿每股溢价. 可转换债券投资的利弊(续)  购回风险  发行人可以赎回可转换债券。  对债券发行人来说，这是一个有价值的债券特征。因此，如果发行人认为当前的股票市场价格被严重低估，那么直接卖出股票就会稀释现有股东的股权。  企业更愿意用发行股票，而不是发行债券的方式融资，因此，企业发行可转换债券，并根据可接受的股票价格来设定转换比率。  被收购风险  公司被收购是投资于可转换债券面临的另一种风险.  由于被收购公司的股票在收购活动完成后可能不再进行交易，因此留给投资者的只是息票利率低于同等风险公司债券息票利率的债券。 可转换债券套利  由于我们在本章中介绍的可转换债券的投资特征其财富特征可以创造出不同的头寸，这些头寸可以从可转换债券的错误定价中获益.  试图从可转换债券的错误定价中获利的行为称为可转换债券套利（convertible bond arbitrage）.  在所有可转换债券套利策略中，第一步都是找出交易价格显著偏离对冲基金经理的可转换债券模型显示的理论价值的可转换债券.  因此，这个过程严重依赖于该估值模型.  对于市场上被认为严重错误估值的可转换债券而言，因此有可转换债券标的普通股和对冲市场风险（这种市场风险可能会对可转换债券套利策略的目标产生不利影响）所需的衍生工具的头寸. 可转换债券套利(续)  可转换债券套利策略所使用债券的特征  再筛选可用于可转换债券套利策略的可转换债券时对冲基金经理，更偏好那些标的普通股和可转换债券本身有某些特征的可转换债券.  在可转换债券套利中，具有下列特征的可转换债券是比较理想的:（1）良好的流动性；（2）低转换溢价；（3）高凸性；（4）低隐含波动率.  具有下列特征的标的普通股是比较理想的：（1）高预期价格波动率；（2）可以方便的借入；（3）支付很少的股息或不支付股息. 可转换债券套利(续)  可转换债券套利策略的类型  现金流套利策略（ a cash flow arbitrage strategy ）的做法是建立相同的可转换债券头寸和标的股票头寸这样在消除或减轻风险时，就能从可转换债券中获得额外的现金流.  在可转换债券交易中，通常是建立一个可转换债券多头，同时建立一个标的股票空头.  波动率交易策略（ volatility trading strategy ）的目标是，不管标的股票价格如何变化，错误定价的可转换债券的价值，都将超过标的股票价值中空头的价值. 可转换债券套利(续)  可转换债券套利策略的类型  和多数期权交易策略一样，头寸必须随着标的工具价格的变动而变化.  有些期权策略需要利用期权delta的预期变动  在可转换债券套利gamma交易策略(gamma trading strategy)中，对冲基金经理不按dela的规定调整标的股票空头，而是根据delta的预期变动持有头寸.  根据预期，股价变动时将产生额外收入. 期权估值方法  购买不可赎回/不可回售可转换债券的投资者实际上进行了两笔独立的交易: • 买入不可赎回/不可回售直接债券 • 买入股票的看涨期权（或认股权证），其中，看涨期权可以购买的股票数量等于转换比率  公允价值取决于影响看涨期权价格的因素.  其中一个关键因素是预期股票价格波动率预期价格波动率越大，买入期权的价值就越高.  用下列公式可以计算出可转换债券的一级近似价值： 可转换债券的价值=直接价值+股票看涨期权的价格  看涨期权的价格需要与直接价值相加，这是因为投资者已经购买了股票的看涨期权. 期权估值方法(续)  考察可转换债券的一项普通特征发行者赎回债券的权利.  如果发行人赎回债券那么投资者损失的反映在市场价格中的超过转换价值的溢价.  因此，对可转换债券的分析必须考虑发行人赎回债券的权利的价值.  这种价值取决于：（1）未来的利率波动率；（2）决定发行人赎回债券是否为最优选择的经济因素.  Black-Scholes期权定价模型无法处理这种情况.  而二项式定价模型可同时用来为债券持有者的股票看涨期权和发行人的债券赎回期权估值.  二项式期权定价模型也适用于为债券持有者的回售权利估值.  为了将利率与股票价格联系起来，必须对这两个历史变量的历史变动进行统计分析，并将估计结果纳入模型中. 要点 ● 可转换债券赋予债券持有者将债券转换为事先确定数量的发行人普通股的权利。转换股数被称为转换比率. ● 2008年以前发行的可转换债券附有某些条款，投资者应该意识到这些条款独有的转换期权：净股结算条款和或有转换条款. ● 分析可转换债券需要计算转换价值、直接价值、市场转换价格、市场转换溢价率和溢价偿还期. ● 可转换债券的跌价风险通常可以通过计算直接价值之上的溢价估计出来，这种指标的局限性是直接价值（下限）随着利率的变动而变动. 要点(续) ● 由于可转换债券基本上属于一种附有标的股票看涨期权的债券，因此期权市场上的参与者使用的期权理论和指标也可用于描述可转换债券的投资特征，这些指标包括delta, gamma, vega和隐含波动率. ● 可转换债券可以根据其投资特征分类为：典型可转换债券（也称为平衡可转换债券）、股权敏感型可转换债券（也称为股权替代可转换债券）、丧失转换价值的可转换债券和不良可转换债券. ● 有几种策略可以用来利用可转换债券的错误定价获利这些策略称为可转换债券套利策略. ● 期权估值法可以用来确定嵌入式看涨期权的公允价值。这种方法运用某种股票期权定价模型（例如，Black-Scholes模型）来估算看涨期权的价值. 本章习题 • pp. 6, 8,10,15 ","date":"0001-01-01","objectID":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/:3:0","tags":null,"title":"","uri":"/7.-%E5%8F%AF%E8%BD%AC%E5%80%BA/"},{"categories":null,"content":"1. Solving Linear Equations ","date":"0001-01-01","objectID":"/1.-solving-linear-equations/:0:0","tags":null,"title":"","uri":"/1.-solving-linear-equations/"},{"categories":null,"content":"1.1 The geometry of linear equations The fundamental problem of linear algebra is to solve $ n $ linear equations in $ n $ unknowns; for example: $$ \\begin{array}{r} 2 x-y=0 \\ -x+2 y=3 \\end{array} $$ In this first lecture on linear algebra we view this problem in three ways. The system above is two dimensional $ (n=2) . $ By adding a third variable $ z $ we could expand it to three dimensions. Row Picture Plot the points that satisfy each equation. The intersection of the plots (if they do intersect) represents the solution to the system of equations. Looking at Figure 1 we see that the solution to this system of equations is $ x=1, y=2 $ Figure 1: The lines $ 2 x-y=0 $ and $ -x+2 y=3 $ intersect at the point (1,2) We plug this solution in to the original system of equations to check our Work: $$ \\begin{array}{r} 2 \\cdot 1-2=0 \\ -1+2 \\cdot 2=3 \\end{array} $$ The solution to a three dimensional system of equations is the common point of intersection of three planes (if there is one). Column Picture In the column picture we rewrite the system of linear equations as a single equation by turning the coefficients in the columns of the system into vectors: $$ x\\left[\\begin{array}{r} 2 \\ -1 \\end{array}\\right]+y\\left[\\begin{array}{r} -1 \\ 2 \\end{array}\\right]=\\left[\\begin{array}{l} 0 \\ 3 \\end{array}\\right] $$ Given two vectors $ \\mathrm{c} $ and $ \\mathrm{d} $ and scalars $ x $ and $ y, $ the sum $ x \\mathrm{c}+y \\mathrm{d} $ is called a linear combination of c and d. Linear combinations are important throughout this course. Geometrically, we want to find numbers $ x $ and $ y $ so that $ x $ copies of vector $ \\left[\\begin{array}{r}2 \\ -1\\end{array}\\right] $ added to $ y $ copies of vector $ \\left[\\begin{array}{r}-1 \\ 2\\end{array}\\right] $ equals the vector $ \\left[\\begin{array}{l}0 \\ 3\\end{array}\\right] . $ As we see from Figure $ 2, x=1 $ and $ y=2, $ agreeing with the row picture in Figure 2 Figure 2: A linear combination of the column vectors equals the vector b. In three dimensions, the column picture requires us to find a linear combination of three 3 -dimensional vectors that equals the vector $ \\mathbf{b} $. Matrix Picture We write the system of equations $$ \\begin{array}{r} 2 x-y=0 \\ -x+2 y=3 \\end{array} $$ as a single equation by using matrices and vectors: $$ \\left[\\begin{array}{rr} 2 \u0026 -1 \\ -1 \u0026 2 \\end{array}\\right]\\left[\\begin{array}{l} x \\ y \\end{array}\\right]=\\left[\\begin{array}{l} 0 \\ 3 \\end{array}\\right] $$ The matrix $ A=\\left[\\begin{array}{rr}2 \u0026 -1 \\ -1 \u0026 2\\end{array}\\right] $ is called the coefficient matrix. The vector $ \\mathbf{x}=\\left[\\begin{array}{l}x \\ y\\end{array}\\right] $ is the vector of unknowns. The values on the right hand side of the equations form the vector b: $$ A \\mathbf{x}=\\mathbf{b} $$ The three dimensional matrix picture is very like the two dimensional one, except that the vectors and matrices increase in size. Matrix Multiplication How do we multiply a matrix $ A $ by a vector $ \\mathbf{x} ? $ $$ \\left[\\begin{array}{ll} 2 \u0026 5 \\ 1 \u0026 3 \\end{array}\\right]\\left[\\begin{array}{l} 1 \\ 2 \\end{array}\\right]=? $$ One method is to think of the entries of $ x $ as the coefficients of a linear combination of the column vectors of the matrix: $$ \\left[\\begin{array}{ll} 2 \u0026 5 \\ 1 \u0026 3 \\end{array}\\right]\\left[\\begin{array}{l} 1 \\ 2 \\end{array}\\right]=1\\left[\\begin{array}{l} 2 \\ 1 \\end{array}\\right]+2\\left[\\begin{array}{l} 5 \\ 3 \\end{array}\\right]=\\left[\\begin{array}{r} 12 \\ 7 \\end{array}\\right] $$ This technique shows that $ A x $ is a linear combination of the columns of $ A $. You may also calculate the product $ A \\mathbf{x} $ by taking the dot product of each row of $ A $ with the vector $ x $ : $$ \\left[\\begin{array}{ll} 2 \u0026 5 \\ 1 \u0026 3 \\end{array}\\right]\\left[\\begin{array}{l} 1 \\ 2 \\end{array}\\right]=\\left[\\begin{array}{l} 2 \\cdot 1+5 \\cdot 2 \\ 1 \\cdot 1+3 \\cdot 2 \\end{array}\\right]=\\left[\\begin{array}{c} 12 \\ 7 \\end{array}\\right] $$ Linear Independence In the column and matrix pictures, the right hand s","date":"0001-01-01","objectID":"/1.-solving-linear-equations/:1:0","tags":null,"title":"","uri":"/1.-solving-linear-equations/"},{"categories":null,"content":"1. 数学建模与数学思想 [toc] ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:0:0","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.1 数学建模 数学建模思维方式：定量思维 探讨对象：自然现象、社会现象、工程技术、人类自身、日常生活中的实际问题 探讨模式： 寻找因素，即识别变量并创建方法量化这些变量 建立变量之间的定量关系，这种定量关系统称为数学模型 然后求解所建立的数学模型并解释、验证求解结果而应用于实际 形成的知识体系： 解决现实问题的同时形成特定的数学思想和建模方法； 建立该类数学模型的理论体系(大学学的内容)； 通过近似计算该模型来预测或解释现实问题的应用体系 数学与现实：数学家通过假设、简化、分析建立数学模型、建立数学理论回归现实解释预测，后来数学家专注于理论研究不回归现实了。历史上三次重大的哲学思潮导致了三次数学研究与背景问题研究的重大分离。 毕达哥拉斯的“万物皆数”形成了古希腊抽象数学体系 “文艺复兴\"时期“科学的本质是数学”的哲学思想，创建了微积分理论体系 1900前后欧洲数学家信奉自由建立纯粹数学结构的思潮，形成了现代纯粹数学和应用数学体系。 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:1:0","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.2 数学方法 在历史的长河中，人类建立了许多建模方法和数学理论体系，归纳起来有两大类 确定性数学(建模)方法：人类面对所研究现象或事件时**，决定现象或事件发展变化的因素即变量，在现象或事件发展变化中不再受其他因素的影响**。在这种情况下形成的建模方法就是确性数学(建模)方法，由此形成的数学理论就是确定性数学理论体系。 不确定性数学(建模)方法：不确定性数学(建模)方法就是人类所面对的现象或事件在发展变化过程中，将不可预知的因素影响。因此，这些现象在一定条件下，并不总是出现相同的结果，即随机现象。 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:2:0","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.2.1 确定性数学方法 1) 初等数学方法 最简定量关系即函数关系(相关性) 建立函数关系的方法： 数据散点图 自然定律 观察并用初等方法建模 拟合插值和回归 初等分析方法/函数论理论体系，比如：苹果从树上自然掉下来影响它运动的就是重力作用，位移与时间关系为$ s= gt^2/2$ 数据拟合 插值方法 应用积分思想 导数思想(变化率) 初等优化方法(求极值) 变量之间呈现代数方程：线性代数方程(组)、(由投入产出问题到填充问题)、空间几何方法建立起非线性代数方程 2) 离散动力学方法 变量间呈现周期的递推关系——差分方程方法 变量间呈现函数方程的形式 3) 连续动力学方法 变量间呈现的函数方程中含有未知函数导数——微分方程 含有偏导数的函数方程称为偏微分方程 4) 连续优化方法 变量之间具有优化效应； 变分法与最优控制 5) 离散优化方法 线性规划建模 整数规划模型 非线性规划建模 动态规划模型 图论模型 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:2:1","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.2.2不确定性数学方法 概率与随机数学：概率论、随机过程、马氏链模型、蒙特卡罗模拟、排队论与随机排队论、存储论与随机存储论、对策论、决策论 统计方法： 统计数据描述和分析 参数估计假设检验 回归分析(一元线性回归、多元线性回归、逐步回归、非线性回归) 方差分析(单因素方差分析、双因素方差分析、方差分析的模型检验) 聚类分析与判别分析 主成分分析和因子分析 对应分析与典型相关分析 时间序列分析 季节模型 条件异方差模型 界限不分明的模糊性问题：模糊数学方法、模糊关系与模糊矩阵、模糊聚类分析方法、模糊模式识别方法、模糊综合评判方法、灰色系统分析方法 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:2:2","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.2.3 其他数学方法 微分几何在广义相对论中的应用 拓扑学在大数据分析中的应用 偏微分方程在瓦斯爆炸的阻隔燝技术和在航空发动机推进技术中的应用 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:2:3","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.3 数学建模的步骤 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:3:0","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.3.1 问题分析 抓住事件本质，想象“理想状态”，确定主要变量，做出合理假设 变量识别 研究的现象或事件中所有变量明确，自变量和因变量都明确 因变量明确，但自变量不明确 自变量明确，但因变量不明确 因变量和自变量都不明确 如何确定主要变量？三点： 抓住事件本质，揭示理想状态，确定主要变量 顺着主要因素，找出相应的其他因素，把这些因素作为变量列出来，完善变量体系 忽略某些自变量：与其他因素相比，影响要小一点；这个变量以几乎相同的方式影响其他各种因素 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:3:1","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.3.2 模型构建 根据所作的假设，分析事件的内在规律，建立数学模型 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:3:2","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.3.3 求解或解释模型 理论与Matlab和R软件的使用 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:3:3","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.3.4 模型检验 数学关系的正确性 是否会有多解或无解的情况出现 数学方法的可行性以及算法的复杂性 有实际意义吗？有普遍意义吗？ 能收集到数据吗？ 进行误差分析和灵敏度分析 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:3:4","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.3.5 模型的改进 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:3:5","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.3.6 论文的写作 要将数学建模的论文当做科技论文的要求来撰写，数学建模的训练过程就是一次科研训练的过程，论文要符合学术论文的写作要求。 摘要：反映论文作者是否真正把握了问题，是否找准最好的方法建模，模型的准确性、改进程度、应用范围、与现实的吻合度 问题的重述：用自己的语言抽象出数学问题 问题的分析：问题的本质、规律、主要变量、主要影响因素、运用的方法及吻合度 问题的假设与符号 问题的解答 结论 参考文献 附录 程序以及某些图标可以放在附录 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:3:6","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.3.7 应用模型解决实际问题 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:3:7","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.4 课程说明 该课程的每一章是一门数学课甚至一个数学分支的诞生过程，本质就是建模的过程，这个过程就是数学思想诞生成长的过程，最后形成了针对这类问题的数学建模方法。本门课程有15个分支，因此涉及广泛的数学内容。 每个数学分支的诞生都是为了解决一类或几类现实问题，解决的过程就是数学思想诞生、数学建模方法形成的过程。这个解决过程中有许多细节正是我们已经学过的微积分或其他课程的应用。 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:4:0","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.4.1 先修课程 高等数学（数学分析）、高等代数、空门解析几何。 实际上，可以降低为微积分、线性代数方程、直线、平面、曲线曲面等在直角坐标系中的方程。 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:4:1","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"1.4.2 参考教材 谭忠编著，《数学建模-冋题、方法与案例分析(基础篇)》，高等教育出版社，2018年11月8日 谭忠编著，《数学建模-问题、方法与案例分析(提高篇)》(即将由高等教育出版社)教材。 谭永基，蔡志杰，俞文眦。数学模型。上海：复旦大学出版社，2006,3 薛毅，陈立萍。統计建模与R软件京：清华大学出版社，2007,4 姜启源，谢金星，叶俊。数学模型(第三版)。北京：高教出版社，2003,8 ","date":"0001-01-01","objectID":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/:4:2","tags":null,"title":"","uri":"/1.-%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%80%9D%E6%83%B3/"},{"categories":null,"content":"2. 初等数据处理方法 [Toc] ","date":"0001-01-01","objectID":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/:0:0","tags":null,"title":"","uri":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"2.1 最简定量关系 ","date":"0001-01-01","objectID":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/:1:0","tags":null,"title":"","uri":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"2.1.1 函数关系 人类建立起来的变量之间最简单、最直观的定量关系就是函数关系。 函数概念的萌芽：古代对图形轨迹的研究；一个就几个量的变化会引起另ー个量的変化，反映量与量之间的相互依赖关系。 函数的力学来源：16-17世纪，欧洲国家争霸，船只定位、炸弹精准落点各种运动 1637年笛卡尔的《几何学》首次涉及到变量，也引入了函数思想 函数解析定义，由英国数学家格雷果里在1667年给出 公认最早提出函数概念——17世纪德国数学家莱布尼茨。 初等数据分析方法，指建立函数关系的三种基本方法 观察法：利用变量之间的比例关系 拟合方法 插值方法 ","date":"0001-01-01","objectID":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/:1:1","tags":null,"title":"","uri":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"2.1.2 数据问题 在现实问题或竞赛命题中，有的提供数据 例2.1: 2004年全国大学生数学建模竞赛题\"奥运场馆周围临时商店的建设问题\"，数据给了很多，这些数据是规划局在前三次亚运会期间采集的数据.采集人大概是“志愿者\"之类的，完全不懂应该采集什么数据才能对解决这类问题有用，而将所有数据记录下来。 有的根本不给数据 例2.2: 2010年全国大学生数学建模竞赛题“试就某个或某几个方面评估上海世博会的影响力”。首先你必须确定从哪个方面评估，什么数据最能体现这个问题的本质，然后去查找相关数据有的问题问你需要什么数据。 例2.3: 2011年全国大学生数学建模竞赛A题《重金属的污染问题》，最后一问\"需要什么数据，我们可以确定城市土质变化的趋势？ 有的问题需要数据，但问题中不仅没有给出数据，就连采集什么数据也没有说明，需要你自己判明应该采集什么数据才能说明这件事情例 例2.4: 2015年国赛B题**“互联网+“时代的出租车资源配置**。有多家公司依托移动互联网建立了打车软件服务平台，实现了乘客与出租车司机之间的信息互通，同时推出了多种出租车的补贴方案，问题是搜集相关数据，建立数学模型研究如下问题： 试建立合理的旨标，并分析不同时空出租车资源的“供求匹配\"程度 分析各公司的出租车补贴方案是否对”缓解打车难有帮助“ 如果要创建一个新的打车软件服务平台，你们将设计什么样的补贴方案，并论证其合理性 另外，无论我们怎样精心设计并极其细心地进行试验我们仍需在拟合模型前评估数据的精确性。数据是如何收集的？收集过程中测量设备的精度如何？有没有疑问的点等。 ","date":"0001-01-01","objectID":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/:1:2","tags":null,"title":"","uri":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"2.2 初等数据分析思想与建模方法 在分析一个数据集合时，可能遇到的问题是 根据收集的数据迸行建模：数据具有明显特征，否则采用插值法 按照选出的一个或多个模型(函数)类型对数据进行拟合 从ー些已经拟合的模型(函数)类型中选取最合适的。例如，判断用指数模型是否比用最佳多项式模型要好 ","date":"0001-01-01","objectID":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/:2:0","tags":null,"title":"","uri":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"2.2.1 观察法和初等数学法 1) 观察法 观察法：通过大量数据，利用变量之向的比例性质，发现自然规律。采集数据可以追溯到7000年前的尼罗河沿岸居民对潮水的数据记录，最简单方式就是数据本身呈现比例关系。17-18世纪应用数据建立了许多物理规律。 例2.5 (Kepler开普勒第三定律): 开普勒曾帮助第谷(Tycho Brahe)收集了13年有关火星的相对运动的观察资料，到1609年已经形成了他的头两条定律： 每个行星都沿一条椭圆轨道运行，太阳在该椭圆的一个焦点处； 对每个行星来说，在相等的时间里，该行星和太阳的联线扫过相等的面积 开普勒花了许多年来验证并形成了第三定律，即轨道周期T(天数)与从太阳到行星的平均距离R之间的关系$ T=cR^{3/2}$。下表中的数据来自1993年的世界年鉴。 行星 周期(天数) 平均距离(百万英里) 水星 88.0 36 金星 224.7 67.25 地球 365.3 93 火星 687.0 141.75 木星 4331.8 483.80 土星 10,760.0 887.97 天王星 30,684.0 1,764.50 冥王星 90.466.8 3,653.90 下图画出了周期对平均距离的3/2次方的图形．该图形近似于一条通过原点的直线．纵坐标是周期(天数)，横坐标量纲是英里 ×(10的负4次方)． \r 任取过原点的这条直线上的两点，很容易估计其斜率(比例常数) $$ k=\\frac{90466.8-88}{220869.1-216} \\approx 0.410 $$ 估计模型为 $ T=0.410 R^{\\frac{3}{2}} $. 例2.6 (波义耳定律, Boyle’s law): 一定量的理想气体的压强P、体积V和绝对温度T之间具有定量关系 $$ P=\\frac{RT}{V} $$ R是普适气体常量，即压强P的变化同时依赖于V和T。 例2.7 (虎克定律, Hooke, 1678年): 一个线性弹簧，它的形变(x)与弹力(F)之间的定量关系为 $$ F=-KX $$ 负号表示形变的方向与弹力方向相反。 例2.8 (牛顿Newton, 万有引力公式1687年): 考虑两个物体之间的相互作用时，对于它们之间的相互吸引力的数学模型 $$ F=k \\frac{m_1 m_2}{r^2} $$ 这个数学模型及其理论是基于大量天文观测数据由牛顿在17世纪创立的。 例2.9 (欧姆定律, Ohm’s law, 1826年)：在同一电路中，通过某一导体的电流跟这段导体两端的电压成正比，跟这段导体的电阻成反比，这就是欧姆定律 $$ I=\\frac{U}{R}, \\quad U=I R, \\quad R=\\frac{U}{I} $$ 公式中物理量的单位：I(电流)的单位是安培(A)，U(电压)的单位是伏特(V)、R(电阻)的单位是欧姆($ \\Omega $)。 大量经济学领域的函数模型如下 (1)生产函数在一定技术条件下生产要素投入量与产品的最大产出量之间的定量关系； (2)需求函数-需求量与价格直降的函数关系。 (3)供给函数-供给量与价格直降的函数关系； (4)总成本函数；(5)总收益函数；(6)总利润函数； (7)效用函数；(8)消费函数；(9)储蓄函数。 2) 初等数学法 通过观察，利用初等数学的知识建立数学模型： 例2.17 由于地面凹凸不平，我们很难将椅子一次放稳，由此提出如下向题：将4条腿长相同的方椅子放在不平的地上，怎样オ能放平？如何才能把它抽象成数学向题？ 问题分析： 假定椅子中心不动，每条腿的着地点视为几何上的点，用A、B、C D表示，把AC和BD连线看做坐标 系中的X轴和y轴，把转动椅子看 做坐标的旋转，如图2.3 \r 用$ \\theta $表示对角线AC转动后与初始位置x轴正向的夹角，设g(θ)表示A,C两腿旋转θ角度后与地面距离之和，f(θ)表示B,D两腿旋转θ角度后与地面距离之和。当地面形成的曲面为连续函数时，f(θ), g(θ)皆为连续函数。 因为三条腿总能同时着地(要么A, C同时着地，要么B, D同时着地)，即对任意θ,总有 $$ f( \\theta ) \\cdot g(\\theta)=0 $$ 不妨设初始位置θ=0时，g(θ)=0, f(θ)\u003e0,于是问题转化为是否存在一个$ \\theta_0 $, 使$ f(\\theta_0)=g(\\theta_0)=0 $. 这样，椅子向题就抽象成如下数学问题：已知f(θ), g(θ)连续，g(0)=0, f(0)\u003e0, 且对任意的θ都有f(θ)・g(θ)=0。求证：存在$ \\theta_0 $，使得 $$ f(\\theta_0)=g(\\theta_0)=0 $$ 证明： 令h(θ)=g(θ)-f(θ)，则h(0)=g(0)-f(0)\u003c0。 将椅子转动$ \\pi/2$，即将AC与BD位置互换，则有$ g(\\pi /2)\u003e0, f(\\pi /2)=0 $, 所以$ h(\\pi /2)=g(\\pi /2)-f(\\pi /2)\u003e0 $。 而h(θ)是连续函数，根据连续函数的零点存在定理，知必存在$ \\theta_0 \\in (0, \\pi /2)$, 使得$ h(\\theta_0)=0$, 即$ g(\\theta_0)=f(\\theta_0) $。 又由条件对任意θ, 恒有f(θ)・g(θ)=0, 所以 $$ f(\\theta_0)=g(\\theta_0)=0 $$ 即存在方向，四条腿能同时着地。 所以椅子问题的答案是：如果地面为光滑曲面，椅子中心不动，最多转动$ \\pi/2 $角度，四条腿一定可以同时着地。 ","date":"0001-01-01","objectID":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/:2:1","tags":null,"title":"","uri":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"2.2.2 数据拟合方法 1) 拟合与插值 类似于统计学用部分数据反映整体或整体的趋势，我们希望通过部分数据获得变量之间的函数关系，即能否根据一组试验观测数据（部分数据）找到变量之间相对准确的函数关系（整体或整体的趋势）。 也就是，从一组试验观测数据$ (x_i, y_i), i=0,1…,n $之中找到自变量x与因变量y之间的函数关系一般可用某个近似函数y=f(x)来表示。 函数y=f(x)的产生办法因观测数据和要求不同而异，通常可采用数据拟合与函数插值两种办法来实现。拟合和插值都是要求通过已知的观测数据去寻求某个近似函数，使得近似函数与已知数据有较高的拟合精度，但两者在数学方法上完全不同。 例如实验测得如下一列数据 X -3 -2 -1 0 1 2 3 Y -8.0942 -3.0942 -0.0942 0.9058 -0.0942 -3.0942 -8.0942 作如下散点图 2.4． \r 拟合问题：请找出一个函数经过所有数据点 实际中，拟合是在求近似函数，不要求过所有的已知数据点，只要求在某种意义下它在这些点上的总偏差最小。主要用来反应数据的基本趋势。 即数据拟合主要是考虑到观测数据受随机观测误差的影响，进而寻求整体误差最小、能较好反映观测数据的近似函数y=f(x), 此时并不要求所得到的近似函数y=f(x)满足$ y_i=x_i, i=0,1,…,n $. 插值问题：请预测当x=3.5时，y的值。 应用插值法求过已知有限个数据点的近似函数时，要求所求的近似函数过已知的数据点，即函数插值要求近似函y=f(x)在每一观测$ x_i $处一定要满足$ y_i=x_i, i=0,1,…, n $。在这种情况下，通常要求观测数据相对比较准确，即不考虑观测误差的影响。 2) 数据拟合的判别准则 数据拟合有几种不同的判别准则： 使偏差的绝对值之和最小 使偏差的最大绝对值最小 使偏差的平方和最小（即最小二乘法） a. Chebyshev近似准则 假设想要对数据点集用一条直线y=aX+b近似，应如何选择a和b，使直线最好地近似数据？从图上看，当多于两个点时，它们不可能精确地处于一条直线上，一些数据点和直线同总存在一些纵向差异，我们称这些纵向差异为绝对偏差 定义2.1 给定m个数据点$ (x_i, y_i), i=0,1…,m $的集合，用直线y=ax+b拟合该集合，确定参数a和b，使任一数据$ (x_i, y_i)$和其对应的直线上的点$ (x_i, ax_i+b) $间的距离之和最小，即：极小化绝对偏差$ \\mid y_i -y(x_i) \\mid$的和。 推广到给定曲线情形：给定某一函数y=f(x), 以及m个数据点$ (x_i, y_i) $的集合，极小化绝对偏差$ \\mid y_i -y(x_i) \\mid$的和，也就是确定函数类型y=f(x)的参数，极小化 $$ \\sum_{i=1}^{m} \\mid y_i -y(x_i) \\mid $$ 定义2.2: 给定m个数据点$ (x_i, y_i), i=0,1…,m $的集合，用直线y=ax+b拟合该集合，确定参数a和b，使任一数据$ (x_i, y_i)$和其对应的直线上的点$ (x_i, ax_i+b) $间的最大距离最小，即：极小化最大绝对偏差$ \\mid y_i -y(x_i) \\mid$。 现推广到给定曲线的情形：给定某一函数y=f(x), 以及m个数据点$ (x_i, y_i) $的集合，极小化最大绝对偏差$ \\mid y_i -y(x_i) \\mid$，也就是确定函数类型y=f(x)的参数，极小化 $$ \\max \\mid y_i -y(x_i) \\mid, i=1,2,\\ldots,m $$ 这一重要的准则常称为Chebyshev近似准则，该准则的困难在于实际应用中通常很复杂，应用这一准则所产生的最优化问题可能需要高级的数学方法。 例2.18 设我们要度量图2.5表示的线段AB, BC和AC，假定你的测量的结果为AB=13, BC=7, AC=19 \r 这时，AB和BC值加起来是20而不是测出的19。现在用 Chebyshev准则来解决这一个单位的差异，也就是用一个方法为三个线段指定数值，使得指定的和观测的任一对应数之间的最大偏差达到极小。 解：假定对毎一次测量有相同的信任度，这样每测量值有相等的权值。于是，差异应均等地分配到每一线段。令$ x_1 $代表线段AB长度的真值，$ x_2 $代表BC的真值。为易于表示，令$ r_1, r_2, r-3 $表示真值和测量值冋的差异。即 $$ \\left{\\begin{array}{l}x_{1}-13=r_{1}(\\text {线段 } \\mathrm{AB}) \\ x_{2}-7=r_{2}(\\text {线段 } \\mathrm{BC}) \\ x_{1}+x_{2}-19=r_{3}(\\text {线段 } \\mathrm{AC})\\end{array}\\right. $$ 数值$ r_1, r_2, r-3 $称为残差 如果用 Chebyshev 近似准则，应指定$ r_{1}, r_{2}, r_{3} $ 的值，使三个数值 $ \\left|r_{1}\\right| ,\\left|r_{2}\\right|, \\left|r_{3}\\right| $ 的最大者达到最小.如果记最大的数为 r，那么我们要求最小化 r , 三个约束条件为 $$ \\left{\\begin{array}{l}\\left|r_{1}\\right| \\leq r \\text { 或 }-r \\leq r_{1} \\leq r \\ \\left|r_{2}\\right| \\leq r \\text { 或 }-r \\leq r_{2} \\leq r \\ \\left|r_{3}\\right| \\leq r \\text { 或 }-r \\leq r_{3} \\leq r\\end{array}\\right. $$ 这些条件的每一个对应两个不等式，即 $$ \\begin{aligned} \u0026 \\min \\mathbf{r} \\ \u0026s.t. \\quad\\left{\\begin{array}{l}r-x_{1}+13 \\geq 0\\left(r-r_{1} \\geq 0\\right) \\ r+x_{1}-13 \\geq 0\\left(r+r_{1} \\geq 0\\right) \\ r-x_{2}+7 \\geq 0\\left(r-r_{2} \\geq 0\\right) \\ r+x_{2}-7 \\geq 0\\left(r+r_{2} \\geq 0\\right) \\ r-x_{1}-x_{2}+19 \\geq 0\\left(r-r_{3} \\geq 0\\right) \\ r+x_{1}+x_{2}-19 \\geq 0\\left(r+r_{3} \\geq 0\\right)\\end{array}\\right. \\end{aligned} $$ 这一问题称为线性规划问题 . 推广这一过程，给定某一函数类型y=f(x)，其参数待定，以及给定 m 个 数据点 $ \\left(x_{i}, y_{i}\\right) $ 的一个集合，并确定出残差为$ r_{i}=y_{i}-\\mathrm{f}\\left(x_{i}\\right) $。 如果 r 代表这些残差的最大绝对值，那么问题表示如下 $$ \\begin{aligned} \u0026 \\min \\mathbf{r} \\ \u0026s.t. \\quad\\left{\\begin{array}{l}r-r_{i} \\geq 0 \\ r+r_{i} \\geq 0\\end{array}\\right. ,i=1,2, \\cdots,m \\end{aligned} $$ b. 最小二乘准则 问题：确定函数类型 y = f(x) 的参数 , 极小化和数 $$ \\sum_{i=1}^{m} \\mid y_{i}-f\\left(x_{i}\\right) \\mid^{2} $$ 例 2.19: 下表是收集到的数据 x 1 2 3 4 z 8.1 22.1 60.1 165 画出这批数据在直角坐标上的散点图，如图2.6。 \r 看起来两者呈指数关系，因此可设 z 与温度 x 的关 系为 $$ z=\\beta e^{\\alpha x} $$ 我们的任务是具体确定常数$ \\alpha, \\beta $，上式两边取对数，令 $$ y=\\ln z, a=\\alpha, b=\\ln \\beta $$ 则原式变成了线性关系 $$ y=a x+b $$ 表格变为 x 1 2 3 4 y=ln z 2.1 3.1 4.1 5.1 散点图变为图 2.7： \r 于是，问题化为找一直线 y = ax + b，即求 a, b使得上表中的数据基本满足这个函数关系。使得所有观测值 yi 与函数值 axi + b 之偏差的平方和$ Q=\\sum_{i=1}^{n}\\left(y_{i}-a x_{i}-b\\right)^{2} $最小。确定常数 a，b 用的就是二元函数求极值的方法，显然 Q 是 a，b 的函数 . 令 $$ \\begin{aligned} \\frac{\\partial Q}{\\partial a}\u0026=-2 \\sum_{i=1}^{n}\\left(y_{i}-a x_{i}-b\\right) x_{i}=2 a \\sum_{i=1}^{n} x_{i}^{2}-2 \\sum_{i=1}^{n} x_{i} y_{i}+2 b \\sum_{i=1}^{n} x_{i}=0 \\ \\frac{\\parti","date":"0001-01-01","objectID":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/:2:2","tags":null,"title":"","uri":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"2.2.3 插值方法 已知某函数y = f(x)(未知)的一组观测或试验数据$ \\left(x_{i}, y_{i}\\right), \\mathrm{i}=0,1,2, \\dots \\cdot, n $, 要寻求一个函数$ \\varphi(x) $，使得 $$ \\varphi\\left(x_{i}\\right)=y_{i}(\\mathrm{i}=0,1,2, \\cdots, \\mathrm{n}) $$ 则$ \\varphi(x) \\approx f(x) $。 具体而言，实际中在不知道函数 y = f(x) 的具体表达式的情况下，对于 $ x=x_i $ 有实验测量值 $ y=y_{i}(i=0,1,2, \\cdots, n) $。寻求另一函数$ \\varphi(x) $使满足 $$ \\varphi\\left(x_{i}\\right)=y_{i}=f\\left(x_{i}\\right) \\quad i=0,1,2, \\cdots, n $$ 称此问题为一维插值问题。并称函数 $ \\varphi(\\mathbf{x}) $ 为 f(x) 的插值函数， $ x_{i}(\\mathrm{i}=0,1,2, \\cdots, \\mathrm{n}) $ 称为**插值结点**, $ \\varphi\\left(x_{i}\\right)=y_{i}(\\mathrm{i}=0,1,2, \\cdot \\cdot \\cdot, n) $ 称为插值条件，则 $ \\varphi(\\mathbf{x}) \\approx \\mathrm{f}(\\mathrm{x}) $。 在实际问题中所遇到的插值问题除了一维插值问题外，还有二维插值问题。 下面介绍几种基本的、常用的一维插值方法： 拉格朗日插值法 牛顿插值法 Hermite插值法 分段线性插值法 三次样条插值法 1) 拉格朗日（Lagrange）插值 a. 插值多项式 已知函数 y=f(x) 在 n+1 个相异点 $ x_{0}, x_{1}, x_{2} ,\\cdots ,x_n$上的函数值为 $ y_{0}, y_{1}, y_{2}, \\cdots, y_{n}, $ 要求一个次数不超过 n 的代数多项式 $$ p_{n}(x)=a_{0}+a_{1} x+a_{2} x^{2}+\\cdots+a_{n} x^{n} $$ 使在结点$ x_i $上有 $ p_{n}\\left(x_{i}\\right)=y_{i}(i=0,1,2, \\cdots, n) $成立，称 $ p_{n}(x) $ 为**插值多项式**。则 f(x) 的 n+1 个待定系数$ a_0, a_1, \\cdots , a_n $满足 $$ \\left{\\begin{array}{c} a_{0}+a_{1} x_{0}+a_{2} x_{0}^{2}+\\cdots+a_{n} x_{0}^{n}=y_{0} \\ a_{0}+a_{1} x_{1}+a_{2} x_{1}^{2}+\\cdots+a_{n} x_{1}^{n}=y_{1} \\ \\cdots \\ a_{0}+a_{1} x_{n}+a_{2} x_{n}^{2}+\\cdots+a_{n} x_{n}^{n}=y_{n} \\end{array}\\right. $$ 记此方程组的系数矩阵为 A，则 $$ \\operatorname{det}(A)=\\left|\\begin{array}{cccc} 1 \u0026 x_{0} \u0026 \\cdots \u0026 x_{0}^{n} \\ 1 \u0026 x_{1} \u0026 \\cdots \u0026 x_{1}^{n} \\ \\vdots \u0026 \\vdots \u0026 \u0026 \\vdots \\ 1 \u0026 x_{n} \u0026 \\cdots \u0026 x_{n}^{n} \\end{array}\\right| $$ 是范德行列式 . 当 $ x_{0}, x_{1}, x_{2}, \\cdots, x_{n} $ 互不相同时 $ , $ 此行列式值不为零。因此，方程组有唯一解。这表明只要 n + 1 个插值节点 $ x_{0}, x_{1}, x_{2}, \\cdots, x_{n} $互异，满足插值条件的插值多项式存在且唯一。 从几何上看，n次多项式插值就是过 n +1 个点 $ (x_i, y_i) $作一条多项式曲线$ y=p_{n}(x) $ 来近似曲线 $ y=f(x), $ 可以证明n次插值问题的解是惟一的。 当 $x \\in [a,b] $ 且 $ x \\neq x_{i}(i=0,1, \\cdots, n) $ 时，$ f(x) \\approx p_{n}(x) $，称被插函数f(x)与插值函数多项式$ p_n(x) $之间的差 $$ R_{n}(x)=f(x)-p_{n}(x) $$ 为插值多项式$ p_n(x) $的截断误差，或插值余项。 用多项式函数$ p_n(x) $作为插值函数时，希望通过解方程组而得到待定系数$ a_{0}, a_{1}, \\cdots \\cdot a_{n} $的做法，**当n比较大时是不现实的**。因此，我们采用拉格朗日(Lagrange)插值多项式。 b. 拉格朗日(Lagrange)插值多项式 首先构造一组基函数，对$ i=0,1 \\cdots, n $, $$ \\begin{aligned} l_{i}(x)\u0026=\\prod_{j=0, j \\neq i}^{n} \\frac{x-x_{j}}{x_{i}-x_{j}} \\ \u0026=\\frac{\\left(x-x_{0}\\right) \\cdots\\left(x-x_{i-1}\\right)\\left(x-x_{i+1}\\right) \\cdots\\left(x-x_{n}\\right)}{\\left(x_{i}-x_{0}\\right) \\cdots\\left(x_{i}-x_{i-1}\\right)\\left(x_{i}-x_{i+1}\\right) \\cdots\\left(x_{i}-x_{n}\\right)} \\end{aligned} $$ 显然 $l_{i}(x)$ 是 $ n $ 次多项式, 且满足 $$ l_{i}\\left(x_{j}\\right)=\\left{\\begin{array}{ll} 0 \u0026 j \\neq i \\ 1 \u0026 j=i \\end{array}\\right. $$ 令 $$ p_{n}(x)=\\sum_{i=0}^{n} y_{i} l_{i}(x)=\\sum_{i=0}^{n} f\\left(x_{i}\\right) \\prod_{j=0, j \\neq i}^{n} \\frac{x-x_{j}}{x_{i}-x_{j}} $$ 则有 $$ p_n(x_i)=y_i $$ 即满足插值条件，称$ p_n(x) $即为n次拉格朗日(Lagrange)插值多项式，同样由唯一性，n+1个节点的n次拉格朗日插值多项式存在且唯一。 当 f(x) 在 [a, b] 上充分光滑时，利用罗尔(Rolle)定理可推出：对于任意 $ x \\in [a,b] $ 插值多项式 $ p_n(x) $ 的余项 $$ R_{n}(x)=f(x)-p_{n}(x)=\\frac{f^{(n+1)}(\\xi)}{(n+1) !} \\prod_{i=0}^{n}\\left(x-x_{i}\\right), \\xi \\in(a, b) $$ 例2.20: 设 $ f(x)=\\sqrt[3]{x}, $ 取结点为 x=1、1.728、2.744，求f(x)的二次拉格朗日插值多项式及$ p_n(x) $其余项的表达式，并计算$ p_2(x) $。$ (\\sqrt[3]{2}=1.2599210 \\cdot \\cdots) $ 解： 取 $ x_{0}=1, x_{1}=1.728, x_{2}=2.744 $ 为插值结点，则函数 $ f(x)=\\sqrt[3]{x} $ 的相应的函数值为 $$ \\begin{array}{ll} f\\left(x_{0}\\right)=1, f\\left(x_{1}\\right)=1.2, f\\left(x_{2}\\right)=1.4 . \\end{array} $$ 于是，由拉格朗日插值公式, $$ \\begin{aligned} f(x) \u0026\\approx p_{2}(x) \\ \u0026=1 \\cdot \\frac{(x-1.728)(x-2.744)}{(1-1.728)(1-2.744)} \\ \u0026+1.2 \\cdot \\frac{( x-1)(x-2.74)}{(1.728-1)(1.728-2.744)} \\ \u0026+1.4 \\cdot \\frac{(x-1)(x-1.728)}{(2.744-1)(2.7 .44-1.728)} \\ \u0026\\approx-0.0447 x^{2}+0.3965 x+0.6481 \\end{aligned} $$ 将 x=2 带入就得到 $$ \\sqrt[3]{2} \\approx p_{2}(2)=1.2626 $$ 它与准确值的差的绝对值(称为绝对误差)约为 0.0027，而由插值余项估计公式，其误差约为 $$ \\left|R_{n}(2)\\right|=\\left| \\frac{5}{81} \\cdot \\frac{(2-1)(2-1.728)(2-2.744)}{\\xi^{3/8}} \\right| \\leq 0.0125 $$ 2) 牛顿(Newton)插值 a. 函数的差商及其性质 设有函数$ f(x) $, 其中取 $ x_{0}, x_{1}, x_{","date":"0001-01-01","objectID":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/:2:3","tags":null,"title":"","uri":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"2.2.4 拟合与插值的MATLAB编程实现 1) Matlab多项式拟合 a = polyfit(x, y, n)：多项式拟合，返回降幂排列的多项式系数。其中x，y是数据点的值，n为拟合的最高次数。 y = polyval(a, x)：计算拟合的多项式在 x 处的值 在处理一些无约束条件的最小二乘拟合时往往会涉及到最小二乘优化，最小二乘优化是一类比较特殊的优化问题，在处理这类问题时，Matlab 也提供了一些强大的函数．在 Matlab 优化工具箱中，用亍求解最小二乘优化问题的凼数有：lsqlin、lsqcurvefit、lsqnonlin、lsqnonneg 二、Matlab 数据拟合工具箱 在 Matlab 中的工作区，输入命令 cftool，便会出现如下拟吅工具箱 5 个命令按钮的功能凾别如下： Data 按钮：可输出、查看和平滑数据； Fitting 按钮：可拟吅数据、比较拟吅曲线和数据集； Exclude 按钮：可以从拟吅曲线中排除特殊的数据点； Ploting 按钮：在选定区间后，单击按钮，可以显示拟吅曲线和数据 三、Matlab 一维插值 用 Matlab 实现凾段线性插值丌需要编制凼 ·数程序，Matlab 中关亍一维插值的凼数为 interp1． y = interp1(x0, y0, x,′ method′), 其中 method 指定插值的方法，默认为线性插值．其值可为：’nearest’ 最近项插值，’linear’ 线性插值，’spline’ 逐段 3 次样条插值，’cubic’ 保凹凸性 3 次插值. 详细情况请使用 help interp1； 四、Matlab 二维插值 当插值节点为网格节点时，命令为 z = interp2(x0, y0, z0, x, y,′ method′), 其中 x0，y0 凾别为 m 维和 n 维向量，表示节点，z0 为 m × n 维矩阵，表示节点值，x，y 为一维数组，表示插值点，x 不 y 应 是方向丌同的向量，即一个是行向量，另一个是列向量，z 为矩 阵，它的行数为 x 的维数，列数为 y 的维数，表示得到的插 值，’method’ 的用法同上面的一维插值． ","date":"0001-01-01","objectID":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/:2:4","tags":null,"title":"","uri":"/2.-%E5%88%9D%E7%AD%89%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"3. 初等分析方法 ","date":"0001-01-01","objectID":"/3.-%E5%88%9D%E7%AD%89%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/:0:0","tags":null,"title":"","uri":"/3.-%E5%88%9D%E7%AD%89%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"3.1 应用积分思想建模 ","date":"0001-01-01","objectID":"/3.-%E5%88%9D%E7%AD%89%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/:1:0","tags":null,"title":"","uri":"/3.-%E5%88%9D%E7%AD%89%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"3.1.1 催生积分思想产生的源头问题 十七世纪，四类问题催生了微积分的诞生 求瞬时速度的问题 求曲线的切线问题 求函数的最大值和最小值问题 求曲线长、曲线围成的面积、曲面围成的体积、物体的重心、一个体积相当大的物体作用于另一物体上的引力 显然，第一类和第二类问题产生了导数思想；第三类问题产生了初等化思想；第四类问题产生了积分思想。 第四类问题的现代提法可以写为： 一维情形：一根长度为L的细的直杆，它在取定的坐标轴 x 上占据闭区间 [0,L]，设其密度为$ \\rho = \\rho (x) $，求此杆的质量; 二维情形：物体是一薄的平板，它在取定的坐标系 xy 平面上占据区域$ \\Omega $，设其密度$ \\rho=\\rho (x,y) $不等于常数，求此板的质量； 三维情形：如果物体占据 xyz 空间区域G，其密度为$ \\rho = \\rho (x,y,z) $，求其质量； 一维不规则情形，第一型曲线积分：设曲线型细长铁丝形成空间上的一根曲线l，其上任一点 (x, y, z) 的线密度为$ \\rho = \\rho (x,y,z) $，试求此铁丝的质量: 二维不规则情形，第一型曲面积分：空间中一薄板呈现曲面$ \\Sigma $形式，任一点 (x, y, z) 的面密度(单位面积上的质量)有分布函数$ \\rho = \\rho (x,y,z) $确定，问如何求此板$ \\Sigma $的总质量； 一维不规则情形，第二型曲线积分：设有一个质 点 , 在平面或(空间)力 场 $ \\mathrm{F}(\\mathrm{x}, \\mathrm{y})(\\mathrm{F}(\\mathrm{x}, \\mathrm{y}, \\mathrm{z})) $ 的作用下，从一光滑的平面(空间)曲线C之一端A至另一端B。试求力场F所做的功 . 二维不机则情形，第二型曲面积分：考虑空间定常流，设其速度场为$ \\mathbf{v}(\\mathbf{x}, \\mathbf{y}, \\mathbf{z})=(\\mathbf{P}(\\mathbf{x}, \\mathbf{y}, \\mathbf{z}), \\mathbf{Q}(\\mathbf{x}, \\mathbf{y}, \\mathbf{z}), \\mathbf{R}(\\mathbf{x}, \\mathbf{y}, \\mathbf{z})) $, $ (x, y, z) \\in \\Omega $试计算流体沿指定的法线方向V通过$ \\Omega $内某光滑曲面$ \\Sigma $的流量(即单位时间内通过$ \\Sigma $的流体体积)。 在解决这些问题的过程中就产生了积分思想，形成了积分建模的方法 。 ","date":"0001-01-01","objectID":"/3.-%E5%88%9D%E7%AD%89%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/:1:1","tags":null,"title":"","uri":"/3.-%E5%88%9D%E7%AD%89%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"3.1.2 积分思想与建模方法 通过回答上述三个问题介绍积分思想。 一维情形的解答：对区间作任意分割， $$ \\Delta: 0=x_{0}\u003cx_{1}\u003c\\cdots\u003cx_{K}=l $$ 并任意取介点 $ \\xi_{i} \\in\\left[x_{i-1}, x_{i}\\right], $ 在每个区间 $ \\left[x_{i-1}, x_{i}\\right] $上，以\"常(密度)代变(密度)\" ，计算出质量的近似值，然后通过一个极限过程，求得此杆的质量 $$ \\begin{aligned} \\boldsymbol{m} \u0026=\\lim {|\\Delta| \\rightarrow 0} \\sum{i=1}^{K} \\rho\\left(\\boldsymbol{\\xi}{i}\\right) \\boldsymbol{\\Delta} \\boldsymbol{x}{i} \\ \u0026=\\int_{0}^{l} \\rho(\\boldsymbol{x}) d \\boldsymbol{x} \\end{aligned} $$ 其中 $ \\Delta x_{i}=x_{i}-x_{i-1}, \\quad|\\Delta|=\\max _{1 \\leq i\u003cK} \\Delta x_{i} $，这样，计算直杆的质量引出一元函数的定积分。同理计算直线上变速运动的路程以及曲边梯形的面积都可以引出一元函数的定积分概念 . 二维情形的解答：现设所考虑的物体是一薄的平板，不妨认为它在取定的坐标系 xy 平面上占据区域$ \\Omega $，设其密度$ \\rho= rho (x, y) $不等于常数。将$ \\Omega $分割为 K 个彼此没有公共内点的闭子域 : $$ \\Omega_{1}, \\cdots, \\Omega_{K} \\ $$ 并任取点$ \\left(x_{i}, y_{i}\\right) \\in \\Omega_{i} $, 求得此板的质量 $$ \\begin{aligned} m \u0026=\\lim _{|\\Delta| \\rightarrow 0} \\sum_{i=1}^{K} \\rho\\left(x_{i}, y_{i}\\right) \\Delta A_{i} \\ \u0026=\\iint_{(x, y) \\in \\Omega} \\rho(x, y) d x d y \\end{aligned} $$ 其中 $ \\triangle A_{i} $ 表示 $ \\Omega_{i} $的面积, 而 $ |\\Delta| $ 表示 $ \\Omega_{i} $中直径最大者。所谓点集 E 的直径，是指 E 中任意两点的距离所构成的数集的上确界 . 三维情形的解答：如果物体占据 xyz 空间区域 G 其密度为$ \\rho= \\rho (x,y,z) $，并取任意点其质量就是： $$ \\begin{aligned} \\boldsymbol{m} \u0026=\\lim {|\\Delta| \\rightarrow 0} \\sum{i=1}^{K} \\rho\\left(\\boldsymbol{x}{i}, \\boldsymbol{y}{i}, \\boldsymbol{z}{i}\\right) \\boldsymbol{\\Delta} \\boldsymbol{V}{i} \\ \u0026=\\iiint_{(x, y, z) \\in G} \\rho(\\boldsymbol{x}, \\boldsymbol{y}, \\boldsymbol{z}) d \\boldsymbol{x} d \\boldsymbol{y} \\boldsymbol{d} \\boldsymbol{z} \\end{aligned} $$ 其中$ \\Delta V $, 表示分割区域 G 所得的闭子域的体积，而$ |\\Delta| $表示 G 中直径最大者。 一维不规则情形，第一型曲线积分的解答：如果密度为$ \\rho = \\rho ( x, y, z) $， 则对此曲线作任意分割所得之微小弧段 $ C_{i}, $ 并任取介点$ \\left(x_{i}, y_{i}, z_{i}\\right), $ 便可求出其质量是： $$ m=\\lim _{|\\Delta| \\rightarrow 0} \\sum_{i=1}^{N} \\rho\\left(x_{i}, y_{i}, z_{i}\\right) \\Delta s_{i}=\\int_{C} \\rho(x, y, z) d s $$ 其中$ \\triangle s $表示分割曲线所得之微小弧段$ C_{i} $的弧长，$ | \\Delta | $表示诸 $ C_{i} $ 的直径中之最大者，这是第一型曲线积分。 二维不规则情形，第一型曲面积分的解答：空间中一薄板呈现曲面$ \\Sigma $形式任一点 (x, y, z) 的面密度(单位面积上的质量)有分布函数$ \\rho= \\rho (x,y,z) $，确定，则对此曲面作任意分割所得之小块曲面的$ \\Sigma_i $，并任取介点 $ \\left(x_{i}, y_{i}, z_{i}\\right) \\in \\sum_{i}, $ 便可求出其质量是 : $$ m=\\lim _{|\\Delta| \\rightarrow 0} \\sum_{i=1}^{N} \\rho\\left(x_{i}, y_{i}, z_{i}\\right) \\Delta S_{i}=\\iint_{\\Sigma} \\rho(x, y, z) d S $$ 称为函数$ \\rho(x,y,z) $在曲面区$ \\Sigma_i $上的第一型曲面积分. 其中$ \\Delta S_i $表示分割曲面所得之小块曲面的$ \\Sigma_i $面积，$ | \\Delta | $表示诸$ \\Sigma_i $的直径中之最大者。 一维不规则情形，第二型曲线积分的解答：如果质点受常力f的作用 沿直线运动，位移为s，那么这个常力所作的功就是 $$ W=|\\mathbf{f}| \\mathbf{s} | \\cos (\\mathbf{f}, \\mathbf{s})=\\mathbf{f} \\cdot \\mathbf{s} $$ 这里$ | \\cdot | $表示向量的模，(f, s)和f · s分别表示两个向量的夹角和内积. 现在问题的难点是，质点所受的力随处 改变，所走的路线弯弯曲曲. 怎么办呢？还是用局部以常代变和折线逼近曲线的方法来解决这个问题。 任意对曲线C进行分割。设分点依次为A $ =A_{0}, A_{1}, \\ldots, A_{k-1}, A_{k}= B $假定F是连续的。在分割相当细密时，F在有向弧段$ \\hat{ A_{k-1} A_{k} } $ 上便可以看作常力$ \\mathrm{F}_{k}=\\mathrm{F}\\left(A_{k}\\right) $. 而弧段$ \\hat{ A_{k-1} A_{k} } $ 便近似于有向线段 $ \\overrightarrow{A_{k-1} A_{k}} $。按照( $ ^{\\star} $ )，常力F沿有向线段 $ \\overline{A_{k-1} A_{k}} $所作的功为$ F_{k} \\cdot \\overrightarrow{A_{k-1} A_{k}} $。 因此，力场F沿曲线弧 $ \\sqrt{A B} $ 所作之功 $$ W \\approx \\sum_{k=1}^{N} \\mathbf{F}_{k} \\cdot \\overrightarrow{A_{k-1} A_{k}} $$ 设$ A_{k}=\\left(x_{k}, y_{k}\\right), \\Delta x_{k}=x_{k}-x_{k-1}, \\Delta y_{k}=y_{k}-y_{k-1}, $ 有向线段 $ {A_{k-1} A_{k}} $ 的长度为 $ \\Delta s_{k} $, 它与X轴的夹角为 $ \\tau_{k} $。 于是，上式右端的和可以表示为 $$ \\sum_{k=1}^{K}\\left[P\\left(x_{k}, y_{k}\\right) \\Delta x_{k}+Q\\left(x_{k}, y_{k}\\right) \\Delta y_{k}\\right]=\\sum_{k=1}^{K}\\left[P\\left(x_{k}, y_{k}\\right) \\cos \\tau_{k}+Q\\left(x_{k}, y_{k}\\right) \\sin \\right. $$ 由此可见 $$ W=\\int_{c} P(x, y) d x+Q(x, y) d y=\\int_{c}(P(x, y) \\cos \\tau+Q(x, y) \\sin \\tau) d s $$ 其中 $ \\tau=\\tau(x, y) $ 是曲线C在点( $ \\mathbf{x}, \\mathbf{y}) $ 处与曲线方向一致的切向量同x轴的夹角. 此外，上式左端还向人们提示了一种新的积分和. 设C是xy平面上一条分段光滑的有向线段，如上所述，$ \\mathrm{F}(\\mathbf{x}, \\mathbf{y})=(\\mathrm{P}(\\mathbf{x}, \\mathbf{y}), \\mathbf{Q}(\\mathbf{x}, \\mathbf{y})) $ 是定义在C上的向量场. 如果下列两式右端 的第一型曲线积分存在，我们就定义 $$ \\begin{array}{l} \\int_{C} P(x, y) d x=\\int_{C} P(x, y) \\cos \\tau d s \\ \\int_{C} Q(x, ","date":"0001-01-01","objectID":"/3.-%E5%88%9D%E7%AD%89%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/:1:2","tags":null,"title":"","uri":"/3.-%E5%88%9D%E7%AD%89%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"3.1.3 案例分析 案例一 已知某种物理量的分布求该物理量 的总量 问题背景 : 设一根长度为 $ l $ 的直线段上分布 热量、电荷量等 “将其平放在x轴的正半轴上，使它的 一头与原点重合，若它在x处的密度（称为 线密度）可由某个连续的分布函数p(x)表 示 . 试由分布函数求该物理量的总量Q . 3.1.3 案例分析 解 由微元法，该物理量在 [X, x+dx] 上 具有 dQ 为 $$ d Q=\\rho(x) d x $$ 对等式两边在 [0,l] 上积分，就得到由分布函 数求总量的公式 $$ Q=\\int_{0}^{l} \\rho(x) d x $$ 3.1.3 案例分析 推广 $ \\quad $ 1、假定物理量分布在一个平面区域上，X 的变化范围为区间[a, b] . 如果过x(a \\leq x \\leq b) 点并且垂直于x轴的直线与该平面区域之交的物 理量的密度可以用f(x)表示，或者说该平面区域 在横坐标位于[X, \\quad X+dx]中的部分上的物理量可 以表示为f(x)dx . 那么，这个区域上的总的物理 量是多少？ 推广 2 、 给定平面其线的参数方程，分布函丟签 f(x) . 则这民虫线上的酋的物理量是多少？进 推广到空间中的某线上・ 推广 3、这种类型的问题不只局限于物理量，元论 自然科学还是社会科学，只要给出某变量的分布 “密度” , 比如人口|问题中的人口出生密度、交 通问题中的车流密度，都能用上述方法求总量・ $ \\sqrt{2} \\cap * \\frac{1}{7} $ xIAMEN UNVERSIT 3.1.3 养例分析 思考题： (一)自然科学： 设有一质量连续分布的物体G，它在任意一点(x,y,z)的密度为 $ p(x, y, z) $ 求此物体的重心。 求转动惯量。 求物体对它外部质点的引力。 二体引力问题与引力场的位势。 求旋牡曲面的面积-设平面上的光滑曲线y $ (x) \\geq 0(x \\in[a, b]), $ 求它绕X轴旋 转一周所得到的旋转曲面的面积。 3.1.3 案例分析 案例二：通讯卫星的电波覆盖的地球面积 问题背景：将通讯卫星发射到赤道的上空 使它位于赤道所在的平面内。如果卫星自西 向东地绕地球飞行一周的时间正好等于地球 自转一周的时间，那么它始终在地球的某一 个位置的上空，即相对静止.这样的卫里称为 地球同步卫星。试计算该卫星的电波所能覆 盖的地球的表面积. Frots 3.1.3 案例分析 解：1、问题的假设： 把地球看成一个球体，且不考虑其它 天体对卫星的影响。设地球的半径 R=6371km, 地 球 自 转 的 角 速 度 $ \\omega=2 \\pi /(24 \\times 3600), $ 设卫里离地面的 高度h。 3.1.3 案例分析 2、问题的分析：由于卫星绕地球飞行 一周的时间，正好等于地球自转一周 的时间，因此\\omega也就是卫里绕地球飞行 的角速度。要使卫星不会脱离其预定 轨道，卫星所受的地球的引力必须与 它绕地球飞行所受的离心力相等，即 $$ \\frac{g M m}{(R+h)^{2}}=m \\omega^{2}(R+h) $$ Fertit 3.1.3 案例分析 其中M为地球的质量，m为卫星的质 量，G是引力常数。由于重力加速度 （即 在 地 面 的 单 位 质 量 所 受 的引 ' $ g=\\frac{G M}{R^{2}} $ 。那么从上式得 $$ (R+h)^{3}=\\frac{G M}{\\omega^{2}} \\frac{G M}{R^{2}} \\frac{R^{2}}{\\omega^{2}}=g \\frac{R^{2}}{\\omega^{2}} $$ 于是 $$ h=\\sqrt[3]{g \\frac{R^{2}}{\\omega^{2}}}-R $$ 3.1.3 至例分析 将 $ \\mathrm{R}=6371000, \\omega=\\frac{2 \\pi}{24 \\times 3600}, \\mathrm{g}=9.8 / \\mathrm{t} $ 入上式，就得到卫星的离地面的高度 为 $$ h=\\sqrt[3]{9.8 \\times \\frac{6371000^{2} \\times 24^{2} \\times 3600^{2}}{4 \\pi^{2}}}-6371000=36000000(m)=36000(k m) $$ 隐个木计 3.1.3 案例分析 3、模型的构建：为计算卫星的电波所 覆盖的地球表面的面积，取地心为坐 标原点。取过地心与卫星中心、方向 从地心到卫星中心的有向直线为z轴(见 图），为简明起见，只画出了xz平面） 则卫星的电波所覆盖的地球表面的面 $ \\begin{array}{ll}\\text { 积为 } \u0026 s=\\iint_{\\mathrm{x}} d S\\end{array} $ 3.1.3 案例分析 其中\\Sigma是上半球面 $ x^{2}+y^{2}+z^{2}=R^{2}(z \\geq 0) \\quad $ 上 足z\\geq Rcos\\alpha的部分，即 利用第一类曲面积分的计算公式 $ s=\\iint_{0} \\sqrt{\\left.1+\\frac{\\partial z}{\\partial x}\\right)^{2}+\\left(\\frac{\\partial z}{\\partial y}\\right)^{2} d x d y}=\\iint \\frac{R}{\\sqrt{R^{2}-x^{2}}} $ 这里D为xy平面上区域 $ \\left.\\quad((x, y)] x^{2}+y^{2} \\leq R^{2} \\sin ^{2} \\alpha\\right} $ 层个ト字 3.1.3 案例分析 4、模型的求解：利用极坐标变换，得 $ S=\\int_{0}^{2 \\pi} d \\theta \\int_{0}^{R_{0} \\operatorname{man}} \\frac{R}{\\sqrt{R^{2}-r^{r}} r d r=2 \\pi R^{2}(1-\\cos \\alpha)} $ 因为 $ \\cos \\alpha=\\frac{n}{n+n}, $ 所以 $ S=2 \\pi R^{2} \\frac{h}{R+h}=2 \\pi \\times 6371000^{2} \\times \\frac{36000000}{6371000+36000000}=2.16575 \\times 10^{8}\\left(k m^{2}\\right) $ 3.1.3 案例分析 由于 $ s=2 \\pi R^{2} \\frac{h}{R+h}=4 \\pi R^{2} \\frac{h}{2(R+h)} \\quad $ 且 $ 4 \\pi R^{2} $ 正是地球的 表面积，而 $$ \\frac{h}{2(R+h)}=\\frac{36000000}{2(6371000+36000000)}=0.4248 $$ 即卫星的电波搜盖了地球表面1/3以上的面 积。 $ \\sqrt{2} \\cap+\\frac{1}{3} $ 3.1.3 养例分析 思考题： 求粗圆周x=acost $ , y= $ bsint $ , 0 \\leq t \\leq 2 \\pi $ 的质量 已知曲线在点M $ (\\mathrm{x}, \\mathrm{y}) $ 处的线密度是 $ \\rho(\\mathrm{x}, \\mathrm{y})=|\\mathrm{y}| $ 求密度为 $ p(\\mathrm{x}, \\mathrm{y}) $ ) = z的披物面壳 $ z=\\frac{1}{2}\\left(x^{2}+y^{2}\\right), 0 \\leq z \\leq 1 $ 质量与重心. 求均匀球面(半径是a，密度是1)对不在该球面上的质 点(质量为1)的引力. 3.1.3 案例分析 思考题： 4. 方向依纵轴的负方向, 且大小等于作用点的横坐标的平方的カ 构成一个力场. 求质量为m的质点沿抛物线 $ y^{2}=1-x / $ 从点(1,0)移 到(0,1)时, 场力所作的功. 5. 设有一高度为h(t)(t为时间)的雪堆在言化过程中，其侧面满足 方程(设长度单位为cm，时间单位为h) $ \\quad z=h(t)-\\frac{2\\left(x^{2}+y^{2}\\right)}{h(t)} $ 已知体积燈少的連率与侧面积成正比(比例系锋为0.9)。间骨度为 130cm的雪堆全部融化需多少时间？? Fer $ A \\neq 7 $ 3.1.3 案例分析 案例三 : 消费者剩余与生产者剩余 问题背景：消费者剩余是经济学中的重要概念，是 指消费者对某种商品所愿意付出的代价超过他实际 付出的代价的金额，即：消费者剩余可用来衡量消 费者所得到的额外满足 . 用公式表示为 : 消费者剩余 = 愿意付出的金额 -实际付出的金额 3.1.3 养例分析 案例三：消费者剩余与生产者剩余 问题背景：消费者剩余是经济学中的重要概念，是 指消费者对某种商品所愿意付出的代价超过他实际 付出的代价的金疾，即：消费者剩余可用来衡量消 费者所得到的额外满足 . 用公式表示为 : 消费者剩余 = 愿意付出的金额 -实际付出的金额 $ (8,7)+3 $ XIAMEN UNIVERS 3.1.3 养例分析 (模型构建] 假定消费者愿意为某种商品所付出的价格 p 是由其需求曲 线 $ \\quad p=D(q) $. (其中 q 为需求量)决定的，它是需求量的减函数 如下图所示： 3.1.3 案例分析 这表明,","date":"0001-01-01","objectID":"/3.-%E5%88%9D%E7%AD%89%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/:1:3","tags":null,"title":"","uri":"/3.-%E5%88%9D%E7%AD%89%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"1. Motivation ","date":"0001-01-01","objectID":"/1.-motivation/:0:0","tags":null,"title":"","uri":"/1.-motivation/"},{"categories":null,"content":"1.1 Nonparametric Estimation Suppose $ \\left{X_{t}\\right} $ is a strictly stationary process with marginal probability density function $ g(x) $ and pairwise joint probability density function $ f_{j}(x, y) $ of $ \\left(X_{t}, X_{t-j}\\right) $, and a random sample $ \\left{X_{t}\\right}_{t=1}^{T} $ of size $ T $ is observed. Then, How to estimate the marginal pdf $ g(x) $ of $ \\left{X_{t}\\right} ? $ How to estimate the pairwise joint pdf $ f_{j}(x, y) $ of $ \\left(X_{t}, X_{t-j}\\right) ? $ How to estimate the autoregression function $ r_{j}(x)=E\\left(X_{t} | X_{t-j}=x\\right) ? $ How to estimate the spectral density (自回归函数的傅里叶变换) $ h(\\omega) $ of $ \\left{X_{t}\\right} ? $ How to estimate the generalized spectral density $ f(\\omega, u, v) $ of $ \\left{X_{t}\\right} ? $ How to estimate the bispectral density $ b\\left(\\omega_{1}, \\omega_{2}\\right) ? $ How to estimate a nonlinear autoregressive conditional heteroskedastic model $ X_{t}=\\mu\\left(X_{t-1}, \\ldots, X_{t-p}\\right)+\\sigma\\left(X_{t-1}, \\ldots, X_{t-q}\\right) \\varepsilon_{t}, \\quad\\left{\\varepsilon_{t}\\right} \\sim i . i . d .(0,1) $ where $ \\mu(\\cdot) $ and $ \\sigma(\\cdot) $ are unknown functions of the past information. Under certain regularity conditions, $ \\mu(\\cdot) $ is the conditional mean of $ X_{t} $ given $ I_{t-1}=\\left{X_{t-1}, X_{t-2}, \\ldots\\right} $ and $ \\sigma^{2}(\\cdot) $ is the conditional variance of $ X_{t} $ given $ I_{t-1} $ How to estimate a semi-nonparametric functional coefficient autoregressive process $ X_{t}=\\sum_{j=1}^{p} \\alpha_{j}\\left(X_{t-d}\\right) X_{t-j}+\\epsilon_{t}, \\quad E\\left(\\varepsilon_{t} | I_{t-1}\\right)=0 $ where $ \\alpha_{j}(\\cdot) $ is unknown, and $ d\u003e0 $ is a time lag parameter? How to estimate a nonparametric additive autoregressive process $$ X_{t}=\\sum_{j=1}^{p} \\mu_{j}\\left(X_{t-j}\\right)+\\varepsilon_{t}, \\quad E\\left(\\varepsilon_{t} | I_{t-1}\\right)=0 \\text { a.s. } $$ where the $ \\mu_{j}(\\cdot) $ functions are unknown? How to estimate a locally linear time-varying regression model $$ Y_{t}=X_{t}^{\\prime} \\beta(t / T)+\\varepsilon_{t} $$ where $ \\beta(\\cdot) $ is an unknown smooth deterministic function of time? How to use these estimators in economic and financial applications? Nonparametric estimation is often called nonparametric smoothing, since a key parameter called smoothing parameter is used to control the degree of the estimated curve. Nonparametric smoothing first arose from spectral density estimation in time series analysis. In a discussion of the seminal paper by Bartlett (1946), Henry Daniels suggested that a possible improvement on spectral density estimation could be made by smoothing the periodogram (see Chapter 3 ), which is the squared discrete Fourier transform of the random sample $ \\left{X_{t}\\right}_{t=1}^{T} $. The theory and techniques were then systematically developed by Bartlett (1948,1950). Thus, smoothing techniques were already prominently featured in time series analysis more than 70 years ago. In the earlier stage of nonlinear time series analysis (see Tong (1990)), the focus was on various nonlinear parametric forms, such as threshold autoregressive models, smooth transition autoregressive models, and Regime-switch Markov chain autoregressive models (see Chapter 8 for details). Recent interest has been mainly in nonparametric curve estimation, which does not require the knowledge of the functional form beyond certain smoothness conditions on the underlying function of interest. Question: Why is nonparametric smoothing popular in statistics and econometrics? There are several reasons for the popularity of nonparametric analysis. In particular, three main reasons are: Demands for nonlinear approaches; Availability of large data sets; Advance in computer technology. Indeed, as Granger (1999) points out, the speed in computing technology increases much faster than the speed at which data grows. ","date":"0001-01-01","objectID":"/1.-motivation/:1:0","tags":null,"title":"","uri":"/1.-motivation/"},{"categories":null,"content":"1.2 Basic Ideas of Nonparametric Smoothing To obtain basic ideas about nonparametric smoothing methods, we now consider two examples, one is the estimation of a regression function, and the other is the estimation of a probability density function. Example 1 [Regression Function]: Consider the first order autoregression function $$ r_{1}(x)=E\\left(X_{t} | X_{t-1}=x\\right) $$ We can write $$ X_{t}=r_{1}\\left(X_{t-1}\\right)+\\varepsilon_{t} $$ where $ E\\left(\\epsilon_{t} | X_{t-1}\\right)=0 $ by construction. We assume $ E\\left(X_{t}^{2}\\right)\u003c\\infty $ Suppose a sequence of bases $ \\left{\\psi_{j}(x)\\right} $ constitutes a complete orthonormal basis for the space of square-integrable functions. Then we can always decompose the function $$ r_{1}(x)=\\sum_{j=0}^{\\infty} \\alpha_{j} \\psi_{j}(x) $$ where the Fourier coefficient $$ \\alpha_{j}=\\int_{-\\infty}^{\\infty} r_{1}(x) \\psi_{j}(x) d x $$ which is the projection of $ r_{1}(x) $ on the base $ \\psi_{j}(x) $. Suppose there is a quadratic function $ r_{1}(x)=x^{2} $ for $ x \\in[-\\pi, \\pi] . $ Then $$ \\begin{aligned} r_{1}(x) \u0026=\\frac{\\pi^{2}}{3}-4\\left(\\cos (x)-\\frac{\\cos (2 x)}{2^{2}}+\\frac{\\cos (3 x)}{3^{2}}-\\cdots\\right) \\ \u0026=\\frac{\\pi^{2}}{3}-4 \\sum_{j=1}^{\\infty}(-1)^{j-1} \\frac{\\cos (j x)}{j^{2}} \\end{aligned} $$ For another example, suppose the regression function is a step function, namely $$ r_{1}(x)=\\left{\\begin{array}{cl} -1 \u0026 \\text { if }-\\pi\u003cx\u003c0 \\ 0 \u0026 \\text { if } x=0 \\ 1 \u0026 \\text { if } 0\u003cx\u003c\\pi \\end{array}\\right. $$ Then we can still expand it as an infinite sum of periodic series, $$ \\begin{aligned} r_{1}(x) \u0026=\\frac{4}{\\pi}\\left[\\sin (x)+\\frac{\\sin (3 x)}{3}+\\frac{\\sin (5 x)}{5}+\\cdots\\right] \\ \u0026=\\frac{4}{\\pi} \\sum_{j=0}^{\\infty} \\frac{\\sin [(2 j+1) x]}{(2 j+1)} \\end{aligned} $$ In general, we do not assume that the function form of $ r_{1}(x) $ is known, except that we still maintain the assumption that $ r_{1}(x) $ is a square-integrable function. Because $ r_{1}(x) $ is square-integrable, we have $$ \\begin{aligned} \\int_{-\\infty}^{\\infty} r_{1}^{2}(x) d x \u0026=\\sum_{j=0}^{\\infty} \\sum_{k=0}^{\\infty} \\alpha_{j} \\alpha_{k} \\int_{-\\infty}^{\\infty} \\psi_{j}(x) \\psi_{k}(x) d x \\ \u0026=\\sum_{j=0}^{\\infty} \\sum_{k=0}^{\\infty} \\alpha_{j} \\alpha_{k} \\delta_{j, k} \\text { by orthonormality } \\ \u0026=\\sum_{j=0}^{\\infty} \\alpha_{j}^{2}\u003c\\infty \\end{aligned} $$ where $ \\delta_{j, k} $ is the Kronecker delta function: $ \\delta_{j, k}=1 $ if $ j=k $ and 0 otherwise. The squares summability implies $ \\alpha_{j} \\rightarrow 0 $ as $ j \\rightarrow \\infty, $ that is, $ \\alpha_{j} $ becomes less important as the order $ j \\rightarrow \\infty . $ This suggests that a truncated sum $$ r_{1 p}(x)=\\sum_{j=0}^{p} \\alpha_{j} \\psi_{j}(x) $$ can be used to approximate $ r_{1}(x) $ arbitrarily well if $ p $ is sufficiently large. The approximation error, or the bias, $$ \\begin{aligned} b_{p}(x) \u0026 \\equiv r_{1}(x)-r_{1 p}(x) \\ \u0026=\\sum_{j=p+1}^{\\infty} \\alpha_{j} \\psi_{j}(x) \\ \u0026 \\rightarrow 0 \\end{aligned} $$ However, the coefficient $ \\alpha_{j} $ is unknown. To obtain a feasible estimator for $ r_{1}(x), $ we consider the following sequence of truncated regression models $$ X_{t}=\\sum_{j=0}^{p} \\beta_{j} \\psi_{j}\\left(X_{t-1}\\right)+\\varepsilon_{p t} $$ where $ p \\equiv p(T) \\rightarrow \\infty $ is the number of series terms that depends on the sample size $ T $. We need $ p / T \\rightarrow 0 $ as $ T \\rightarrow \\infty, $ i.e., the number of $ p $ is much smaller than the sample size $ T . $ Note that the regression error $ \\varepsilon_{p t} $ is not the same as the true innovation $ \\varepsilon_{t} $ for each given $ p $. Instead, it contains the true innovation $ \\varepsilon_{t} $ and the bias $ b_{p}\\left(X_{t-1}\\right) $. The ordinary least squares estimator $$ \\begin{aligned} \\hat{\\beta} \u0026=\\left(\\Psi^{\\prime} \\Psi\\right)^{-1} \\Psi^{\\prime} X \\ \u0026=\\left(\\sum_{t=2}^{T} \\psi_{t} \\psi_{t}^{\\prime}\\right)^{-1} \\sum_{t=2}^{T} \\psi_{t} X_{t} \\end{aligned} $$ where $$ \\Psi=\\left(\\psi_{1}^{\\prime}, \\ldots, \\psi_{T}^{\\p","date":"0001-01-01","objectID":"/1.-motivation/:2:0","tags":null,"title":"","uri":"/1.-motivation/"},{"categories":null,"content":"1.3 Advantages \u0026 Disadvantages of nonparametric smoothing Question: What are the advantages of nonparametric smoothing methods? They require few assumptions or restrictions on the data generating process. In particular, they do not assume a specific functional form for the function of interest (of course certain smoothness condition such as differentiability is required). They can deliver a consistent estimator for the unknown function, no matter whether it is linear or nonlinear. Thus, nonparametric methods can effectively reduce potential systematic biases due to model misspecification, which is more likely to be encountered for parametric modeling. Question: What are the disadvantages of nonparametric methods? Nonparametric methods require a large data set for reasonable estimation. Furthermore, there exists a notorious problem of “curse of dimensionality”, when the function of interest contains multiple explanatory variables. This will be explained below. There exists another notorious “boundary effect” problem for nonparametric estimation near the boundary regions of the support. This occurs due to asymmetric coverage of data in the boundary regions. Coefficients are usually difficult to interpret from an economic point of view. There exists a danger of potential overfitting, in the sense that nonparametric method, due to its flexibility, tends to capture non-essential features in a data which will not appear in out-of-sample scenarios. The above two motivating examples are the so-called orthogonal series expansion methods. There are other nonparametric methods, such as splines smoothing, kernel smoothing, $ k $ -near neighbor, and local polynomial smoothing. As mentioned earlier, series expansion methods are examples of so-called global smoothing, because the coefficients are estimated using all observations, and they are then used to evaluate the values of the underlying function over all points in the support of $ X_{t} . $ A nonparametric series model is an increasing sequence of parametric models, as the sample size $ T $ grows. In this sense, it is also called a sieve estimator. In contrast, kernel and local polynomial methods are examples of the so-called local smoothing methods, because estimation only requires the observations in a neighborhood of the point of interest. Below we will mainly focus on kernel and local polynomial smoothing methods, due to their simplicity and intuitive nature. ","date":"0001-01-01","objectID":"/1.-motivation/:3:0","tags":null,"title":"","uri":"/1.-motivation/"},{"categories":null,"content":"2. Kernel Density Method ","date":"0001-01-01","objectID":"/2.-kernel-density-method/:0:0","tags":null,"title":"","uri":"/2.-kernel-density-method/"},{"categories":null,"content":"2.1 Univariate Density Estimation ","date":"0001-01-01","objectID":"/2.-kernel-density-method/:1:0","tags":null,"title":"","uri":"/2.-kernel-density-method/"},{"categories":null,"content":"2.1.0 Parametric Approach Suppose $ \\left{X_{t}\\right} $ is a strictly stationary time series process with unknown marginal PDF $ g(x) $. Question: How to estimate the marginal PDF $ g(x) $ of the time series process $ \\left{X_{t}\\right} ? $ We first consider a parametric approach. Assume that $ g(x) $ is an $ N\\left(\\mu, \\sigma^{2}\\right) $ PDF with unknown $ \\mu $ and $ \\sigma^{2} $ ? Then we know the functional form of $ g(x) $ up to two unknown parameters $ \\theta=\\left(\\mu, \\sigma^{2}\\right)^{\\prime} $ $$ g(x, \\theta)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left[-\\frac{1}{2 \\sigma^{2}}(x-\\mu)^{2}\\right], \\quad-\\infty\u003cx\u003c\\infty $$ To estimate $ g(x, \\theta), $ it suffices to estimate two unknown parameters $ \\mu $ and $ \\sigma^{2} $. Based on the random sample $ \\left{X_{t}\\right}_{t=1}^{T}, $ we can obtain the maximum likelihood estimators (MLE), $$ \\begin{aligned} \\hat{\\mu} \u0026=\\frac{1}{T} \\sum_{t=1}^{T} X_{t} \\ \\hat{\\sigma}^{2} \u0026=\\frac{1}{T} \\sum_{t=1}^{T}\\left(X_{t}-\\hat{\\mu}\\right)^{2} \\end{aligned} $$ The approach taken here is called a parametric approach, that is, assuming that the unknown PDF is a known functional form up to some unknown parameters. It can be shown that the parameter estimator $ \\theta $ converges to the unknown parameter value $ \\theta_{0} $ at a root-$ T $ **convergence rate** in the sense that $ \\sqrt{T}\\left(\\hat{\\theta}-\\theta_{0}\\right)=O_{P}(1), $ or $ \\hat{\\theta}-\\theta_{0}=O_{P}\\left(T^{-1 / 2}\\right) $ where $ \\hat{\\theta}=\\left(\\hat{\\mu}, \\hat{\\sigma}^{2}\\right)^{\\prime}, \\theta_{0}=\\left(\\mu_{0}, \\sigma_{0}^{2}\\right)^{\\prime}, $ and $ O_{P}(1) $ denotes boundedness in probability. The root-$ T $ convergence rate is called the parametric convergence rate for $ \\hat{\\theta} $ and $ g(x, \\hat{\\theta}) $. As we will see below, nonparametric density estimators will have a slower convergence rate. Question: What is the definition of $ O_{P}\\left(\\delta_{T}\\right) ? $ Let $ \\left{\\delta_{T}, T \\geq 1\\right} $ be a sequence of positive numbers. A random variable $ Y_{T} $ is said to be at most of order $ \\delta_{T} $ in probability, written $ Y_{T}=O_{P}\\left(\\delta_{T}\\right), $ if the sequence $ \\left{Y_{T} / \\delta_{T}, T \\geq 1\\right} $ is tight, that is, if $$ \\lim {\\lambda \\rightarrow \\infty} \\lim {T \\rightarrow \\infty} \\sup P\\left(\\left|Y{T} / \\delta{T}\\right|\u003e\\lambda\\right)=0 $$ Tightness is usually indicated by writing $ Y_{T} / \\delta_{T}=O_{P}(1) $ Question: What is the advantage of the parametric approach? By the mean-value theorem, we obtain $$ \\begin{aligned} g(x, \\hat{\\theta})-g(x) \u0026=g\\left(x, \\theta_{0}\\right)-g(x)+\\frac{\\partial}{\\partial \\theta} g(x, \\bar{\\theta})\\left(\\hat{\\theta}-\\theta_{0}\\right) \\ \u0026=0+\\frac{1}{\\sqrt{T}} \\frac{\\partial}{\\partial \\theta} g(x, \\bar{\\theta}) \\sqrt{T}\\left(\\hat{\\theta}-\\theta_{0}\\right) \\ \u0026=0+O_{P}\\left(T^{-1 / 2}\\right) \\ \u0026=O_{P}\\left(T^{-1 / 2}\\right) \\end{aligned} $$ Intuitively, the first term, $ g\\left(x, \\theta_{0}\\right)-g(x), $ is the bias of the density estimator $ g(x, \\hat{\\theta}) $ which is zero if the assumption of correct model specification holds. The second term, $ \\frac{\\partial}{\\partial \\theta} g(x, \\bar{\\theta})\\left(\\hat{\\theta}-\\theta_{0}\\right), $ is due to the sampling error of the estimator $ \\hat{\\theta}, $ which is unavoidable no matter whether the density estimator $ g(x, \\theta) $ is correctly specified. This term converges to zero in probability at the parametric root-$ T $ rate. Question: What happens if the correct model specification assumption fails? That is, what happens if $ g(x, \\theta) \\neq g(x) $ for all $ \\theta $? When the density model $ g(x, \\theta) $ is not correctly specified for the unknown PDF $ g(x) $, the estimator $ g(x, \\hat{\\theta}) $ will not be consistent for $ g(x) $ because the bias $ g\\left(x, \\theta^{}\\right)-g(x) $ never vanishes no matter how large the sample size $ T $ is, where $ \\theta^{}=p \\lim \\hat{\\theta} $. We now introduce a nonparametric estimation method for $ g(x) $ which w","date":"0001-01-01","objectID":"/2.-kernel-density-method/:1:1","tags":null,"title":"","uri":"/2.-kernel-density-method/"},{"categories":null,"content":"2.1.1 Kernel Density Estimator Kernel smoothing is a kind of local smoothing. The purpose of nonparametric probability density estimation is to construct an estimate of a PDF without imposing restrictive functional form assumptions. Typically the only condition imposed on the unknown PDF is that it has at least first two order bounded derivatives. In this circumstance, we may use only local information about the value of the PDF at any given point in the support. That is, the value of the PDF of a point $ x $ must be calculated from data values that lie in a neighborhood of $ x, $ and to ensure consistency the neighborhood must shrink to zero as the sample size $ T $ increases. In the case of kernel density estimation, the radius of the effective neighborhood is roughly equal to the so-called “bandwidth” of a kernel density estimator, which is essentially a smoothing parameter. Under the assumption that the PDF is univariate with at least first two order bounded derivatives, and using a nonnegative kernel function, the size of bandwidth that optimizes the performance of the estimator in term of the mean squared error (MSE) criterion is proportional to the rate $ T^{-1 / 5} $. The number of “parameters” needed to model the unknown PDF within a given interval is approximately equal to the number of bandwidths that can be fitted into that interval, and so is roughly of size $ T^{1 / 5} $. Thus, nonparametric density estimation involves the adaptive fitting of approximately $ T^{1 / 5} $ parameters, with this number growing with the sample size $ T $. Suppose we are interested in estimating the value of the PDF $ g(x) $ at a given point $ x $ in the support of $ X_{t} $. There are **two basic instruments in kernel estimation**: the kernel function $ K(\\cdot) $ and the bandwidth $ h . $ Intuitively, the former gives weighting to the observations in an interval containing the point $ x $, and the latter controls the size of the interval containing observations. We first introduce an important instrument for local smoothing. This is called a kernel function. Definition [Second Order Kernel $ K(\\cdot) $]: A second order or positive kernel function $ K(\\cdot) $ is a pre-specified symmetric PDF such that non-negative (PDF) $ \\int_{-\\infty}^{\\infty} K(u) d u=1 $ $ \\int_{-\\infty}^{\\infty} K(u) u d u=0 $ $ \\int_{-\\infty}^{\\infty} u^{2} K(u) d u=C_{K}\u003c\\infty $ $ \\int_{-\\infty}^{\\infty} K^{2}(u) d u=D_{K}\u003c\\infty $ Intuitively, the kernel function $ K(\\cdot) $ is a weighting function that will “discount” the observations whose values are more away from the point $ x $ of interest. The kernel functions satisfying the above condition are called a second order or positive kernel. It should be emphasized that the kernel $ K(\\cdot) $ has nothing to do with the unknown PDF $ g(x) $ of $ \\left{X_{t}\\right} ; $ it is just a weighting function for observations when constructing a kernel density estimator. More generally, we can define a $ q $-th order kernel $ K(\\cdot), $ where $ q \\geq 2 $. Definition [$q$th Order Kernel]: $ K(\\cdot) $ satisfies the conditions that $ \\int_{-\\infty}^{\\infty} K(u) d u=1 $ $ \\int_{-\\infty}^{\\infty} u^{j} K(u) d u=0 $ for $ 1 \\leq j \\leq q-1 $ $ \\int_{-\\infty}^{\\infty} u^{q} K(u) d u\u003c\\infty $ $ \\int_{-\\infty}^{\\infty} K^{2}(u) d u\u003c\\infty $ For a higher order kernel (i.e., $ q\u003e2 $ ), $ K(\\cdot) $ will take some negative values at some points. Question: Why is a higher order kernel useful? Can you give an example of a third order kernel? And an example of a fourth order kernel? Higher order kernels can reduce the bias of a kernel estimator to a higher order. An example of higher order kernels is given in Robinson (1991). We now consider some examples of second order kernels: Uniform kernel $$ K(u)=\\frac{1}{2} 1(|u| \\leq 1) $$ Gaussian kernel $$ K(u)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{1}{2} u^{2}\\right) ; \\quad-\\infty\u003cu\u003c\\infty $$ Epanechnikov Kernel $$ K(u)=\\frac{3}{4}\\left(1-u^{2}\\right) 1(|u| \\leq 1) $$ Quartic kern","date":"0001-01-01","objectID":"/2.-kernel-density-method/:1:2","tags":null,"title":"","uri":"/2.-kernel-density-method/"},{"categories":null,"content":"2.1.2 Asymptotic Bias and Boundary Effect Our purpose is to show that $ \\hat{g}(x) $ is a consistent estimator for $ g(x) $ for a given point $ x $ in the support. Now we decompose $$ \\hat{g}(x)-g(x)=[E \\hat{g}(x)-g(x)]+[\\hat{g}(x)-E \\hat{g}(x)] $$ It follows that the mean squared error of the kernel density estimator $ \\hat{g}(x) $ is given by $$ \\begin{aligned} \\operatorname{MSE}(\\hat{g}(x)) \u0026=[E \\hat{g}(x)-g(x)]^{2}+E[\\hat{g}(x)-E \\hat{g}(x)]^{2} \\ \u0026=\\operatorname{Bias}^{2}[\\hat{g}(x)]+\\operatorname{var}[\\hat{g}(x)] \\end{aligned} $$ The first term is the squared bias of the estimator $ \\hat{g}(x), $ which is nonstochastic, and the second term is the variance of $ \\hat{g}(x) $ at the point $ x $. We shall show that under suitable regularity conditions, both the bias and the variance of $ \\hat{g}(x) $ vanish to zero as the sample size $ T $ goes to infinity. We first consider the bias. For any given point $ x $ in the interior region $ [a+h, b-h] $ of the support $ [a, b] $ of $ X_{t}, $ we have $$ \\begin{aligned} E[\\hat{g}(x)]-g(x) =\u0026\\frac{1}{T} \\sum_{t=1}^{T} E K_{h}\\left(x-X_{t}\\right)-g(x) \\ =\u0026E\\left[K_{h}\\left(x-X_{t}\\right)\\right]-g(x)\\text { (by identical distribution)} \\ =\u0026\\int_{a}^{b} \\frac{1}{h} K\\left(\\frac{x-y}{h}\\right) g(y) d y-g(x)\\ =\u0026\\int_{(a-x) / h}^{(b-x) / h} K(u) g(x+h u) d u-g(x) \\text{ (by change of variable $ \\frac{y-x}{h}=u $)}\\ =\u0026 \\int_{-1}^{1} K(u) g(x+h u) d u-g(x) \\ =\u0026 g(x) \\int_{-1}^{1} K(u) d u-g(x) \\ \u0026+h g^{\\prime}(x) \\int_{-1}^{1} u K(u) d u \\ \u0026+\\frac{1}{2} h^{2} \\int_{-1}^{1} u^{2} K(u) g^{\\prime \\prime}(x+\\lambda h u) d u \\text{ (by Taylor expansion)}\\ =\u0026 \\frac{1}{2} h^{2} C_{K} g^{\\prime \\prime}(x)+\\frac{1}{2} h^{2} \\int_{-1}^{1}\\left[g^{\\prime \\prime}(x+\\lambda h u)-g^{\\prime \\prime}(x)\\right] u^{2} K(u) d u \\ =\u0026 \\frac{1}{2} h^{2} C_{K} g^{\\prime \\prime}(x)+o\\left(h^{2}\\right) \\end{aligned} $$ where the second term $$ \\int_{-1}^{1}\\left[g^{\\prime \\prime}(x+\\lambda h u)-g^{\\prime \\prime}(x)\\right] u^{2} K(u) d u \\rightarrow 0 $$ as $ h \\rightarrow 0 $ by Lebesgue’s dominated convergence theorem, and the boundedness and continuity of $ g^{\\prime \\prime}(\\cdot) $ and $ \\int_{-1}^{1} u^{2} K(u) d u\u003c\\infty $. Therefore, for the point $ x $ in the interior region $ [a+h, b-h], $ the bias of $ \\hat{g}(x) $ is proportional to $ h^{2} . $ Thus, we must let $ h \\rightarrow 0 $ as $ T \\rightarrow \\infty $ in order to have the bias vanish to zero as $ T \\rightarrow \\infty $. The above result for the bias is obtained under the identical distribution assumption on $ \\left{X_{t}\\right} . $ It is irrelevant to whether $ \\left{X_{t}\\right} $ is IID or serially dependent. In other words, **it is robust to serial dependence** in $ \\left{X_{t}\\right} $. Question: What happens to the bias of $ \\hat{g}(x) $ if $ x $ is outside the interior region $ [a+h, b-h] $? We say that $ x $ is outside the interior region $ [a+h, b-h] $ if $ x $ in $ [a, a+h] $ or $ [b-h, b] $ These two regions are called boundary regions of the support. Their sizes are equal to $ h $ and so vanish to zero as the sample size $ T $ increases. Suppose $ x=a+\\lambda h \\in[a, a+h) $, where $ \\lambda \\in[0,1) $. We shall call $ x $ is a point in the left boundary region of the support $ [a, b] $. Then $$ \\begin{aligned} E[\\hat{g}(x)]-g(x) \u0026=E\\left[K_{h}\\left(x-X_{t}\\right)\\right]-g(x) \\ \u0026=\\frac{1}{h} \\int_{a}^{b} K\\left(\\frac{x-y}{h}\\right) g(y) d y-g(x) \\ \u0026=\\int_{(a-x) / h}^{(b-x) / h} K(u) g(x+h u) d u-g(x) \\ \u0026=\\int_{-\\lambda}^{1} K(u) g(x+h u) d u-g(x) \\ \u0026=g(x) \\int_{-\\lambda}^{1} K(u) d u-g(x) \\ \u0026+h \\int_{-\\lambda}^{1} u K(u) g^{\\prime}(x+\\tau h u) d u \\ \u0026=g(x)\\left[\\int_{-\\lambda}^{1} K(u) d x-1\\right]+O(h) \\ \u0026=O(1) \\end{aligned} $$ if $ g(x) $ is bounded away from zero, that is, if $ g(x) \\geq \\epsilon\u003e0 $ for all $ x \\in[a, b] $ for any small but fixed constant $ \\epsilon $. Note that the $ O(1) $ term arises since $ \\int_{-\\lambda}^{1} K(u) d x=1 $ for any $ \\lambda\u003c1 $. Thus, if $ x \\in[a, a+h) $ or $ (b-h, b], $ the bias $ E[\\hat{g}(x)","date":"0001-01-01","objectID":"/2.-kernel-density-method/:1:3","tags":null,"title":"","uri":"/2.-kernel-density-method/"},{"categories":null,"content":"2.1.3 Asymptotic Variance Question: We have dealt with the bias of $ \\hat{g}(x) $. Then, what is the variance of $ \\hat{g}(x) $? For the time being, in order to simplify the analysis, we assume an IID random sample. We will explain that the asymptotic result for the variance of $ \\hat{g}(x) $ remains true when $ \\left{X_{t}\\right} $ is not IID under certain regularity restrictions on temporal dependence in $ \\left{X_{t}\\right} $. Assumption A.3 [IID Observations]: The random sample $ \\left{X_{t}\\right}_{t=1}^{T} $ is IID. The IID assumption simplifies our calculating the asymptotic variance of $ \\hat{g}(x) $. Later, we can relax the independence assumption for $ \\left{X_{t}\\right} $ such that $ \\left{X_{t}\\right} $ is an $ \\alpha $-mixing process, a condition that allows weak temporal dependence (see Chapter 2). This will not change the asymptotic variance result for $ \\hat{g}(x) $. Given any point $ x $ in the support $ [a, b], $ put $$ Z_{t} \\equiv Z_{t}(x)=K_{h}\\left(x-X_{t}\\right)-E\\left[K_{h}\\left(x-X_{t}\\right)\\right] $$ Then $ \\left{Z_{t}\\right}_{t=1}^{T} $ is $ \\mathrm{IID} $ with mean zero. It follows that the variance of $ \\hat{g}(x) $, $$ \\begin{aligned} E[\\hat{g}(x)-E \\hat{g}(x)]^{2} \u0026=E\\left(T^{-1} \\sum_{t=1}^{T} Z_{t}\\right)^{2} \\ \u0026=\\frac{1}{T^{2}} \\sum_{t=1}^{T} \\operatorname{var}\\left(Z_{t}\\right) \\ \u0026=\\frac{1}{T} \\operatorname{var}\\left(Z_{t}\\right) \\ \u0026=\\frac{1}{T}\\left[E\\left[K_{h}^{2}\\left(x-X_{t}\\right)\\right]-\\left[E K_{h}\\left(x-X_{t}\\right)\\right]^{2}\\right] \\ \u0026=\\frac{1}{T h^{2}} \\int_{a}^{b} K^{2}\\left(\\frac{x-y}{h}\\right) g(y) d y \\ \u0026\\quad -\\frac{1}{T}\\left[\\frac{1}{h} \\int_{a}^{b} K\\left(\\frac{x-y}{h}\\right) g(y) d y\\right]^{2} \\ \u0026=\\frac{1}{T h} g(x) \\int_{-1}^{1} K^{2}(u) d u[1+o(1)]+O\\left(T^{-1}\\right) \\ \u0026=\\frac{1}{T h} g(x) D_{k}+o\\left(T^{-1} h^{-1}\\right) \\end{aligned} $$ where the last second equality follows by change of variable $ \\frac{x-y}{h}=u $ and Taylor expansion. The variance of $ \\hat{g}(x) $ is proportional to $ (T h)^{-1}, $ which is the approximate sample size for the observations which fall into the interval $ [x-h, x+h]$. Next, we discuss the impact of serial dependence in $ \\left{X_{t}\\right} $ on the asymptotic variance of $ \\hat{g}(x) $ Question: What happens to the variance of $ \\hat{g}(x) $ if $ \\left{X_{t}\\right} $ is **serially dependent**. Intuition (strictly proof below can be skipped): Hart (1996) provides a nice intuition for this result. Suppose the kernel $ K(\\cdot) $ has support on $ [-1,1], $ as assumed in this chapter. Then the kernel density estimator at the point $ x $ uses only the local data points inside the local interval $ [x-h, x+h] $. The observations whose values fall into this local interval are generally far away from each other in time. Thus, although the data $ \\left{X_{t}\\right}_{t=1}^{T} $ in the original sequence may be highly correlated, the dependence for the new subsequence in the local interval around $ x $ can be much weaker. As a result, the local data look like those from an independent sample. Hence, one would expect that the asymptotic variance of the kernel density estimator is the same as that for the independent observations when certain mixing conditions are imposed. \r Suppose $ \\left{X_{t}\\right} $ is a strictly stationary $ \\alpha $-mixing process. Then under suitable conditions on the $ \\alpha $-mixing coefficient $ \\alpha(j), $ for example, $ \\alpha(j) \\leq C j^{-\\beta} $ for $ \\beta\u003e\\frac{5}{2}, $ we have the same MSE formula for $ \\hat{g}(x) $ as we have when $ \\left{X_{t}\\right} $ is $ \\mathrm{IID} $. This is formally established in Robinson $ (1983) . $ See Robinson $ (1983, $ Journal of Time Series Analysis) for details. Intuitively, when a bounded support kernel $ K(u) $ is used, the kernel density estimator is an weighted average of nonlinear functions of observations $ \\left{X_{t}\\right}_{t=1}^{T} $ which fall into the small interval $ [x-h, x+h] . $ The observations that fall into the small interval are determined by the c","date":"0001-01-01","objectID":"/2.-kernel-density-method/:1:4","tags":null,"title":"","uri":"/2.-kernel-density-method/"},{"categories":null,"content":"2.1.4 MSE and Optimal Bandwidth It follows that the mean squared error (MSE) of $ \\hat{g}(x) $ is given by $$ \\begin{aligned} \\operatorname{MSE}[\\hat{g}(x)] \u0026=E[\\hat{g}(x)-g(x)]^{2} \\ \u0026=\\operatorname{var}[\\hat{g}(x)]+\\operatorname{Bias}^{2}[\\hat{g}(x), g(x)] \\ \u0026=\\frac{1}{T h} g(x) D_{K}+\\frac{1}{4} h^{4}\\left[g^{\\prime \\prime}(x)\\right]^{2} C_{K}^{2}+o\\left(T^{-1} h^{-1}+h^{4}\\right) \\ \u0026=O\\left(T^{-1} h^{-1}+h^{4}\\right) \\end{aligned} $$ By Chebyshev’s inequality, for any given point $ x $ in the interior region $ [a+h, b-h], $ we have $$ \\hat{g}(x)-g(x)=O_{P}\\left(T^{-1 / 2} h^{-1 / 2}+h^{2}\\right) $$ Therefore, for $ \\hat{g}(x) \\stackrel{p}{\\rightarrow} g(x), $ we need $ T h \\rightarrow \\infty, h \\rightarrow 0 $ as $ T \\rightarrow \\infty $. Under the stated assumptions, the estimator $ \\hat{g}(x) $ is always consistent for the unknown density $ g(x) $ but at a slower rate than the parametric $ T^{-1 / 2} $. This means that a large sample is needed to obtain a reasonable estimate for $ g(x) $. Moreover, the bias of $ \\hat{g}(x) $ depends on the smoothness of the unknown function $ g(\\cdot) $ In particular, if the second derivative $ g^{\\prime \\prime}(x) $ has a relatively sharp spike at the point $ x $ it is difficult to obtain a good estimate $ g(\\cdot) $ at the point $ x $. We can also obtain a r elative MSE criterion when $ g(x)\u003e0 $ $$ \\begin{aligned} \\operatorname{MSE}[\\hat{g}(x) / g(x)] \u0026=\\frac{M S E[\\hat{g}(x)]}{g^{2}(x)} \\ \u0026=E\\left[\\frac{\\hat{g}(x)-g(x)}{g(x)}\\right]^{2} \\ \u0026=\\frac{1}{T h g(x)} D_{K}+\\frac{1}{4} h^{4}\\left[\\frac{g^{\\prime \\prime}(x)}{g(x)}\\right]^{2} C_{K}^{2} \\ \u0026+o\\left(T^{-1} h^{-1}+h^{4}\\right) \\ \u0026=O\\left(T^{-1} h^{-1}+h^{4}\\right) \\end{aligned} $$ The expression of the relative MSE indicates that it is very difficult to obtain a reasonable estimate of $ g(x) $ in the sparse area where relatively few observations are available (i.e., when $ g(x) $ is small), or in the area where $ g(\\cdot) $ changes dramatically (i.e., when the curvature $ g^{\\prime \\prime}(x) / g(x) $ is large in absolute value). As can be seen from the MSE formula for $ \\hat{g}(x), $ a small bandwidth $ h $ will reduce the bias but inflate the variance, and a large bandwidth will increase the bias but reduce the variance. The bandwidth is a smoothing parameter. When the bandwidth $ h $ is so small such that the squared bias is smaller than the variance, we say that there exists undersmoothing; when the bandwidth is so large such that its squared bias is larger than the variance, we say that there exists oversmoothing. Optimal smoothing is achieved if the bandwidth balances the squared bias and the variance of $ \\hat{g}(x) $. We now consider the optimal choice of the bandwidth $ h . $ The optimal bandwidth can be obtained by minimizing $ M S E[\\hat{g}(x)] $: $$ h_{0}=\\left[\\frac{D_{K}}{C_{K}^{2}} \\frac{1 / g(x)}{\\left[g^{\\prime \\prime}(x) / g(x)\\right]^{2}}\\right]^{\\frac{1}{5}} T^{-1 / 5} $$ **The less smooth the PDF $ g(x) $ is or the more sparse the observations are around the point $ x, $ the smaller the optimal bandwidth $ h_{0} $ for any given sample size $ T $.** The optimal bandwidth $ h_{0} $ gives the optimal convergence rate for $ \\hat{g}(x) $: $$ \\hat{g}(x)-g(x)=O_{P}\\left(T^{-2 / 5}\\right) $$ The convergence rate $ T^{-2 / 5} $ is slower than the parametric rate $ T^{-1 / 2} $. The optimal bandwidth $ h_{0} $ is unknown, because it depends on the unknown density function $ g(x) $ and its second order derivative $ g^{\\prime \\prime}(x) $. Question: How to obtain a consistent estimator of this optimal bandwidth in practice? Since we have obtained a closed form expression for the optimal bandwidth $ h_{0}, $ we can obtain a consistent estimator of $ h_{0} $ by plugging in some preliminary consistent estimators for $ g(x) $ and $ g^{\\prime \\prime}(x) . $ Suppose we have some initial preliminary estimators, say $ \\tilde{g}(x) $ and $ \\tilde{g}^{\\prime \\prime}(x), $ for $ g(x) $ and $ g^{\\prime \\prime}(x) $ respectively. Then w","date":"0001-01-01","objectID":"/2.-kernel-density-method/:1:5","tags":null,"title":"","uri":"/2.-kernel-density-method/"},{"categories":null,"content":"2.2 Multivariate Density Estimation We now extend the kernel method to estimate a multivariate density function when $ X_{t} $ is a strictly stationary vector-valued time series process. Question: How to estimate a joint PDF $ f(x) $ of $ X_{t}=\\left(X_{1 t}, X_{2 t}, \\ldots, X_{d t}\\right)^{\\prime} $, where $ x= \\left(x_{1}, x_{2}, \\ldots, x_{d}\\right)^{\\prime} $ is a $ d \\times 1 $ vector? Example 1: How to estimate the joint $ \\operatorname{PDF} f_{j}(x, y) $ of $ \\left(X_{t}, X_{t-j}\\right) $? To estimate $ f(x), $ we define a product kernel density estimator $$ \\begin{aligned} \\hat{f}(x) \u0026=\\frac{1}{T} \\sum_{t=1}^{T} \\prod_{i=1}^{d} K_{h}\\left(x_{i}-X_{i t}\\right) \\ \u0026=\\frac{1}{T} \\sum_{t=1}^{T} \\mathcal{K}_{h}\\left(x-X_{t}\\right) \\end{aligned} $$ where $$ \\mathcal{K}{h}\\left(x-X{t}\\right)=\\prod_{i=1}^{d} K_{h}\\left(x_{i}-X_{i t}\\right) $$ For simplicity, we have used the same bandwidth $ h $ for every coordinate. Different bandwidths could be used for different coordinates. In practice, before using the same bandwidth $ h, $ one can standardize all $ \\left{X_{i t}\\right}_{t=1}^{T} $ by dividing by their sample standard deviations respectively for all $ i=1, \\ldots, d $. ","date":"0001-01-01","objectID":"/2.-kernel-density-method/:2:0","tags":null,"title":"","uri":"/2.-kernel-density-method/"},{"categories":null,"content":"2.2.1 Asymptotic Bias We first consider the bias of $ \\hat{f}(x) $. Suppose $ x $ is an interior point $ x $ such that $ x_{i} \\in \\left[a_{i}+h, b_{i}-h\\right] $ for all $ i=1, \\ldots, d $. This implies that $ x $ is in a **$ d $-dimensional box** each side of which is $ \\left[a_{i}+h, b_{i}-h\\right] $ for $ i=1, \\ldots, d $. It follows that the bias of $ \\hat{f}(x) $, $$ \\begin{aligned} E[\\hat{f}(x)]-f(x) \u0026=E \\mathcal{K}{h}\\left(x-X{t}\\right)-f(x) \\ \u0026=E \\prod_{i=1}^{d} K_{h}\\left(x_{i}-X_{i t}\\right)-f(x) \\ \u0026=\\int \\cdots \\int\\left[\\prod_{i=1}^{d} \\frac{1}{h} K\\left(\\frac{x_{i}-y_{i}}{h}\\right)\\right] f(y) d y-f(x) \\ \u0026=\\prod_{i=1}^{d} \\int_{\\left(a_{i}-x_{i}\\right) / h}^{\\left(b_{i}-x_{i}\\right) / h} K\\left(u_{i}\\right) f(x+h u) d u-f(x) \\ \u0026=\\int_{-1}^{1} \\cdots \\int_{-1}^{1} \\prod_{i=1}^{d} K\\left(u_{i}\\right) f(x+h u) d u-f(x) \\ \u0026=f(x) \\prod_{i=1}^{d} \\int_{-1}^{1} K\\left(u_{i}\\right) d u_{i}-f(x) \\ \u0026 +h \\sum_{i=1}^{d} f_{i}(x) \\int_{-1}^{1} u_i K\\left(u_{i}\\right) d u_{i} \\prod_{j \\neq i}^{d} \\int_{-1}^{1} K\\left(u_{j}\\right) d u_{j}\\ \u0026+\\frac{1}{2} h^{2} \\sum_{i=1}^{d} \\sum_{j=1}^{d} \\int_{-1}^{1} \\int_{-1}^{1} u_{i} u_{j} K\\left(u_{i}\\right) K\\left(u_{j}\\right) f_{i j}(x+\\lambda u h) d u_{i} d u_{j} \\prod_{k \\neq i,j}^{d} \\int_{-1}^{1} K\\left(u_{k}\\right) d u_{k}\\ \u0026=\\frac{1}{2} h^{2} C_{K} \\sum_{i=1}^{d} f_{i i}(x)+o\\left(h^{2}\\right) \\text{ (by Cross item=0)} \\ \u0026=O\\left(h^{2}\\right) \\end{aligned} $$ where $ u=(u_1,u_2,\\ldots, u_d), f_{i}(x)=\\frac{\\partial}{\\partial x_{i}} f(x), f_{i j}(x)=\\frac{\\partial^{2}}{\\partial x_{i} \\partial x_{j}} f(x), $ and the quantity $ \\sum_{i=1}^{d} f_{i i}(x) $ is called the Laplace of the joint PDF $ f(x) $. And d-dimension Taylor expenion $$ f(x+hu)=f(x)+\\sum_{i=1}^d hu_i\\frac{\\partial}{\\partial x_{i}} f(x) + \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d h^2 u_i u_j \\frac{\\partial^{2}}{\\partial x_{i} \\partial x_{j}} f(x+\\lambda hu) $$ And the fact that $$ \\begin{aligned} \\iint u^{2} K^{2}(u) d u d u \u0026=\\int \\int u^{2} K(x) d u d K^{*}(u) \\ \u0026=C_{k} \\int d K^{*}(u)=C_k \\end{aligned} $$ where $ k^*(u)=K(u) $. ","date":"0001-01-01","objectID":"/2.-kernel-density-method/:2:1","tags":null,"title":"","uri":"/2.-kernel-density-method/"},{"categories":null,"content":"2.2.2 Asymptotic Variance Next, put $$ \\begin{aligned} Z_{t} \u0026 \\equiv Z_{t}(x) \\ \u0026=\\mathcal{K}_{h}\\left(x-X_{t}\\right)-E \\mathcal{K}_{h}\\left(x-X_{t}\\right) \\ \u0026=\\prod_{i=1}^{d} K_{h}\\left(x_{i}-X_{i t}\\right)-E \\prod_{i=1}^{d} K_{h}\\left(x-X_{i t}\\right) \\end{aligned} $$ Then $ \\left{Z_{t}\\right} $ is IID with mean zero given that $ \\left{X_{t}\\right} $ is $ \\mathrm{IID} $. It follows that the variance of $ \\hat{f}(x) $ $$ \\begin{aligned} E[\\hat{f}(x)-E \\hat{f}(x)]^{2} \u0026=E\\left[T^{-1} \\sum_{t=1}^{T}\\left[\\mathcal{K}_{h}\\left(x-X_{t}\\right)-E \\mathcal{K}_{h}\\left(x-X_{t}\\right)\\right]\\right]^{2} \\ \u0026=\\frac{1}{T^{2}} \\sum_{t=1}^{T} E\\left(Z_{t}^{2}\\right)(\\text { by independence }) \\ \u0026=\\frac{1}{T} E\\left[\\prod_{i=1}^{d} K_{h}\\left(x_{i}-X_{i t}\\right)-E \\prod_{i=1}^{d} K_{h}\\left(x_{i}-X_{i t}\\right)\\right]^{2} \\ \u0026=\\frac{1}{T}\\left[E \\prod_{i=1}^{d} K_{h}^{2}\\left(x_{i}-X_{i t}\\right)-\\left[E \\prod_{i=1}^{d} K_{h}\\left(x_{i}-X_{i t}\\right)\\right]^{2}\\right] \\ \u0026=\\frac{1}{T h^{d}} f(x) D_{K}^{d}+o\\left(T^{-1} h^{-d}\\right) \\end{aligned} $$ We note that the asymptotic variance of $ \\hat{f}(x) $ is proportional to the inverse of $ T h^{d} $ where $ T h^{d} $ is approximately the effective sample size for observations falling into a $ d $-dimensional subspace centered at point $ x, $ with each size equal to $ 2 h $. The asymptotic variance of $ \\hat{f}(x) $ remains valid under a suitable $ \\alpha $-mixing condition for the time series $ \\left{X_{t}\\right} $. ","date":"0001-01-01","objectID":"/2.-kernel-density-method/:2:2","tags":null,"title":"","uri":"/2.-kernel-density-method/"},{"categories":null,"content":"2.2.3 MSE and Optimal Bandwidth It follows that the MSE of $ f(x) $ is $$ \\begin{aligned} M S E[\\hat{f}(x)] =\u0026 \\frac{1}{T h^{d}} f(x) D_{K}^{d}+\\frac{1}{4} C_{K}^{2} h^{4}\\left[\\sum_{i=1}^{d} f_{i i}(x)\\right]^{2} \\ \u0026+o\\left(T^{-1} h^{-d}+h^{4}\\right) \\ =\u0026 O\\left(T^{-1} h^{-d}+h^{4}\\right) \\end{aligned} $$ With a suitable choice of bandwidth $ h, $ the optimal MSE convergence rate of $ \\hat{f}(x) $ to $ f(x) $ is $ T^{-\\frac{4}{4+a}}, $ which can be obtained by setting the bandwidth $$ h_{0}=\\left[\\frac{d D_{K}^{2}}{C_{K}^{2}} \\frac{1 / f(x)}{\\left[\\sum_{i=1}^{d} f_{i i}(x) / f(x)\\right]^{2}}\\right]^{\\frac{1}{d+4}} T^{-\\frac{1}{d+4}} $$ Curse of dimensionality Thus, the MSE convergence rate is $ T^{-4/(d+4)}$ $ \\operatorname{MSE}[\\hat{f}(x)] \\propto T^{-\\frac{4}{3}} $ if $ d=1 $ $ \\operatorname{MSE}[\\hat{f}(x)] \\propto T^{-\\frac{2}{3}} $ if $ d=2 $ $ \\operatorname{MSE}[\\hat{f}(x)] \\propto T^{-\\frac{4}{7}} $ if $ d=3 $ The larger dimension $ d $, the slower convergence of $ \\hat{f}(x) $. This is the so-called “curse of dimensionality” associated with multivariate nonparametric estimation. It implies that a large sample size is needed in order to have a reasonable estimation for $ f(x) $. In particular, the same size $ T $ has to be increased exponentially fast as the dimension $ d $ increases in order to achieve the same level of estimation accuracy and precision. For most typical sample sizes encountered in economics and finance, it is rare to see nonparametric estimation with dimension $ d\u003e5 $. Question: How to deal with the curse of dimensionality? There are various methods to deal with the curse of dimensionality. For example, Imposing Multiplicability or Additicability Conditions Suppose multiplicability conditions for $ f(x) $ holds such as $$ f(x)=\\prod_{i=1}^{d} g_{i}\\left(x_{i}\\right) $$ Then one can estimate $ g_{i}\\left(x_{i}\\right) $ separately. When $ f(x) $ is a joint density function, multiplicability occurs when and only when $ X_{1 t}, X_{2 t}, \\ldots, X_{d t} $ are mutually independent. For functions other than probability densities, an additivity condition can be imposed to reduce the dimension of estimation. Projection Pursuit This approach assumes that a multivariate function is an unknown function of the linear combination of $ d $ explanatory variables, and then use a nonparametric method to estimate the unknown function and the combination coefficients. A well-known class of models in econometrics is single-index models for which a function of $ X_{t} $ is assumed to be an unknown function of a linear combination of the components of $ X_{t}, $ where the combination coefficients are also unknown. Imposing the Markov Condition Suppose the time series $ \\left{X_{t}\\right} $ is a Markov process. Namely, $$ \\begin{aligned} f\\left(X_{t} | I_{t-1}\\right) \u0026=f\\left(X_{t} | X_{t-1}\\right) \\ \u0026=\\frac{f\\left(X_{t}, X_{t-1}\\right)}{g\\left(X_{t-1}\\right)} \\end{aligned} $$ where $ I_{t-1}=\\left(X_{t-1}, X_{t-2}, \\ldots\\right) $ is the set of infinite dimension. Here, $ f\\left(X_{t}, X_{t-1}\\right) $ depends on only $ X_{t} $ and $ X_{t-1} $. ","date":"0001-01-01","objectID":"/2.-kernel-density-method/:2:3","tags":null,"title":"","uri":"/2.-kernel-density-method/"},{"categories":null,"content":"2.3 Application Kernel density estimators have been widely used in time series econometrics and financial econometrics. For example, Ait-Sahalia (1996, Review of Financial Studies) uses the kernel-based marginal density estimator $ \\hat{g}(x) $ to test the adequacy of a diffusion model for short-term interest rates, i.e., to test whether model is correctly specified. Gallant and Tauchen (1996, Econometric Theory) use the Hermite polynomial-based estimator for the conditional PDF of $ X_{t} $ given $ I_{t-1} $ to estimate continuous-time models efficiently. Hong and Li (2005, Review of Financial Studies) use the kernel-based joint density estimator $ \\hat{f}_{j}(x, y) $ to test the adequacy of continuous-time models and consider an application to affine term structure models of interest rates. \r Hong and White (2005, Econometrica) use the kernel-based joint density estimator $ \\hat{f}_{j}(x, y) $ to construct a nonparametric entropy-density measure for serial dependence with a well-defined asymptotic distribution. Su and White (2008, Econometric Theory) propose a Hellinger metric-based test for conditional dependence test which is applicable to test for general Granger causality by checking whether $$ f\\left(X_{t} | X_{t-1}, \\ldots, X_{t-p}\\right)=f\\left(X_{t} | X_{t-1}, \\ldots, X_{t-p}, Y_{t-1}, \\ldots, Y_{t-q}\\right) $$ where the conditional PDFs are estimated using the kernel method, and the lag orders $ p $ and $ q $ are given in advance. Granger causality: \r de Matos and Fernandes (2007, Journal of Econometrics) propose a test for the Markov property of a time series process: $$ f\\left(X_{t} | I_{t-1}\\right)=f\\left(X_{t} | X_{t-1}\\right) $$ They compare two kernel estimators for the conditional PDFs, $$ f\\left(X_{t} | X_{t-1}, X_{t-j}\\right)=\\frac{f\\left(X_{t}, X_{t-1}, X_{t-j}\\right)}{f\\left(X_{t-1}, X_{t-j}\\right)} $$ and $$ f\\left(X_{t} | X_{t-1}\\right)=\\frac{f\\left(X_{t}, X_{t-1}\\right)}{f\\left(X_{t-1}\\right)} $$ Wang and Hong (2017, Econometric Theory) propose a test for conditional independence which is applicable to test the Markov property and Granger causality in distribution. They estimate the conditional characteristic function rather than the conditional density function of $ \\left{X_{t}\\right}, $ thus *avoiding the curse of dimensionality problem* when the dimension $ d $ of $ X_{t} $ is large. There are other possible approaches to testing the Markov property. In general, this requires checking whether $$ f\\left(X_{t}=x | I_{t-1}\\right)=f\\left(X_{t}=x | X_{t-1}\\right) $$ where $ I_{t-1}=\\left{X_{t}, X_{t-1}, \\ldots,\\right} . $ A possible approach to testing the Markov property of a time series can be based on the following lemma. Lemma [Probability Integral Transforms of Markov Process]: Suppose $ \\left{X_{t}\\right} $ is a strictly stationary process. Denote the conditional PDF of $ X_{t} $ given $ X_{t-1} $ as $$ f(x | y)=f\\left(X_{t}=x | X_{t-1}=y\\right) $$ Define the probability integral transform $$ Z_{t}=\\int_{-\\infty}^{X_{t}} f\\left(x | X_{t-1}\\right) d x=F_{t}\\left(X_{t}\\right) $$ where $ F_{t}(x)=P\\left(X_{t} \\leq x | X_{t-1}\\right) . $ If $ \\left{X_{t}\\right} $ is Markovian, then $$ \\left{Z_{t}\\right} \\sim \\text { IID } U[0,1]. $$ ","date":"0001-01-01","objectID":"/2.-kernel-density-method/:3:0","tags":null,"title":"","uri":"/2.-kernel-density-method/"},{"categories":null,"content":"3. Nonparametric Regression Estimation Question: How to estimate a regression function $ E\\left(Y_{t} | X_{t}\\right) $ using an observed bivariate sample $ \\left{Y_{t}, X_{t}\\right}_{t=1}^{T} $? Note that both $ Y_{t} $ and $ X_{t} $ are random variables here. We first consider a few examples of regression functions. Example 1: The auto-regression function $$ r_{j}\\left(X_{t-j}\\right)=E\\left(X_{t} | X_{t-j}\\right) $$ We can write $$ X_{t}=r_{j}\\left(X_{t-j}\\right)+\\varepsilon_{t} $$ where $ E\\left(\\varepsilon_{t} | X_{t-j}\\right)=0 $. Example 2: The conditional variance $$ \\begin{aligned} \\sigma_{j}^{2}(x) \u0026=\\operatorname{var}\\left(X_{t} | X_{t-j}\\right) \\ \u0026=E\\left(X_{t}^{2} | X_{t-j}\\right)-\\left[E\\left(X_{t} | X_{t-j}\\right)\\right]^{2} \\end{aligned} $$ Example 3: The conditional distribution function $$ \\begin{aligned} F_{t}(x) \u0026=P\\left(X_{t} \\leq x | I_{t-1}\\right) \\ \u0026=E\\left[1\\left(X_{t} \\leq x\\right) | I_{t-1}\\right] \\end{aligned} $$ where $ I_{t-1} $ is an information set available at time $ t-1 . $ If we assume that $ \\left{X_{t}\\right} $ is an Markovian process. Then $$ F_{t}(x)=E\\left[1\\left(X_{t} \\leq x\\right) | X_{t-1}\\right] $$ This is a generalized regression function of $ 1\\left(X_{t} \\leq x\\right) $ on $ X_{t-1} $. Example 4: The conditional characteristic function $$ \\varphi_{t}(u)=E\\left[\\exp \\left(i u X_{t}\\right) | I_{t-1}\\right] $$ If $ \\left{X_{t}\\right} $ is an Markovian process, then $$ \\varphi_{t}(u)=E\\left[\\exp \\left(i u X_{t}\\right) | X_{t-1}\\right] $$ This is a generalized regression function of $ \\exp \\left(i u X_{t}\\right) $ on $ X_{t-1} $. ","date":"0001-01-01","objectID":"/3.-nonparametric-regression-estimation/:0:0","tags":null,"title":"","uri":"/3.-nonparametric-regression-estimation/"},{"categories":null,"content":"3.1 Kernel Regression Estimation We first impose a regularity condition on the data generating process and the regression function. Assumption [DGP]: (i) Suppose $ \\left{Y_{t}, X_{t}\\right}^{\\prime} $ is an **IID** sequence such that the regression function $ r(x) \\equiv E\\left(Y_{t} | X_{t}=x\\right) $ exists and is *twice continuously differentiable*; (ii) $ X_{t} $ is a continuous random variable with support $ [a, b] $ and probability density $ g(x) $ which is also *twice continuously differentiable* over $ [a, b] . $ Furthermore, $ g(x)\u003e0 $ for all $ x \\in[a, b] $. We will relax the IID assumption to a serially dependent time series process at a later stage. Like in the case of density estimation, allowing mild serial dependence (e.g., $ \\alpha $-mixing) in $ \\left{Y_{t}, X_{t}\\right}^{\\prime} $ will not affect the asymptotic results derived under the IID assumption. Question: How to estimate the regression function $ r(x) $? We can always write $$ Y_{t}=r\\left(X_{t}\\right)+\\varepsilon_{t} $$ where $ E\\left(\\varepsilon_{t} | X_{t}\\right)=0 $ and $ \\operatorname{var}\\left(\\varepsilon_{t} | X_{t}\\right)=\\sigma^{2}\\left(X_{t}\\right) $. Note that conditional heteroskedasticity may exist. We impose a continuity condition on $ \\sigma^{2}(x) $. Assumption [Conditional Heteroskedasticity]: $ \\sigma^{2}(x) $ is continuous over the support $ [a, b] $ of $ X_{t} $. ###3.1.1 Nadaraya-Watson Estimator For any given $ x $ in the support of $ X_{t}, $ define a kernel-based regression estimator $$ \\hat{r}(x)=\\frac{\\hat{m}(x)}{\\hat{g}(x)} $$ where the numerator $$ \\hat{m}(x)=\\frac{1}{T} \\sum_{t=1}^{T} Y_{t} K_{h}\\left(x-X_{t}\\right) $$ is a weighted sample mean of $ \\left{Y_{t}\\right}, $ and, as before, the denominator $$ \\hat{g}(x)=T^{-1} \\sum_{t=1}^{T} K_{h}\\left(x-X_{t}\\right) $$ is the kernel estimator for density $ g(x) $ at point $ x . $ This kernel regression estimator was proposed by Nadaraya (1964) and Watson (1964) and so is also called the Nadaraya-Watson estimator. Alternatively, we can express $$ \\hat{r}(x)=\\sum_{t=1}^{T} \\hat{W}_{t}(x) Y_{t} $$ where the weighting function $$ \\hat{W}{t}(x)=\\frac{K{h}\\left(x-X_{t}\\right)}{\\sum_{t=1}^{T} K_{h}\\left(x-X_{t}\\right)} $$ which sums to unity, that is, $$ \\sum_{t=1}^{T} \\hat{W}_{t}=1 $$ Therefore, the Nadaraya-Watson estimator is a local weighted sample mean of $ \\left{Y_{t}\\right}_{t=1}^{n} $ where the weight $ \\hat{W}_{t}(x) $ is zero outside the interval $ [x-h, x+h] $ when the kernel $ K(u) $ has bounded support on [-1,1]. We first provide a geometric interpretation for $ \\hat{r}(x) . $ When the uniform kernel $ K(u)= $ $ \\frac{1}{2} 1(|u| \\leq 1) $ is used, the Nadaraya-Watson estimator becomes $$ \\hat{r}(x)=\\frac{\\sum_{t=1}^{T} Y_{t} 1\\left(\\left|X_{t}-x\\right| \\leq h\\right)}{\\sum_{t=1}^{T} 1\\left(\\left|X_{t}-x\\right| \\leq h\\right)} $$ This is a local sample mean, that is, the average of the observations $ \\left{Y_{t}\\right}_{t=1}^{T} $ for which the values of the corresponding explanatory variables $ \\left{X_{t}\\right} $ fall into the interval $ [x-h, x+h] $. Tukey (1961) calls this estimator the regressogram. **Intuitively**, suppose $ r(\\cdot) $ is a smooth function, and we consider a small interval $ [x-h, x+h], $ which is centered at point $ x $ and has size $ 2 h, $ where $ h $ is small. Then $ r(\\cdot) $ will be nearly a constant over this small interval and can be estimated by taking an average of the observations $ \\left{Y_{t}\\right} $ which correspond to those $ \\left{X_{t}\\right} $ whose values fall into the small interval. More generally, we can assign different weights to observations of $ \\left{Y_{t}\\right}_{t=1}^{T} $ according to their distances to the location $ x $. This makes sense because the observations $ \\left{X_{t}\\right}_{t=1}^{T} $ closer to $ x $ will contain more information about $ r(x) $ at point $ x . $ The use of $ K(\\cdot) $ is to assign different weights for observations $ \\left{Y_{t}, X_{t}\\right}_{t=1}^{T} $. Kernel regression is a special co","date":"0001-01-01","objectID":"/3.-nonparametric-regression-estimation/:1:0","tags":null,"title":"","uri":"/3.-nonparametric-regression-estimation/"},{"categories":null,"content":"3.1.2 MSE and Optimal Bandwidth Question: How to derive the asymptotic MSE of $ \\hat{r}(x) $? The Nadaraya-Watson estimator $ \\hat{r}(x) $ is a ratio of two random variables. To simplify asymptotic analysis, for any given point $ x $, we consider the decomposition $$ \\begin{aligned} \\hat{r}(x)-r(x)=\u0026 \\frac{\\hat{m}(x)-r(x) \\hat{g}(x)}{\\hat{g}(x)} \\ =\u0026 \\frac{\\hat{m}(x)-r(x) \\hat{g}(x)}{E[\\hat{g}(x)]} \\ \u0026+\\frac{[\\hat{m}(x)-r(x) \\hat{g}(x)]}{E[\\hat{g}(x)]} \\cdot \\frac{[E[\\hat{g}(x)]-\\hat{g}(x)]}{\\hat{g}(x)} \\ =\u0026 \\frac{\\hat{m}(x)-r(x) \\hat{g}(x)}{E[\\hat{g}(x)]} \\ \u0026+\\text { higher order term. } \\end{aligned} $$ Here the second term is of a higher order term because $$ \\hat{g}(x)-E[\\hat{g}(x)] \\stackrel{p}{\\rightarrow} 0 \\text { as } T \\rightarrow \\infty $$ by law of large number. And by Taylor expansion $$ E[\\hat{g}(x)] \\rightarrow g(x) \\int_{-1}^{1} K(u) d u\u003e0 \\text { as } h \\rightarrow 0 $$ It can be shown that the second term is of a higher order term that vanishes faster than the first term (how?). As a consequence, the convergence rate of $ \\hat{r}(x) $ to $ r(x) $ is determined by the first term, which is the dominant term. For the first term, using $ Y_{t}=r\\left(X_{t}\\right)+\\varepsilon_{t}, $ we can write the numerator as follows: $$ \\begin{aligned} \\hat{m}(x)-r(x) \\hat{g}(x)=\u0026 \\frac{1}{T} \\sum_{t=1}^{T}\\left[Y_{t}-r(x)\\right] K_{h}\\left(x-X_{t}\\right) \\ =\u0026 \\frac{1}{T} \\sum_{t=1}^{T} \\varepsilon_{t} K_{h}\\left(x-X_{t}\\right) \\ \u0026+\\frac{1}{T} \\sum_{t=1}^{T}\\left[r\\left(X_{t}\\right)-r(x)\\right] K_{h}\\left(x-X_{t}\\right) \\ =\u0026 \\hat{V}(x)+\\hat{B}(x), \\quad \\text { say }\\ =\u0026 \\text{variance component + bias component} \\end{aligned} $$ Here, the first term $ \\hat{V}(x) $ is a variance effect, and the second term $ \\hat{B}(x) $ is a bias effect. We first consider the variance term. For simplicity, we first assume that $ \\left{Y_{t}, X_{t}\\right} $ is an IID sequence. Then for the variance component, we have $$ \\begin{aligned} E\\left[\\hat{V}(x)^{2}\\right] \u0026=E\\left[\\frac{1}{T} \\sum_{t=1}^{T} \\varepsilon_{t} K_{h}\\left(x-X_{t}\\right)\\right]^{2} \\ \u0026=\\frac{1}{T^{2}} E\\left[\\sum_{t=1}^{T} \\varepsilon_{t} K_{h}\\left(x-X_{t}\\right)\\right]^{2}\\ \u0026=\\frac{1}{T^{2}} \\sum_{t=1}^{T} E\\left[\\varepsilon_{t}^{2} K_{h}^{2}\\left(x-X_{t}\\right)\\right] \\text { (by independence, and } E\\left(\\varepsilon_{t} | X_{t}\\right)=0)\\ \u0026=\\frac{1}{T} E\\left[\\varepsilon_{t}^{2} K_{h}^{2}\\left(x-X_{t}\\right)\\right] \\ \u0026=\\frac{1}{T} E\\left[\\sigma^{2}\\left(X_{t}\\right) K_{h}^{2}\\left(x-X_{t}\\right)\\right]\\left(\\text { by } E\\left(\\varepsilon_{t}^{2} | X_{t}\\right)=\\sigma^{2}\\left(X_{t}\\right)\\right) \\ \u0026=\\frac{1}{T} \\int_{a}^{b}\\left[\\frac{1}{h} K\\left(\\frac{x-y}{h}\\right)\\right]^{2} \\sigma^{2}(y) g(y) d y \\ \u0026=\\frac{1}{T h} \\sigma^{2}(x) g(x) \\int_{-1}^{1} K^{2}(u) d u[1+o(1)] \\text{ (by Taylor expansion for g(y) and $ \\sigma^2(y) $ simultaneously)} \\end{aligned} $$ by change of variable, and the continuity of $ \\sigma^{2}(\\cdot) g(\\cdot), $ where $ \\sigma^{2}(x)=E\\left(\\varepsilon_{t}^{2} | X_{t}=x\\right) $ is the conditional variance of $ \\varepsilon_{t} $ or $ Y_{t} $ given $ X_{t}=x . $ Note that the variance $ E[\\hat{V}(x)]^{2} $ is proportional to the inverse of $ T h $ because $ T h $ can be viewed as the effective sample size of the observations falling into the interval $ [x-h, x+h] $. On the other hand, for the denominator, we have as $ h \\rightarrow 0 $, $$ \\begin{aligned} E[\\hat{g}(x)] \u0026=E\\left[K_{h}\\left(x-X_{t}\\right)\\right] \\ \u0026=\\int_{a}^{b} \\frac{1}{h} K\\left(\\frac{x-y}{h}\\right) g(y) d y \\ \u0026 \\rightarrow g(x) \\int_{-1}^{1} K(u) d u=g(x) \\end{aligned} $$ if $ \\int_{-1}^{1} K(u) d u=1 . $ It follows that $$ E\\left[\\frac{\\hat{V}(x)}{E[\\hat{g}(x)}\\right]^{2}=\\frac{1}{T h} \\frac{\\sigma^{2}(x)}{g(x)} \\int_{-1}^{1} K^{2}(u) d u[1+o(1)] $$ Thus, the asymptotic variance of $ \\hat{r}(x) $ is proportional to $ (T h)^{-1}, $ where $ T h $ is the approximate (effective) sample size of the observations in the interval $ [x-h, x+h] . $ The asymptotic variance of $ \\hat{r}(","date":"0001-01-01","objectID":"/3.-nonparametric-regression-estimation/:1:1","tags":null,"title":"","uri":"/3.-nonparametric-regression-estimation/"},{"categories":null,"content":"3.2 Local Polynomial Estimation To provide an motivation for local polynomial smoothing, we now provide an alternative interpretation for the Nadaraya-Watson estimator $ \\hat{r}(x) $. First, we consider a sum of squared residuals (SSR) minimization problem $$ \\min {r} \\sum{t=1}^{T}\\left(Y_{t}-r\\right)^{2} $$ where $ r $ is a constant. The optimal solution is the sample mean $$ \\hat{r}=\\bar{Y} \\equiv \\frac{1}{T} \\sum_{t=1}^{T} Y_{t} $$ Next, we consider a local weighted sum of squared residuals minimization problem $$ \\min {r} \\sum{t=1}^{T}\\left(Y_{t}-r\\right)^{2} K_{h}\\left(x-X_{t}\\right) $$ where $ r $ is, again, a real-valued constant. When $ K(u) $ has bounded support on [-1,1] this is the weighted sum of squared residuals to predict the observations $ \\left{Y_{t}\\right} $ for which the values of the corresponding explanatory variables $ \\left{X_{t}\\right} $ fall into the interval $ [x- $ $ h, x+h] $. The FOC is given by $$ \\sum_{t=1}^{T}\\left(Y_{t}-\\hat{r}\\right) K_{h}\\left(x-X_{t}\\right)=0 $$ It follows that $$ \\begin{aligned} \\hat{r} \u0026 \\equiv \\hat{r}(x) \\ \u0026=\\frac{\\sum_{t=1}^{T} Y_{t} K_{h}\\left(x-X_{t}\\right)}{\\sum_{t=1}^{T} K_{h}\\left(x-X_{t}\\right)} \\ \u0026=\\frac{\\hat{m}(x)}{\\hat{g}(x)} \\end{aligned} $$ This is a local constant estimator. In other words, the Nadaraya-Watson estimator can be viewed as a locally weighted sample mean which minimizes a locally weighted sum of squared residuals. Question: Why only use a local constant estimator? Why not use a local linear function? More generally, why not use a local polynomial? Question: What are the gains, if any, to use a local polynomial estimator? ","date":"0001-01-01","objectID":"/3.-nonparametric-regression-estimation/:2:0","tags":null,"title":"","uri":"/3.-nonparametric-regression-estimation/"},{"categories":null,"content":"3.2.1 Local Polynomial Estimator We now consider a local polynomial estimator for the regression function $ r(x), $ where $ x $ is a given point in the support of $ X_{t} $. Suppose $ z $ is an arbitrary point in a small neighborhood of $ x, $ and $ r(z) $ is continuously differentiable with respect to $ z $ up to order $ p+1 $ in this neighborhood. Then by a $ (p+1) $-order Taylor series expansion, we have for all $ z $ in a small neighborhood of a fixed point $ x $, $$ \\begin{aligned} r(z) \u0026=\\sum_{j=0}^{p} \\frac{1}{j !} r^{(j)}(x)(z-x)^{j}+\\frac{1}{(p+1) !} r^{(p+1)}(\\bar{x})(z-x)^{p+1} \\ \u0026=\\sum_{j=0}^{p} \\alpha_{j}(z-x)^{j}+\\frac{1}{(p+1) !} r^{(p+1)}(\\bar{x})(z-x)^{p+1} \\end{aligned} $$ where $ \\bar{x} $ lies in the segment between $ x $ and $ z, $ and the polynomial coefficient $$ \\begin{aligned} \\alpha_{j} \u0026 \\equiv \\alpha_{j}(x) \\ \u0026=\\frac{1}{j !} r^{(j)}(x), \\quad j=0,1, \\ldots, p \\end{aligned} $$ depends on the point $ x . $ This relation suggests that one can use a local polynomial model to fit the function $ r(z) $ in the neighborhood of $ x $ as long as the observations in this neighborhood are “sufficiently rich.” Therefore, we consider the following local weighted sum of squared residuals minimization problem $$ \\min {\\alpha} \\sum{t=1}^{T}\\left[Y_{t}-\\sum_{j=0}^{p} \\alpha_{j}\\left(X_{t}-x\\right)^{j}\\right]^{2} K_{h}\\left(x-X_{t}\\right)=\\sum_{t=1}^{T}\\left(Y_{t}-\\alpha^{\\prime} Z_{t}\\right)^{2} K_{h}\\left(x-X_{t}\\right) $$ where $ \\alpha=\\left(\\alpha_{0}, \\alpha_{1}, \\ldots, \\alpha_{p}\\right)^{\\prime} $ and $ Z_{t}=Z_{t}(x)=\\left[1,\\left(X_{t}-x\\right), \\ldots,\\left(X_{t}-x\\right)^{p}\\right]^{\\prime} . $ Note that $ Z_{t}=Z_{t}(x) $ is a $ (p+1) $-dimensional local polynomial vector which depends on location $ x $. The resulting local weighted least squares estimator, $$ \\hat{r}(z)=\\sum_{j=0}^{p} \\hat{\\alpha}_{j}(z-x)^{j} \\text{ for all } z \\text{ near } x $$ is the so-called local polynomial estimator of $ r(z) $ for $ z $ near $ x, $ where $ \\hat{\\alpha} $ is the locally weighted least squares estimator, whose expression will be given below. We now show that the intercept estimator $ \\hat{\\alpha}{0} $ is an estimator for $ r(x), $ and $ \\nu ! \\hat{\\alpha}{\\nu} $ is an estimator for the derivative $ r^{(\\nu)}(x), $ where $ 1 \\leq \\nu \\leq p $. Since $ \\hat{r}(z)=\\sum_{j=0}^{p} \\hat{\\alpha}_{j}(z-x)^{j}, $ for all $ z $ near $ x $, the regression estimator at point $ x $ is then given by $$ \\hat{r}(x)=\\sum_{j=0}^{p} \\hat{\\alpha}_{j}(x-x)^{j}=\\hat{\\alpha}_{0} $$ Moreover, the derivative estimator of $ r^{(\\nu)}(z) $ for $ z $ near the point $ x $ is given by $$ \\hat{r}^{(\\nu)}(z)=\\sum_{j=\\nu}^{p} j(j-1) \\cdots(j-\\nu+1) ! \\hat{\\alpha}_{j}(z-x)^{j-\\nu} \\text { for } \\nu \\leq p $$ Thus, we have the $ \\nu $ -th order derivative estimator at point $ x $ $$ \\hat{r}^{(\\nu)}(x)=\\nu ! \\hat{\\alpha}_{\\nu} $$ Interestingly, we can obtain $ \\hat{r}(x) $ and $ \\hat{r}^{(\\nu)}(x) $ for $ 1 \\leq \\nu \\leq p $ simultaneously. Local polynomial smoothing is rather convenient for estimating the $ r^{(\\nu)}(x), \\nu= $ $ 0,1, \\ldots, p, $ simultaneously. When $ p=0, $ we obtain a local constant estimator, i.e., the Nadaraya-Watson estimator. Obviously, a local polynomial estimator with $ p\u003e0 $ always has a smaller sum of weighted squared residuals than the local constant estimator, because for any local polynomial model, one can always simply set all slope coefficients equal to zero. This implies that the sum of weighted squared residuals will never be larger than that of the local constant estimator. To compute the local polynomial estimator, one has to choose $ p, $ the order of local polynomial, $ h, $ the bandwidth, and $ K(\\cdot), $ the kernel function. Often, a nonnegative kernel function $ K(\\cdot) $ is used, which corresponds to a second order kernel function. The choices of $ (p, h) $ jointly determine the complexity of the local polynomial model. The choice of $ h $ is more important than the choice of $ p $ (why? Higher order do","date":"0001-01-01","objectID":"/3.-nonparametric-regression-estimation/:2:1","tags":null,"title":"","uri":"/3.-nonparametric-regression-estimation/"},{"categories":null,"content":"3.2.2 Equivalent Kernel To exploit the advantages of the local polynomial estimator for $ r(x), $ we now investigate its asymptotic properties. Suppose our interest is in estimating $ r^{(\\nu)}(x), $ where $ 0 \\leq v \\leq p $. Recalling the weighting function $$ W_{t}=K_{h}\\left(x-X_{t}\\right)=\\frac{1}{h} K\\left(\\frac{x-X_{t}}{h}\\right) $$ we define a $ j $-th order locally weighted sample moment $$ \\begin{aligned} \\hat{s}{j} \u0026=\\hat{s}{j}(x) \\ \u0026=\\sum_{t=1}^{T}\\left(X_{t}-x\\right)^{j} K_{h}\\left(X_{t}-x\\right) \\ \u0026=\\sum_{t=1}^{T}\\left(X_{t}-x\\right)^{j} W_{t}, \\quad j=0,1, \\ldots, p \\end{aligned} $$ and let $$ \\begin{aligned} \\hat{S} \u0026=\\hat{S}(x) \\ \u0026=Z^{\\prime} W Z \\ \u0026=\\sum_{t=1}^{T} Z_{t} W_{t} Z_{t}^{\\prime} \\ \u0026=\\left[\\hat{s}_{(i-1)+(j-1)}\\right]_{(i, j)} \\end{aligned} $$ be a $ (p+1) \\times(p+1) $ stochastic symmetric matrix, whose $ (i, j) $-th element is $ \\hat{s}{(i-1)+(j-1)}= \\hat{s}{i+j-2} $. Then we have the local weighted least squares estimator $$ \\hat{\\alpha}=\\hat{S}^{-1} Z^{\\prime} W Y $$ Denote $ e_{\\nu+1} $ for the $ (p+1) \\times 1 $ unit vector with 1 at the $ (\\nu+1) $-th position and 0 elsewhere. Then the $ \\nu $ -th component of $ \\hat{\\alpha} $ is given by $$ \\begin{aligned} \\hat{\\alpha}{\\nu} \u0026=e{\\nu+1}^{\\prime} \\hat{\\alpha} \\ \u0026=e_{\\nu+1}^{\\prime} \\hat{S}^{-1} Z^{\\prime} W Y \\ \u0026=e_{\\nu+1}^{\\prime} \\hat{S}^{-1} \\sum_{t=1}^{T} Z_{t} W_{t} Y_{t} \\ \u0026=\\sum_{t=1}^{T} e_{\\nu+1}^{\\prime} \\hat{S}^{-1}\\left(\\begin{array}{c} 1 \\ \\left(X_{t}-x\\right) \\ \\cdots \\ \\left(X_{t}-x\\right)^{p} \\end{array}\\right) \\frac{1}{h} K\\left(\\frac{X_{t}-x}{h}\\right) Y_{t} \\ \u0026=\\sum_{t=1}^{T} \\hat{W}_{\\nu}\\left(\\frac{X_{t}-x}{h}\\right) Y_{t}, \\text { say } \\end{aligned} $$ where the effective kernel $ \\hat{W}_{\\nu}(\\cdot) $ is the multiplication of the kernel function $ K(\\cdot) $ with a polynomial function, namely $$ \\begin{aligned} \\hat{W}{\\nu}(u) \u0026=e{\\nu+1}^{\\prime} \\hat{S}^{-1}\\left(\\begin{array}{l} 1 \\ h u \\ \\cdots \u0026 \\ (h u)^{p} \\end{array}\\right) \\frac{1}{h} K(u) \\ \u0026=e_{\\nu+1}^{\\prime} \\hat{S}^{-1} H P(u) \\frac{1}{h} K(u) \\end{aligned} $$ where $$ H=\\operatorname{diag}\\left{1, h, \\ldots, h^{p}\\right} $$ is a $ (p+1) \\times(p+1) $ diagonal matrix, and $$ P(u)=\\left(1, u, \\ldots, u^{p}\\right)^{\\prime} $$ is a $ (p+1) \\times 1 $ vector of a $ p $-th order polynomial in $ u $. Note that we will make change of variable $ u=\\left(X_{t}-x\\right) / h . $ Obviously, the local polynomial estimator differs from the Nadaraya-Watson estimator in using a different weighting function $ \\hat{W}_{v}\\left(\\frac{X_{t}-x}{h}\\right) $ for $ \\left{Y_{t}\\right}_{t=1}^{T} $. Question: What properties does the effective kernel $ \\hat{W}_{\\nu}(u) $ have? $ \\hat{W}_{\\nu}(u) $ is data driven, then it’s also random. Lemma [Sample Orthogonality between $ \\hat{W}{\\nu}(u) \\text { and }\\left(X{t}-x\\right)^{q} $]: Let $ \\hat{W}{v}(u) $ be defined as above. Then $$ \\sum{t=1}^{T} \\hat{W}{\\nu}\\left(\\frac{X{t}-x}{h}\\right)\\left(X_{t}-x\\right)^{q}=\\delta_{\\nu, q} \\text { for } 0 \\leq \\nu, q \\leq p $$ where $ \\delta_{v, q} $ is the Kronecker delta function, namely $ \\delta_{\\nu, q}=1 $ if $ \\nu=q $ and $ \\delta_{\\nu, q}=0 $ otherwise. The sample orthogonality between $ \\hat{W}{\\nu}(u) $ and $ \\left(X{t}-x\\right)^{q} $ is very useful in deriving the bias of the local polynomial estimator $ \\hat{\\alpha}_{\\nu} $. Question: What is the intuition behind this orthogonality? Proof: Observing $ \\left(X_{t}-x\\right)^{q}=Z_{t}^{\\prime} e_{q+1} $ and $ \\hat{S}=Z^{\\prime} W Z, $ we have $$ \\begin{aligned} \\sum_{t=1}^{T} \\hat{W}_{\\nu}\\left(\\frac{X_{t}-x}{h}\\right) Z_{t}^{\\prime} e_{q+1} \u0026=e_{\\nu+1}^{\\prime} \\hat{S}^{-1}\\left(\\sum_{t=1}^{T} Z_{t} W_{t} Z_{t}^{\\prime}\\right) e_{q+1} \\ \u0026=e_{\\nu+1}^{\\prime} I_{p+1} e_{q+1} \\ \u0026=\\delta_{\\nu q} \\end{aligned} $$ Now, let $ S $ be a non-stochastic $ (p+1) \\times(p+1) $ matrix whose $ (i, j) $ th element $ S_{(i, j)} $ is $ s_{(i-1)+(j-1)}=s_{i+j-2}, $ where $$ s_{j}=\\int_{-1}^{1} u^{j} K(u) d u, \\quad j=0,1, \\ldots, p $$ Then $$ S=\\int_{-1}^{1} P","date":"0001-01-01","objectID":"/3.-nonparametric-regression-estimation/:2:2","tags":null,"title":"","uri":"/3.-nonparametric-regression-estimation/"},{"categories":null,"content":"3.2.3 Asymptotic Properties of Local Polynomial Estimator Question: What is the MSE of $ \\hat{\\alpha} $? Noting $ Y_{t}=r\\left(X_{t}\\right)+\\varepsilon_{t}, $ we first write the $ v $-th component of $ \\hat{\\alpha} $ $$ \\begin{aligned} \\hat{\\alpha}{\\nu}-\\alpha{\\nu} \u0026=\\sum_{t=1}^{T} \\hat{W}_{\\nu}\\left(\\frac{X_{t}-x}{h}\\right) Y_{t}-\\alpha_{\\nu} \\ \u0026=\\sum_{t=1}^{T} \\hat{W}_{\\nu}\\left(\\frac{X_{t}-x}{h}\\right) \\varepsilon_{t}+\\left[\\sum_{t=1}^{T} \\hat{W}_{\\nu}\\left(\\frac{X_{t}-x}{h}\\right) r\\left(X_{t}\\right)-\\alpha_{\\nu}\\right] \\ \u0026=\\hat{V}+\\hat{B}, \\text { say. } \\end{aligned} $$ For the first term $ \\hat{V}, $ using the formula $$ \\hat{S}=T g(x) H S H\\left[1+O_{P}\\left(a_{T}\\right)\\right] $$ which has been proven earlier, we can write $$ \\begin{aligned} \\hat{V} \u0026=\\sum_{t=1}^{T} \\hat{W}_{\\nu}\\left(\\frac{X_{t}-x}{h}\\right) \\varepsilon_{t} \\ \u0026=e_{\\nu+1}^{\\prime} \\hat{S}^{-1} Z^{\\prime} W \\varepsilon \\ \u0026=\\frac{1}{T h^{\\nu} g(x)} e_{\\nu+1}^{\\prime} S^{-1} H^{-1} Z^{\\prime} W \\varepsilon\\left[1+O_{P}\\left(a_{T}\\right)\\right] \\end{aligned} $$ where we used the fact that $ e_{\\nu+1}^{\\prime} H^{-1}=h^{-\\nu} e_{\\nu+1}^{\\prime} . $ Furthermore, assuming that $ \\left{Y_{t}, X_{t}\\right} $ is IID, we have $$ \\begin{aligned} \u0026 E\\left(Z^{\\prime} W \\varepsilon \\varepsilon^{\\prime} W Z\\right) \\ =\u0026 E\\left[\\sum_{t=1}^{T} \\varepsilon_{t} Z_{t} K_{h}\\left(X_{t}-x\\right)\\right]\\left[\\sum_{s=1}^{T} \\varepsilon_{s} Z_{s}^{\\prime} K_{h}\\left(X_{s}-x\\right)\\right] \\ =\u0026 \\sum_{t=1}^{T} E\\left[\\varepsilon_{t}^{2} Z_{t} K_{h}^{2}\\left(X_{t}-x\\right) Z_{t}^{\\prime}\\right]\\left(\\text { by } E\\left(\\varepsilon_{t} | X_{t}\\right)=0\\right) \\ =\u0026 T E\\left[\\varepsilon_{t}^{2} Z_{t} K_{h}^{2}\\left(X_{t}-x\\right) Z_{t}^{\\prime}\\right](\\text { by independence }) \\ =\u0026 \\frac{T}{h} \\sigma^{2}(x) g(x) H S^{*} H[1+o(1)] \\end{aligned} $$ by change of variable and continuity of $ \\sigma^{2}(\\cdot), $ where $ S^{* *} $ is a $ (p+1) \\times(p+1) $ matrix with $ (i, j) $-th element $$ \\begin{aligned} S_{(i, j)}^{*} \u0026=s_{(i-1)+(j-1)}^{*} \\ \u0026=\\int_{-1}^{1} u^{(i-1)+(j-1)} K^{2}(u) d u \\end{aligned} $$ Note that $ S^{*} \\neq S $ because the integral here is weighted by $ K^{2}(u) $ rather than $ K(u) $. It follows that the asymptotic variance of $ \\hat{\\alpha}_{\\nu} $ $$ \\begin{aligned} \\operatorname{avar}(\\hat{V}) \u0026=\\frac{1}{T h^{\\nu} g(x)} e_{\\nu+1}^{\\prime} S^{-1} H^{-1} E\\left(Z^{\\prime} W \\varepsilon \\varepsilon^{\\prime} W Z\\right) H^{-1} S^{-1} \\frac{1}{T h^{\\nu} g(x)} \\ \u0026=\\frac{1}{T h^{2 \\nu+1}} \\frac{\\sigma^{2}(x)}{g(x)} e_{\\nu+1}^{\\prime} S^{-1} S^{*} S^{-1} e_{\\nu+1}[1+o(1)] \\ \u0026=\\frac{1}{T h^{2 \\nu+1}} \\frac{\\sigma^{2}(x)}{g(x)} \\int_{-1}^{1} \\tilde{K}^{2}_{\\nu}(u) d u[1+o(1)] \\ \u0026=O\\left(T^{-1} h^{-2 \\nu-1}\\right) \\end{aligned} $$ where, as before, $ \\tilde{K}{\\nu}(u)=e{\\nu+1}^{\\prime} S^{-1} P(u) K(u) $ is the equivalent kernel, and $$ \\begin{aligned} \\int_{-1}^{1} \\tilde{K}_{\\nu}^{2}(u) d u \u0026=\\int_{-1}^{1}\\left[e_{\\nu+1}^{\\prime} S^{-1} P(u) K(u)\\right]\\left[K(u) P(u)^{\\prime} S^{-1} e_{\\nu+1}\\right] d u \\ \u0026=e_{\\nu+1}^{\\prime} S^{-1}\\left[\\int_{-1}^{1} P(u) K^{2}(u) P(u)^{\\prime} d u\\right] S^{-1} e_{\\nu+1} \\ \u0026=e_{\\nu+1}^{\\prime} S^{-1} S^{*} S^{-1} e_{\\nu+1} \\end{aligned} $$ Note that this result is obtained under the IID assumption for $ \\left{Y_{t}, X_{t}\\right}_{t=1}^{T} . $ It still holds under a suitable mixing condition, using an analogous reasoning to that for the kernel density estimator $ \\hat{g}(x) $. Question: How to compute the order of magnitude of the bias $ \\hat{B} $? Taking a Tailor series expansion around a small neighborhood of $ x, $ up to order $ p+1 $ namely, $$ \\begin{aligned} r\\left(X_{t}\\right) \u0026=\\sum_{j=0}^{p} \\frac{1}{j !} r^{(j)}(x)\\left(X_{t}-x\\right)^{j}+\\frac{1}{(p+1) !} r^{(p+1)}\\left(\\bar{x}_{t}\\right)\\left(X_{t}-x\\right)^{p+1} \\ \u0026=\\sum_{j=0}^{p} \\frac{1}{j !} r^{(j)}(x)\\left(X_{t}-x\\right)^{j}+R\\left(x, X_{t}\\right) \\end{aligned} $$ where $ \\bar{x}{t} $ lies in the segment between $ x $ and $ X{t} $, we have $$ \\begin{aligned} \\hat{B}=\u0026 ","date":"0001-01-01","objectID":"/3.-nonparametric-regression-estimation/:2:3","tags":null,"title":"","uri":"/3.-nonparametric-regression-estimation/"},{"categories":null,"content":"3.2.4 Boundary Behavior of Local Polynomial Estimator Question: The above asymptotic results, namely MSE and asymptotic normality, hold for $ x $ in the interior region, i.e., $ x \\in[a+h, b-h] $. What happens if $ x $ is in the boundary region? For simplicity, we assume $ [a, b]=[0,1] $ and consider a left boundary point $ x=\\tau h $ for $ \\tau \\in[0,1] . $ Then following reasoning analogous to what we have done for an interior point, we can obtain $$ \\begin{aligned} \\operatorname{MSE}\\left[\\hat{\\alpha}{\\nu}(\\tau h)\\right]=\u0026 E\\left[\\hat{\\alpha}{\\nu}(\\tau h)-\\alpha_{\\nu}(0)\\right]^{2} \\ =\u0026 \\frac{1}{T h^{2 \\nu+1}} \\frac{\\sigma^{2}(0)}{g(0)} e_{\\nu+1}^{\\prime} S_{\\tau}^{-1} S_{\\tau}^{*} S_{\\tau}^{-1} e_{\\nu+1} \\ \u0026+\\left[\\frac{h^{p+1-\\nu} r^{(p+1)}(0)}{(p+1) !}\\right]^{2} e_{\\nu+1}^{\\prime} S_{\\tau}^{-1} C_{\\tau} C_{\\tau}^{\\prime} S_{\\tau}^{-1} e_{\\nu+1} \\end{aligned} $$ where $ S_{\\tau}, S_{\\tau}^{*} $ and $ C_{\\tau} $ are defined in the same way as $ S, S^{*} $ and $ C, $ with the lower bounds of all integrals involved being changed from -1 to $ -\\tau . $ For example, $ S_{\\tau} $ is a $ (p+1) \\times(p+1) $ matrix, with its $ (i, j) $-th element equal to $$ s_{(i-1)+(j-1), \\tau}=\\int_{-\\tau}^{1} u^{(i-1)+(j-1)} K(u) d u $$ Then optimal bandwidth $$ h^_\\tau=C^_\\tau T^{-\\frac{1}{2p+3}} $$ Interestingly, the bias of $ \\hat{\\alpha}_{\\nu}(x) $ is of the same order of magnitude no matter whether $ x $ is in the interior region or in the boundary region of $ [a, b]=[0,1] $. Of course, the proportionality does depend on the location of $ x, $ namely $ \\tau $ if $ x $ is in the boundary region. Thus, the local polynomial estimator automatically adapts to the boundary region and does not suffer from the boundary bias problem as the standard kernel method. Question: What is the intuition behind this? Why does the local polynomial regression estimator behave differently from the Nadaraya-Watson regression estimator? The latter has a bias equal to $ O(h) $ in the boundary region. We consider the local linear estimator (i.e., $ p=1 $ ) as an example. The key here is the joint use of the local intercept and local slope. The latter provides flexibility to adapt to asymmetric data coverage such as those in the boundary regions. As a result, the bias of the local linear estimator in the boundary region is much smaller than without using a slope parameter. An alternative interpretation is that the local polynomial estimator is equivalent to a kernel estimator but with a known density $ g(x) $, even when $ x $ is in the boundary region. Thus, the boundary bias due to density estimation does not arise for the local polynomial estimator. We can also obtain an analogous asymptotic normality for $ \\hat{\\alpha}_{\\nu}(\\tau h) $ in the boundary region. Theorem [Asymptotic Normality]: Suppose $ h=O\\left(T^{-1 /(2 p+3)}\\right) $ and $ r^{(p+1)}(x) $ is continuous. Then as $ T \\rightarrow \\infty $, $$ \\sqrt{T h}\\left[H[\\hat{\\alpha}(\\tau h)-\\alpha(0)]-\\frac{h^{p+1} r^{(p+1)}(0)}{(p+1) !} S_{\\tau}^{-1} C_{\\tau}\\right] \\rightarrow^{d} N\\left(0, \\frac{\\sigma^{2}(0)}{g(0)} S_{\\tau}^{-1} S_{\\tau}^{*} S_{\\tau}^{-1}\\right) $$ where $$ \\alpha(0)=\\left[r(0), r^{(1)}(0), \\ldots, r^{(p)}(0) / p !\\right]^{\\prime} $$ Therefore, as $ T \\rightarrow \\infty $, $$ \\begin{array}{l} \\quad \\sqrt{T h^{2 \\nu+1}}\\left[\\hat{r}^{(\\nu)}(\\tau h)-r^{(\\nu)}(0)-\\frac{h^{p+1-\\nu} r^{(p+1)}(0)}{(p+1) !} \\int_{-\\tau}^{1} u^{p+1} \\tilde{K}_{\\nu, \\tau}(u) d u\\right] \\ \\rightarrow^{d} N\\left(0, \\frac{(\\nu !)^{2} \\sigma^{2}(0)}{g(0)} \\int_{-\\tau}^{1} \\tilde{K}_{\\nu, \\tau}^{2}(u) d u\\right) \\end{array} $$ where the equivalent boundary kernel $$ \\tilde{K}{\\nu, \\tau}(u)=e{\\nu+1}^{\\prime} S_{\\tau}^{-1} P(u) K(u) $$ Proof: The proof is similar to the derivation of the asymptotic MSE for the local polynomial estimator in the interior region. Question: Why is the local polynomial estimator useful in economic applications? First of all, it avoids the well-known boundary problem in smoothed kernel regres","date":"0001-01-01","objectID":"/3.-nonparametric-regression-estimation/:2:4","tags":null,"title":"","uri":"/3.-nonparametric-regression-estimation/"},{"categories":null,"content":"3.2.5 Curse of Dimensionality and Dimension Reduction Like in multivariate probability density estimation, we will also encounter the curse of dimensionality for regression estimation, when the dimension $ d $ of the regressors vector $ X_{t} $ is high. Again, **some simplifying assumptions can be made to reduce the curse of dimensionality**. Some restrictions on the unknown functions of interest may come from economic theory. For example, a demand function must satisfy the property of a homogeneous function of degree zero. Below are a few examples often used in econometrics and time series econometrics: Example [single Index Model]: $$ Y_{t}=m\\left(X_{t}^{\\prime} \\beta^{0}\\right)+\\varepsilon_{t} $$ where $ E\\left(\\varepsilon_{t} | X_{t}\\right)=0, $ the linear combination $ X_{t}^{\\prime} \\beta^{0} $ is a scalar variable, and the functional form $ m(\\cdot) $ is unknown. Often, the interest is inference of unknown parameter $ \\beta^{0} $. See Stoker (1986) for more discussion. Example [Partially Linear Regression Model]: $$ Y_{t}=X_{t}^{\\prime} \\beta^{0}+m\\left(Z_{t}\\right)+\\varepsilon_{t} $$ where $ E\\left(\\varepsilon_{t} | X_{t}, Z_{t}\\right)=0, $ and $ m(\\cdot) $ is an unknown function of $ Z_{t} $ only. Here, the interest is inference of the marginal effect of the economic variables $ X_{t} $. However, one has to consistently estimate the unknown function $ m\\left(Z_{t}\\right) $ in order to obtain consistent estimation of parameter $ \\beta^{0} $. Example [Functional Coefficient Model]: $$ Y_{t}=X_{t}^{\\prime} \\beta\\left(Z_{t}\\right)+\\varepsilon_{t} $$ where $ E\\left(\\varepsilon_{t} | X_{t}, Z_{t}\\right)=0, $ and the parameter vector $ \\beta(\\cdot) $ is an unknown function of $ Z_{t} $ only. Example [Additive Nonlinear Autoregressive Model]: $$ X_{t}=\\sum_{j=1}^{p} \\alpha_{j}\\left(X_{t-j}\\right)+\\varepsilon_{t} $$ where the $ \\alpha_{j}(\\cdot) $ functions are unknown. ","date":"0001-01-01","objectID":"/3.-nonparametric-regression-estimation/:2:5","tags":null,"title":"","uri":"/3.-nonparametric-regression-estimation/"},{"categories":null,"content":"4. Nonparametric Estimation of Time-Varying Models In this section, we consider smoothed nonparametric estimation of time-varying models, where model parameters are deterministic functions of time. For simplicity, we focus on estimating a known time trend function in a time series process. ","date":"0001-01-01","objectID":"/4.-nonparametric-estimation-of-time-varying-models/:0:0","tags":null,"title":"","uri":"/4.-nonparametric-estimation-of-time-varying-models/"},{"categories":null,"content":"4.1 Slow-Varying Time Trend Estimation Suppose we observe a bivariate time series random sample $ \\left{Y_{t}, X_{t}\\right}_{t=1}^{T} $ of size $ T, $ where $$ Y_{t}=m(t / T)+X_{t}, \\quad t=1, \\ldots, T $$ where $ m(\\cdot) $ is a smooth but unknown time-trend function and $ \\left{X_{t}\\right} $ is a strictly stationary process with $ E\\left(X_{t}\\right)=0 $ and auto-covariance function $ \\gamma(j)=\\operatorname{cov}\\left(X_{t}, X_{t-j}\\right) . $ Thus the mean of $ Y_{t} $ is changing over time, $ \\left{Y_{t}\\right} $ is non-stationary. Question: Why is the trend function $ m(\\cdot) $ assumed to be a function of normalized time $ t / T $ rather than time $ t $ only? This is a crucial device for consistent estimation of the trend function $ m(\\cdot) $ as the sample size $ T \\rightarrow \\infty $. *Suppose $ m(\\cdot) $ is a function of time $ t $ only, then when sample size $ T $ increases, new information about future times becomes available, but the information about the function $ m(\\cdot) $ around a given time point, say, $ t_{0}, $ does not increase. Therefore, it is impossible to obtain a consistent estimation of $ m\\left(t_{0}\\right) $. In contrast, with $ m\\left(t_{0} / T\\right) $ as a function of $ t_{0} / T, $ more and more information about $ m(\\cdot) $ in a neighborhood of $ t_{0} / T $ ($ t \\in [t_0-Th,t_0+Th ] $) will become available when $ T $ increases, which ensures consistent estimation of $ m\\left(t_{0} / T\\right) $.* Question: How to estimate the time trend function $ m(t / T) ? $ We can separate the smooth trend from the stochastic component with smoothed nonparametric estimation. Suppose $ m(\\cdot) $ is continuously differentiable on [0,1] up to order $ p, $ and we are interested in estimating the function $ m\\left(t_{0} / T\\right) $ at a time point $ t_{0} $ such that $ t_{0} / T \\rightarrow \\tau_{0} \\in[0,1] $, where $ \\tau_{0} $ is a fixed point. Then by a Taylor series expansion, we have, for all $ t $ such that $ t / T $ lies in a neighborhood of $ \\tau_{0}=t_{0} / T $, i.e. $ |t/T-\\tau_0| \\leq h$, $$ m(t / T)=\\sum_{j=0}^{p} \\frac{1}{\\nu !} m^{(\\nu)}\\left(\\tau_{0}\\right)\\left(\\frac{t-t_{0}}{T}\\right)^{j}+\\frac{1}{(p+1) !} m^{(p+1)}\\left(\\bar{\\tau}_{t}\\right)\\left(\\frac{t-t_{0}}{T}\\right)^{p+1} $$ where $ \\bar{\\tau}{t} $ lies in the segment between $ \\tau{0} $ and $ t / T . $ Thus, we consider local polynomial smoothing by solving the local weighted sum of squared residuals minimization problem $$ \\min {\\alpha} \\sum{t=1}^{T}\\left[Y_{t}-\\sum_{j=0}^{p} \\alpha_{j}\\left(\\frac{t-t_{0}}{T}\\right)^{j}\\right]^{2} K_{h}\\left(\\frac{t-t_{0}}{T}\\right)=\\sum_{t=1}^{T}\\left(Y_{t}-\\alpha^{\\prime} Z_{t}\\right)^{2} W_{t} $$ where $ \\alpha=\\left(\\alpha_{0}, \\alpha_{1}, \\ldots, \\alpha_{p}\\right)^{\\prime}, $ $$ Z_{t}=\\left[1,\\left(\\frac{t-t_{0}}{T}\\right), \\ldots,\\left(\\frac{t-t_{0}}{T}\\right)^{p}\\right]^{\\prime} $$ and $$ W_{t} \\equiv K_{h}\\left(\\frac{t-t_{0}}{T}\\right)=\\frac{1}{h} K\\left(\\frac{t-t_{0}}{T h}\\right) $$ Then the solution for $ \\alpha $ is $$ \\begin{aligned} \\hat{\\alpha} \u0026=\\left(\\sum_{t=1}^{T} Z_{t} W_{t} Z_{t}^{\\prime}\\right)^{-1} \\sum_{t=1}^{T} Z_{t} W_{t} Y_{t} \\ \u0026=\\left(Z^{\\prime} W Z\\right)^{-1} Z^{\\prime} W Y \\end{aligned} $$ In particular, we have $$ \\hat{\\alpha}{\\nu}=e{\\nu+1}^{\\prime} \\hat{\\alpha}, \\quad 0 \\leq \\nu \\leq p $$ where $ e_{\\nu+1} $ is a $ (p+1) \\times 1 $ unit vector with the $ \\nu+1 $ element being unity and all others being zero. Question: What are the asymptotic properties of $ \\hat{\\alpha}_{\\nu} $ for $ 0 \\leq \\nu \\leq p ? $ We first derive the asymptotic MSE of $ \\hat{\\alpha}_{\\nu} . $ Put $$ \\hat{S}=Z^{\\prime} W Z^{\\prime} $$ a nonstochastic $ (p+1) \\times(p+1) $ matrix, whose $ (i, j) $ -element is $$ \\hat{S}{(i, j)}=\\sum{t=1}^{T}\\left(\\frac{t-t_{0}}{T}\\right)^{(i-1)+(j-1)} K_{h}\\left(t-t_{0}\\right) $$ We first decompose $$ \\begin{aligned} \\hat{\\alpha}{\\nu}-\\alpha{\\nu}=\u0026 e_{\\nu+1}^{\\prime} \\hat{S}^{-1} \\sum_{t=1}^{T} Z_{t} W_{t} X_{t} \\ \u0026+e_{\\nu+1}^{\\prime} \\hat{S}^{-1} \\sum_{t=1}","date":"0001-01-01","objectID":"/4.-nonparametric-estimation-of-time-varying-models/:1:0","tags":null,"title":"","uri":"/4.-nonparametric-estimation-of-time-varying-models/"},{"categories":null,"content":"5 Nonparametric Estimation in Frequency Domain Questions: Given a time series random sample $ \\left{X_{t}\\right}_{t=1}^{\\text {of }} $ size $ T $, How to estimate the power spectral density $ h(\\omega) $ of $ \\left{X_{t}\\right} ? $ How to estimate the bispectral density $ b\\left(\\omega_{1}, \\omega_{2}\\right) $ of $ \\left{X_{t}\\right} ? ? ? ? ? ? $ How to estimate the generalized spectral density $ f(\\omega, u, v) $ of $ \\left{X_{t}\\right} ? $ 5.1 Sample Periodogram Suppose $ \\left{X_{t}\\right} $ is a weakly stationary time series with autocovariance function $ \\gamma(j) $ and $ d $ it. Then we can estimate $ \\gamma(j) $ the sample autocovariance function $$ \\hat{\\gamma}(j)=T^{-1} \\sum_{t=j+1}^{T} X_{t} X_{t-j j}, \\quad j=0,\\pm 1, \\ldots, \\pm(T-1) $$ If $ \\mu $ is unknown, we should use the sample autooovariance function $$ \\hat{\\gamma}(j)=T^{-1} \\sum_{t=j j+1}^{t}\\left(X_{t}-\\bar{X}\\right)\\left(X_{t-j j}-\\bar{X}\\right) $$ where $ \\bar{X} $ is the sample mean. The asymptotic analysis is a bit more tedious but the same results can be obtained since the replacement of $ \\mu $ by $ \\bar{X} $ has no impact on the $ e e $ asymptotic results below. In this section, our interest is in consistent estimation of the spectral density function $ h(\\omega) $ based on an observed random sample $ \\left{X_{t}\\right}_{t 1}^{1} $. Recall the spectral density function $$ h(\\omega)=\\frac{1}{2 \\pi} \\sum_{j=-\\infty}^{\\infty} \\gamma(j) e^{-i j \\omega} $$ For a $ \\mathrm{WN}\\left(0, \\sigma^{2}\\right) $ process, the spectral density $$ h(\\omega)=\\frac{1}{2 \\pi} \\gamma(0), \\quad \\omega \\in[-\\pi, \\pi] $$ where $ \\gamma(0)=\\operatorname{var}\\left(X_{t}\\right) . $ In this case, a spectral density estimator is $$ \\hat{h}(\\omega)=\\frac{1}{2 \\pi} \\hat{\\gamma}(0) $$ For an MA(1) process, the spectral density $$ h(\\omega)=\\frac{1}{2 \\pi} \\gamma(0)+\\frac{1}{\\pi} \\gamma(1) \\cos (\\omega) $$ The corresponding spectral estimator is $$ \\hat{h}(\\omega)=\\frac{1}{2 \\pi} \\hat{\\gamma}(0)+\\frac{1}{\\pi} \\hat{\\gamma}(1) \\cos (\\omega) $$ For an ARMA $ (p, q) $ process, the spectral density function $$ h(\\omega)=\\frac{\\sigma^{2}}{2 \\pi}\\left|\\frac{1+\\sum_{j=1}^{q} \\theta_{j} e^{-i j \\omega}}{1-\\sum_{j=1}^{p} \\phi_{j} e^{-i j \\omega}}\\right|^{2} $$ where $ \\sigma^{2}=E\\left(\\varepsilon_{t}^{2}\\right) $ is the variance of innovation $ \\varepsilon_{t} $ A spectral density estimator is $$ \\hat{h}(\\omega)=\\frac{\\hat{\\sigma}^{2}}{2 \\pi}\\left|\\frac{1+\\sum_{j=1}^{q} \\hat{\\theta}_{j} e^{-i j \\omega}}{1-\\sum_{j=1}^{p} \\hat{\\phi}_{j} e^{-i j \\omega}}\\right|^{2} $$ where $ \\left(\\hat{\\theta}_{j}, \\hat{\\phi}_{j}\\right) $ are consistent parameter estimators, and $$ \\hat{\\sigma}^{2}=\\frac{1}{T} \\sum_{t=\\max (p, q)+1}^{T} \\hat{\\varepsilon}_{t}^{2} $$ where $$ \\hat{\\varepsilon}_{t}=X_{t}-\\sum_{j=1}^{p} \\hat{\\phi}_{j} X_{t-j}-\\sum_{j=1}^{q} \\hat{\\theta}_{j} \\hat{\\varepsilon}_{t-j} $$ with the initial values $ \\hat{\\varepsilon}_{t}=0 $ for all $ t \\leq 0 $. For a general linear process (or when we do not know what process $ X_{t} $ is), we may like to use the sample periodogram as a spectral density estimator: $$ \\begin{aligned} \\hat{I}(\\omega) \u0026=\\left.\\frac{1}{2 \\pi T} \\sum_{t=1}^{T} X_{t} e^{i t \\omega}\\right|^{2} \\ \u0026=\\frac{1}{2 \\pi} \\sum_{j=1-T}^{T-1}\\left(1-\\frac{|j|}{T}\\right) \\hat{\\gamma}(j) e^{-i j \\omega} \\ \u0026=\\frac{1}{2 \\pi} \\hat{\\gamma}(0)+\\frac{1}{\\pi} \\sum_{j=1}^{T-1}\\left(1-\\frac{j}{T}\\right) \\hat{\\gamma}(j) \\cos (j \\omega) \\end{aligned} $$ The sample periodogram $ \\hat{I}(\\omega), $ based on the time series random sample $ \\left{X_{t}\\right}_{t=1}^{T}, $ is the squared modulus of the discrete Fourier transform of $ \\left{X_{t}\\right}_{t=1}^{T} $. Unfortunately, this sample periodogram $ \\hat{I}(\\omega) $ is not consistent for the spectral density $ h(\\omega) . $ Why? To explain, let us consider the simplest case when $ \\left{X_{t}\\right} $ is IID. Then we have $ h(\\omega)= $ $$ \\begin{array}{l} \\frac{1}{2 \\pi} \\gamma(0), \\text { and } \\ \\qquad E \\hat{I}(\\omega)=\\frac{1}{2 \\pi} \\gamm","date":"0001-01-01","objectID":"/5.-nonparametric-estimation-in-frequency-domain/:0:0","tags":null,"title":"","uri":"/5.-nonparametric-estimation-in-frequency-domain/"},{"categories":null,"content":"股票多因子模型的回归检验: https://zhuanlan.zhihu.com/p/40984029 ","date":"0001-01-01","objectID":"/multifactormodel/:0:0","tags":null,"title":"","uri":"/multifactormodel/"},{"categories":null,"content":"事件研究法：大投资公司进场对收益率的影响 不要做左侧投资，进行右侧投资，当底部过去之后再追涨 锚定效应使投资者倾向于卖出盈利股票，留下亏损的股票 Turn off the TV and stop surfing the Internet for advice(stop the noise Develop a simple process, one that you can explain to anyone(mine is trend following) Create a security selection process based upon momentum Devise a simple set of prudent and reasonable rules and guidelines Follow your process with discipline; without it, you will fail If you do not have the discipline to do this, seek professional help from someone who does Do not be upset with yourself if you do not have the discipline at times; be proud of yourself for recognizing it Do not confuse luck with skill Listen and learn from the market- it is always right Read this list often 躲过最差的10天，退出市场，风险 技术分析的同质性 期货市场趋势性比较明显，投机者不建议做反向交易 恒生沪深港通AH股溢价指数 ","date":"0001-01-01","objectID":"/transaction/:0:0","tags":null,"title":"","uri":"/transaction/"},{"categories":null,"content":"A reverse merger, also referred to as a “reverse takeover” or a “backdoor listing,” is a process whereby a private company is merged into a public company and the private company’s ownership team takes over control of the combined publicly traded company. At the time of the merger, the public company may be an empty “shell” (i.e., a dormant and non-operating entity) or it may have an existing set of assets and liabilities. Shell values reflect the marginal benefit/cost to the firms engaged in a RM, they are a “shadow price” for the economic cost of gaining access to China’s public equity market. This extreme imbalance between supply and demand gives rise to listed firms’ “shell value,” which reflects a firm’s scarce and valuable public listing status in China. In the first stage, we manually collect detailed information on a comprehensive sample of reverse mergers (RMs) in China that were completed in January 2007 to April 2016. ","date":"0001-01-01","objectID":"/ipo/:0:0","tags":null,"title":"","uri":"/ipo/"},{"categories":null,"content":"屈源育 会计研究 2018 题目：上市公司売价值与资源配置效率 提出了度量上市公司股价中壳价值含量的指标ESV/MV。我们定义一个上市公司的期望壳价值(ESV)等于借壳交易能够实现的壳价值(SV)与被借壳概率(Pr)的乘积，并用期望壳价值(ESV)与市值(MV)比来度量上市公司的壳价值含量(ESV/MV)。 $$ E S V / M V=\\frac{S V \\times \\operatorname{Pr}}{M V} $$ 其中，SV与Pr分别通过以下回归模型估计 $$ \\begin{aligned} \\ln S V_{i t}\u0026=a_{0}+a_{1} \\ln M V_{i-1}+a_{2}\\left(\\ln M V_{i t-1}\\right)^{2}+a_{3} S O E_{i t-1}+e_{i} \\ P r_{i t}\\left(\\right.\\text { Shell }_{i t}=1)\u0026=b_{0}+b_{1} \\text { Rsize }_{i-1}+b_{2} O P_{i t-1}+b_{3} \\text { Sales }_{-} g_{i t-1} \\ \u0026+b_{4} S T_{i t-1}+b_{5} I P O_{-} r e j_{i t-1}+b_{6} \\text { Insider }_{i t-1} \\ \u0026+b_{7} \\text { Return }_{i t-1}+e_{i t} \\end{aligned} $$ 问题：Pr估计中Logistic回归的非平衡问题 ","date":"0001-01-01","objectID":"/ipo/:1:0","tags":null,"title":"","uri":"/ipo/"},{"categories":null,"content":"Latex杂记 ","date":"0001-01-01","objectID":"/latex/:0:0","tags":null,"title":"","uri":"/latex/"},{"categories":null,"content":"1. 公式 ","date":"0001-01-01","objectID":"/latex/:1:0","tags":null,"title":"","uri":"/latex/"},{"categories":null,"content":"1.1 公式环境 Markdown中通过$$可引入Latex公式的编辑环境()，以下用法也能在该环境中实现 a. 行内公式：$公式内容$ b. 行间公式 $$%公式内容 $$ c. 带编号公式 编号随文章equation环境公式个数自动变化 \\begin{equation} 公式内容 \\end{equation} 注：Markdown中equation环境可以用\\\\实现换行，而Latex则不行 d. 公式引用 \\usepackage{hyperref}%开头导入依赖包 \\begin{equation}\\label{公式标签} 公式内容 \\end{equation} \\autoref{公式标签}%正文引用处 注：Markdown中不能使用 e. 多行公式 多行公式共用一个编号 \\usepackage{amsmath}%开头导入依赖包 \\begin{equation} \\begin{split} 公式内容 \\\\ 公示内容... \\end{split} \\end{equation} align环境，每行公式单独编号 \\begin{align} 公式内容 \\\\ 公示内容... \\end{align} align*与align功能相同，但不编号 \\begin{align*} 公式内容 \\\\ 公示内容... \\end{align*} aligned环境，必须依赖于已经构建好的公式环境，例如$$、equation等，常用于构建多列公式 $$\\begin{aligned} %公式内容 \\\\ %公示内容... \\end{aligned} $$ 注： 多行公式中可以在每行\u0026对齐； Markdown中可以直接使用amsmath包的功能； Typora中可以直接用\\\\输入多行公式，但其他平台不允许，建议添加多行公式环境。 f. 分段函数 \\usepackage{amsmath}%开头导入依赖包 \\begin{equation} Y(x)= \\begin{cases} 0\u0026, \\text{if $x\u003c0$}\\\\ x+1\u0026, \\text{if $x\u003e0$}\\\\ 1\u0026, \\text{otherwise.} \\end{cases} \\end{equation} ","date":"0001-01-01","objectID":"/latex/:1:1","tags":null,"title":"","uri":"/latex/"},{"categories":null,"content":"1.2 常用符号 1.2.1关系运算符 输入 输出 大于等于 \\ge或者\\geq $\\ge$ 小于等于 \\le或者\\leq $\\le$ 相似/服从 \\sim $\\sim$ 数学运算符 输入 输出 导数 f'(x) $f'(x)$ 极限 \\lim x $ \\lim x $ 箭头 输入 输出 右箭头 \\to $\\to$ 推导出 \\impliesor\\Rightarrow $ \\Rightarrow $ 被推导出 \\impliedby $\\impliedby$ 不推导出 \\nRightarrow $\\nRightarrow$ 等价于 \\iff $\\iff$ 箭头加文字 \\underrightarrow{p} $\\underrightarrow{p}$ 输入 输出 下划线 \\underline{x} $\\underline{x}$ ","date":"0001-01-01","objectID":"/latex/:1:2","tags":null,"title":"","uri":"/latex/"},{"categories":null,"content":"1.3 公式内部字体大小设置 在Mrakdown中通过以下方式可以调整公式部分字体大小 {\\small a} x+1 而Latex中要更麻烦一些 {\\tiny a} x+1 \\begin{equation} \\text{{\\Large $a$}}x+1 \\end{equation} 常用的字号调整符 符号 效果 \\tiny $\\tiny smallest$ \\scriptsize $\\scriptsize very small$ \\small $\\small small$ \\normalsize $\\normalsize normall$ \\large $\\large large$ \\Large $\\Large Large$ \\LARGE $\\LARGE LARGE$ \\huge $\\huge huge$ \\Huge $\\Huge Huge$ \r ","date":"0001-01-01","objectID":"/latex/:1:3","tags":null,"title":"","uri":"/latex/"},{"categories":null,"content":"1.4 字体 符号 效果 加粗 \\mathbf或\\bf $ \\bf{bf} $ 黑板报粗体 \\mathbb或\\Bbb $\\Bbb{Bbb}$ 粗体希腊字母 \\boldsymbol $\\boldsymbol{bold}$ 手写体/花体 \\mathcal $\\mathcal{MATHCAL}$ ","date":"0001-01-01","objectID":"/latex/:1:4","tags":null,"title":"","uri":"/latex/"},{"categories":null,"content":"1.5 文字相对位置 符号 效果 上加文字 \\overset $ \\overset{0.96}{9.89} $ 下加文字 \\underset $ \\underset{0.96}{9.89} $ 上加文字 \\stackrel $\\stackrel{p}{\\rightarrow}$ 上加括号文字 \\overbrace $\\overbrace{kkk}^{sss}$ 公式人工添加编号 $$ a = b \\tag{1.1} $$ ","date":"0001-01-01","objectID":"/latex/:1:5","tags":null,"title":"","uri":"/latex/"},{"categories":null,"content":"2. 表格 添加居中表格 \\begin{table}[h] \\centering \\begin{tabular}{lll} …… \\end{tabular} \\end{table} 详细参见 Tex用户主页http://www.tug.org/ Texi英文社区https://tex.stackexchange.com/ Alex工作室https://www.latexstudio.net/ 武汉大学黄正华老师的中文Latex安装与使用 北京大学李东风老师的Latex排版心得 《 Latex入门》刘海洋著 《 Alex2e完全学习手册》胡伟著 ","date":"0001-01-01","objectID":"/latex/:2:0","tags":null,"title":"","uri":"/latex/"},{"categories":null,"content":"利用R语言做结构方程模型分析 lavaan简明教程 install.packages(\"lavaan\", dependencies=TRUE) install.packages(\"semPlot\") 基本流程实现 library(lavaan) ## This is lavaan 0.6-7 ## lavaan is BETA software! Please report any bugs. data(PoliticalDemocracy) model \u003c- ' # main regressions/主要回归方程 dem60 ~ ind60 dem65 ~ ind60 + dem60 # latent variable definitions/定义潜变量 ind60 =~ x1 + x2 + x3 dem60 =~ y1 + a*y2 + b*y3 + c*y4 dem65 =~ y5 + a*y6 + b*y7 + c*y8 # residual correlations/方差和协方差 y1 ~~ y5 y2 ~~ y4 + y6 y3 ~~ y7 y4 ~~ y8 y6 ~~ y8 #截距项 dem60 ~ 1 dem65 ~ 1 ' fit \u003c- sem(model, data = PoliticalDemocracy) ## Warning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING: ## The variance-covariance matrix of the estimated parameters (vcov) ## does not appear to be positive definite! The smallest eigenvalue ## (= 1.579160e-17) is close to zero. This may be a symptom that the ## model is not identified. # summary()只适合展示结果 summary(fit, fit.measures = TRUE) ## lavaan 0.6-7 ended normally after 65 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of free parameters 44 ## Number of equality constraints 3 ## ## Number of observations 75 ## ## Model Test User Model: ## ## Test statistic 40.179 ## Degrees of freedom 36 ## P-value (Chi-square) 0.290 ## ## Model Test Baseline Model: ## ## Test statistic 730.654 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.994 ## Tucker-Lewis Index (TLI) 0.991 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1548.818 ## Loglikelihood unrestricted model (H1) -1528.728 ## ## Akaike (AIC) 3179.636 ## Bayesian (BIC) 3274.653 ## Sample-size adjusted Bayesian (BIC) 3145.432 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.039 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.094 ## P-value RMSEA \u003c= 0.05 0.574 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.052 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(\u003e|z|) ## ind60 =~ ## x1 1.000 ## x2 2.180 0.138 15.751 0.000 ## x3 1.818 0.152 11.971 0.000 ## dem60 =~ ## y1 1.000 ## y2 (a) 1.191 0.139 8.551 0.000 ## y3 (b) 1.175 0.120 9.755 0.000 ## y4 (c) 1.251 0.117 10.712 0.000 ## dem65 =~ ## y5 1.000 ## y6 (a) 1.191 0.139 8.551 0.000 ## y7 (b) 1.175 0.120 9.755 0.000 ## y8 (c) 1.251 0.117 10.712 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(\u003e|z|) ## dem60 ~ ## ind60 1.471 0.392 3.750 0.000 ## dem65 ~ ## ind60 0.600 0.226 2.661 0.008 ## dem60 0.865 0.075 11.554 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(\u003e|z|) ## .y1 ~~ ## .y5 0.583 0.356 1.637 0.102 ## .y2 ~~ ## .y4 1.440 0.689 2.092 0.036 ## .y6 2.183 0.737 2.960 0.003 ## .y3 ~~ ## .y7 0.712 0.611 1.165 0.244 ## .y4 ~~ ## .y8 0.363 0.444 0.817 0.414 ## .y6 ~~ ## .y8 1.372 0.577 2.378 0.017 ## ## Intercepts: ## Estimate Std.Err z-value P(\u003e|z|) ## .dem60 0.000 0.238 0.000 1.000 ## .dem65 0.000 0.112 0.000 1.000 ## .x1 5.054 0.084 60.127 0.000 ## .x2 4.792 0.173 27.657 0.000 ## .x3 3.558 0.161 22.066 0.000 ## .y1 5.465 0.164 33.308 0.000 ## .y2 4.256 0.253 16.843 0.000 ## .y3 6.563 0.230 28.592 0.000 ## .y4 4.453 0.174 25.604 0.000 ## .y5 5.136 0.172 29.947 0.000 ## .y6 2.978 0.196 15.219 0.000 ## .y7 6.196 0.195 31.754 0.000 ## .y8 4.043 0.163 24.741 0.000 ## ind60 0.000 ## ## Variances: ## Estimate Std.Err z-value P(\u003e|z|) ## .x1 0.081 0.019 4.182 0.000 ## .x2 0.120 0.070 1.729 0.084 ## .x3 0.467 0.090 5.177 0.000 ## .y1 1.855 0.433 4.279 0.000 ## .y2 7.581 1.366 5.549 0.000 ## .y3 4.956 0.956 5.182 0.000 ## .y4 3.224 0.723 4.458 0.000 ## .y5 2.313 0.479 4.831 0.000 ## .y6 4.968 0.921 5.393 0.000 ## .y7 3.560 0.710 5.018 0.000 ## .y8 3.308 0.704 4.701 0.000 ## ind60 0.449 0.087 5.175 0.000 ## .dem60 3.875 0.866 4.477 0.000 ## .dem65 0.164 0.227 0.725 0.469 # parameterEstimates()会返回一","date":"0001-01-01","objectID":"/structuralequationmodeling/:0:0","tags":null,"title":"结构方程模型","uri":"/structuralequationmodeling/"}]