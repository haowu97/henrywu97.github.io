<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title> | </title><meta name="Description" content="About uBlogger Theme"><meta property="og:title" content="" />
<meta property="og:description" content="4. Important Probability Distributions [toc]
A class of probability measures, say PMF/ PDF $ f(x, \theta) $, indexed by parameter $ \theta, $ where the functional form $ f(\cdot, \cdot) $ is known
The family of probability distributions is called a class of parametric probability distribution models.
4.1 Discrete Probability Distributions 4.1.1 Discrete Uniform Distribution A random variable $ X $ has a discrete uniform $ (1, \mathrm{N}) $ distribution if" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://henrywu97.github.io/4.-important-probability-distributions/" />
<meta property="og:image" content="https://henrywu97.github.io/logo.png"/>
<meta property="article:modified_time" content="2021-02-06T20:37:07+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://henrywu97.github.io/logo.png"/>

<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="4. Important Probability Distributions [toc]
A class of probability measures, say PMF/ PDF $ f(x, \theta) $, indexed by parameter $ \theta, $ where the functional form $ f(\cdot, \cdot) $ is known
The family of probability distributions is called a class of parametric probability distribution models.
4.1 Discrete Probability Distributions 4.1.1 Discrete Uniform Distribution A random variable $ X $ has a discrete uniform $ (1, \mathrm{N}) $ distribution if"/>
<meta name="application-name" content="uBlogger">
<meta name="apple-mobile-web-app-title" content="uBlogger"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://henrywu97.github.io/4.-important-probability-distributions/" /><link rel="prev" href="https://henrywu97.github.io/7.-convergences-and-limit-theorems/" /><link rel="next" href="https://henrywu97.github.io/3.-random-variable-and-univariate-distributions/" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/henrywu97.github.io\/4.-important-probability-distributions\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/henrywu97.github.io\/images\/Apple-Devices-Preview.png",
                            "width":  3200 ,
                            "height":  2048 
                        }],"genre": "posts","wordCount":  3233 ,
        "url": "https:\/\/henrywu97.github.io\/4.-important-probability-distributions\/","dateModified": "2021-02-06T20:37:07+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/henrywu97.github.io\/images\/avatar.png",
                    "width":  528 ,
                    "height":  560 
                }},"author": {
                "@type": "Person",
                "name": "天天.zh-cn"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script>(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('light' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'light' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Henry Wu" class="header-logo"><span class="header-title-pre"><i class='fas fa-pencil-alt fa-fw'></i></span>Henry Wu</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/tags/"> Tags </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="/about/"> About </a><a class="menu-item" href="https://github.com/henrywu97" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i>  </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item language" title="Select Language">English<i class="fas fa-chevron-right fa-fw"></i>
                        <select class="language-select" id="language-select-desktop" onchange="location = this.value;"><option value="/4.-important-probability-distributions/" selected>English</option></select>
                    </a><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Henry Wu" class="header-logo"><span class="header-title-pre"><i class='fas fa-pencil-alt fa-fw'></i></span>Henry Wu</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/tags/" title="">Tags</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="/about/" title="">About</a><a class="menu-item" href="https://github.com/henrywu97" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a><a href="javascript:void(0);" class="menu-item" title="Select Language">English<i class="fas fa-chevron-right fa-fw"></i>
                    <select class="language-select" onchange="location = this.value;"><option value="/4.-important-probability-distributions/" selected>English</option></select>
                </a></div>
    </div>
</header><div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div><main class="main"><div class="container content-article page-toc theme-classic"><div class="toc" id="toc-auto">
            <div class="toc-title">Contents</div>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><div class="header-post">
        <div class="post-title">

            <div class="post-all-meta">
            <div class="breadcrumbs">
    <a href="/">Home </a>/ <a href="/">  </a>
</div>
            <h1 class="single-title animated flipInX"></h1><div class="post-meta">
                <div class="post-meta-line">&nbsp;&nbsp;&nbsp;&nbsp;<i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time class="timeago" datetime="0001-01-01">0001-01-01</time>&nbsp;&nbsp;&nbsp;&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;3233 words
                    &nbsp;&nbsp;&nbsp;&nbsp;<i class="far fa-clock fa-fw"></i>&nbsp;16 minutes</div>
            </div>
        </div>


    </div>

    </div>

        <article class="single toc-start">

        <div class="content-block content-block-first content-block-position">

        <div class="post"><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#41-discrete-probability-distributions">4.1 Discrete Probability Distributions</a>
      <ul>
        <li><a href="#411-discrete-uniform-distribution">4.1.1 Discrete Uniform Distribution</a></li>
        <li><a href="#412-bernoulli-distribution">4.1.2 Bernoulli Distribution</a></li>
        <li><a href="#413-binomial-distribution">4.1.3 Binomial Distribution</a></li>
        <li><a href="#414-geometric-distribution">4.1.4 Geometric Distribution</a></li>
        <li><a href="#415-negative-binomial-distribution">4.1.5 Negative Binomial Distribution</a></li>
        <li><a href="#416-hyper-geometric-distribution">4.1.6 Hyper-geometric Distribution</a></li>
        <li><a href="#417-poisson-distribution">4.1.7 Poisson Distribution</a>
          <ul>
            <li><a href="#poisson-process">Poisson process</a></li>
            <li><a href="#poisson-approximation">Poisson approximation</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#42-continuous-probability-distributions">4.2 Continuous Probability Distributions</a>
      <ul>
        <li><a href="#421-continuous-uniform-distribution">4.2.1 Continuous Uniform Distribution</a></li>
        <li><a href="#422-beta-distribution">4.2.2 Beta Distribution</a></li>
        <li><a href="#423-gamma-distribution">4.2.3 Gamma Distribution</a></li>
        <li><a href="#424-normal-distribution">4.2.4 Normal Distribution</a></li>
        <li><a href="#425-chi-square-distribution">4.2.5 Chi-Square Distribution</a></li>
        <li><a href="#426-the-students-t-distribution">4.2.6 The Student&rsquo;s t Distribution</a></li>
        <li><a href="#427-the-f-distribution">4.2.7 The F distribution</a></li>
        <li><a href="#428-log-normal-distribution">4.2.8 Log-normal Distribution</a></li>
        <li><a href="#429-cauchy-distribution">4.2.9 Cauchy Distribution</a></li>
        <li><a href="#4210-exponential-distribution">4.2.10 Exponential Distribution</a></li>
        <li><a href="#4211-double-exponential-distribution">4.2.11 Double Exponential Distribution</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><span class="post-update">
                    <b>Updated on 2021-02-06</b>
                </span><h1 id="4-important-probability-distributions">4. Important Probability Distributions</h1>
<p>[toc]</p>
<p>A class of probability measures, say PMF/ PDF $ f(x, \theta) $, indexed by parameter $ \theta, $ where the functional form $ f(\cdot, \cdot) $ is known</p>
<p>The family of probability distributions is called a class of parametric probability distribution models.</p>
<h2 id="41-discrete-probability-distributions">4.1 Discrete Probability Distributions</h2>
<h3 id="411-discrete-uniform-distribution">4.1.1 Discrete Uniform Distribution</h3>
<p>A random variable $ X $ has a discrete uniform $ (1, \mathrm{N}) $ distribution if</p>
<p>$$
P(X=x \mid N)=\frac{1}{N}, x=1,2, \cdots, N
$$</p>
<p>where $ N $ is a specified integer. The distribution puts equal mass on each of the outcomes $ 1,2, \cdots, N $.</p>
<p>Since</p>
<p>$$
\sum_{i=1}^{k} i=\frac{k(k+1)}{2} \text { and } \sum_{i=1}^{n} i^{2}=\frac{k(k+1)(2 k+1)}{6}
$$</p>
<p>Hence, we have</p>
<p>$$
E(X)=\sum_{x=1}^{N} x P(X=x \mid N)=\sum_{x=1}^{N} x \frac{1}{N}=\frac{N+1}{2}
$$</p>
<p>and</p>
<p>$$
E X^{2}=\sum_{x=1}^{N} x^{2} \frac{1}{N}=\frac{(N+1)(2 N+1)}{6}
$$</p>
<p>and so</p>
<p>$$
\operatorname{Var}(X)=E X^{2}-(E X)^{2}=\frac{(N+1)(2 N+1)}{6}-\left(\frac{N+1}{2}\right)^{2}=\frac{(N+1)(N-1)}{12}
$$</p>
<h3 id="412-bernoulli-distribution">4.1.2 Bernoulli Distribution</h3>
<p>Bernoulli distribution has wide applications in economics and finance. Model binary data where the outcome only has two possibilities. $ X \sim $ Bernoulli $ (p) $, if $ \mathrm{PMF} $</p>
<p>$$
\begin{aligned}
f_{X}(x) &amp;=\left{\begin{array}{ll}
p &amp; \text { if } x=1 \<br>
1-p &amp; \text { if } x=0
\end{array}\right.\<br>
&amp;=p^{x}(1-p)^{1-x}, \quad x \in{0,1}
\end{aligned}
$$</p>
<p>where the parameter $ p \in(0,1) $</p>
<ul>
<li>$ \boldsymbol{E}\left(X^{k}\right)=p $ for all $ k=1,2, \ldots $</li>
<li>$ \operatorname{Var}(X)=p(1-p) $</li>
<li>$ M_{X}(t)=p e^{t}+1-p, \quad t \in \mathbb{R} $</li>
<li>The parameter $ p $ (the probability that the binary random variable takes value 1 ) will vary across individuals as a function of observable variables, say $ Z $. For example, logit model assume</li>
</ul>
<p>$$
P(X=1 \mid Z)=\frac{1}{1+\exp \left(-\beta^{\prime} Z\right)}
$$</p>
<h3 id="413-binomial-distribution">4.1.3 Binomial Distribution</h3>
<p>If $ X_{1}, X_{2}, \ldots, X_{n} $ are i.i.d. Bernoulli $ (p) $ random variables, then</p>
<p>$$
X=\sum_{i=1}^{n} X_{i}
$$</p>
<p>follows a $ B(n, p) $ distribution. $ B(n, p) $ is the distribution of the number of successes in a sequence of $ n $ independent Bernoulli trials, each of which yields success with probability $ p $. Bernoulli $ (p) $ is in fact $ B(1, p) $.</p>
<p>$ X \sim B(n, p) $, if PMF</p>
<p>$$
f_{X}(x)=\left(\begin{array}{l}
n \<br>
x
\end{array}\right) p^{x}(1-p)^{n-x}, \quad x=0,1,2, \ldots, n
$$</p>
<p>where parameter $ n \in \mathbb{N} $ and parameter $ p \in(0,1) $</p>
<p><strong>Theorem</strong>: For any real numbers $ x $ and $ y $ and integer $ n \geq 0 $,</p>
<p>$$
(x+y)^{n}=\sum_{i=0}^{n}\left(\begin{array}{l}
n \<br>
i
\end{array}\right) x^{i} y^{n-i}
$$</p>
<p>If we take $$ x=p $$ and $$ y=1-p, $$ we get</p>
<p>$$
1=(p+1-p)^{n}=\sum_{i=0}^{n}\left(\begin{array}{l}
n \<br>
i
\end{array}\right) p^{i}(1-p)^{n-i}
$$</p>
<ul>
<li>$ E(X)=n p $</li>
<li>$ \operatorname{Var}(X)=n p(1-p) $</li>
<li>$ M_{X}(t)=\left(p e^{t}+1-p\right)^{n} $</li>
</ul>
<h3 id="414-geometric-distribution">4.1.4 Geometric Distribution</h3>
<p>Geometric distribution is the distribution of <strong>the number of independent Bernoulli trials needed to get one success</strong>,
supposing each trial has a probability $ p $ of success.</p>
<p>$ X \sim \text { Geom }(p) $, if PMF</p>
<p>$$
f_{X}(x)=(1-p)^{x-1} p, \quad x=1,2, \ldots
$$</p>
<p>where parameter $ p \in(0,1) $.</p>
<ul>
<li>$ E(X)=1 / p $</li>
<li>$ \operatorname{Var}(X)=(1-p) / p^{2} $</li>
<li>$ M_{X}(t)=\frac{p e^{t}}{1-(1-p) e^{t}}, \quad t&lt;-\ln (1-p) $</li>
</ul>
<p><strong>Memoryless property</strong>: For integers $ s, t&gt;0 $,</p>
<p>$$
P(X&gt;t+s \mid X&gt;t)=P(X&gt;s)
$$</p>
<p>To show this, first note that</p>
<p>$$
\begin{aligned}
P(X&gt;t) &amp;=1-P(X \leqslant t) \<br>
&amp;=1-\sum_{x=1}^{t} p(1-p)^{x-1} \<br>
&amp;=1-\frac{p-p(1-p)^{t}}{1-(1-p)} \<br>
&amp;=1-\left[1-(1-p)^{t}\right]=(1-p)^{t}
\end{aligned}
$$</p>
<p>and $ P(X&gt;s)=(1-p)^{s}, P(X&gt;t+s)=(1-p)^{t+s} $. Then</p>
<p>$$
\begin{aligned}
P(X&gt;t+s \mid X&gt;t) &amp;=\frac{P(X&gt;t+s, X&gt;t)}{P(X&gt;t)} \<br>
&amp;=\frac{P(X&gt;t+s)}{P(X&gt;t)} \<br>
&amp;=\frac{(1-p)^{t+s}}{(1-p)^{t}} \<br>
&amp;=(1-p)^{s}=P(X&gt;s)
\end{aligned}
$$</p>
<h3 id="415-negative-binomial-distribution">4.1.5 Negative Binomial Distribution</h3>
<p>$ N B(r, p) $ is the distribution of the number of independent Bernoulli trials such that the $ r $-th success occurs at the $ X $-th trial, supposing each trial has a probability $ p $ of success, e.g. A family wish to have some fixed number of boys.</p>
<p>If $ X_{1}, X_{2}, \ldots, X_{r} $ are i.i.d. Geom $ (p) $ random variables, then</p>
<p>$$
X=\sum_{i=1}^{r} X_{i}
$$</p>
<p>follows a $ N B(r, p) $ distribution.</p>
<p>$ X \sim N B(r, p) $, if PMF</p>
<p>$$
\begin{aligned}
f_{X}(x)=&amp;\left[\left(\begin{array}{l}
x-1 \<br>
r-1
\end{array}\right) p^{r-1}(1-p)^{(x-1)-(r-1)}\right] \cdot p \<br>
=&amp;\left(\begin{array}{l}
x-1 \<br>
r-1
\end{array}\right) p^{r}(1-p)^{x-r}, \quad x=r, r+1, \ldots
\end{aligned}
$$</p>
<p>where parameter $ p \in(0,1) $.</p>
<ul>
<li>$ E(X)=r / p $</li>
<li>$ \operatorname{Var}(X)=r(1-p) / p^{2} $</li>
<li>$ M_{X}(t)=\left[\frac{p e^{t}}{1-(1-p) e^{t}}\right]^{r}, \quad t&lt;-\ln (1-p) $</li>
</ul>
<p><strong>Example</strong>: Suppose that in a population of fruit flies, we are interested in the proportion having vestigial wings and decide to sample until we have found 100 such flies. The probability that we will have to examine at least $ N $ flies is</p>
<p>$$
\begin{aligned}
P(X \geq N) &amp;=\sum_{x=N}^{\infty}\left(\begin{array}{c}
x-1 \<br>
99
\end{array}\right) p^{100} (1-p)^{x-100} \<br>
&amp;=1-\sum_{x=100}^{N-1}\left(\begin{array}{c}
x-1 \<br>
99
\end{array}\right) p^{100}(1-p)^{x-100}
\end{aligned}
$$</p>
<h3 id="416-hyper-geometric-distribution">4.1.6 Hyper-geometric Distribution</h3>
<p>Large urn with $ N $ balls that are identical in every way except that $ M $ are red and $ N-M $ are green. We reach in, blindfolded, and select $ K $ balls at random. What is the probability that exactly $ x $ of the balls are red?</p>
<p>The numbers of</p>
<ul>
<li>size $ K $ that can be drawn from $ N $ balls is $ C^K_N $.</li>
<li>choosing $ x $ red balls is $ C_M^x $</li>
<li>choosing $ K-x $ green balls is $ C^{K-x}_{N-M} $</li>
</ul>
<p>Then $ X $ has a hyper-geometric distribution given by</p>
<p>$$
P(X=x \mid N, M, K)=\frac{ C_M^x C^{K-x}_{N-M} }{ C^K_N }
$$</p>
<p>Support: $ M \geq x $ and $ N-M \geq K-x \Rightarrow M-(N-K) \leq x \leq M$</p>
<p>Expectation</p>
<p>$$
E X=\sum_{x=0}^{K} x \frac{\left(\begin{array}{c}
M \<br>
x
\end{array}\right)\left(\begin{array}{c}
N-M \<br>
K-x
\end{array}\right)}{\left(\begin{array}{c}
N \<br>
K
\end{array}\right)}
$$</p>
<p>by using the fact $ \sum_{x=1}^{K} \frac{\left(\begin{array}{c}M-1 \ x-1\end{array}\right)\left(\begin{array}{c}N-M \ K-x\end{array}\right)}{\left(\begin{array}{c}N-1 \ K-1\end{array}\right)}=1, $ we have</p>
<p>$$
E X=\frac{K M}{N} \sum_{x=1}^{K} \frac{\left(\begin{array}{c}
M-1 \<br>
x-1
\end{array}\right)\left(\begin{array}{c}
N-M \<br>
K-x
\end{array}\right)}{\left(\begin{array}{c}
N-1 \<br>
K-1
\end{array}\right)}=\frac{K M}{N}
$$</p>
<p>Variance</p>
<p>$$
\operatorname{Var}(X)=\frac{K M}{N}\left(\frac{(N-M)(N-K)}{N(N-1)}\right)
$$</p>
<h3 id="417-poisson-distribution">4.1.7 Poisson Distribution</h3>
<p>$ X \sim $ Poisson $ (\lambda) $, if $ \mathrm{PMF} $</p>
<p>$$
f_{X}(x)=e^{-\lambda} \frac{\lambda^{x}}{x !}, \quad x=0,1,2, \ldots
$$</p>
<p>where the intensity parameter $ \lambda&gt;0 $.</p>
<p>Recall the Taylor series expansion of $ e^{y}, e^{y}=\sum_{i=0}^{\infty} \frac{y^{i}}{i !} $.</p>
<p>$$
\sum_{x=0}^{\infty} P(X=x \mid \lambda)=e^{-\lambda} \sum_{x=0}^{\infty} \frac{\lambda^{x}}{x !}=e^{-\lambda} e^{\lambda}=1
$$</p>
<ul>
<li>Mean of $ X $</li>
</ul>
<p>$$
\begin{aligned}
E X &amp;=\sum_{x=0}^{\infty} x \frac{e^{-\lambda} \lambda^{x}}{x !}=\sum_{x=1}^{\infty} x \frac{e^{-\lambda} \lambda^{x}}{x !} \<br>
&amp;=\lambda e^{-\lambda} \sum_{x=1}^{\infty} \frac{\lambda^{x-1}}{(x-1) !}=\lambda
\end{aligned}
$$</p>
<ul>
<li>$ \operatorname{Var}(X)=\lambda $</li>
<li>$ \boldsymbol{M}_{X}(t)=e^{\lambda\left(e^{t}-1\right)} $</li>
</ul>
<h4 id="poisson-process">Poisson process</h4>
<p>Counting <strong>the number of random events</strong> starting from $ t=0 $. $ N(t)= $ the number of events that have occurred during the time period $ [0, t] $.</p>
<p>Assumptions:</p>
<ol>
<li>Stationary: For all $ m \geqslant 0 $ and for any time intervals $ \Delta_{1}=\Delta_{2}, P\left(m \text { events in } \Delta_{1}\right)=P\left(m \text { events in } \Delta_{2}\right) $</li>
<li>Independent increments: For all $ m \geqslant 0 $ and for any time interval $ (t, t+s], P(m \text { events in }(t, t+s]) $ is independent of how many events have occurred earlier or how they have occurred;</li>
<li>Sequencing: The occurring of two or more events in a very small time interval is practically impossible, i.e. $ \lim _{\Delta \rightarrow 0} P(N(\Delta)&gt;1) / \Delta=0 $</li>
</ol>
<p><strong>A stochastic process $ {N(t)} $ satisfying these three assumptions is called a stationary Poisson process.</strong></p>
<p>Suppose $ N(0)=0, $ then there exists $ \lambda&gt;0 $ such that
$$
P(N(t)=m)=\frac{(\lambda t)^{m} e^{-\lambda t}}{m !}, \quad m=0,1,2, \ldots
$$</p>
<p>that is, for $ t&gt;0, N(t) \sim $ Poisson $ (\lambda t) $</p>
<p>$ \lambda $ is the <strong>average number of events during a unit time period</strong>.</p>
<h4 id="poisson-approximation">Poisson approximation</h4>
<p>We can use a Poisson $ (\lambda) $ distribution to approximate a $ B(n, p) $ distribution when $ p \rightarrow 0 $ and $ n p \rightarrow \lambda $ as $ n \rightarrow \infty $.</p>
<p>The MGF of the binomial distribution $ B(n, p) $ is
$$
M_{B}(t)=\left(p e^{t}+1-p\right)^{n}=\left[1+\frac{n p \cdot\left(e^{t}-1\right)}{n}\right]^{n}
$$</p>
<p>For every $ t, $ when $ n \rightarrow \infty $ and $ n p \rightarrow \lambda, $ we have</p>
<p>$$
M_{B}(t) \rightarrow e^{\lambda\left(e^{t}-1\right)}=M_{P}(t)
$$</p>
<p>which is the MGF of Poisson $ (\lambda) $.</p>
<p>We can also show that the PMF of the binomial distribution
$ B(n, p) $ converges to the PMF of Poisson distribution.</p>
<p><strong>Example</strong>: A typesetter on average makes one error in every 500 words typeset. What is the probability that there will be no more than two errors in 1500 words?</p>
<p>Use binomial distribution: Assume that making error in typing a word follows a Bernoulli distribution with $ p=1 / 500 $. Then the number of errors in 1500 words, $ X, $ follows a binomial distribution B(1500, p). So,</p>
<p>$$
P(X \leqslant 2)=\sum_{x=0}^{2}
\left(\begin{array}{c}
1500 \<br>
x
\end{array}\right)
\left(\frac{1}{500}\right) \times\left(\frac{499}{500}\right)^{1500-x} \approx 0.4230
$$</p>
<p>Use Poisson approximation: Let $ \lambda=n p=3 $, then</p>
<p>$$
P(X \leqslant 2) \approx \sum_{x=0}^{2} \frac{\lambda^{x} e^{-\lambda}}{x !}=e^{-3}\left(1+\frac{3}{1 !}+\frac{3^{2}}{2 !}\right) \approx 0.4232
$$</p>
<h2 id="42-continuous-probability-distributions">4.2 Continuous Probability Distributions</h2>
<h3 id="421-continuous-uniform-distribution">4.2.1 Continuous Uniform Distribution</h3>
<p>$ X \sim U[a, b] $, if $ \mathrm{PDF} $
$$
f_{X}(x)=\left{\begin{array}{ll}
\frac{1}{b-a} &amp; \text { if } a \leqslant x \leqslant b \<br>
0 &amp; \text { otherwise }
\end{array}\right.
$$</p>
<ul>
<li>$ \boldsymbol{E}(X)=\frac{a+b}{2}, \quad E\left(X^{k}\right)=\frac{1}{b-a} \cdot \frac{b^{k+1}-a^{k+1}}{k+1} $</li>
<li>$ \operatorname{Var}(X)=\frac{1}{12}(b-a)^{2} $</li>
<li>$ M_{X}(t)=\frac{1}{t(b-a)}\left(e^{t b}-e^{t a}\right), \quad t \in \mathbb{R} $</li>
</ul>
<h3 id="422-beta-distribution">4.2.2 Beta Distribution</h3>
<p>$ X \sim \operatorname{Beta}(\alpha, \beta) $, if $ \mathrm{PDF} $</p>
<p>$$
f_{X}(x)=\frac{1}{B(\alpha, \beta)} x^{\alpha-1}(1-x)^{\beta-1}, \quad 0 \leqslant x \leqslant 1
$$</p>
<p>where $ \alpha&gt;0, \beta&gt;0, $ and $ B(\alpha, \beta) $ is the Beta function</p>
<p>$$
B(\alpha, \beta)=\int_{0}^{1} t^{\alpha-1}(1-t)^{\beta-1} \mathrm{d} t
$$</p>
<p>Moments</p>
<p>$$
\begin{aligned}
E X^{n} &amp;=\frac{1}{B(\alpha, \beta)} \int_{0}^{1} x^{n} x^{\alpha-1}(1-x)^{\beta-1} d x \<br>
&amp;=\frac{1}{B(\alpha, \beta)} \int_{0}^{1} x^{\alpha+n-1}(1-x)^{\beta-1} d x
\end{aligned}
$$</p>
<p>We now recognize the integrand as the kernel of a beta $ (\alpha+n, \beta) $ PDF, which means</p>
<p>$$
E\left(X^{n}\right)=\frac{B(\alpha+n, \beta)}{B(\alpha, \beta)}=\frac{\Gamma(\alpha+n) \Gamma(\alpha+\beta)}{\Gamma(\alpha+\beta+n) \Gamma(\alpha)}
$$</p>
<p>Set $ \mathrm{n}=1, \mathrm{n}=2, $ We then have</p>
<p>$$
E X=\frac{\alpha}{\alpha+\beta} \text { and } \operatorname{Var}(X)=\frac{\alpha \beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}
$$</p>
<p>And $ \boldsymbol{M}<em>{X}(t)=1+\sum</em>{j=1}^{\infty}\left(\prod_{i=0}^{j-1} \frac{\alpha+i}{\alpha+\beta+i}\right) \frac{t^{j}}{j !}, \quad t \in \mathbb{R} $.</p>
<h3 id="423-gamma-distribution">4.2.3 Gamma Distribution</h3>
<p>$ X \sim \text { Gamma }(\alpha, \beta) $, if $ \mathrm{PDF} $</p>
<p>$$
f_{X}(x)=\left{\begin{array}{ll}
\frac{1}{\beta^{\alpha} \Gamma(\alpha)} x^{\alpha-1} e^{-x / \beta} &amp; \text { if } x&gt;0 \<br>
0 &amp; \text { if } x \leqslant 0
\end{array}\right.
$$</p>
<p>where $ \alpha&gt;0 $ is the shape parameter, $ \beta&gt;0 $ is the scale parameter, and $ \Gamma(\alpha)=\int_{0}^{\infty} t^{\alpha-1} e^{-t} \mathrm{d} t $ is the **Gamma function**.</p>
<p><strong>Remark: Gamma function is an extension of the factorial function</strong>.</p>
<ol>
<li>$ \Gamma(k)=(k-1) ! $ if $ k \in \mathbb{N} $</li>
<li>$ \Gamma(\alpha+1)=\alpha \Gamma(\alpha) $</li>
<li>$ \Gamma(1 / 2)=\sqrt{\pi} $</li>
</ol>
<p>Mean</p>
<p>$$
E X=\frac{1}{\Gamma(\alpha) \beta^{\alpha}} \int_{0}^{\infty} x x^{\alpha-1} e^{-x / \beta} d x
$$</p>
<p>Since $ \int_{0}^{\infty} x^{\alpha-1} e^{-x / \beta} d x=\Gamma(\alpha) \beta^{\alpha} $ (property of PDF),</p>
<p>$$
\begin{aligned}
E X &amp;=\frac{1}{\Gamma(\alpha) \beta^{\alpha}} \int_{0}^{\infty} x^{\alpha} e^{-x / \beta} d x \<br>
&amp;=\frac{1}{\Gamma(\alpha) \beta^{\alpha}} \Gamma(\alpha+1) \beta^{\alpha+1} \<br>
&amp;=\frac{\alpha \Gamma(\alpha) \beta}{\Gamma(\alpha)} \<br>
&amp;=\alpha \beta
\end{aligned}
$$</p>
<p>Moreover, we have</p>
<ul>
<li>$ E\left(X^{k}\right)=\beta^{k} \prod_{i=0}^{k-1}(\alpha+i) $</li>
<li>$ \operatorname{Var}(X)=\alpha \beta^{2} $</li>
<li>$ \boldsymbol{M}_{X}(t)=(1-\beta t)^{-\alpha}, \quad t&lt;1 / \beta $</li>
</ul>
<p>If $ X_{1}, X_{2}, \ldots, X_{n} $ are independent and $ X_{i} \sim \operatorname{Gamma}\left(\alpha_{i}, \beta\right) $ for $ i=1,2, \ldots, n, $ then</p>
<p>$$
\sum_{i=1}^{n} X_{i} \sim \text { Gamma }\left(\sum_{i=1}^{n} \alpha_{i}, \beta\right)
$$</p>
<p>If $ X \sim \operatorname{Gamma}(\alpha, \beta) $ and $ c&gt;0, $ then</p>
<p>$$
c X \sim \text { Gamma }(\alpha, c \beta)
$$</p>
<p>Gamma-Poisson Relation, If $ X $ is a Gamma $ (\alpha, \beta) $ random variable, where $ \alpha $ is an integer, then for any $ X $</p>
<p>$$
P(X \leq x)=P(Y \geq \alpha)
$$</p>
<p>If we set $ \alpha=p / 2, $ where $ p $ is an integer, and $ \beta=2, $ then the gamma PDF becomes</p>
<p>$$
f(x \mid p)=\frac{1}{\Gamma(p / 2) 2^{p / 2}} x^{p / 2-1} e^{-x / 2}
$$</p>
<p>which is the <strong>Chi-squared PDF</strong> with p degrees of freedom.</p>
<p>When we set $ \alpha=1, $ we then have</p>
<p>$$
f(x \mid \beta)=\frac{1}{\beta} e^{-x / \beta}, 0&lt;x&lt;\infty
$$</p>
<p>which is an <strong>exponential PDF</strong>. It is also memoryless (lifetimes).</p>
<h3 id="424-normal-distribution">4.2.4 Normal Distribution</h3>
<p>Importance of the normal distribution: Central Limit Theorem
Under certain conditions, the <strong>sample average</strong> of sufficiently
large number of i.i.d. random variables, <strong>will be approximately</strong>
<strong>normally distributed</strong>, regardless of the underlying distribution.</p>
<p>$ X \sim N\left(\mu, \sigma^{2}\right) $, if $ \mathrm{PDF} $</p>
<p>$$
f_{X}(x)=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}, \quad x \in \mathbb{R}
$$</p>
<p>where $ \mu \in \mathbb{R} $ and $ \sigma^{2}&gt;0 $, and</p>
<p>$$
E(X)=\mu, \quad \operatorname{Var}(X)=\sigma^{2}
$$</p>
<p>Standardize normal distributions:</p>
<p>$$
\begin{array}{l}
X \sim N\left(\mu, \sigma^{2}\right) \Rightarrow \frac{X-\mu}{\sigma} \sim N(0,1) \<br>
X \sim N(0,1) \Rightarrow \mu+\sigma X \sim N\left(\mu, \sigma^{2}\right)
\end{array}
$$</p>
<p>The MGF of $ Y \sim N(0,1) $ is</p>
<p>$$
\begin{aligned}
M_{Y}(t)&amp;=E\left(e^{t Y}\right)=\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{t y} e^{-y^{2} / 2} d y \<br>
&amp;=\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}\left(y^{2}-2 t y+t^{2}\right)+\frac{1}{2} t^{2}} d y \<br>
&amp;=e^{t^{2} / 2} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2}(y-t)^{2}} \mathrm{d} y=e^{t^{2} / 2}
\end{aligned}
$$</p>
<p>Then the MGF of $ X=\mu+\sigma Y \sim N\left(\mu, \sigma^{2}\right) $ is</p>
<p>$$
\begin{aligned}
M_{X}(t)&amp;=E\left(e^{t X}\right)=E \left[e^{t(\mu+\sigma Y)}\right]=e^{\mu t} E\left(e^{\sigma t Y}\right) \<br>
&amp;=e^{\mu t} M_{Y}(\sigma t)=e^{\mu t} e^{\sigma^{2} t^{2} / 2}=e^{\mu t+\sigma^{2} t^{2} / 2}
\end{aligned}
$$</p>
<p>All odd central moments, $ E(X-\mu)^{2 k-1} $ for $ k \in \mathbb{N}, $ are $ 0 . $</p>
<p>Even central moments are given by</p>
<p>$$
E(X-\mu)^{2 k}=\sigma^{2 k}(2 k-1) ! !, \quad k \in \mathbb{N}
$$</p>
<p>Calculate the even central moments</p>
<ol>
<li>Brute-force integration</li>
</ol>
<p>$$
\int_{-\infty}^{\infty}(x-\mu)^{2 k} \cdot \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} \mathrm{d} x
$$</p>
<ol start="2">
<li>Differentiate the MGF of $ Y=X-\mu $ and evaluate $ M_{Y}^{(2 k)}(0) $</li>
<li>Using Stein&rsquo;s Lemma</li>
</ol>
<p><strong>Lemma (Stein&rsquo;s Lemma)</strong>: Suppose $ X \sim N\left(\mu, \sigma^{2}\right), $ and $ g(\cdot) $ is a differentiable function satisfying $ E\left|g^{\prime}(X)\right|&lt;\infty . $ Then</p>
<p>$$
E[g(X)(X-\mu)]=\sigma^{2} E\left[g^{\prime}(X)\right]
$$</p>
<p>Apply Stein&rsquo;s Lemma to calculate $ E(X-\mu)^{4} $ :</p>
<p>Let $ g(X)=(X-\mu)^{3}, $ then</p>
<p>$$
E(X-\mu)^{4}=E\left[(X-\mu)^{3}(X-\mu)\right]=E[g(X)(X-\mu)]
$$</p>
<p>By Stein&rsquo;s Lemma,</p>
<p>$$
E(X-\mu)^{4}=\sigma^{2} E\left[3(X-\mu)^{2}\right]=3 \sigma^{4}
$$</p>
<p>Thus, the kurtosis of normal distribution is $ 3 . $</p>
<p><strong>Normal Approximation</strong>: Let $ X \sim $Binomial $ (25,0.6) $. We can approximate $ X $ with a normal random variable, $ Y, $ with mean $ \mu=25(0.6)=15 $ and standard deviation $ \sigma=(25(0.6)(0.4))^{1 / 2}=2.45 . $ Thus</p>
<p>$$
P(X \leq 13) \approx P(Y \leq 13)=P\left(Z \leq \frac{13-15}{2.45}\right)=P(Z \leq-0.82)=0.206
$$</p>
<p>The exact binomial calculation gives</p>
<p>$$
P(X \leq 13)=\sum_{x=0}^{13}\left(_{x}^{25}\right)(0.6)^{x}(0.4)^{25-x}=0.267
$$</p>
<h3 id="425-chi-square-distribution">4.2.5 Chi-Square Distribution</h3>
<p>$ \chi_{\nu}^{2} $ is the distribution of the sum of $ \nu $ squared independent
standard normal random variables. $ X \sim \chi_{\nu}^{2} $, if $ \mathrm{PDF} $</p>
<p>$$
f_{X}(x)=\left{\begin{array}{ll}
\frac{1}{2^{\nu / 2} \Gamma(\nu / 2)} \times^{\frac{\nu}{2}-1} e^{-\frac{x}{2}} &amp; \text { if } x&gt;0 \<br>
0 &amp; \text { if } x \leqslant 0
\end{array}\right.
$$</p>
<p>where $ \nu&gt;0 $ is the degree of freedom.</p>
<ul>
<li>$ \chi_{\nu}^{2} $ is Gamma $ (\nu / 2,2) $</li>
<li>$ E(X)=\nu $</li>
<li>$ \operatorname{Var}(X)=2 \nu $</li>
<li>$ M_{X}(t)=(1-2 t)^{-\nu / 2}, \quad t&lt;\frac{1}{2} $</li>
</ul>
<h3 id="426-the-students-t-distribution">4.2.6 The Student&rsquo;s t Distribution</h3>
<p>Let $ X \sim N(0,1) $ and $ Y_{n} \sim \chi_{n}^{2}, $ where $ X $ and $ Y_{n} $ are independent. Then the distribution of the random variable</p>
<p>$$
T_{n}=\frac{X}{\sqrt{Y_{n} / n}}
$$</p>
<p>is called <strong>the (Student&rsquo;s) t distribution</strong> with n degrees of freedom and is denoted by $ t_{n} $.</p>
<p>The conditional density $ h_{n}(x \mid y) $ of $ T_{n} $ given $ Y_{n}=y $ is the density of the $ N(1, n / y) ; $ hence, the unconditional density of $ T_{n} $ is</p>
<p>$$
\begin{aligned}
h_{n}(x) &amp;=\frac{\exp \left(-\left(x^{2} / n\right) y / 2\right)}{\sqrt{n / y} \sqrt{2 \pi}} \times \frac{y^{n / 2-1} \exp (-y / 2)}{\Gamma(n / 2) 2^{n / 2}} d y \<br>
&amp;=\frac{\Gamma((n+1) / 2)}{\sqrt{n \pi} \Gamma(n / 2)\left(1+x^{2} / n\right)^{(n+1) / 2}}
\end{aligned}
$$</p>
<ul>
<li>The expectation of $ T_{n} $ does not exist if $ n=1, $ and is zero for $ n \geq 2 $ by symmetry.</li>
<li>The variance of $ T_{n} $ is infinite for $ n=2, $ whereas for $ n \geq 3 $ var $ \left(T_{n}\right)=E\left(T_{n}^{2}\right)=\frac{n}{n-2} $</li>
</ul>
<h3 id="427-the-f-distribution">4.2.7 The F distribution</h3>
<p>Let $ X \sim \chi_{m}^{2} $ and $ Y_{n} \sim \chi_{n}^{2}, $ where $ X_{m} $ and $ Y_{n} $ are independent. Then the distribution of the random variable</p>
<p>$$
F=\frac{X_{m} / m}{Y_{n} / n}
$$</p>
<p>is said to be $ \mathrm{F} $ with $ \mathrm{m} $ and $ \mathrm{n} $ degrees of freedom and is denoted by $ F_{m, n} $.</p>
<p>Its' density is</p>
<p>$$
h_{m, n}(x)=\frac{m^{m / 2} \Gamma(m / 2+n / 2) x^{m / 2-1}}{n^{m / 2} \Gamma(m / 2) \Gamma(n / 2)[1+m x / n]^{m / 2+n / 2}}
$$</p>
<p>Expectation</p>
<p>$$
E(F)=\left{\begin{array}{ll}
n / n-2, &amp; \text { if } n \geq 3 \<br>
=\infty, &amp; \text { if } n=1,2
\end{array}\right.
$$
Variance</p>
<p>$$
\operatorname{var}(F)=\left{\begin{array}{ll}
\frac{2 n^{2}(m+n-4)}{m(n-2)^{2}(n-4)}, &amp; \text { if } n \geq 5 \<br>
=\infty, &amp; \text { if } n=3,4 \<br>
=\text { not defined }, &amp; \text { if } n=1,2
\end{array}\right.
$$</p>
<h3 id="428-log-normal-distribution">4.2.8 Log-normal Distribution</h3>
<p>$ X \sim $ Log-normal $ \left(\mu, \sigma^{2}\right) $, if $ \mathrm{PDF} $</p>
<p>$$
f_{X}(x)=\left{\begin{array}{ll}
\frac{1}{\sqrt{2 \pi} \sigma x} e^{-\frac{(\ln x-\mu)^{2}}{2 \sigma^{2}}} &amp; \text { if } x&gt;0 \<br>
0 &amp; \text { if } x \leqslant 0
\end{array}\right.
$$</p>
<p>If $ Y \sim N\left(\mu, \sigma^{2}\right), $ then $ X=e^{Y} \sim $ Log-normal $ \left(\mu, \sigma^{2}\right) $</p>
<p>Calculate the moments of log-normal distribution using the MGF of normal distribution</p>
<p>$$
E\left(X^{k}\right)=E\left(e^{k Y}\right)=M_{Y}(k)=e^{k \mu+\sigma^{2} k^{2} / 2}
$$</p>
<p>So $ E(X)=e^{\mu+\sigma^{2} / 2} $, and $ \operatorname{Var}(X)=e^{2 \mu+\sigma^{2}}\left(e^{\sigma^{2}}-1\right) $. But MGF does not exist.</p>
<p><strong>Example</strong>: Assume</p>
<p>$$
X_{t}=X_{t-1}\left(1+Y_{t}\right)
$$</p>
<p>and $ \left{Y_{t}\right} $ is a sequence of i.i.d. random variables such that $ Y_{t} $ is independent of $ X_{t-1} $. Then</p>
<p>$$
X_{T}=X_{0} \prod_{t=1}^{T}\left(1+Y_{t}\right) \Rightarrow \ln X_{T}=\ln X_{0}+\sum_{t=1}^{T} \ln \left(1+Y_{t}\right)
$$</p>
<p>By the CLT, for sufficiently large $ T, \sum_{t=1}^{T} \ln \left(1+Y_{t}\right) $ will be approximately normally distributed, after suitable standardization. Thus, $ \prod_{t=1}^{n}\left(1+Y_{t}\right) $ and $ X_{T} $ are approximately log-normally distributed.</p>
<h3 id="429-cauchy-distribution">4.2.9 Cauchy Distribution</h3>
<p>$ X \sim \operatorname{Cauchy}(\mu, \sigma) $, if PDF</p>
<p>$$
f_{X}(x)=\frac{1}{\pi \sigma}\left[1+\left(\frac{x-\mu}{\sigma}\right)^{2}\right]^{-1}, \quad x \in \mathbb{R}
$$</p>
<p>where $ \mu $ is the location parameter and $ \sigma&gt;0 $ is the scale parameter.</p>
<p>Cauchy distribution is symmetric about $ \mu $ but <strong>has heavier tails than normal distribution</strong>.</p>
<ul>
<li>$ E\left(X^{k}\right) $ does not exist for any $ k \geqslant 1 $</li>
<li>MGF does not exist.</li>
</ul>
<h3 id="4210-exponential-distribution">4.2.10 Exponential Distribution</h3>
<p>$ X \sim \operatorname{Exp}(\beta) $, if $ \mathrm{PDF} $</p>
<p>$$
f_{X}(x)=\left{\begin{array}{ll}
\frac{1}{\beta} e^{-x / \beta} &amp; \text { if } x&gt;0 \<br>
0 &amp; \text { if } x \leqslant 0
\end{array}\right.
$$</p>
<p>where $ \beta&gt;0 $ is the scale parameter.</p>
<ul>
<li>$ \operatorname{Exp}(\beta) $ is Gamma $ (1, \beta) $</li>
<li>$ E(X)=\beta $</li>
<li>$ \operatorname{Var}(X)=\beta^{2} $</li>
<li>$ \boldsymbol{M}_{X}(t)=\frac{1}{1-\beta t}, \quad t&lt;1 / \beta $</li>
</ul>
<p><strong>Exponential distribution is the only continuous distribution with memoryless property</strong>.</p>
<p>In a Poisson process with intensity/rate $ \lambda, $ <strong>the waiting time between consecutive events</strong> follows $ \operatorname{Exp}(1 / \lambda) $ distribution.</p>
<p>Exponential distribution is the only continuous distribution that has a constant hazard rate.</p>
<p>Remark: Hazard rate or hazard function is defined as</p>
<p>$$
\begin{aligned}
\lambda(x) &amp;=\lim <em>{\Delta x \rightarrow 0^{+}} \frac{P(X \leqslant x+\Delta x \mid X&gt;x)}{\Delta x} \<br>
&amp;=\lim <em>{\Delta x \rightarrow 0^{+}} \frac{P(x&lt;X \leqslant x+\Delta x)}{P(X&gt;x) \cdot \Delta x} \<br>
&amp;=\frac{1}{P(X&gt;x)} \lim <em>{\Delta x \rightarrow 0^{+}} \frac{\int</em>{x}^{x+\Delta x} f</em>{X}(u) \mathrm{d} u}{\Delta x} \<br>
&amp;=\frac{f</em>{X}(x)}{P(X&gt;x)}=\frac{f_{X}(x)}{1-F_{X}(x)}
\end{aligned}
$$</p>
<h3 id="4211-double-exponential-distribution">4.2.11 Double Exponential Distribution</h3>
<p>The double exponential distribution is formed by <strong>reflecting the exponential distribution around its mean</strong>, The PDF is given by</p>
<p>$$
f(x \mid \mu, \sigma)=\frac{1}{2 \sigma} e^{-|x-\mu| / \sigma},-\infty&lt;x&lt;\infty,-\infty&lt;\mu&lt;\infty, \sigma&gt;0
$$</p>
<p>Mean and Variance</p>
<p>$$
E X=\mu \text { and } \operatorname{Var}(X)=2 \sigma^{2}
$$</p>
</div>

            <div class="post"><div class="post-info-share">
    <span><a class="share-icon share-twitter" href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://henrywu97.github.io/4.-important-probability-distributions/" data-title="" data-via="xxxx"><i class="fab fa-twitter fa-fw"></i></a><a class="share-icon share-facebook" href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://henrywu97.github.io/4.-important-probability-distributions/"><i class="fab fa-facebook-square fa-fw"></i></a><a class="share-icon share-line" href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://henrywu97.github.io/4.-important-probability-distributions/" data-title=""><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@2.14.0/icons/line.svg"></i></a><a class="share-icon share-weibo" href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://henrywu97.github.io/4.-important-probability-distributions/" data-title=""><i class="fab fa-weibo fa-fw"></i></a></span>
</div>
<div class="footer-post-author"style="border-radius: 10px;border-bottom: solid 2px #ececec">
    <div class="author-avatar"><a href="" target="_blank"><img alt="" src="" border="0"></a></div>
    <div class="author-info">
        <div class="name"><a href="" target="_blank"></a></div>
        <div class="number-posts"></span></div>
    </div>
</div><div class="post-footer" id="post-footer"><div class="post-navigation"><div class="post-nav-box nav-box-prev">
            <a class="nav-box" href="/7.-convergences-and-limit-theorems/"><span class="nav-icon"><svg aria-hidden="true" data-prefix="fas" data-icon="chevron-circle-left" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256 504C119 504 8 393 8 256S119 8 256 8s248 111 248 248-111 248-248 248zM142.1 273l135.5 135.5c9.4 9.4 24.6 9.4 33.9 0l17-17c9.4-9.4 9.4-24.6 0-33.9L226.9 256l101.6-101.6c9.4-9.4 9.4-24.6 0-33.9l-17-17c-9.4-9.4-24.6-9.4-33.9 0L142.1 239c-9.4 9.4-9.4 24.6 0 34z"></path></svg></span><div style="text-align: right;padding-left: 10px"><div class="nav-text-h">Next article</div><span class="nav-text"></span></div></a>
        </div>
        <div class="post-nav-box nav-box-next">
            <a class="nav-box" href="/3.-random-variable-and-univariate-distributions/"><div style="padding-right: 10px"><div class="nav-text-h">Next article</div><span class="nav-text"></span></div><span class="nav-icon"><svg aria-hidden="true" data-prefix="fas" data-icon="chevron-circle-right" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256 8c137 0 248 111 248 248S393 504 256 504 8 393 8 256 119 8 256 8zm113.9 231L234.4 103.5c-9.4-9.4-24.6-9.4-33.9 0l-17 17c-9.4 9.4-9.4 24.6 0 33.9L285.1 256 183.5 357.6c-9.4 9.4-9.4 24.6 0 33.9l17 17c9.4 9.4 24.6 9.4 33.9 0L369.9 273c9.4-9.4 9.4-24.6 0-34z"></path></svg></span></a>
        </div></div></div>
</div>
        </div>
    <div id="toc-final"></div>
    </article><div class="page single comments content-block-position"><div id="comments"><div id="remark42" class="comment" style="padding-top: 1.5rem"></div>
            <script>
                var themeRemark = document.body.getAttribute('theme')
                var remark_config = {
                    host: 'https:\/\/comments.upagge.ru',
                    site_id: 'documentation',
                    components: ['embed'],
                    theme: themeRemark,
                    locale: 'en',
                    show_email_subscription: '',
                    page_title: ''
                };

                (function(c) {
                    for(var i = 0; i < c.length; i++){
                        var d = document, s = d.createElement('script');
                        s.src = remark_config.host + '/web/' +c[i] +'.js';
                        s.defer = true;
                        (d.head || d.body).appendChild(s);
                    }
                })(remark_config.components || ['embed']);
            </script></div></div></div></main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.80.0">Hugo</a> | Theme - <a href="https://ublogger.netlify.app/?utm_source=https://henrywu97.github.io/&utm_medium=footer&utm_campaign=config&utm_term=1.2.0" target="_blank" title="uBlogger 1.2.0"><i class="fas fa-pencil-alt fa-fw"></i> uBlogger</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span>2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank"></a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><script src="https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js"></script><script src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.37.1/dist/autocomplete.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.2.0/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.2/lazysizes.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js"></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"search":{"algoliaAppID":"PASDMWALPK","algoliaIndex":"index.en","algoliaSearchKey":"b42948e51daaa93df92381c8e2ac0f93","highlightTag":"em","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"algolia"}};</script><script src="/js/theme.min.js"></script><script src="/js/jquery-3.5.1.min.js"></script>
    <script>
        (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
            m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
        (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

        ym("70532758", "init", {
            clickmap:true,
            trackLinks:true,
            accurateTrackBounce:true
        });
    </script>
    <noscript><div><img src="https://mc.yandex.ru/watch/69594475" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
    </body>
</html>
